---
title: "STAT 479: Lecture 3"
subtitle: Fitting More Complex XG Models
format: revealjs
execute:
  cache: true
  
---

## Recap

- In [Lecture 2](../../lectures/lecture02.qmd), fit two XG models
  - Model 1 accounted for body part
  - Model 2 accounted for body part + technique
- Which model is better?
  - Which fits the observed data best?
  - Which will predict new data best? 

```{r}
#| label: load-data
#| eval: true
#| echo: false
#| output: false
#| message: false
#| warning: false
wi_shots <-
  StatsBombR::FreeCompetitions() |> 
  dplyr::filter(competition_gender == "female" & competition_international)|> 
  StatsBombR::FreeMatches() |> 
  StatsBombR::free_allevents() |> 
  StatsBombR::allclean() |> 
  StatsBombR::get.opposingteam() |>
  dplyr::filter(type.name == "Shot" & shot.body_part.name != "Other") |>  
  dplyr::mutate(Y = ifelse(shot.outcome.name == "Goal", 1, 0)) 
```

## Our Simple XG Models

:::{.panel-tabset}

### Model 1
```{r}
#| label: xg-model1
#| eval: true
#| echo: true
xg_model1 <- 
  wi_shots |>
  dplyr::group_by(shot.body_part.name) |>
  dplyr::summarise(XG1 = mean(Y))
```
### Model 2
```{r}
#| label: xg-model2
#| eval: true
#| echo: true
xg_model2 <-
  wi_shots |>
  dplyr::group_by(shot.body_part.name, shot.technique.name) |>
  dplyr::summarise(XG2 = mean(Y), .groups = "drop") 
```
### Concatenating Predictions
```{r}
#| label: concatenate-simple
#| eval: true
#| echo: true
simple_preds <-
  wi_shots |>
  dplyr::select(Y, shot.body_part.name, shot.technique.name) |>
  dplyr::left_join(y = xg_model1, by = c("shot.body_part.name")) |>
  dplyr::left_join(y = xg_model2, by = c("shot.body_part.name", "shot.technique.name"))
```
:::

## Qualitative Comparisons

- Consider two shots:
  - [Right-footed half-volley](https://youtu.be/lVGiGZN2gdw?t=148) by Beth Mead against Sweden
  - [Right-footed backheel](https://youtu.be/rsuLFOJCCUA?t=7) by Alessia Russo
```{r}
simple_preds |>
  dplyr::filter(shot.body_part.name == "Right Foot" & 
                  shot.technique.name %in% c("Half Volley", "Backheel")) |>
  dplyr::select(shot.body_part.name, shot.technique.name, XG1, XG2) |>
  dplyr::mutate(XG1 = round(XG1, digits = 3), XG2 = round(XG2, digits = 3)) |>
  unique()
```

. . .

- Model 2 accounts for more factors
- Intuitively expect it is more accurate

## Setup & Notation
- Data: $n$ shots represented by pairs $(\boldsymbol{\mathbf{x}}_{1}, y_{1}), \ldots, (\boldsymbol{\mathbf{x}}_{n}, y_{n})$
  - Outcomes: $y_{i} = 1$ if shot $i$ results in a goal & 0 otherwise
  - Feature vector: $\boldsymbol{\mathbf{x}}_{i}$
- Assumption: Data is a representative sample from an infinite super-population of shots
$$
\textrm{XG}(\boldsymbol{\mathbf{x}}) = \mathbb{E}[Y \vert \boldsymbol{\mathbf{X}} = \boldsymbol{\mathbf{x}}]
$$

. . .

- $\hat{p}_{i}$: predicted $\textrm{XG}$ for shot $i$ from fitted model
- How close is $\hat{p}_{i}$ to $y_{i}$?

## Misclassification Rate (Definition)

- $\hat{p}_{i} > 0.5:$ model predicts $y_{i} = 1$ more likely than $y_{i} = 0$
- Ideal: $\hat{p}_{i} > 0.5$ when $y_{i} = 1$ and $\hat{p}_{i} < 0.5$ when $y_{i} = 0$
- If too many $\hat{p}_{i}$'s on wrong-side of 50%, model is badly calibrated.

. . .

$$
\textrm{MISS} = n^{-1}\sum_{i = 1}^{n}{\mathbb{I}(y_{i} \neq \mathbb{I}(\hat{p}_{i} \geq 0.5))},
$$

## Misclassification Rate (Example)

```{r}
#| label: misclass-code
#| eval: false
#| echo: true
misclass <- function(y, phat){ 
  return( mean( (y != 1*(phat >= 0.5))))
}
misclass(simple_preds$Y, simple_preds$XG1)
misclass(simple_preds$Y, simple_preds$XG2)
```

. . .

```{r}
#| label: misclass-eval
#| eval: true
#| echo: false
misclass <- function(y, phat){ # <1>
  return( mean( (y != 1*(phat >= 0.5))))
}

cat("Model 1 misclassification", #<2>
    round(misclass(simple_preds$Y, simple_preds$XG1), digits = 3), "\n")
cat("Model 2 misclassificaiton", 
    round(misclass(simple_preds$Y, simple_preds$XG2), digits = 3), "\n")
```

. . .

- Why do Models 1 & 2 have the same misclassification rate???


## Brier Score (Definition)
- MISS only cares whether $\hat{p}_{i}$ is on the wrong-side of 50%
  - Forecasting $\hat{p} = 0.501$ and $\hat{p} = 0.999$ have same loss when $y = 0$
  - Doesn't penalize how **far** $\hat{p}$ is from $Y$

. . .

- Brier Score penalizes distance b/w forecast $\hat{p}$ & $Y$
$$
\text{Brier} = n^{-1}\sum_{i = 1}^{n}{(y_{i} - \hat{p}_{i})^2}.
$$
- Just Mean Square Error applied to binary outcomes

## Brier Score (Example)
```{r}
#| label: brier-code
#| eval: false
#| echo: true

brier <- function(y, phat){
  return(mean( (y - phat)^2 ))
}

brier(simple_preds$Y, simple_preds$XG1)
brier(simple_preds$Y, simple_preds$XG2)
```


```{r}
#| label: brier-eval
#| eval: true
#| echo: false
brier <- function(y, phat){
  return(mean( (y - phat)^2 ))
}

cat("Model 1 Brier Score:", 
    round(brier(simple_preds$Y, simple_preds$XG1), digits = 3), "\n")
cat("Model 2 Brier Score:", 
    round(brier(simple_preds$Y, simple_preds$XG2) , digits = 3), "\n")
```



## Log-Loss (Definition)
- Like Brier Score but penalizes extreme mistakes more severely

$$
\textrm{LogLoss} = -1 \times \sum_{i = 1}^{n}{\left[ y_{i} \times \log(\hat{p}_{i}) + (1 - y_{i})\times\log(1-\hat{p}_{i})\right]}.
$$

. . .

- Theoretically, can be infinite (when $\hat{p} = 1-y$)
- In practice, truncate $\hat{p}$ to $[\epsilon,1-\epsilon]$ to avoid $\log(0)$
- Also known as cross-entropy loss in ML literature

## Log-Loss Score (Example)
```{r}
#| label: logloss-code
#| eval: false
#| echo: true
#| code-line-numbers: "2-3|"

logloss <- function(y, phat){
  if(any(phat < 1e-12)) phat[phat < 1e-12] <- 1e-12
  if(any(phat > 1-1e-12)) phat[phat > 1-1e-12] <- 1-1e-12
  return(-1 * mean( y * log(phat) + (1-y) * log(1-phat)))
}

logloss(simple_preds$Y, simple_preds$XG1)
logloss(simple_preds$Y, simple_preds$XG2)
```

. . . 

```{r}
#| label: logloss-eval
#| eval: true
#| echo: false
logloss <- function(y, phat){
  if(any(phat < 1e-12)) phat[phat < 1e-12] <- 1e-12 # <1>
  if(any(phat > 1-1e-12)) phat[phat > 1-1e-12] <- 1e-12
  return(-1 * mean( y * log(phat) + (1-y) * log(1-phat)))
}

cat("Model 1 Log-Loss:", 
    round(logloss(simple_preds$Y, simple_preds$XG1), digits = 3), "\n")
cat("Model 2 Log-Loss:", 
    round(logloss(simple_preds$Y, simple_preds$XG2) , digits = 3), "\n")
```


# Estimating Out-of-Sample Error

## In-sample Error
- Recall our setup:
  - Data is a sample from super-population $\mathcal{P}$
  - Fit model to *estimate* $\mathbb{E}[Y \vert \boldsymbol{\mathbf{X}} = \boldsymbol{\mathbf{x}}]$
  - Fitted model returns predictions $\hat{p}(\boldsymbol{\mathbf{x}})$
- Computed $\textrm{MISS}, \textrm{Brier},$ and $\textrm{LogLoss}$ w/ same data used to fit model
- This only checks whether model fits **observed** data well?

. . . 

- How well does model predict *new*, previously unseen data?

  
## Out-of-Sample Error

- Say we had a second dataset $(\boldsymbol{\mathbf{x}}_{1}^{\star}, y_{1}^{\star}), \ldots, (\boldsymbol{\mathbf{x}}_{m}^{\star}, y_{M}^{\star})$ from $\mathcal{P}$
- Compute predictions $\hat{p}^{\star}_{m}:= \hat{p}(\boldsymbol{\mathbf{x}}_{m}^{\star})$
  - Important: $(\boldsymbol{\mathbf{x}}_{m}^{\star}, y_{m}^{\star})$'s not used to fit the model
  
. . .

- Assess how close $\hat{p}^{\star}_{m}$'s are to $y^{\star}_{m}$'s
  - Could use misclassification rate, Brier score, or log-loss
  - Result is the out-of-sample loss



## Cross-Validation

- Problem: we don't have access to a second dataset

. . .

- Solution: create *random* training/testing split
  - Train on a random subset containing 75% of the data
  - Evaluate using the remaining held-out portion

## The Training/Testing Paradigm{.scrollable}

  
:::{.panel-tabset}


### Create Split
```{r}
#| label: create-split
#| eval: true
#| echo: true
#| code-line-numbers: "1-3|5|8-10|11-13|7|"
n <- nrow(wi_shots)
n_train <- floor(0.75 * n)
n_test <- n - n_train

wi_shots <- wi_shots |> dplyr::mutate(id = 1:n)

set.seed(479)
train_data <-
  wi_shots |>
  dplyr::slice_sample(n = n_train) 
test_data <-
  wi_shots |>
  dplyr::anti_join(y = train_data, by = "id")
```
### Fit Models
```{r}
#| label: single-fold-simple-train
#| eval: true
#| echo: true
#| code-line-numbers: "|2,6|9-12"
model1 <- 
  train_data |>
  dplyr::group_by(shot.body_part.name) |>
  dplyr::summarise(XG1 = mean(Y))
model2 <-
  train_data |>
  dplyr::group_by(shot.body_part.name, shot.technique.name) |>
  dplyr::summarise(XG2 = mean(Y), .groups = "drop")
train_preds <-
  train_data |>
  dplyr::inner_join(y = model1, by = c("shot.body_part.name")) |>
  dplyr::inner_join(y = model2, by = c("shot.body_part.name", "shot.technique.name"))
```

### Evaluate Models
```{r}
#| label: single-fold-simple-test-code
#| eval: false
#| echo: true
test_preds <-
  test_data |>
  dplyr::inner_join(y = model1, by = c("shot.body_part.name")) |>
  dplyr::inner_join(y = model2, by = c("shot.body_part.name", "shot.technique.name"))

logloss(train_preds$Y, train_preds$XG1)
logloss(test_preds$Y, test_preds$XG1)

logloss(train_preds$Y, train_preds$XG2)
logloss(test_preds$Y, test_preds$XG2)
```

. . .

```{r}
#| label: single-fold-simple-test-eval
#| eval: true
#| echo: false
test_preds <-
  test_data |>
  dplyr::inner_join(y = model1, by = c("shot.body_part.name")) |>
  dplyr::inner_join(y = model2, by = c("shot.body_part.name", "shot.technique.name"))

cat("BodyPart train log-loss:",
    round(logloss(train_preds$Y, train_preds$XG1), digits = 3), 
    "test log-loss:",
    round(logloss(test_preds$Y, test_preds$XG1), digits = 3), "\n")

cat("BodyPart+Technique train log-loss:",
    round(logloss(train_preds$Y, train_preds$XG2), digits = 3), 
    "test log-loss:",
    round(logloss(test_preds$Y, test_preds$XG2), digits = 3), "\n")
```

:::

## Multiple Splits

- Recommend averaging over many train/test splits (e.g., 100)
```{r}
#| label: cv-simple
#| eval: true
#| echo: false
n_sims <- 100
train_logloss <- 
  matrix(nrow = 2, ncol = n_sims,
         dimnames = list(c("XG1","XG2"), c()))
test_logloss <- 
  matrix(nrow = 2, ncol = n_sims,
         dimnames = list(c("XG1","XG2"), c()))

for(r in 1:n_sims){
  set.seed(479+r) 
  train_data <-
    wi_shots |>
    dplyr::slice_sample(n = n_train) 
  test_data <-
    wi_shots |>
    dplyr::anti_join(y = train_data, by = "id") 
  
  model1 <- 
    train_data |>
    dplyr::group_by(shot.body_part.name) |>
    dplyr::summarise(XG1 = mean(Y))
  
  model2 <-
    train_data |>
    dplyr::group_by(shot.body_part.name, shot.technique.name) |>
    dplyr::summarise(XG2 = mean(Y), .groups = "drop")
  
  train_preds <-
    train_data |>
    dplyr::inner_join(y = model1, by = c("shot.body_part.name")) |>
    dplyr::inner_join(y = model2, by = c("shot.body_part.name", "shot.technique.name"))
  
  test_preds <-
    test_data |>
    dplyr::inner_join(y = model1, by = c("shot.body_part.name")) |>
    dplyr::inner_join(y = model2, by = c("shot.body_part.name", "shot.technique.name"))
  
  train_logloss["XG1", r] <- logloss(train_preds$Y, train_preds$XG1) # <3>
  train_logloss["XG2",r] <- logloss(train_preds$Y, train_preds$XG2)
  
  test_logloss["XG1", r] <- logloss(test_preds$Y, test_preds$XG1)
  test_logloss["XG2",r] <- logloss(test_preds$Y, test_preds$XG2)
}
cat("Model 1 training logloss:", round(mean(train_logloss["XG1",]), digits = 3), "\n") # <4>
cat("Model 2 training logloss:", round(mean(train_logloss["XG2",]), digits = 3), "\n")

cat("Model 1 test logloss:", round(mean(test_logloss["XG1",]), digits = 3), "\n")
cat("Model 2 test logloss:", round(mean(test_logloss["XG2",]), digits = 3), "\n")
```

. . .

- Simpler model appaers to have slightly smaller out-of-sample log-loss!
  
# Logistic Regression
  
## Motivation: Accounting for Distance
  
- Model 1 gives same prediction for
  - A header from 1m away
  - A header from 15m away
- How to account for continuous feature like `DistToGoal`?
  
. . .

- Idea: Divide into discrete bins and then average within bins
  - Problem: sensitivity to bin sizes (1m , 3m, 10m)
  - Problem: potential small sample issues
  
## Logistic Regression{.smaller}

- Binary outcome $Y$ and numerical predictors $X_{1}, \ldots, X_{p}$:
$$
\log\left(\frac{\mathbb{P}(Y= 1 \vert \boldsymbol{\mathbf{X}})}{\mathbb{P}(Y = 0 \vert \boldsymbol{\mathbf{X}})}\right) = \beta_{0} + \beta_{1}X_{1} + \cdots + \beta_{p}X_{p}.
$$
  
- Keeping all other predictors constant, a one unit change in $X_{j}$ associated with a $\beta_{j}$ change in the log-odds
- Say $\beta_{j} = 1$. Increasing $X_{j}$ by 1 unit moves $\mathbb{P}(Y = 1)$
  - From 4.7% to 11.2% (log-odds from -3 to -2)
  - From 37.8% to 62.2% (log-odds from -0.5 to 0.5)
  - From 73.1% to 88.1% (log-odds from 1 to 2)

## Logistic & Inverse Logistic Functions

- Logistic function: $f(t) = [1 + e^{-t}]^{-1}$
- Inverse logistic: $f^{-1}(p) = \log{p/(1-p)}$
```{r}
#| label: fig-logistic
#| fig-width: 6
#| fig-asp: 0.5625
#| fig-align: center
#| fig-cap: Logistic and inverse logistic functions

par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0), mfrow = c(1,2))
p_seq <- seq(0.01, 0.99, by = 0.01)
t_seq <- log(p_seq/(1-p_seq))

plot(1, type = "n", 
     xlim = c(-5,5), ylim = c(0,1), 
     xlab = "t", ylab = "p", main = "Logistic")
lines(t_seq, p_seq)

x <- c(-3, -2, -0.5, 0.5, 1, 2)
y <- 1/(1 + exp(-1 * x))
points(x = x, y = y, pch = 16)

lines(x = c(-3,-2), y = 1/(1 + exp(-1*c(-3,-3))), lty = 2)
lines(x = c(-0.5, 0.5), y = 1/(1 + exp(-1*c(-0.5, -0.5))), lty = 2)
lines(x = c(1,2), y = 1/(1 + exp(-1*c(1,1))), lty = 2)

lines(x = c(-2,-2), y = 1/(1 + exp(-1*c(-3,-2))), lty = 2)
lines(x = c(0.5, 0.5), y = 1/(1 + exp(-1*c(-0.5, 0.5))), lty = 2)
lines(x = c(2,2), y = 1/(1 + exp(-1*c(1,2))), lty = 2)

plot(1, type = "n",
     xlim = c(0,1), ylim = c(-5,5),
     xlab = "p", ylab = "t", main = "Inverse Logistic")
lines(p_seq, t_seq)
```

## An Initial Model{.scrollable}
- $X_{1}$: distance from shot to goal (`DistToGoal`)
- $\log\left(\frac{\mathbb{P}(Y = 1)}{\mathbb{P}(Y = 0)} \right) = \beta_{0} + \beta_{1} X_{1}$
  
:::{.panel-tabset}

### Model Fitting
```{r}
#| label: dist2goal-fit
#| eval: true
#| echo: true
#| code-line-numbers: "1-7|8-10|"
set.seed(479)
train_data <-
  wi_shots |>
  dplyr::slice_sample(n = n_train)
test_data <-
  wi_shots |>
  dplyr::anti_join(y = train_data, by = "id") 

fit1 <- glm(formula = Y~DistToGoal, data = train_data,  
            family = binomial("logit")) #<3>
```

### Model Summary
```{r}
#| label: summarize-fit1
#| eval: true
#| echo: true
summary(fit1)
```
:::
  
## Assessing Model Performance{.smaller}
- Use `predict()` to make test set predictions
```{r}
#| label: assess-preds1-code
#| eval: false
#| echo: true
#| code-line-numbers: "|3,7|4,8|"
train_pred1 <- 
  predict(object = fit1,
          newdata = train_data,
          type = "response") 
test_pred1 <-
  predict(object = fit1,
          newdata = test_data,
          type = "response")

logloss(train_data$Y, train_pred1)
logloss(test_data$Y, test_pred1)
```

. . .

```{r}
#| label: assess-preds1-eval
#| eval: true
#| echo: false
train_pred1 <- 
  predict(object = fit1,
          newdata = train_data,
          type = "response") 
test_pred1 <-
  predict(object = fit1,
          newdata = test_data,
          type = "response")

cat("Dist training logloss:", round(logloss(train_data$Y, train_pred1), digits = 3), "\n")
cat("Dist testing logloss:", round(logloss(test_data$Y, test_pred1), digits = 3), "\n")
```

. . .

- Averaging across 100 train/test splits: distance-based model better than body-part-based model
```{r}
#| label: cv-dist
#| eval: true
#| echo: false
n_sims <- 100
train_logloss <- rep(NA, times = n_sims)
test_logloss <- rep(NA, times = n_sims)

for(r in 1:n_sims){
  set.seed(479+r)
  train_data <-
    wi_shots |>
    dplyr::slice_sample(n = n_train)
  test_data <-
    wi_shots |>
    dplyr::anti_join(y = train_data, by = "id") 
  
  fit <- glm(Y~DistToGoal, 
             data = train_data, family = binomial("logit"))
  
  train_preds <-
    predict(object = fit,
            newdata = train_data,
            type = "response")
  
  test_preds <-
    predict(object = fit,
            newdata = test_data,
            type = "response")
  
  train_logloss[r] <-
    logloss(train_data$Y, train_preds)
  test_logloss[r] <-
    logloss(test_data$Y, test_preds)
}
cat("Dist*BodyPart training logloss:", round(mean(train_logloss), digits = 4), "\n")
cat("Dist*BodyPart testing logloss:", round(mean(test_logloss), digits = 4), "\n")
```

## Including Multiple Predictors{.smaller}

- Distance-based model is more accurate than body-part based model
- What if we account for body part and distance?
  
. . .

- Body part is a categorical predictor

```{r}
table(wi_shots$shot.body_part.name)
```

- Convert to `factor` type and one-hot encode
```{r}
#| label: body-factor
#| eval: true
#| echo: true
wi_shots <- wi_shots |>
  dplyr::mutate(shot.body_part.name = factor(shot.body_part.name))
```

## Model Specification

$$
\beta_{0} + \beta_{1}\times \textrm{DistToGoal} + \\ \beta_{\textrm{LeftFoot}}\times \mathbb{I}(\textrm{LeftFoot}) + \beta_{\textrm{RightFoot}} \times \mathbb{I}(\textrm{RightFoot})
$$

- Different predictions based on the body part used to attempt the shot

- For a shot taken at distance $d$:
  - Header: log-odds $ = \beta_{0} + \beta_{1}d$
  - Left-footed shot: $ = \beta_{0} + \beta_{1}d + \beta_{\textrm{LeftFoot}}$ 
  - Right-footed shot: $ = \beta_{0} + \beta_{1}d + \beta_{\textrm{RightFoot}}$
  
## Fitted Model{.scrollable}
  
```{r}
#| label: model3-fit
#| eval: true
#| echo: true
fit <- glm(formula = Y~DistToGoal + shot.body_part.name, 
           data = train_data, family = binomial("logit"))
summary(fit)
```

## Interactions{.smaller}
- Model assumes effect of distance is the same regardless of body part
- Interactions allow the effect of one factor to vary based on the value of another.
$$
  \begin{align}
&\beta_{0} + \beta_{\textrm{LeftFoot}} \times \mathbb{I}(\textrm{LeftFoot}) + \beta_{\textrm{RightFoot}} * \mathbb{I}(\textrm{RightFoot}) +  \\
&+[\beta_{\textrm{Dist}} + \beta_{\textrm{Dist:LeftFoot}}*\mathbb{I}(\textrm{LeftFoot}) + \beta_{\textrm{Dist:RightFoot}}\mathbb{I}(\textrm{RightFoot})] \times \textrm{Dist}   
  \end{align}
$$
  
  
## Fitting Interactive Model{.scrollable}
```{r}
#| label: interaction
#| eval: true
#| echo: true
#| code-line-numbers: "|2"
fit <- 
  glm(formula = Y~DistToGoal * shot.body_part.name, 
      data = train_data, family = binomial("logit"))
summary(fit)
```

## Cross-Validation{.smaller}


- Fit & assess models w/ 100 train/test splits
- Model w/ interactions is slightly better than others

:::{.panel-tabset}

### Simple Models
```{r}
#| label: cv-body
n_sims <- 100
train_logloss <- 
  matrix(nrow = 2, ncol = n_sims,
         dimnames = list(c("XG1","XG2"), c()))
test_logloss <- 
  matrix(nrow = 2, ncol = n_sims,
         dimnames = list(c("XG1","XG2"), c()))

for(r in 1:n_sims){
  set.seed(479+r) 
  train_data <-
    wi_shots |>
    dplyr::slice_sample(n = n_train) 
  test_data <-
    wi_shots |>
    dplyr::anti_join(y = train_data, by = "id") 
  
  model1 <- 
    train_data |>
    dplyr::group_by(shot.body_part.name) |>
    dplyr::summarise(XG1 = mean(Y))
  
  model2 <-
    train_data |>
    dplyr::group_by(shot.body_part.name, shot.technique.name) |>
    dplyr::summarise(XG2 = mean(Y), .groups = "drop")
  
  train_preds <-
    train_data |>
    dplyr::inner_join(y = model1, by = c("shot.body_part.name")) |>
    dplyr::inner_join(y = model2, by = c("shot.body_part.name", "shot.technique.name"))
  
  test_preds <-
    test_data |>
    dplyr::inner_join(y = model1, by = c("shot.body_part.name")) |>
    dplyr::inner_join(y = model2, by = c("shot.body_part.name", "shot.technique.name"))
  
  train_logloss["XG1", r] <- logloss(train_preds$Y, train_preds$XG1) # <3>
  train_logloss["XG2",r] <- logloss(train_preds$Y, train_preds$XG2)
  
  test_logloss["XG1", r] <- logloss(test_preds$Y, test_preds$XG1)
  test_logloss["XG2",r] <- logloss(test_preds$Y, test_preds$XG2)
}
cat("BodyPart training logloss:", round(mean(train_logloss["XG1",]), digits = 3), "\n") # <4>
cat("BodyPart+Technique training logloss:", round(mean(train_logloss["XG2",]), digits = 3), "\n")

cat("BodyPart test logloss:", round(mean(test_logloss["XG1",]), digits = 3), "\n")
cat("BodyPart+Technique test logloss:", round(mean(test_logloss["XG2",]), digits = 3), "\n")
```

### Distance Only
```{r}
#| label: cv-model3
#| eval: true
#| echo: false
n_sims <- 100
train_logloss <- rep(NA, times = n_sims)
test_logloss <- rep(NA, times = n_sims)

for(r in 1:n_sims){
  set.seed(479+r)
  train_data <-
    wi_shots |>
    dplyr::slice_sample(n = n_train)
  test_data <-
    wi_shots |>
    dplyr::anti_join(y = train_data, by = "id") 
  
  fit <- glm(Y~DistToGoal, 
             data = train_data, family = binomial("logit"))
  
  train_preds <-
    predict(object = fit,
            newdata = train_data,
            type = "response")
  
  test_preds <-
    predict(object = fit,
            newdata = test_data,
            type = "response")
  
  train_logloss[r] <-
    logloss(train_data$Y, train_preds)
  test_logloss[r] <-
    logloss(test_data$Y, test_preds)
}
cat("Dist*BodyPart training logloss:", round(mean(train_logloss), digits = 4), "\n")
cat("Dist*BodyPart testing logloss:", round(mean(test_logloss), digits = 4), "\n")
```


### Distance + BodyPart
```{r}
#| label: cv-model-multi
#| eval: true
#| echo: false
n_sims <- 100
train_logloss <- rep(NA, times = n_sims)
test_logloss <- rep(NA, times = n_sims)

for(r in 1:n_sims){
  set.seed(479+r)
  train_data <-
    wi_shots |>
    dplyr::slice_sample(n = n_train)
  test_data <-
    wi_shots |>
    dplyr::anti_join(y = train_data, by = "id") 
  
  fit <- glm(Y~DistToGoal+shot.body_part.name, 
             data = train_data, family = binomial("logit"))
  
  train_preds <-
    predict(object = fit,
            newdata = train_data,
            type = "response")
  
  test_preds <-
    predict(object = fit,
            newdata = test_data,
            type = "response")
  
  train_logloss[r] <-
    logloss(train_data$Y, train_preds)
  test_logloss[r] <-
    logloss(test_data$Y, test_preds)
}
cat("Dist*BodyPart training logloss:", round(mean(train_logloss), digits = 4), "\n")
cat("Dist*BodyPart testing logloss:", round(mean(test_logloss), digits = 4), "\n")
```
### Distance*Body Part
```{r}
#| label: cv-model-interact
#| eval: true
#| echo: false
n_sims <- 100
train_logloss <- rep(NA, times = n_sims)
test_logloss <- rep(NA, times = n_sims)

for(r in 1:n_sims){
  set.seed(479+r)
  train_data <-
    wi_shots |>
    dplyr::slice_sample(n = n_train)
  test_data <-
    wi_shots |>
    dplyr::anti_join(y = train_data, by = "id") 
  
  fit <- glm(Y~DistToGoal*shot.body_part.name, 
             data = train_data, family = binomial("logit"))
  
  train_preds <-
    predict(object = fit,
            newdata = train_data,
            type = "response")
  
  test_preds <-
    predict(object = fit,
            newdata = test_data,
            type = "response")
  
  train_logloss[r] <-
    logloss(train_data$Y, train_preds)
  test_logloss[r] <-
    logloss(test_data$Y, test_preds)
}
cat("Dist*BodyPart training logloss:", round(mean(train_logloss), digits = 4), "\n")
cat("Dist*BodyPart testing logloss:", round(mean(test_logloss), digits = 4), "\n")
```
:::

# Random Forests

## Including Even More Features

- StatsBomb records many potentially important features
```{r}
#| label: shot-features
#| echo: false
#| eval: true
shot_vars <- c("shot.type.name", 
    "shot.technique.name", "shot.body_part.name",
    "DistToGoal", "DistToKeeper", # dist. to keeper is distance from GK to goal
    "AngleToGoal", "AngleToKeeper",
    "AngleDeviation", 
    "avevelocity","density", "density.incone",
    "distance.ToD1", "distance.ToD2",
    "AttackersBehindBall", "DefendersBehindBall",
    "DefendersInCone", "InCone.GK", "DefArea")
shot_vars
```

. . .

- How much more predictive accuracy can we gain by accounting for these?
- Challenge: hard to specify nonlinearities & interactions correctly in `glm()`

##  Regression Trees I{.smaller}

:::: {.columns}

:::{.column width="48%"}
![](regression_tree.png){fig-align="center"}
:::
:::{.column width = "48%"}
![](partition.png){fig-align="center"}
:::

::::

- Regression trees can elegantly model interactions and non-linearities
- Regression trees are just step-functions

## Regression Trees II

![](tree_appx_wide.png)

- Regression trees can approximate functions arbitrarily well
- But often need very deep and complicated trees

## Tree Ensembles

![](sum_of_trees.png)

- Complicated trees can be written as sums of shallow trees!

## Random Forests

- Approximate $\mathbb{E}[Y \vert \boldsymbol{\mathbf{X}} = \boldsymbol{\mathbf{x}}]$ w/ tree ensemble
- No need to pre-specify functional form or interactions
- Scales nicely even if $\boldsymbol{\mathbf{X}}$ is high-dimensional
- We will use implementation in **ranger** package

## Fitting a Random Forests Model{.scrollable}

:::{.panel-tabset}

### Data Preparation
```{r}
#| label: prepare-rf-data
#| eval: true
#| echo: true
shot_vars <-
  c("Y",
    "shot.type.name", 
    "shot.technique.name", "shot.body_part.name",
    "DistToGoal", "DistToKeeper", # dist. to keeper is distance from GK to goal
    "AngleToGoal", "AngleToKeeper",
    "AngleDeviation", 
    "avevelocity","density", "density.incone",
    "distance.ToD1", "distance.ToD2",
    "AttackersBehindBall", "DefendersBehindBall",
    "DefendersInCone", "InCone.GK", "DefArea")

wi_shots <-
  wi_shots |>
  dplyr::mutate(
    shot.type.name = factor(shot.type.name),
    shot.body_part.name = shot.body_part.name,
    shot.technique.name = shot.technique.name)

set.seed(479)
train_data <-
  wi_shots |>
  dplyr::slice_sample(n = n_train) |>
  dplyr::select(dplyr::all_of(c("id",shot_vars)))

test_data <-
  wi_shots |>
  dplyr::anti_join(y = train_data, by = "id") |>
  dplyr::select(dplyr::all_of(c("id", shot_vars)))

y_train <- train_data$Y
y_test <- test_data$Y

train_data <-
  train_data |>
  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |>
  dplyr::select(-id)
test_data <-
  test_data |>
  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |>
  dplyr::select(-id)
```

### Model Fitting
```{r}
#| label: fit-rf
#| message: false
#| warning: false
#| eval: true
#| echo: true
library(ranger)
fit <- ranger(formula = Y~., 
  data = train_data, probability = TRUE)
```

### Assessing Predictions
```{r}
#| label: assess-rf
#| eval: true
#| echo: true
#| 
train_preds <- 
  predict(object = fit,
          data = train_data)$predictions[,2] 

test_preds <- 
  predict(object = fit,
          data = test_data)$predictions[,2]

logloss(y_train, train_preds)
logloss(y_test, test_preds)
```
:::
