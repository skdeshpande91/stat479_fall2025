---
title: "STAT 479 Lecture 24"
subtitle: "Decathlon Performance"
format: revealjs
execute:
  cache: true
---

# Motivation & Setup

## Decathlon Overview

![](decathlon.png){width=90% fig-align="center"}

## Best Decathlon Performances

![](decathlon-scores.png){fig-align="center"}

- Can someone break 9200? What would it take?
- Max out in one event? Be good but not elite at all?

## Statistical Model

- $j$-th performance in event $e$ by athlete $i$:
$$
\begin{align}
y_{e,i,j} &= \alpha_{e,i} + \sum_{d = 1}^{D}{\gamma_{e,d}\phi_{d}(\text{age}_{i,j})} \\
~& + \beta_{e,1}Y_{1,i,j} + \beta_{e,2}Y_{2,i,j} + \cdots + \beta_{e,e-1}Y_{e-1,i,j} + \epsilon_{e,i,j}
\end{align}
$$
- Athlete-event random effect: $\alpha_{e,i} \sim N(\mu_{e}, \sigma_{e})$
- $\phi_{d}(x) = x^{d}$: allows for non-linear performance evolution

<!--
## Understanding the model

- Model has loads of parameters:
  - $\alpha_{e,i}$'s: capture each athlete's latent skill in each event
  - $\gamma_{e,d}$'s: govern how performances change over time
  - $\beta_{e,e'}$: captures inter-event relationships
  
- If we knew the parameter values, we could 
  - Predict age curve for each event & overall score
  - Estimate prob. of breaking record via simulation
-->
## Model Fitting

- Classical approach: maximum likelihood estimation
  - Find $\boldsymbol{\alpha}, \boldsymbol{\beta},$ and $\boldsymbol{\gamma}$ that make observed data most likely
  - Take STAT 310/312/410 for details

- Use $\boldsymbol{\hat \alpha}, \boldsymbol{\hat \beta},$ and $\boldsymbol{\hat \gamma}$ to make predictions

- Problem: how to propagate *estimation* uncertainty to predictions?

# Digression: A Bayesian Approach
  
## Types of Uncertainty

- Aleatoric: inherent uncertainty in a system
- Epistemic: due to ignorance/lack of knowledge

. . . 

- Eg: consider a coin with unknown bias $\mathbb{P}(\textrm{heads})$
- Epistemic uncertainty about $\mathbb{P}(\textrm{heads})$
  - If we repeatedly flip, we can learn $\mathbb{P}(\textrm{heads})$
  - The more data we get, the less uncertainty!

- Aleatoric: uncertainty about result of next flip
  - Even if we know $\mathbb{P}(\textrm{heads})$, can't perfectly predict next flip
  - Can't be reduced!


## The Big Bayesian Picture

- Quantify **all** uncertainties using probability distributions
- Update, combine, and propagate uncertainty using probability calculus

- General workflow: data $Y$ and unknown parameter $\theta$
  - Specify likelihood $p(y \vert \theta)$ & prior $p(\theta)$
  - Compute posterior distribution $p(\theta \vert y)$
- Posterior quantifies uncertainty **given the observed data**

## Example: Tea, Milk, Octopi, & FIFA

- [Paul the Octopus](https://en.wikipedia.org/wiki/Paul_the_Octopus): predicted football matches
  - Picked from food boxes w/ nations' flags
  - Correctly predicted $Y_{1} = 8$ out of 8 games!

- Milk or tea first? 
  - [Muriel Bristol](https://en.wikipedia.org/wiki/Muriel_Bristol) claimed to taste difference
  - Correctly determined order in $Y_{2} = 8$ out of 8 cup! 




## Example: Classical Statistics{.smaller}

- $Y_{1} \sim \textrm{Binomial}(8, \theta_{1})$ & $Y_{2} \sim \textrm{Binomial}(8, \theta_{2}).$
- We observe $Y_{1} = Y_{2} = 8$
- Classical Statistics: $\hat{\theta}_{1} = \hat{\theta}_{2} = 1$

```{r}
#| label: fig-likelihood
#| eval: true
#| echo: false
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center
oi_colors <- palette.colors(palette = "Okabe-Ito")
par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))
theta_seq <- seq(0, 1, by = 0.001)

likelihood <- dbinom(x = 8, size = 8, prob = theta_seq)

plot(1, type = "n", xlim = c(0, 1), ylim = c(0, 1),
     xlab = "theta", ylab = "P(Y = 8 | theta)", main = "Likelihood")
lines(theta_seq, likelihood, col = oi_colors[9])
lines(x = c(1, 1), y = c(par("usr")[3], dbinom(8,8,1)), lty = 2)
points(x = 1, y = dbinom(8, 8, 1), pch = 16, col = oi_colors[9])
```


## Example: Prior for $\theta_{1}${.smaller}

- Relatively implausible that octopi know soccer
- More likely than not Paul is randomly guessing
- $\theta_{1} \sim \textrm{Beta}(20,20)$: 

```{r}
#| label: fig-prior-paul
#| eval: true
#| echo: false
#| fig-width: 7.2
#| fig-height: 4.05
#| fig-align: "center"

oi_colors <- palette.colors(palette = "Okabe-Ito")
theta_seq <- seq(0, 1, by = 0.001)

a1_prior <- 10
b1_prior <- 10
y1 <- 8
n1 <- 8

a1_post <- a1_prior + y1
b1_post <- b1_prior + n1 - y1

prior1 <- dbeta(theta_seq, shape1 = a1_prior, shape2 = b1_prior)
post1 <- dbeta(theta_seq, shape1 = a1_post, shape2 = b1_post)

theta1_ix_prior <- 
  which(theta_seq >= qbeta(0.025, a1_prior, b1_prior) & theta_seq <= qbeta(0.975, a1_prior, b1_prior))

x1_prior <- theta_seq[theta1_ix_prior]
y1_prior_top <- prior1[theta1_ix_prior]
y1_prior_bot <- rep(0, times = length(theta1_ix_prior))

prior1_mean <- a1_prior/(a1_prior + b1_prior)


theta1_ix_post <- 
  which(theta_seq >= qbeta(0.025, a1_post, b1_post) & theta_seq <= qbeta(0.975, a1_post, b1_post))
x1_post <- theta_seq[theta1_ix_post]
y1_post_top <- post1[theta1_ix_post]
y1_post_bot <- rep(0, times = length(theta1_ix_post))

post1_mean <- a1_post/(a1_post + b1_post)

par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))
plot(1, type = "n", 
     xlim = c(0,1), ylim = c(0, max(c(prior1, post1))),
     main = c("Prior (theta1)"), xlab = "theta1", ylab = "Density")
abline(h = 0)
lines(theta_seq, prior1, col = oi_colors[2])
polygon(x = c(x1_prior, rev(x1_prior)),
        y = c(y1_prior_top, rev(y1_prior_bot)),
        col = adjustcolor(col = oi_colors[2], alpha.f = 0.5),
        border = NA)
lines(x = c(prior1_mean, prior1_mean), y = c(0, dbeta(prior1_mean, a1_prior, b1_prior)), lty = 2,
      col = oi_colors[2])
points(x = prior1_mean, y = dbeta(prior1_mean, a1_prior, b1_prior), pch = 16, col = oi_colors[2])
```
## Example: Posterior for $\theta_{1}${.smaller}

- Posterior: $\theta_{1} \vert Y_{1} = 8 \sim \textrm{Beta}(18, 10)$
  - Best guess: $\hat{\theta}_{1} = $ `{r} round(post1_mean, digits = 2)`
  - 95% interval: `{r} round( qbeta(c(0.025, 0.975), a1_post, b1_post), digits = 2)`

```{r}
#| label: fig-post-paul
#| eval: true
#| echo: false
#| fig-width: 7.2
#| fig-height: 4.05
#| fig-align: "center"
par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))
plot(1, type = "n", 
     xlim = c(0,1), ylim = c(0, max(c(prior1, post1))),
     main = c("Prior & Posterior"), xlab = "theta1", ylab = "Density")
abline(h = 0)
lines(theta_seq, prior1, col = oi_colors[2])
polygon(x = c(x1_prior, rev(x1_prior)),
        y = c(y1_prior_top, rev(y1_prior_bot)),
        col = adjustcolor(col = oi_colors[2], alpha.f = 0.5),
        border = NA)
lines(x = c(prior1_mean, prior1_mean), y = c(0, dbeta(prior1_mean, a1_prior, b1_prior)), lty = 2,
      col = oi_colors[2])
points(x = prior1_mean, y = dbeta(prior1_mean, a1_prior, b1_prior), pch = 16, col = oi_colors[2])

lines(theta_seq, post1, col = oi_colors[3])
polygon(x = c(x1_post, rev(x1_post)),
        y = c(y1_post_top, rev(y1_post_bot)),
        col = adjustcolor(col = oi_colors[3], alpha.f = 0.5),
        border = NA)
lines(x = c(post1_mean, post1_mean), y = c(0, dbeta(post1_mean, a1_post, b1_post)), lty = 2,
      col = oi_colors[3])
points(x = post1_mean, y = dbeta(post1_mean, a1_post, b1_post), pch = 16, col = oi_colors[3])

legend("topleft", legend = c("Prior","Posterior"), pch = c(15, 15),
       col = adjustcolor(col = oi_colors[2:3], alpha.f = 0.5), border = NA)
```

## Example: Prior for $\theta_{2}${.smaller}

- Plausible to refine palette over decades of daily tea consumption
- $\theta_{2}$ is probably over 50%
- $\theta_{2} \sim \textrm{Beta}(15,5)$

```{r}
#| label: fig-prior-tea
#| eval: true
#| echo: false
#| fig-width: 7.2
#| fig-height: 4.05
#| fig-align: "center"
a2_prior <- 15
b2_prior <- 5
y2 <- 8
n2 <- 8

a2_post <- a2_prior + y2
b2_post <- b2_prior + n2 - y2

prior2 <- dbeta(theta_seq, shape1 = a2_prior, shape2 = b2_prior)
post2 <- dbeta(theta_seq, shape1 = a2_post, shape2 = b2_post)

theta2_ix_prior <- 
  which(theta_seq >= qbeta(0.025, a2_prior, b2_prior) & theta_seq <= qbeta(0.975, a2_prior, b2_prior))

x2_prior <- theta_seq[theta2_ix_prior]
y2_prior_top <- prior2[theta2_ix_prior]
y2_prior_bot <- rep(0, times = length(theta2_ix_prior))

prior2_mean <- a2_prior/(a2_prior + b2_prior)


theta2_ix_post <- 
  which(theta_seq >= qbeta(0.025, a2_post, b2_post) & theta_seq <= qbeta(0.975, a2_post, b2_post))
x2_post <- theta_seq[theta2_ix_post]
y2_post_top <- post2[theta2_ix_post]
y2_post_bot <- rep(0, times = length(theta2_ix_post))

post2_mean <- a2_post/(a2_post + b2_post)

par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))
plot(1, type = "n", 
     xlim = c(0,1), ylim = c(0, max(c(prior2, post2))),
     main = c("Prior (theta2)"), xlab = "theta1", ylab = "Density")
abline(h = 0)
lines(theta_seq, prior2, col = oi_colors[5])
polygon(x = c(x2_prior, rev(x2_prior)),
        y = c(y2_prior_top, rev(y2_prior_bot)),
        col = adjustcolor(col = oi_colors[5], alpha.f = 0.5),
        border = NA)
lines(x = c(prior2_mean, prior2_mean), y = c(0, dbeta(prior2_mean, a2_prior, b2_prior)), lty = 2,
      col = oi_colors[5])
points(x = prior2_mean, y = dbeta(prior2_mean, a2_prior, b2_prior), pch = 16, col = oi_colors[5])

```


## Example: Posterior for $\theta_{2}${.smaller}
- $\theta_{2} \vert Y_{2} = 8 \sim \textrm{Beta}(23, 5)$
  - Best guess: `{r} round(post2_mean, digits = 2)`
  - 95% interval: `{r} round( qbeta(c(0.025, 0.975), a2_post, b2_post), digits = 2)`
  
```{r}
#| label: fig-post-tea
#| eval: true
#| echo: false
#| fig-width: 7.2
#| fig-height: 4.05
#| fig-align: "center"
par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))
plot(1, type = "n", 
     xlim = c(0,1), ylim = c(0, max(c(prior2, post2))),
     main = c("Prior & Posterior"), xlab = "theta2", ylab = "Density")
abline(h = 0)
lines(theta_seq, prior2, col = oi_colors[5])
polygon(x = c(x2_prior, rev(x2_prior)),
        y = c(y2_prior_top, rev(y2_prior_bot)),
        col = adjustcolor(col = oi_colors[5], alpha.f = 0.5),
        border = NA)
lines(x = c(prior2_mean, prior2_mean), y = c(0, dbeta(prior2_mean, a2_prior, b2_prior)), lty = 2,
      col = oi_colors[5])
points(x = prior2_mean, y = dbeta(prior2_mean, a2_prior, b2_prior), pch = 16, col = oi_colors[5])

lines(theta_seq, post2, col = oi_colors[4])
polygon(x = c(x2_post, rev(x2_post)),
        y = c(y2_post_top, rev(y2_post_bot)),
        col = adjustcolor(col = oi_colors[4], alpha.f = 0.5),
        border = NA)
lines(x = c(post2_mean, post2_mean), y = c(0, dbeta(post2_mean, a2_post, b2_post)), lty = 2,
      col = oi_colors[4])
points(x = post2_mean, y = dbeta(post2_mean, a2_post, b2_post), pch = 16, col = oi_colors[4])

legend("topleft", legend = c("Prior","Posterior"), pch = c(15, 15),
       col = adjustcolor(col = oi_colors[5:4], alpha.f = 0.5), border = NA)
```

## Example: Posterior Predictive{.smaller}

- What if we asked Paul & Muriel for 4 more predictions?
- $\mathbb{P}(Y_{1}^{\star} = k \vert Y_{1} = 8) = \int{ \mathbb{P}(Y_{1}^{\star} = k \vert \theta)p(\theta \vert Y_{1} = 8)d\theta}$
- Simulation: for $m = 1, \ldots, M$
  - Draw a posterior sample $\theta_{1}^{(m)}$
  - Then simulate $Y_{1}^{\star} \sim \textrm{Binomial}(4, \theta_{1}^{(m)})$

```{r}
#| label: post-pred1
#| eval: true
#| echo: true
n_sims <- 5e5
theta1_draws <- rbeta(n = 5e5, shape1 = a1_post, shape2 = b1_post)
ystar1 <- rbinom(n = 5e5, size = 4, prob = theta1_draws)
round(table(ystar1)/n_sims, digits = 2)
```

```{r}
#| label: post-pred2
#| eval: true
#| echo: false
theta2_draws <- rbeta(n = 5e5, shape1 = a2_post, shape2 = b2_post)
ystar2 <- rbinom(n = 5e5, size = 4, prob = theta2_draws)
round(table(ystar2)/n_sims, digits = 2)
```

## Example: Uncertainty about functionals{.smaller}

- Best estimate for $\theta_{1} - \theta_{2}$: `{r} round( mean(theta1_draws - theta2_draws), digits = 2)`; 95% interval `{r} round( quantile(theta1_draws - theta2_draws, probs = c(0.025, 0.975)), digits = 2)`
- $\mathbb{P}(\theta_{1} > \theta_{2} \vert Y_{1} = Y_{2} = 8)$ = `{r} round( mean(theta1_draws > theta2_draws), digits = 2)`

```{r}
#| label: fig-hist-theta-diff
#| eval: true
#| echo: false
#| fig-width: 7.2
#| fig-height: 4.05
#| fig-align: center
hist(theta1_draws - theta2_draws, breaks = 100,
     xlab = "theta1-theta2", main = "Posterior of theta1-theta2")
abline(v = 0, col = oi_colors[8])
```



## Advantages of Bayesian Modeling

- Incorporate prior knowledge!
- Multiple parameter values can provide good fits to data
- No longer forced to commit to single estimate to drive downstream prediction
- Can make probabilistic statements: "93% prob. $\theta$ lies in this interval"
- Sequential nature: use data to update prior beliefs

## Bayesian Modeling in Practice

- Most posteriors are not standard distributions
- Must *approximate* posterior means, quantiles, etc.
- Markov chain Monte Carlo: construct a Markov chain whose invariant dist. is the posterior


# Decathlon Modeling Results

## Inter-event Dependence

![](decathlon-beta.png){width=50% fig-align="center"}

- .1 second decrease in 100m associated w/ ~24 second increase in 1500m time
- Posterior fully concentrated on negative values

## Posterior Predictive Simulation

- For each posterior sample $\boldsymbol{\alpha}^{(m)}, \boldsymbol{\beta}^{(m)}, \boldsymbol{\gamma}^{(m)}, \boldsymbol{\sigma}^{(m)}$:
$$
\begin{align}
y^{\star(m)}_{e,i,j} &= \alpha^{(m)}_{e,i} + \sum_{d = 1}^{D}{\gamma^{(m)}_{e,d}\phi_{d}(\text{age}_{i,j})} \\
~& + \beta^{(m)}_{e,1}Y_{1,i,j} + \beta^{(m)}_{e,2}Y_{2,i,j} + \cdots + \beta^{(m)}_{e,e-1}Y_{e-1,i,j} + \sigma^{(m)}\epsilon^{\star (m)}_{e,i,j},
\end{align}
$$
where $\epsilon^{\star(m)} \sim N(0,1)$

- $y^{\star (1)}_{e,i,j}, \ldots, y^{\star (M)}_{e,i,j}$: sample from posterior of $Y_{e,i,j}$ given all data.


## Posterior Predictive Simulations

![](decathlon-predictive.png){width=50% fig-align="center"}

## Predicted Age Curve


![](decathlon-age-curves.png){width=85% fig-align="center"}


## Athlete Profiles


![](decathlon-quantiles.png){width=85% fig-align="center"}

## Breaking 9200

![](decathlon-profiles){fig-align="center"}


