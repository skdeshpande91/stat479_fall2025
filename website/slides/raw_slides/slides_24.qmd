---
title: "STAT 479 Lecture 24"
subtitle: "Decathlon Performance"
format: revealjs
execute:
  cache: true
---

# Motivation & Setup

## Decathlon Overview

![](decathlon.png){width=90% fig-align="center"}

- Events spread over 2 days
- Success requires speed, strength, & skill
- Individual event scores aggregated into a single score

## Best Decathlon Performances

![](decathlon-scores.png){fig-align="center"}

- Can someone break 9200? What would it take?
- Max out in one event? Be good but not elite at all?

## Statistical Model

- $j$-th performance in event $e$ by athlete $i$:
$$
\begin{align}
y_{e,i,j} &= \alpha_{e,i} + \sum_{d = 1}^{D}{\gamma_{e,d}\phi_{d}(\text{age}_{i,j})} \\
~& + \beta_{e,1}Y_{1,i,j} + \beta_{e,2}Y_{2,i,j} + \cdots + \beta_{e,e-1}Y_{e-1,i,j} + \epsilon_{e,i,j}
\end{align}
$$
- Athlete-event random effect: $\alpha_{e,i} \sim N(\mu_{e}, \sigma_{e})$
- $\phi_{d}(x) = x^{d}$: allow non-linear performance evolution

## Understanding the model

- Model has loads of parameters:
  - $\alpha_{e,i}$'s: capture each athlete's latent skill in each event
  - $\gamma_{e,d}$'s: govern how performances change over time
  - $\beta_{e,e'}$: captures inter-event relationships
  
- If we knew the parameter values, we could 
  - Predict age curve for each event & overall score
  - Estimate prob. of breaking record via simulation

## Model Fitting

- Classical approach: maximum likelihood estimation
  - Find $\boldsymbol{\alpha}, \boldsymbol{\beta},$ and $\boldsymbol{\gamma}$ that make observed data most likely
  - Take STAT 310/312/410 for details

- Use $\boldsymbol{\hat \alpha}, \boldsymbol{\hat \beta},$ and $\boldsymbol{\hat \gamma}$ to make predictions

- How to propagate *estimation* uncertainty to downstream predictions?

# Digression: A Bayesian Approach
  
## Types of Uncertainty

- Aleatoric: inherent uncertainty in a system
- Epistemic: due to ignorance/lack of knowledge

. . . 

- Eg: consider a coin with unknown bias $\mathbb{P}(\textrm{heads})$
- Epistemic uncertainty about $\mathbb{P}(\textrm{heads})$
  - If we repeatedly flip, we can learn $\mathbb{P}(\textrm{heads})$
  - The more data we get, the less uncertainty!

- Aleatoric: uncertainty about result of next flip
  - Even if we know $\mathbb{P}(\textrm{heads})$, there's still uncertainty about result of next flip
  - Can't be reduced with more data


## The Big Bayesian Picture

- Quantify **all** uncertainties using probability distributions
- Update, combine, and propagate uncertainty using probability calculus

- General workflow: data $Y$ and unknown parameter $\theta$
  - Specify likelihood $p(y \vert \theta)$ & prior $p(\theta)$
  - Compute posterior distribution $p(\theta \vert y)$
- Posterior quantifies all post-data uncertainty **given the observed data*

## Advantages of Bayesian Modeling

- Multiple parameter values can provide good fits to data
- No longer forced to commit to single estimate to drive downstream prediction
- Can make probabilistic statements: "93% prob. $\theta$ lies in this interval"
- Sequential nature: use data to update prior beliefs


## Example: Flipping a Strange Coin

- Consider a coin with unknown $\mathbb{P}(\textrm{heads}) = \theta \in [0,1]$
- Flip the coin 10 times and observe $Y = 4$ heads.
- We will flip 5 more times. What's your prediction for $Y^{\star}$? 

. . . 

- Classical approach: $\hat{\theta} = 0.4$
- $\mathbb{P}(Y^{\star} = k \vert \theta) = \binom{5}{k}\theta^{k}(1 - \theta)^{5-k}$
- $E[Y^{\star} \vert \theta = 0.4] = 2$ and $\mathbb{P}(Y^{\star} = 2 \vert \theta = 0.4)$ is largest


```{r}
#| label: fig-coin-classical
#| eval: true
#| echo: false
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center

par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))
plot(1, type = "n",
     xlim = c(0, 5), ylim = c(0, 0.5),
     xlab = "k", ylab = "Prob", main = "Distribution of Y*")
for(x in 0:5){
  points(x = x, y = dbinom(x, size = 5, prob = 0.4))
  lines(x = c(x,x), y = c(par("usr")[3], dbinom(x, size = 5, prob = 0.4)))
}

```

- Then, we'll flip the coin an additional 5 times & count $Y^{\star}$



## Example: Likelihood Specification

- Consider a coin with unknown $\mathbb{P}(\textrm{heads}) = \theta \in [0,1]$
- We will flip $n = 10$ times & record number of heads $Y$
- Goal: Quantify our uncertainty about $\theta$ after observing $Y = 4.$

. . . 

- Likelihood: $p(y \vert \theta) = \binom{10}{y}\theta^{y}(1 - \theta)^{10-k}$
  - If $\theta = 0.5,$ prob. of seeing $Y = 4$ is 20.5%
  - If $\theta = 0.4,$ prob. of seeing $Y = 4$ is 25.1%
  - If $\theta = 0.1,$ prob. of seeing $Y = 4$ is 1%

## Example: Prior Specification
- What do we believe about the coin **prior** to flipping it & seeing data?
- Absent more information, uniform prior not unreasonable: $p(\theta) = 1$
- Prior to seeing any data:
  - Best estimate of $\theta$ is *prior mean*: $\hat{\theta} = 1/2$
  - 95% prior prob. that $\theta \in [0.025, 0.975]$ 

```{r}
#| label: fig-prior-density
#| eval: true
#| echo: false
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center

oi_colors <- palette.colors(palette = "Okabe-Ito")
theta_seq <- seq(0, 1, by = 0.005)
a <- 1
b <- 1
prior_dens <- rep(1, times = length(theta_seq))
theta_ix <- which(theta_seq >= 0.025 & theta_seq <= 0.975)

prior_ci_x <- theta_seq[theta_ix]
prior_ci_y_top <- prior_dens[theta_ix]
prior_ci_y_bot <- rep(par("usr")[3], times = length(theta_ix))


par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))
plot(1, type = "n", xlim = c(0,1), ylim = c(0, 3),
     xlab = "theta", ylab = "Density", main = "Prior & Posterior")
lines(theta_seq, prior_dens, col = oi_colors[3])

polygon(x = c(prior_ci_x, rev(prior_ci_x)),
        y = c(prior_ci_y_bot, rev(prior_ci_y_top)),
        col = adjustcolor(col = oi_colors[3], alpha.f = 0.5),
        border = NA)

```

## Example: Posterior Distribution

- $p(\theta \vert Y = 4) \propto \theta^{4+1-1}(1 - \theta)^{6+1-1}$ meaning $\theta \vert Y = 4 \sim \textrm{Beta}(5,7)$
  - Best guess: $\hat{\theta} = (4 + 1)/(10 + 1)$
  - 95% prob. that $\theta$ in `{r} round(qbeta(c(0.025, 0.975), 5, 7), digits = 2)`
```{r}
#| label: fig-beta-binomial
#| eval: true
#| echo: false
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: "center"

y <- 4
n <- 10

post_dens <- dbeta(theta_seq, shape1 = y+a, shape2 = n-y+b)

theta_ix_post <- which(theta_seq >= qbeta(0.025, a+y, b+n-y) & theta_seq <= qbeta(0.975, a+y, b+n-y))

post_ci_x <- theta_seq[theta_ix_post]
post_ci_y_top <- post_dens[theta_ix_post]
post_ci_y_bot <- rep(par("usr")[3], times = length(theta_ix_post))

par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))
plot(1, type = "n", xlim = c(0,1), ylim = c(0, 3)),
     xlab = "theta", ylab = "Density", main = "Posterior")
#lines(theta_seq, prior_dens, col = oi_colors[3])
lines(theta_seq, post_dens, col = oi_colors[4])

polygon(x = c(post_ci_x, rev(post_ci_x)),
        y = c(post_ci_y_bot, rev(post_ci_y_top)),
        col = adjustcolor(col = oi_colors[2], alpha.f = 0.5),
        border = NA)

```


## Example: Posterior Predictive

- Say we flip the coin another 5 times; $Y^{\star}$ is number of heads
- $\mathbb{P}(Y^{\star} = k \vert Y = 4) = \int{ \mathbb{P}(Y^{\star} = k \vert \theta)p(\theta \vert Y = 4)d\theta}$


```{r}
#| label: fig-post-pred
#| eval: true
#| echo: false


theta_draws <- rbeta(n = 1e5, shape1 = y+n, shape2 = n-y+b)
ystar <- rbinom(n = 1e5, size = 5, prob = theta_draws)
post_pred <- table(ystar)/1e5



```


## Bayesian Modeling in Practice

- Likelihood & prior: specify a complete data generating process:
  - To generate data $Y$, first draw $\theta$ and then draw $Y \vert \theta$
- Output is a whole distribution not just a point estimate
- In practice, we obtain *samples* from the posterior



# Decathlon Modeling Results

## Inter-event Dependence

![](decathlon-beta.png){width=50%}

- .1 second decrease in 100m associated w/ ~24 second increase in 1500m time
- Posterior fully concentrated on negative values

## Posterior Predictive Simulation

- For each posterior sample $\boldsymbol{\alpha}^{(m)}, \boldsymbol{\beta}^{(m)}, \boldsymbol{\gamma}^{(m)}, \boldsymbol{\sigma}^{(m)}$:
$$
\begin{align}
y^{\star(m)}_{e,i,j} &= \alpha^{(m)}_{e,i} + \sum_{d = 1}^{D}{\gamma^{(m)}_{e,d}\phi_{d}(\text{age}_{i,j})} \\
~& + \beta^{(m)}_{e,1}Y_{1,i,j} + \beta^{(m)}_{e,2}Y_{2,i,j} + \cdots + \beta^{(m)}_{e,e-1}Y_{e-1,i,j} + \sigma^{(m)}\epsilon^{\star (m)}_{e,i,j},
\end{align}
$$
where $\epsilon^{\star(m)} \sim N(0,1)$ indepently of other parameters

- $y^{\star (1)}_{e,i,j}, \ldots, y^{\star} (M)}_{e,i,j}$: sample from posterior of $Y_{e,i,j}$ given all data.

## Posterior Predictive Simulations

![](decathlon-predictive.png){width=50% fig-align="center"}



