---
title: "Lecture 15: Markov Chains II"
format: html
execute: 
  cache: true
---
<!--
  Create a "Votes" matrix. v_{s,s'} is how many votes team s "gives" to team s'
  Imagine the fan currently supporting the team S. They choose to move .
  Because they're looking for the best possible team, the fan will move from S to S' if 
  they believe S' is better. 
  Many ways to measure better so let's imagine that there's some out-going votes.
  For now, we'll say 
  
  Under this construction, we'll say 
  
  
  Of course, we might also want to allow teams to vote for itself! To make this a bit more formal,


-->




## Overview

In [Lecture 14](lecture14.qmd), we estimated the matrix of transition probabilities for 25 different game-states in baseball.
24 of these states corresponded to the combinations of outs and base runner configurations.
The final state was the three-out state, representing the end of the half-inning.
We simulated Markov chains that randomly walked between these states according to the estimated transition matrix



## Example

We will discuss how to construct the transition matrices a little bit later




## Overview


In [Lecture 14](lecture14.qmd), we estimated the matrix of transition probabilities for 25 different game-states in baseball.
24 of these states corresponded to the combinations of outs and base runner configurations.
The final state was the three-out state, representing the end of the half-inning.
We simulated Markov chains that randomly walked between these states according to the estimated transition matrix

In this lecture, we will explore how Markov chains can be used to derive power rankings for teams from head-to-head competitions.
Unlike our [rankings derived from Bradley-Terry models](lecture12.qmd), these Markov chain-based rankings cannot immediately be used to estimate the probability of one team beating another. 
Instead, they are based on mathematical properties of specifically constructed Markov chain models over the state space of teams.
These Markov chains model the behavior of a hypothetical"bandwagon" fan who tries to identify the "best" team by randomly switching their support from team $s$ to team $s'$ based on how often and/or by how many points team $s'$ beats team $s.$
If we simulate a Markov chain modeling the fan's behavior, we can estimate the relative frequencies with which the fan supports each team.
These relatively frequencies immediately give us a rank-ordering of the teams based on how often the bandwagon fan think that they are best.

We begin in @sec-fourteam-example by illustrating such a Markov chain inWe first 


## The Random Behavior of a Fair-weather Fan {#sec-fourteam-example}

Imagine a bandwagon fan who randomly switches their support each day between four teams (`W`, `O`, `M`, and `C`) according to the transition probabilities in @fig-fourstate-graph. 
For instance, if they currently support `W` today, tomorrow they will support

  * Team `W` with probability 0.05 (i.e., they stay at vertex `W`)
  * Team `M` with probability 0.05 (i.e., they move from vertex `W` to vertex `M`)
  * Team `O` with probability 0.85 (i.e., they move from vertex `W` to vertex `O`)
  * Team `C` with probability 0.05 (i.e., they move from vertex `W` to vertex `C`)
  
Once the fan selects their new team to support, the process begins again: based on their current favorite team, they pick a new team based on the weights if the outgoing edges from the corresponding vertex in the graph.

![Graphical representation of the transition matrix of a fan who randomly choose which team to support](figures/four-state-graph.png){#fig-fourstate-graph width=50% fig-align="center"}

The following code defines a *function* that simulates the Markov chain describing the fans behavior.
The function takes three arguments: the starting state (i.e., the team the fan initially supports); the number of iterations to simulate; and the transition matrix $\boldsymbol{\mathbf{P}}.$
It then returns the relative proportion of times that the chain spends in each state (i.e., how often the fan supports each team).

```{r}
#| label: four-state-example-fn

fan_walk <- function(init_state, n_iter, P){
  states <- colnames(P)
  states_visited <- rep(NA, times = n_iter)
  states_visited[1] <- init_state
  for(r in 2:n_iter){
    states_visited[r] <- 
      sample(states, size = 1, 
             prob = P[states_visited[r-1],])
  }
  props <- table(states_visited)[states]/n_iter
  return(props)
}
```

Now we define the matrix $\boldsymbol{\mathbf{P}}$ correspond to the graph in @fig-fourstate-graph
```{r}
teams <- c("W", "O", "C", "M")
P <- matrix(c(0.05, 0.85, 0.05, 0.05,
              0.425, 0.075, 0.075, 0.425,
              0.05, 0.85, 0.05, 0.05,
              0.53, 0.32, 0.075, 0.075), 
            byrow = TRUE, nrow = 4, ncol = 4,
            dimnames = list(teams, teams))

set.seed(479)
round(fan_walk(init_state = "W", n_iter = 1e6, P = P), digits = 3)
round(fan_walk(init_state = "O", n_iter = 1e6, P = P), digits = 3)
round(fan_walk(init_state = "C", n_iter = 1e6, P = P), digits = 3)
round(fan_walk(init_state = "M", n_iter = 1e6, P = P), digits = 3)

```
The long-run frequency that the chain spends in each state is remarkably stable: regardless of their initial preference, the fan spends about 30\% of their time supporting `W`; 42\% of the time supporting `O`; 7\% of the time supporting `C`; and 21\% of the time supporting `M`. 



## Limiting Distributions of Markov Chains

The stability in the long-run proportion of time spent in each state in the four-team example is not coincidental!

### Distribution of $X_{n}$ {#sec-nstep-dist}

Suppose that $\{X_{t}\}$ is a Markov chain defined on the states $\{1, 2, \ldots, S\}$ with transition matrix $\boldsymbol{\mathbf{P}}.$
Let $\boldsymbol{\pi}_{0} = (\pi_{0,1}, \ldots, \pi_{0,S})^{\top}$ be a vector of probabilities.
Now suppose that we initialize the Markov chain randomly according to $\boldsymbol{\pi}_{0}.$
That is, for each state $s \in \{1, 2, \ldots, S\},$ $\mathbb{P}(X_{0} = s) = \pi_{0,s}.$
If we initialize the Markov chain according to $\boldsymbol{\pi}_{0}$ and then simulate the Markov chain for $n$ steps, what is the probability that the chain is in state $s$ after $n$ steps? 

Before jumping into the general case for the $n$-th visited step, we'll start by carefully computing the distribution of the first visited state (i.e., $X_{1}$).
To this end, observe that
$$
\mathbb{P}(X_{1} = s') = \sum_{s}{\mathbb{P}(X_{1} = s' \textrm{ and } X_{0} = s)},
$$
where the sum is taken over the entire state space $\{1, 2, \ldots, S\}.$
We can further compute each summand in as the product of atransition probability and an initial probability:
$$
\mathbb{P}(X_{1} = s' \textrm{ and } X_{0} = s) = \mathbb{P}(X_{1} = s' \vert X_{0} = s)\mathbb{P}(X_{0} = s).
$$
Since we know $\mathbb{P}(X_{0} = s) \pi_{0,s}$ and $\mathbb{P}(X_{1} = s' \vert X_{0} = s) = p_{s,s'},$ we conclude that
$$
\mathbb{P}(X_{1} = s') = \sum_{s}{\pi_{0,s}p_{s,s'}}
$$
If we let if $\boldsymbol{\pi}_{1} = (\pi_{1,1}, \ldots, \pi_{1,S})^{\top}$ where $\pi_{1,s} = \mathbb{P}(X_{1} = s)$ be the probability distribution of $X_{1},$ we can re-write the above equation in matrix form as
$$
\boldsymbol{\pi}_{1}^{\top} = \boldsymbol{\pi}_{0}^{\top}\boldsymbol{\mathbf{P}}
$$
Using essentially the same calculation, we conclude that for each $n$,
$$
\boldsymbol{\pi}_{n}^{\top} = \boldsymbol{\pi}_{n-1}^{\top}\boldsymbol{\mathbf{P}},
$$
where $\boldsymbol{\pi}_{n} = (\pi_{n,1}, \ldots, \pi_{n,S})^{\top}$ and $\pi_{n,s} = \mathbb{P}(X_{n} = s).$
Recalling the definition of the $n$-step transition probability matrix $\boldsymbol{\mathbf{P}}^{(n)}$ from [Lecture 14](lecture14.qmd#sec-nstep), we have
$$
\boldsymbol{\pi}_{n}^{\top} = \boldsymbol{\pi}_{0}^{\top}\boldsymbol{\mathbf{P}}^{n},
$$

### Invariant Distributions {#sec-invariant-dist}
Returning to the 4 team example from @fig-fourstate-graph, suppose that the bandwagon fan choose the initial team to support uniformly at random.
The following code computes $\boldsymbol{\pi}_{1}, \ldots, \boldsymbol{\pi}_{10}$
```{r}
#| label: n-step-probs-1
#| eval: true
#| echo: true
pi0 <- rep(0.25, times = 4)
state_probs <- matrix(nrow = 10, ncol = 4, dimnames = list(c(), teams))
state_probs[1,] <- pi0
for(i in 2:10){
  state_probs[i,] <- state_probs[i-1,] %*% P #<1>
}
round(state_probs, digits = 2)
```
1. The $i$-th row of `state_probs` holds the entries in the vector $\boldsymbol{\pi}_{i-1}$ (i.e., the distribution of $X_{i-1}$).

We notice that after about four or five iterations, it appears that the vectors $\boldsymbol{\pi}_{n-1}$ and $\boldsymbol{\pi}_{n}$ are essentially identical.
We observe the same phenomenon using a very different initial distribution
```{r}
#| label: n-step-probs-2
#| eval: true
#| echo: true
pi0 <- c(0.9, 0.05, 0.025, 0.025)
state_probs <- matrix(nrow = 10, ncol = 4, dimnames = list(c(), teams))
state_probs[1,] <- pi0
for(i in 2:10){
  state_probs[i,] <- state_probs[i-1,] %*% P #<1>
}
round(state_probs, digits = 2)
```
It would appear that if every $\boldsymbol{\pi}_{n} \approx (0.31, 0.42, 0.07, 0.21)^{\top},$ then $\boldsymbol{\pi}_{n+1} \approx \boldsymbol{\pi}_{n}.$
In other words, after a while, the uncertainty about what state the Markov chain is in appears to be *invariant*. 

:::{.callout-note}

## Definition: Invariant Distribution

Suppose $\{X_{t}\}$ is a Markov chain over state space $\{1, 2, \ldots, S\}$ with transition probability matrix $\boldsymbol{\mathbf{P}}.$
The probability distribution $\boldsymbol{\pi} = (\pi_{1}, \ldots, \pi_{S})^{\top}$ is **invariant** to $\boldsymbol{\mathbf{P}}$ if 
$$
\boldsymbol{\pi}^{\top}\boldsymbol{\mathbf{P}} = \boldsymbol{\pi}.
$$
Notice that if $\boldsymbol{\pi}$ is invariant, if the chain is initialized according to $\boldsymbol{\pi}$ then $\mathbb{P}(X_{t} = s) = \pi_{s}$ for all time steps $t.$

:::


It turns out that for a broad class of Markov chains --- specifically, those that are positive recurrent[^positiverecurrent], aperiodic[^aperiodic], and irreducible[^irreducible] --- then

  1. There is a unique invariant distribution $\boldsymbol{\pi}$ satisfying $\boldsymbol{\pi}^{\top}\boldsymbol{\mathbf{P}} = \boldsymbol{\pi}.$
  2. As $n \rightarrow \infty,$ $\boldsymbol{\pi}_{n} \rightarrow \boldsymbol{\pi}.

[^aperiodic]: The *period* of state $s$ is the greatest common divisor of all $n$ such that $(\boldsymbol{P}^{n})_{s,s} > 0.$ If state $s$ has period $d,$ this means that the chain has non-zero probability of returning to $s$ every $d$ steps. An **aperiodic** chain is one in which the period of every state is 1. That is, in an aperiodic chain, there is always a chance of returning to the same state in one step.

[^irreducible]: Informally, irreducible Markov chains are those in which there is a any state $s'$ can be reached from any other state in a **finite** number of steps. A sufficient (but not necessary) condition is that $p_{s,s'} > 0$ for all pairs of state $s$ and $s'$

[^positiverecurrent]: A chain is said to be positive recurrent if for all states $s,$ a chain begun in $s$ is guaranteed to return to $s$ in a **finite** number of steps with probability 1. A sufficient (but not necessary) condition is for $p_{s,s} > 0$ for all states $s.$



### Computing Invariant Distributions


That is, under suitable conditions, the limiting distribution of the states visited by the Markov chain converges to the invariant distribution.
This means that in order to determine the long-run frequency of the chain being in state $s,$ we do not actualy have to simulate the Markov chain.
Instead, we need find a probability vector $\boldsymbol{\pi}$ such that $\boldsymbol{\pi}^{\top}\boldsymbol{\mathbf{P}} = \boldsymbol{\pi}^{\top}.$
This relation implies that in fact, the invariant probability vector is a **left** eigenvector of the matrix $\boldsymbol{\mathbf{P}}$ with eigenvalue 1.
The following code defines a function that return the invariant distribution given a transition matrix $\boldsymbol{\mathbf{P}}.$

```{r}
#| label: get-invariant-fn
#| eval: true
#| code-fold: true
#| code-summary: Function to compute leading left eigenvector of transition matrix

get_invariant <- function(P, tol = 1e-16){ #<1>
  states <- colnames(P)
  if(!all( abs(rowSums(P) - 1) < tol) ){ #<2>
    stop("Rows sums of P are not all 1. P may not be a valid transition matrix") #<2
  } #<2>
  
  decomp <- eigen(t(P)) #<3>
  pi_raw <- decomp$vectors[,1] #<4>
  
  if(!all(Re(pi_raw) < tol)){
    stop("Leading eigenvalue of P appears to be complex and not real-valued")
  } #<5>
  pi <- Re(pi_raw)
  pi <- pi/sum(pi)
  names(pi) <- states
  
  return(pi)
}
```
1. When doing linear algebra in R, it's helpful to work up to some degree of numerical precision. For most of our calculations, it'll be enough to use `1e-16`
2. Transition matrices have row sums of 1. This checks that the row sums of the provided matrix `P` are sufficiently close to 1. If they are not, the function throws an error
3. By default `eigen()` returns right eigenvectors. The left eigenvectors of a matrix $M$ are the right eigenvector of its transpose $M^{\top}$
4. `eigen()` scales eigenvectors to have squared norm 1 and not a sum of 1. We need to re-scale this vector.
5. Generally speaking, the eigenvalues and eigenvectors of a transition matrix are complex-valued and not real-valued. For a Markov chain with a limiting, invariant distribution the leading eigenvector should only have real values. This code checks that the imaginary parts are 0 (or at least smaller than than our small tolerance `tol`)


As an illustration, in the code below we first compute the invariant distribution for our 4-team Markov chain.
Then, we look at the different $\boldsymbol{\pi}^{\top} - \boldsymbol{\pi}^{\top}\boldsymbol{\mathbf{P}}.$
We find that that differences are all of the other $10^{-16},$ which for our purposes are small enough to be considered zero.

```{r}
#| label: verify-invariant
#| eval: true
#| echo: true
pi <- get_invariant(P)
cat("pi:", round(pi, digits = 6), "\n")
cat("pi %*% P:", round(pi %*% P, digits = 6), "\n")
cat("Difference: ", pi - pi %*% P, "\n")
```

If we interpret the bandwagon fans' transitions from team $s$ to $s'$ to signify that team is $s'$ is, in some sense, better than $s,$ then we can rank teams by the amount of time the fan spends support thing.
In our 4-team example, the bandwagon fan supports:

  * `W` about 30% of the time
  * `O` about 42% of the time
  * `C` about 7% of the time
  * `M` about 21% of the time.

So, we would obtain the ranking `O`, `W`, `M,` and `C` based on this specific Markov chain. 


## Markov chain-based Rankings 

We're now in a position to apply this general procedure to our D1 hockey data.
The main steps involve:

  1. Construct a Markov chain model describing how a bandwagon fan could randomly switch their support from team to team
  2. Compute the invariant distribution of the Markov chain

There are **many** ways to carry out the first step and there is no objectively or normatively *best* way to do so.
By convention, we usually set up the Markov chain model so that $p_{s,s'}$, the probability that the fan switches their support from $s$ to $s'$, quantifies how much better $s'$ is than $s.$
Most often, $p_{s,s'}$ is selected to depend on the number of times $s'$ beats $s$ and/or the number of points $s'$ scores against $s.$












### Constructing $\boldsymbol{\mathbf{P}}$

There are *many* ways to construct the transition matrix $\boldsymbol{\mathbf{P}}.$
All of these involve first introducing edge weights $w_{s,s'}$ for all possibly pairs of teams $s$ and $s'.$
In addition to specifying the weight of the edge from $s$ to $s'$ (i.e., $w_{s,s'}$) and the weight of the edge from $s'$ to $s$ (i.e., $w_{s',s}$), we also must specify the weight for the self-loop from $s$ back to itself (i.e., $w_{s,s}$).
Once we do that, we can then form a *stochastic matrix* $\boldsymbol{\mathbf{P}}$ whose rows sum to 1 and use this matrix as the transition matrix in our Markov chain.
To ensure that the the resulting Markov chain has a unique limiting distribution, we often need to add a slight *damping* factor.

Formally, let $\boldsymbol{\mathbf{A}}$ be the diagonal matrix containing the *row* sums of $\boldsymbol{\mathbf{W}}$ with entries $a_{s,s'} = 0$ whenever $s \neq s'$ and 
$$
a_{s,s} = \sum_{s'}{w_{s,s'}}.
$$
Additionally let $\boldsymbol{\mathbf{J}}$ be the $T \times T$ matrix consisting of all $1$'s.
Then for some damping factor $0 < \beta < 1$, we set
$$
\boldsymbol{\mathbf{P}} = \beta \times \boldsymbol{\mathbf{A}}^{-1}\boldsymbol{\mathbf{W}} + \frac{1 - \beta}{T}\boldsymbol{\mathbf{J}}
$$
Essentially the additional damping ensures that the fan walking along a graph always has the potential to (i) not change their support in one-step and (ii) can switch their support to every other team from the current team.
This is especially helpful in situations where not every team plays every other team or if one team goes undefeated. 




#### Loss-based Edge Weights
Perhaps the simplest edge-weight specification sets $w_{s,s'}$ to be the number of times team $s$ beats team $s'.$
Let $\boldsymbol{\mathbf{W}}$ be the matrix with entries $w_{s,s'}.$

<!--
 Weights are loss. w(s,s') = -1 each time s loses to s'. Intuitively: if s loses to s' many times, we'd like for the the fan to move to s' more often  from s.
 Set beta = 0.85
-->


#### Margin-Based Weights

<!--
  w_{s,s'} : when s loses to s', look up total point differential by which they lost.
  is w_{s,s'} < w_{s,t}, we want to encourage fan to move to the team that beat s by more points!
  if s never loses to s', set w_{s,s'} = 0.
-->

#### Score-based Weights

<!--
  w_{s,s'} number of points s' scored when it played s
  w_{s',s} number of points s score when it played s'.
  
  
--> 

To illustrate, let us load in the NCAA D1 Hockey Data



Given a Markov chain

Under suitable conditions[^markovchain], one can show that a Markov chain 






[^fairweather]: See [this letter](https://www.nytimes.com/2024/03/05/magazine/fair-weather-fan-recommendation.html) for a counter-point.





Each edge from team $s$ to team $s'$ will be associated with a weight $w_{s,s'}.$

Every time


We can now imagine a fan picking a node uniformly at random and then taking a random walk on the graph, visiting a sequence of vertices corresponding to different teams.
The probability that the fan walks from vertex $s$ to vertex $s'$ will be proportional to $w_{s,s'}.$


Now imagine a fan takes a random

imagine creating a directed graph that contains a node (or vertex) for every team and that contains directed edges between  
 



a fair-weather fan taking a random walk between teams and trying to identify the best team.


The fan can start at any team and

The central idea



We also clear out our workspace, deleting all the objects used for our baseball modeling.
```{r}
#| label: load-hockey
#| eval: true
#| echo: true
rm(list = ls())
load("wd1hockey_regseason_2024_2025.RData")
```



