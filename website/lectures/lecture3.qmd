---
title: "Actual and Expected Performance"
format: html
execute: 
  cache: true
---
## Motivation: Beth Meade's Performance at EURO2022

<!--
Beth Mead scored 6 goals in the EURO2022 and won the tournament's Golden Boot award.
Let's take a look at some of her goals.
Which was the most impressive?


-->


:::: {.columns}

:::{.column width="49%"}
{{< video https://youtube.com/clip/UgkxS9IXis1jcLq7Q5oTB2ED6Dkb34FT7TmM?si=-2MwgWJsy2HkeWgb >}}
:::

:::{.column width="49%"}
{{< video https://youtube.com/clip/UgkxJKtmAvMQ9eXTygNxJUkLL1BvVN7o2njn?si=TUaiNTdkohxUWfqr >}}
:::

::::


<!--
We can argue about this endlessly.
In doing so, we found ourselves dissecting various aspects -- the footedness, the type of shot, the circumstances ---
To resolve this question, we can try to *quantify* the differences between these goals.
One way -- not the only way -- is to imagine what might have happened if we could reply every shot.
We could count up the time
As we will see in this lecture, we can provide a *quantitative* answer to this question through the framework of expected goals


Why is XG compelling for assessing the quality of a chance? If something is expected to go in, we shouldn't be surprised when it does. Similarly, if it is unlikely to go in, then we should give a lot of credit to the player for converting the chance!
-->



## Working with soccer event data

We will make use of high-resolution tracking data provided by the company Huddle StatsBomb.
As a bit of background, StatsBomb extracts player locations using <!-- video processing --> 
To their great credit, StatsBomb releases a small snapshot of their data for public use.
We can access this data directly in R using the **StatsBombR** package.

You can check if the package is installed using the code `"StatsBombR" %in% rownames(installed.packages()).`
If that code returns `FALSE`, then you can install the package using the code `devtools::install_github("statsbomb/StatsBombR")`.

```{r xg-setup}
#| output: false
library(tidyverse)
# save a nice color-blind friednly color palette into our environment
oi_colors <- palette.colors(palette = "Okabe-Ito") 
```

StatsBomb organizes its free data by competition/tournament.
The screenshot below shows a table of all the available competitions.
We can load this table into our R environment using the function `StatsBombR::FreeCompetitions()`
<!--
Screenshot of the table of matches and table of matches for 
-->
Each competition and season have unique id and we can also see whether it was a men's or women's competition.
To see which matches from selected competitions have publicy available data, we can pass the corresponding rows of this table to the function `StatsBombR::FreeMatches()`.
Figure 2 shows the table of matches from the 2022 EURO Competition; StatsBomb graciously provided data for all matches from the tournament, which can be obtained using the code below.
```{r get-euro2022-matches}
#| eval: false
StatsBombR::FreeCompetitions() %>%
  filter(competition_id == 53 & season_id == 106) %>% # Finds competition corresponding to EURO 2022
  StatsBombR::FreeMatches() 
```

Finally, to get raw-event level data for certain matches, we need to pass the corresponding rows to the function `StatsBombR::free_allevents().`
StatsBomb also recommends running some basic pre-processing, all of which is nicely packaged together in the functions `StatsBombR::allclean()` and  `StatsBombR::get.opposingteam().`

As an example, the code chunk below pulls out publicly available event data for every women's international match.
```{r get-wi}
#| output: false
#| message: false
#| warning: false
wi_events <-
  StatsBombR::FreeCompetitions() %>% # get table of available competitions
  filter(competition_gender == "female" & 
           competition_international) %>% # filter to women's internationals
  StatsBombR::FreeMatches() %>% # Gets match data for all women's internationals
  StatsBombR::free_allevents() %>% # Gets all events for all matches
  StatsBombR::allclean() %>% # Apply StatsBomb's pre-processing
  StatsBombR::get.opposingteam()
```


::: {.callout-important}

## Navigating complex code

It is not easy to code complicated pipelines like the above in a single attempt.
In fact, I had to build the code line-by-line.
For instance, I initially ran just the first line and manually inspected the table of free competitions (using `View()`) to figure out which variables I needed to `filter()` on in the second line.
It is very helpful to develop pipelines incrementally and to check intermediate results before putting everything together in one block of code.
:::

## An initial expected goals model

Suppose we observe a dataset consisting of $n$ shots.
For each shot $i = 1, \ldots, n,$ let $Y_{i}$ be a binary indicator of whether the shot resulted in a goal ($Y_{i} = 1$) or not ($Y_{i} = 0$).
From the high-resolution tracking data, we can extract a potentially huge number of features about the shot at the moment of it was taken.
Possible features include, but are certainly not limited to, the player taking the shot, the body part and side of the body used, the positions of the defenders and goal keepers, and contextual information like the score. 
Mathematically, we can collect all these features into a (potentially large) vector $\boldsymbol{\mathbf{X}}_{i}.$

Expected goals (XG) models work by (i) positing an infinite super-population of shots represented by pairs $(\boldsymbol{\mathbf{X}}, Y)$ of feature vector $\boldsymbol{\mathbf{X}}$ and binary outcome $Y$; and (ii) assuming that the shots in our dataset constitute a random sample $(\boldsymbol{\mathbf{X}}_{1}, Y_{1}), \ldots, (\boldsymbol{\mathbf{X}}_{n}, Y_{n})$ from that population.
::: {.callout-note icon=false}
## Conditional Expectations

For each combination of features $\boldsymbol{\mathbf{x}}$, the expect goals given $\boldsymbol{\mathbf{x}},$ which we will denote by $\textrm{XG}(\boldsymbol{\mathbf{X}})$ is just the average value of $Y$ among the (assumed infinite) sub-population of shots with features $\boldsymbol{\mathbf{X}} = \boldsymbol{\mathbf{x}}.$
Mathematically, XG is *conditional expectation*:
$$
\textrm{XG}(\boldsymbol{\mathbf{x}}) = \mathbb{E}[Y \vert \boldsymbol{\mathbf{X}} = \boldsymbol{\mathbf{x}}],
$$
:::

Because the shot outcome $Y$ is binary, $\textrm{XG}(\boldsymbol{\mathbf{x}})$ is the proportion of goals scored within the sub-population of shots defined by the feature combinations $\boldsymbol{\mathbf{x}}.$
In other words, it is the *conditional probability* of a goal given the shot features $\boldsymbol{\mathbf{x}}.$
On this view, $\textrm{XG}(\boldsymbol{\mathbf{x}})$ provides a quantitative answer to our motivating question "If we were to replay a particular shot over and over again, what fraction of the time does it result in a goal?"


The StatsBomb variable `shot.body_part.name` records the body part with which each shot was taken.
Within our dataset of women's international matches, we can see the breakdown of these body parts
```{r wi-body-part}
table(wi_events$shot.body_part.name)
```
For this analysis, we will focus on fitting XG models using data from shots taken with a player's feet or head.
```{r wi-shots-foot}
#| output: false
#| message: false
#| warning: false
wi_shots <-
  wi_events %>% # Adds opposing team information
  filter(type.name == "Shot" & shot.body_part.name != "Other") %>%  # Subsets only shot event data
  mutate(Y = ifelse(shot.outcome.name == "Goal", 1, 0))
```

Later, it will be useful for us to focus only on the shots from EURO2022, so we will also create a table `euro2022_shots` of all shots from that competition using similar code.
```{r euro2022-shots}
#| code-fold: true
#| output: false
#| message: false
#| warning: false
euro2022_shots <-
  StatsBombR::FreeCompetitions() %>% # get table of available competitions
  filter(competition_id == 53 & season_id == 106) %>% # Finds competition corresponding to EURO 2022
  StatsBombR::FreeMatches() %>% # Gets match data for all EURO 2022
  StatsBombR::free_allevents() %>% # Gets all events for all matches
  StatsBombR::allclean() %>% # Apply StatsBomb's pre-processing
  StatsBombR::get.opposingteam() %>%
  filter(type.name == "Shot" & shot.body_part.name != "Other") %>%
  mutate(Y = ifelse(shot.outcome.name == "Goal", 1, 0))
```

Now suppose we only include the body part in $\boldsymbol{\mathbf{X}}$. 
If we had full access to the infinite super-population of women's international shots, then we could compute
$$\textrm{XG}(\text{right-footed shot}) = \mathbb{P}(\text{goal} \vert \text{right-footed shot})$$
by (i) forming a sub-group containing only those right-footed shots and then (ii) calculating the proportion of goals scored within that sub-group.
We could similarly compute $\textrm{XG}(\text{left-footed shot})$ and $\textrm{XG}(\text{header})$ by calcuating the proportion of goals scored within the sub-groups containing, resptively, only left-footed shots and only headers.

Of course, we don't have access to the infinite super-population of shots.
However, on the assumption that our observed data constitute a sample from that super-population, we can *estimate* $\textrm{XG}$ by mimicking the idealized calculations described above:

  1. Break the dataset of all observed shots in women's international matches into several groups based on the body part
  2. Within these two groups, compute the proportion of goals

To keep things simple, we dropped the 23 shots that were taken with a body part other than the feet or the head.
```{r wi-xg-body}
xg_model1 <-
  wi_shots %>%
  group_by(shot.body_part.name) %>%
  summarize(XG1 = mean(Y),
            n = n())
xg_model1
```



::: {.callout-warning}

## Generalizing our model results

A key assumption of all XG models is that the observed data is a random sample drawn from the super-population.
The only women's internationals matches for which StatsBomb data were from the 2019 and 2023 World Cup and the 2022 EURO tournaments.
These matches are arguably **not** highly representative of all women's international matches, meaning that we should exercise some caution when using models fitted to these data to analyze matches from other competitions (e.g., an international friendly or a match in a domestic league).
:::

### Accounting for additional features

We can now create a table of just Beth Mead's shots from EURO 2022 and add a column with the XG for each shot.
To do this, we first filter our table `wi_shots` using the player name (note, StatsBomb uses her full name!).
Then, for every left-footed shot Mead attempted, we want to copy over the corresponding value from the table `xg_model1`, which in this cse is 0.113.
Similarly, we want to copy over the corresponding values for right-footed shots and headers from `xg_model1` into our table for Mead's shots.
We can do this using an [left join](https://r4ds.hadley.nz/joins.html#sec-mutating-joins).
In the code below, we actually create a temporary version of `xg_model1` that drops the column recording the overall counts of the body part used for the shots in `wi_shots`. 
This way, when we perform the join, we don't create a new column with these counts. 

```{r mead-xg-foot-tech}
tmp_xg1 <- xg_model1 %>% select(!n) # temporary copy of the table without sample size column

mead_shots <-
  euro2022_shots %>%
  filter(player.name == "Bethany Mead") %>%
  left_join(y = tmp_xg1, by = c("shot.body_part.name"))
rm(tmp_xg1) # delete temporary copy
```

We can now look at the what our model says about the three goals from above.
The first, against Austria in the 15th minute; the second, against Norway in the 37th minute, and the third against Sweden in the 33rd minute
When These turn out to be in rows 1, 4, and 14 of the table `mead_shots`
```{r mead-xg1-goals}
mead_shots %>%
  select(OpposingTeam, minute, shot.body_part.name, Y, XG1) %>%
  slice(c(1, 4, 14))
```
According to our first model, these three goals appear equally impressive: our model put the respective chances of each shot resulting in a goal at about 10%, 11%, and 10%.
But, watching the videos a bit more closely, this conclusions is **not** especially satisfying: Mead scored the first goal in a one-on-one situation but had to shoot through several defenders on the second and third goal
The discrepancy between our qualitative comparisons and our quantitative modeling results stems from the fact that we only conditioned on the body part and did not account for the other ways that the shots are different.
In other words, our initial XG model is much too coarse to quantify the differences between the three chances that we believe are important.


To better compare the three goals, we need to build a a finer XG model that conditions on more features, including ones that differ between the two shots.
To this end, notice that Mead uses a different *technique* on the three shots: she lobs the ball into the net on the first goal, shoots the ball from the ground on the second goal, and scores the third goal off of a half volley, striking the ball as it bounced up off the ground. 
The StatsBomb variable `shot.technique.name` records the technique of each shot type
```{r wi-tech}
table(wi_shots$shot.technique.name)
```
By conditioning on both body part and technique, we can begin to build a more refined XG model.
The code to do this is almost identical to the code used in our first model.
The only difference is that we now group by two variables `shot.body_part.name` and `shot.technique_name`.
Because we are grouping by two variables, specify the argument `.groups="drop"` argument when calling `summarize`; this prevents a (mostly innocuous) warning message[^groupnote].
We additionally append our new XG estimates to the table containing all of Mead's shots.

[^groupnote]: See [this StackOverflow post](https://stackoverflow.com/questions/78422148/understanding-the-purpose-of-groups-drop-in-dplyrs-summarise-function) and the [documentation](https://dplyr.tidyverse.org/reference/summarise.html#arg--groups) for `summarise` for details.

```{r wi-xg-foot-tech}
xg_model2 <-
  wi_shots %>%
  group_by(shot.body_part.name, shot.technique.name) %>%
  summarize(XG2 = mean(Y),
            n = n(), .groups = "drop")

tmp_xg2 <- xg_model2 %>% select(-n)
mead_shots <-
  mead_shots %>%
  inner_join(y = xg_model2, by = c("shot.body_part.name", "shot.technique.name"))
rm(tmp_xg2)

mead_shots %>%
  select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y, XG2) %>%
  slice(c(1, 4, 14))
```

## Conditioning on even more features

At least for these three goals, our new XG estimates seem more reasonable: the one-on-one lob against Austria has a much higher XG than the shots through traffic against Norway and Sweden. 
But are we fully satisfied with this model?
One could credibly argue that even though our model returns somewhat more sensible XG estimates, it is still too coarse for to meaningfully compare the shots above.
After all, because it does not condition on distance, our model would return exactly the same XG for right-footed volleys taken one meter and 15 meters away from the goal.
Similarly, we could try to account for the number of defenders between the shot and the goal and the position of the keeper.

If we had access to the infinite super-population of shots, conditioning on more features is conceptually straightforward: we look at the corresponding sub-group of the super-population defined by a particular combination of features and compute the average $Y.$
Unfortunately, with finite data, trying to "bin-and-average" using lots of features can lead to erratic estimates.
For instance, here are the five largest and five smallest XG estimates based on body-part and shot technique.
```{r xg2-counts}
xg_model2 %>% arrange(desc(XG2)) %>% filter(row_number() %in% c(1:5, (n()-4):n()))
```
Because none of the 3 left-footed lobs in our dataset led to goals, our model estimates $\textrm{XG}(\text{left-footed lob})$ as 0.
Similarly, the rather large $\textrm{XG}(\text{right-footed lob})$ estimate of around 33% is based on only 12 shots.
Attempting to condition on even more variables would result in estimates based on even smaller sample sizes!


So, it would appear that we're stuck between a rock and a hard place. 
On the one hand, our XG model with two features is too coarse to quantify important differences between the motivating shots.
But on the other hand, binning and averaging with even more features carries the risk of producing highly erratic, extreme, and somewhat nonsensical estimates[^cromwell] To overcome this challenge, we can build a *statistical model*.



[^cromwell]: 
  Indeed, it seems absurd to claim that, at least in women's international soccer, players will **never** score off left-footed lobs! As the statistician Dennis Lindley put it we must "never believe in anything absolutely" and should "leave a little probability for the moon being made of green cheese; it can be as small as 1 in a million, but have it there since otherwise an army of astronauts returning with samples of the said cheese will leave you unmoved" ( [source](https://archive.org/details/makingdecisions00dvli/page/104/mode/2up)). Lindley termed this principle ["Cromwell's Rule"](https://en.wikipedia.org/wiki/Cromwell%27s_rule), a reference to a [Oliver Cromwell's famous quote](https://www.olivercromwell.org/Letters_and_speeches/letters/Letter_129.pdf) "I beesech you, in the bowels of Christ, think it possible that you may be mistaken."

Later in the course, we will discuss how to estimate more granular XG models with loads more features.
But for today, we'll use XG estimates from StatsBomb LLC (Huddl?). 
Although this is a proprietary model, StatsBomb has published a lot of information about what all goes into it.
In addition to the body part and technique, they also construct several features based on location of the players (in 2 dimensions) and the ball (in 3 dimensions).


<!--
  1. Even this model seems a bit too coarse
  2. Also notice that there are some pretty extreme values; see left foot lob and a right foot lob
  3. Issue is sample size; although we define XG in terms of "binning and averaging" in an infinte super-population, with a finite dataset, this isn't always practical (or advised). 
  4. Statistics has developed a whole range of **regression models** precisely to overcome this issue. They chiefly operate by making different assumptions and attempting to "borrow strength" using data about one (x,y) value to make a prediction about a "nearby" x value. They differ in how they operationalize this strength borrowing
  5. We will consider these methods in more detail later in the course. But for now, we will content ourselves with using a fairly sophistical model developed by StatsBomb

One advantage of conditioning on a small number of features is that we have more data per combination with which to estimate XG.
The downside, of course, is that the models are a bit too coarse and can't account for all the context around the shot.
-->

```{r mead-sbxg}
mead_shots %>%
  filter(Y == 1) %>%
  select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y, shot.statsbomb_xg)
```
Recall that XG quantifies a certain hypothetical long-term frequency of scoring a goal: if the shot was replayed under exactly the conditions quantified by the feature vector $\boldsymbol{\mathbf{x}}$, $\textrm{XG}(\boldsymbol{\mathbf{x}})$ is the proportion of times a goal is scored.
So, we should be fairly impressed when a player scores a goal on a shot with very low XG.
We can quantify the degree to which players under- or over-perform the model expectations by comparing the 
In the case of Beth Mead, we want to sum the difference $Y_{i} = \textrm{XG}_{i}$ where $\textrm{XG}_{i}$ is the StatsBomb XG estimate.
```{r mead-oe}
sum(mead_shots$Y - mead_shots$shot.statsbomb_xg)
```
During Euro 2022, Beth Mead scored a total of 6 goals, which was about 2.9 goals **more** than what the StatsBomb XG model expected, based on the contexts in which she attempted shots.
We can repeat this calculation -- summing over the difference between shot outcome $Y$ and $\textrm{XG}$ --- for all players in EURO 2022 to find the players that most over-performed and most under-performed the model expectations.
```{r euro2022-oe}
goe <- 
  euro2022_shots %>%
  mutate(diff = Y - shot.statsbomb_xg) %>%
  group_by(player.name) %>%
  summarise(GOE = sum(diff),
            n = n()) %>%
  arrange(desc(GOE))

goe
```
It turns out that Alexandra Popp, the German captain, outperformed StatsBomb's XG model expectations, by an even wider margin than Beth Mead.
Like Mead, Popp scored 6 goals during the tournament off a similar number of shots (16 for Popp and 15 for Mead).
Interestingly, Mead won the Golden Boot because she had one more assist... 



