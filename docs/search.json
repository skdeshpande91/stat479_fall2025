[
  {
    "objectID": "slides/raw_slides/slides_05.html#motivation",
    "href": "slides/raw_slides/slides_05.html#motivation",
    "title": "STAT 479: Lecture 5",
    "section": "Motivation",
    "text": "Motivation\n\nHow do NBA players help their teams win?\nHow do we quantify contributions?\n\n\n\nPlus/Minus: Easy to compute\n\nHard to separate skill from opportunities\nDoesn’t adjust for teammate quality\n\n\n\n\n\nAdjusted Plus/Minus:\n\nRegress point differential per 100 possession on signed on-court indicators\nIntroduces fairly arbitrary baseline\nAssumes all baseline players have same underlying skill"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#the-original-model",
    "href": "slides/raw_slides/slides_05.html#the-original-model",
    "title": "STAT 479: Lecture 5",
    "section": "The Original Model",
    "text": "The Original Model\n\n\\(n\\): total number of stints\n\\(p\\): total number of players\n\\(Y_{i}\\): point differential per 100 possessions in stint \\(i\\)\n\n\\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#matrix-notation",
    "href": "slides/raw_slides/slides_05.html#matrix-notation",
    "title": "STAT 479: Lecture 5",
    "section": "Matrix Notation",
    "text": "Matrix Notation\n\nFor each stint \\(i\\) and player \\(j\\), signed indicator \\(x_{ij}\\):\n\n\\(x_{ij} = 1\\) if player \\(j\\) on-court at home in stint \\(i\\)\n\\(x_{ij} = -1\\) if player \\(j\\) on-court and away in stint \\(i\\)\n\\(x_{ij} = 0\\) if player \\(j\\) off-court in stint \\(i\\)\n\n\n\n\n\\(\\boldsymbol{\\mathbf{X}}\\): \\(n \\times p\\) matrix of signed indicators\n\nRows correspond to stints:\nColumns correspond to players\n\n\\(\\boldsymbol{\\mathbf{Z}}\\): \\(n \\times (p+1)\\) matrix\n\nFirst column is all ones; remaining are \\(\\boldsymbol{\\mathbf{X}}\\)\n\\(i\\)-th row is \\(\\boldsymbol{\\mathbf{x}}_{i}\\)\n\n\\(Y_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#problems-w-original-model",
    "href": "slides/raw_slides/slides_05.html#problems-w-original-model",
    "title": "STAT 479: Lecture 5",
    "section": "Problems w/ Original Model",
    "text": "Problems w/ Original Model\n\nIndividual \\(\\alpha_{j}\\)’s are not statistically identifiable\n\nE.g. \\(\\alpha_{j} \\rightarrow \\alpha_{j}+5\\) yields same fit to data\n\n\\(\\boldsymbol{\\mathbf{Z}}\\) is not of full-rank\n\nCan’t invert \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\)\n\nCan’t estimate \\(\\boldsymbol{\\alpha}\\) with least squares!"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#a-re-parametrized-model",
    "href": "slides/raw_slides/slides_05.html#a-re-parametrized-model",
    "title": "STAT 479: Lecture 5",
    "section": "A Re-parametrized Model",
    "text": "A Re-parametrized Model\n\nAssume \\(\\alpha_{j} = \\mu\\) for all baseline players \\(j\\)\nFor all non-baseline players, \\(\\beta_{j} = \\alpha_{j} - \\mu\\)\n\\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\): drop baseline columns from \\(\\boldsymbol{\\mathbf{Z}}\\)\n\\(Y_{i} = \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta} + \\epsilon_{i}\\)\nCan estimate \\(\\boldsymbol{\\beta}\\) with least squares"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#problems-w-re-parametrized-model",
    "href": "slides/raw_slides/slides_05.html#problems-w-re-parametrized-model",
    "title": "STAT 479: Lecture 5",
    "section": "Problems w/ Re-Parametrized Model",
    "text": "Problems w/ Re-Parametrized Model\n\n250 minute cut-off for baseline is very arbitrary\nRestrictive to assume baseline players have same skill\n\n\n\nBaseline assumption needed to use least squares\n… but what if we don’t use least squares"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#ridge-regression",
    "href": "slides/raw_slides/slides_05.html#ridge-regression",
    "title": "STAT 479: Lecture 5",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nOriginal APM problem: minimize \\(\\sum_{i = 1}^{n}{\\left(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha}\\right)^{2}}\\)\n\n\n\nInstead for a fixed \\(\\lambda &gt; 0\\) let’s minimize \\(\\sum_{i = 1}^{n}{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}} + \\lambda \\times \\sum_{j = 0}^{p}{\\alpha_{j}^{2}},\\)\n\n\n\n\nFirst term: minimized when all \\(Y_{i} \\approx \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha}\\)\nSecond term: shrinkage penalty tries to keep all \\(\\alpha_{j}\\)’s near 0\n\\(\\lambda\\): trades-off these two terms"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#analytic-solution",
    "href": "slides/raw_slides/slides_05.html#analytic-solution",
    "title": "STAT 479: Lecture 5",
    "section": "Analytic Solution",
    "text": "Analytic Solution\n\nFor all \\(\\lambda\\), minimizer is $ () = (^{} + I ){-1}{}.$\nThis is almost the OLS solution\n\nSlightly perturb \\(\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) so it becomes invertible\nE.g., by adding \\(\\lambda\\) to its diagonal"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#cross-validation-for-lambda",
    "href": "slides/raw_slides/slides_05.html#cross-validation-for-lambda",
    "title": "STAT 479: Lecture 5",
    "section": "Cross-Validation for \\(\\lambda\\)",
    "text": "Cross-Validation for \\(\\lambda\\)\n\nIdea: select \\(\\lambda\\) yielding smallest out-of-sample prediction error\nProblem: don’t have a separate validation dataset to compute out-of-sample error\n\n\n\nSolution: cross-validation to estimate out-of-sample error\n\nCreate a grid of \\(\\lambda\\) values & several train/test splits\n\nFor each \\(\\lambda\\) and train/test split:\n\nCompute \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda)\\) w/ training data\nEvaluate prediction error using testing data\n\n\nFor each \\(\\lambda,\\) average testing prediction error across splits\nIdentify \\(\\hat{\\lambda}\\) w/ smallest average testing error\n\n\n\n\n\nCompute \\(\\hat{\\boldsymbol{\\alpha}}(\\hat{\\lambda})\\) w/ all data"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#ridge-regression-in-practice",
    "href": "slides/raw_slides/slides_05.html#ridge-regression-in-practice",
    "title": "STAT 479: Lecture 5",
    "section": "Ridge Regression in Practice",
    "text": "Ridge Regression in Practice\n\nImplemented in the glmnet package\ncv.glmnet(): performs cross-validation\n\nAutomatically creates grid of \\(\\lambda\\)\nDefault: 10 train/test splits\n\nImportant to set standardize = FALSE\n\n\nset.seed(479) \nlibrary(glmnet) \ncv_fit &lt;-cv.glmnet(x = X_full, y = Y, \n            alpha = 0,\n            standardize = FALSE)"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#finding-hatlambda",
    "href": "slides/raw_slides/slides_05.html#finding-hatlambda",
    "title": "STAT 479: Lecture 5",
    "section": "Finding \\(\\hat{\\lambda}\\)",
    "text": "Finding \\(\\hat{\\lambda}\\)\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nlambda_min &lt;- cv_fit$lambda.min"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#regularized-adjusted-plusminus",
    "href": "slides/raw_slides/slides_05.html#regularized-adjusted-plusminus",
    "title": "STAT 479: Lecture 5",
    "section": "Regularized Adjusted Plus/Minus",
    "text": "Regularized Adjusted Plus/Minus\n\nExtract \\(\\boldsymbol{\\alpha}(\\hat{\\lambda})\\)Top-10 RAPMDončić vs Davis\n\n\n\nlambda_index &lt;- which(cv_fit$lambda == lambda_min) \nfit &lt;-glmnet(x = X_full, y = Y, alpha = 0,\n         lambda = cv_fit$lambda, \n         standardize = FALSE)\n\nalpha_hat &lt;- fit$beta[,lambda_index] \n\n\n\n\nrapm &lt;- data.frame(id = names(alpha_hat), rapm = alpha_hat) |&gt;\n  dplyr::inner_join(y = player_table |&gt; dplyr::select(id,Name), by = \"id\")\n\n\n\n        id     rapm                    Name\n1  1628983 3.602143 Shai Gilgeous-Alexander\n2  1627827 3.472249     Dorian Finney-Smith\n3  1630596 3.415232             Evan Mobley\n4  1629029 3.352736             Luka Doncic\n5   202699 3.183716           Tobias Harris\n6   203999 2.846327            Nikola Jokic\n7  1631128 2.829463         Christian Braun\n8  1628384 2.813794              OG Anunoby\n9   203507 2.646488   Giannis Antetokounmpo\n10 1626157 2.641075      Karl-Anthony Towns\n\n\n\n\n\nExpect to score 3.4 fewer points per 100 possessions with Davis instead of Dončić\n\n\nluka_rapm &lt;- rapm |&gt; dplyr::filter(Name == \"Luka Doncic\") |&gt; dplyr::pull(rapm)\nad_rapm &lt;- rapm |&gt; dplyr::filter(Name == \"Anthony Davis\") |&gt; dplyr::pull(rapm)\nad_rapm - luka_rapm\n\n[1] -3.417006"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#other-penalties",
    "href": "slides/raw_slides/slides_05.html#other-penalties",
    "title": "STAT 479: Lecture 5",
    "section": "Other Penalties",
    "text": "Other Penalties\n\nFor some \\(a \\in [0,1]\\) glmnet() and cv.glmnet() actually minimize \\[\n\\sum_{i = 1}^{n}{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}} + \\lambda \\times  \\sum_{j = 0}^{p}{\\left[a \\times \\lvert\\alpha_{j}\\rvert + (1-a) \\times \\alpha_{j}^{2}\\right]},\n\\]\nValue of \\(a\\) specified with alpha argument.\nalpha = 0: penalty is \\(\\sum_{j}{\\alpha_{j}^{2}}\\)\n\nPenalty encourage small (but non-zero) \\(\\alpha_{j}\\) values\nRidge regression; \\(\\ell_{2}\\) or Tikohonov regularization\n\nalpha = 1: penalty is \\(\\sum_{j}{\\lvert \\alpha_{j} \\rvert}\\)\n\nPenalty encourages sparsity (i.e., sets many \\(\\alpha_{j} = 0\\))\nLeast Absolute Shrinkage and Selection Operator (LASSO); \\(\\ell_{1}\\) regularization\n\n\\(0 &lt;\\)alpha\\(&lt;1\\): Elastic Net regression"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#the-bootstrap-high-level-idea",
    "href": "slides/raw_slides/slides_05.html#the-bootstrap-high-level-idea",
    "title": "STAT 479: Lecture 5",
    "section": "The Bootstrap (High-Level Idea)",
    "text": "The Bootstrap (High-Level Idea)\n\nSay we compute some statistic \\(T(\\boldsymbol{y})\\) using observed data \\(\\boldsymbol{y}\\)\n\nE.g., RAPM estimate \\(\\hat{\\alpha}_{j}\\) for a single player \\(j\\)\nEstimated difference \\(\\hat{\\alpha}_{j} - \\hat{\\alpha}_{j'}\\) b/w players \\(j\\) and \\(j'\\)\nSomething more exotic: e.g. \\(\\max_{j}\\hat{\\alpha}_{j} - \\min_{j}\\hat{\\alpha_{j}}\\)\n\nThese are estimates and there is always estimation uncertainty\n\n\n\nRepeatedly re-sample data and re-compute statistic\n\nRe-sampled datasets: \\(\\boldsymbol{y}^{(1)}, \\ldots, \\boldsymbol{y}^{(B)}\\)\nCorresponding statistics: \\(T(\\boldsymbol{y}^{(1)}), \\ldots, T(\\boldsymbol{y}^{(B)})\\)\n\nBoostrapped statistics gives a sense of estimate’s variability"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#bootstrapping-rapm-plan",
    "href": "slides/raw_slides/slides_05.html#bootstrapping-rapm-plan",
    "title": "STAT 479: Lecture 5",
    "section": "Bootstrapping RAPM (plan)",
    "text": "Bootstrapping RAPM (plan)\n\nCompute \\(\\hat{\\boldsymbol{\\alpha}}(\\hat{\\lambda})\\) using original dataset\nDraw \\(B\\) re-samples of size \\(n\\)\n\nSample observed stints with replacement\n\nFor each re-sampled dataset, compute \\(\\hat{\\boldsymbol{\\alpha}}(\\hat{\\lambda})\\)\n\nNo need to re-run cross-validation to compute optimal \\(\\lambda\\)\nUse the optimal \\(\\lambda\\) from original dataset\n\nSave the \\(B\\) bootstrap estimates of \\(\\boldsymbol{\\alpha}\\) in an array"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#a-single-iteration",
    "href": "slides/raw_slides/slides_05.html#a-single-iteration",
    "title": "STAT 479: Lecture 5",
    "section": "A Single Iteration",
    "text": "A Single Iteration\n\nRe-sampling DataCompute \\(\\boldsymbol{\\alpha}\\)Bootstrapped Estimate\n\n\n\nset.seed(479)\nn &lt;- nrow(X_full)\np &lt;- ncol(X_full)\nboot_index &lt;- sample(1:n, size = n, replace = TRUE)\n\n\n\n [1]  1  2  3  4  5  6  6  9 10 11 13 14 14 15 16 17 18 18 19 20\n\n\n\n\n\nfit &lt;- glmnet(x = X_full[boot_index,], y = Y[boot_index],\n              lambda = cv_fit$lambda,\n              alpha = 0, standardize = FALSE)\n\n\n\n\n\n                      Name     Orig Bootstrap\n1  Shai Gilgeous-Alexander 3.602143 1.9823702\n2      Dorian Finney-Smith 3.472249 1.8454962\n3              Evan Mobley 3.415232 3.9265593\n4              Luka Doncic 3.352736 2.6188868\n5            Tobias Harris 3.183716 2.3872891\n6             Nikola Jokic 2.846327 3.1117201\n7          Christian Braun 2.829463 2.6171955\n8               OG Anunoby 2.813794 1.9276271\n9    Giannis Antetokounmpo 2.646488 0.8409495\n10      Karl-Anthony Towns 2.641075 4.2211787"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#full-bootstrap",
    "href": "slides/raw_slides/slides_05.html#full-bootstrap",
    "title": "STAT 479: Lecture 5",
    "section": "Full Bootstrap",
    "text": "Full Bootstrap\n\nB &lt;- 500 \nplayer_names &lt;- colnames(X_full) \nboot_rapm &lt;- matrix(nrow = B, ncol = p, dimnames = list(c(), player_names)) \n\nfor(b in 1:B){\n  set.seed(479+b)\n  boot_index &lt;- sample(1:n, size = n, replace = TRUE) \n  \n  fit &lt;- glmnet(x = X_full[boot_index,], y = Y[boot_index], \n                lambda = cv_fit$lambda,\n                alpha = 0,standardize = FALSE)\n  tmp_alpha &lt;- fit$beta[,lambda_index]\n  boot_rapm[b, names(tmp_alpha)] &lt;- tmp_alpha \n}"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#dončić-davis-contrast",
    "href": "slides/raw_slides/slides_05.html#dončić-davis-contrast",
    "title": "STAT 479: Lecture 5",
    "section": "Dončić-Davis Contrast",
    "text": "Dončić-Davis Contrast\n\nBootstrap Dist.Uncertainty Interval\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bootstrap distribution of difference between Doncic & Davis\n\n\n\n\n\n\n\n\ndoncic_id &lt;- player_table |&gt; dplyr::filter(Name == \"Luka Doncic\") |&gt; dplyr::pull(id) \ndavis_id &lt;- player_table |&gt; dplyr::filter(Name == \"Anthony Davis\") |&gt; dplyr::pull(id) \nboot_diff &lt;- boot_rapm[,davis_id] - boot_rapm[,doncic_id]\n\nci &lt;- quantile(boot_diff, probs = c(0.025, 0.975))\nround(ci, digits = 3)\n\n  2.5%  97.5% \n-6.378 -0.484"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#visualizing-rapm-uncertainty",
    "href": "slides/raw_slides/slides_05.html#visualizing-rapm-uncertainty",
    "title": "STAT 479: Lecture 5",
    "section": "Visualizing RAPM Uncertainty",
    "text": "Visualizing RAPM Uncertainty\n\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#uncertainty-in-ranking",
    "href": "slides/raw_slides/slides_05.html#uncertainty-in-ranking",
    "title": "STAT 479: Lecture 5",
    "section": "Uncertainty in Ranking",
    "text": "Uncertainty in Ranking\n\nrank() returns sample ranks of vector elements\n\n\nx &lt;- c(100, -1, 3, 2.5, -2)\nrank(x)\n\n[1] 5 2 4 3 1\n\n\n\n\nrank(-1*x)\n\n[1] 1 4 2 3 5"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#sgas-rapm-ranking-uncertainty",
    "href": "slides/raw_slides/slides_05.html#sgas-rapm-ranking-uncertainty",
    "title": "STAT 479: Lecture 5",
    "section": "SGA’s RAPM Ranking Uncertainty",
    "text": "SGA’s RAPM Ranking Uncertainty\n\nPoint EstimateBootstrapped RanksSGA’s Rank\n\n\n\nrapm &lt;- rapm |&gt; dplyr::mutate(rank_rapm = rank(-1 * rapm))\nrapm |&gt; dplyr::filter(Name == \"Shai Gilgeous-Alexander\")\n\n       id     rapm                    Name rank_rapm\n1 1628983 3.602143 Shai Gilgeous-Alexander         1\n\n\n\n\n\nMARGIN = 1 applies function to each row\n\n\nboot_rank &lt;- apply(-1*boot_rapm, MARGIN = 1, FUN = rank)\nsga_id &lt;- \n  player_table |&gt; dplyr::filter(Name == \"Shai Gilgeous-Alexander\") |&gt; dplyr::pull(id)\nsga_ranks &lt;- boot_rank[sga_id,]\ntable(sga_ranks)[1:10]\n\nsga_ranks\n 1  2  3  4  5  6  7  8  9 10 \n58 59 46 55 36 21 19 12 17 17 \n\n\n\nSGA had\n\nHighest RAPM in 58/500 bootstrap re-samples\n2nd highest RAPM in 59/500 re-samples\n\n\n\n\n\nConsiderable uncertainty in ranks!\n\n\nquantile(sga_ranks, probs = c(0.025, 0.975))\n\n  2.5%  97.5% \n 1.000 51.525"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#recap",
    "href": "slides/raw_slides/slides_03.html#recap",
    "title": "STAT 479: Lecture 3",
    "section": "Recap",
    "text": "Recap\n\nIn Lecture 2, fit two XG models\n\nModel 1 accounted for body part\nModel 2 accounted for body part + technique\n\nWhich model is better?\n\nWhich fits the observed data best?\nWhich will predict new data best?"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#our-simple-xg-models",
    "href": "slides/raw_slides/slides_03.html#our-simple-xg-models",
    "title": "STAT 479: Lecture 3",
    "section": "Our Simple XG Models",
    "text": "Our Simple XG Models\n\nModel 1Model 2Concatenating Predictions\n\n\n\nxg_model1 &lt;- \n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\n\n\n\n\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\") \n\n\n\n\nsimple_preds &lt;-\n  wi_shots |&gt;\n  dplyr::select(Y, shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::left_join(y = xg_model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::left_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#qualitative-comparisons",
    "href": "slides/raw_slides/slides_03.html#qualitative-comparisons",
    "title": "STAT 479: Lecture 3",
    "section": "Qualitative Comparisons",
    "text": "Qualitative Comparisons\n\nConsider two shots:\n\nRight-footed half-volley by Beth Mead against Sweden\nRight-footed backheel by Alessia Russo\n\n\n\n\n# A tibble: 2 × 4\n  shot.body_part.name shot.technique.name   XG1   XG2\n  &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Right Foot          Half Volley         0.111 0.089\n2 Right Foot          Backheel            0.111 0.103\n\n\n\n\nModel 2 accounts for more factors\nIntuitively expect it is more accurate"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#setup-notation",
    "href": "slides/raw_slides/slides_03.html#setup-notation",
    "title": "STAT 479: Lecture 3",
    "section": "Setup & Notation",
    "text": "Setup & Notation\n\nData: \\(n\\) shots represented by pairs \\((\\boldsymbol{\\mathbf{x}}_{1}, y_{1}), \\ldots, (\\boldsymbol{\\mathbf{x}}_{n}, y_{n})\\)\n\nOutcomes: \\(y_{i} = 1\\) if shot \\(i\\) results in a goal & 0 otherwise\nFeature vector: \\(\\boldsymbol{\\mathbf{x}}_{i}\\)\n\nAssumption: Data is a representative sample from an infinite super-population of shots \\[\n\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}]\n\\]\n\n\n\n\\(\\hat{p}_{i}\\): predicted \\(\\textrm{XG}\\) for shot \\(i\\) from fitted model\nHow close is \\(\\hat{p}_{i}\\) to \\(y_{i}\\)?"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#misclassification-rate-definition",
    "href": "slides/raw_slides/slides_03.html#misclassification-rate-definition",
    "title": "STAT 479: Lecture 3",
    "section": "Misclassification Rate (Definition)",
    "text": "Misclassification Rate (Definition)\n\n\\(\\hat{p}_{i} &gt; 0.5:\\) model predicts \\(y_{i} = 1\\) more likely than \\(y_{i} = 0\\)\nIdeal: \\(\\hat{p}_{i} &gt; 0.5\\) when \\(y_{i} = 1\\) and \\(\\hat{p}_{i} &lt; 0.5\\) when \\(y_{i} = 0\\)\nIf too many \\(\\hat{p}_{i}\\)’s on wrong-side of 50%, model is badly calibrated.\n\n\n\\[\n\\textrm{MISS} = n^{-1}\\sum_{i = 1}^{n}{\\mathbb{I}(y_{i} \\neq \\mathbb{I}(\\hat{p}_{i} \\geq 0.5))},\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#misclassification-rate-example",
    "href": "slides/raw_slides/slides_03.html#misclassification-rate-example",
    "title": "STAT 479: Lecture 3",
    "section": "Misclassification Rate (Example)",
    "text": "Misclassification Rate (Example)\n\nmisclass &lt;- function(y, phat){ \n  return( mean( (y != 1*(phat &gt;= 0.5))))\n}\nmisclass(simple_preds$Y, simple_preds$XG1)\nmisclass(simple_preds$Y, simple_preds$XG2)\n\n\n\n\nModel 1 misclassification 0.112 \nModel 2 misclassificaiton 0.112 \n\n\n\n\n\nWhy do Models 1 & 2 have the same misclassification rate???"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#brier-score-definition",
    "href": "slides/raw_slides/slides_03.html#brier-score-definition",
    "title": "STAT 479: Lecture 3",
    "section": "Brier Score (Definition)",
    "text": "Brier Score (Definition)\n\nMISS only cares whether \\(\\hat{p}_{i}\\) is on the wrong-side of 50%\n\nForecasting \\(\\hat{p} = 0.501\\) and \\(\\hat{p} = 0.999\\) have same loss when \\(y = 0\\)\nDoesn’t penalize how far \\(\\hat{p}\\) is from \\(Y\\)\n\n\n\n\nBrier Score penalizes distance b/w forecast \\(\\hat{p}\\) & \\(Y\\) \\[\n\\text{Brier} = n^{-1}\\sum_{i = 1}^{n}{(y_{i} - \\hat{p}_{i})^2}.\n\\]\nJust Mean Square Error applied to binary outcomes"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#brier-score-example",
    "href": "slides/raw_slides/slides_03.html#brier-score-example",
    "title": "STAT 479: Lecture 3",
    "section": "Brier Score (Example)",
    "text": "Brier Score (Example)\n\nbrier &lt;- function(y, phat){\n  return(mean( (y - phat)^2 ))\n}\n\nbrier(simple_preds$Y, simple_preds$XG1)\nbrier(simple_preds$Y, simple_preds$XG2)\n\n\n\nModel 1 Brier Score: 0.1 \n\n\nModel 2 Brier Score: 0.099"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#log-loss-definition",
    "href": "slides/raw_slides/slides_03.html#log-loss-definition",
    "title": "STAT 479: Lecture 3",
    "section": "Log-Loss (Definition)",
    "text": "Log-Loss (Definition)\n\nLike Brier Score but penalizes extreme mistakes more severely\n\n\\[\n\\textrm{LogLoss} = -1 \\times \\sum_{i = 1}^{n}{\\left[ y_{i} \\times \\log(\\hat{p}_{i}) + (1 - y_{i})\\times\\log(1-\\hat{p}_{i})\\right]}.\n\\]\n\n\nTheoretically, can be infinite (when \\(\\hat{p} = 1-y\\))\nIn practice, truncate \\(\\hat{p}\\) to \\([\\epsilon,1-\\epsilon]\\) to avoid \\(\\log(0)\\)\nAlso known as cross-entropy loss in ML literature"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#log-loss-score-example",
    "href": "slides/raw_slides/slides_03.html#log-loss-score-example",
    "title": "STAT 479: Lecture 3",
    "section": "Log-Loss Score (Example)",
    "text": "Log-Loss Score (Example)\n\nlogloss &lt;- function(y, phat){\n  if(any(phat &lt; 1e-12)) phat[phat &lt; 1e-12] &lt;- 1e-12\n  if(any(phat &gt; 1-1e-12)) phat[phat &gt; 1-1e-12] &lt;- 1-1e-12\n  return(-1 * mean( y * log(phat) + (1-y) * log(1-phat)))\n}\n\nlogloss(simple_preds$Y, simple_preds$XG1)\nlogloss(simple_preds$Y, simple_preds$XG2)\n\n\n\n\nModel 1 Log-Loss: 0.351 \nModel 2 Log-Loss: 0.348"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#in-sample-error",
    "href": "slides/raw_slides/slides_03.html#in-sample-error",
    "title": "STAT 479: Lecture 3",
    "section": "In-sample Error",
    "text": "In-sample Error\n\nRecall our setup:\n\nData is a sample from super-population \\(\\mathcal{P}\\)\nFit model to estimate \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}]\\)\nFitted model returns predictions \\(\\hat{p}(\\boldsymbol{\\mathbf{x}})\\)\n\nComputed \\(\\textrm{MISS}, \\textrm{Brier},\\) and \\(\\textrm{LogLoss}\\) w/ same data used to fit model\nThis only checks whether model fits observed data well?\n\n\n\nHow well does model predict new, previously unseen data?"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#out-of-sample-error",
    "href": "slides/raw_slides/slides_03.html#out-of-sample-error",
    "title": "STAT 479: Lecture 3",
    "section": "Out-of-Sample Error",
    "text": "Out-of-Sample Error\n\nSay we had a second dataset \\((\\boldsymbol{\\mathbf{x}}_{1}^{\\star}, y_{1}^{\\star}), \\ldots, (\\boldsymbol{\\mathbf{x}}_{m}^{\\star}, y_{M}^{\\star})\\) from \\(\\mathcal{P}\\)\nCompute predictions \\(\\hat{p}^{\\star}_{m}:= \\hat{p}(\\boldsymbol{\\mathbf{x}}_{m}^{\\star})\\)\n\nImportant: \\((\\boldsymbol{\\mathbf{x}}_{m}^{\\star}, y_{m}^{\\star})\\)’s not used to fit the model\n\n\n\n\nAssess how close \\(\\hat{p}^{\\star}_{m}\\)’s are to \\(y^{\\star}_{m}\\)’s\n\nCould use misclassification rate, Brier score, or log-loss\nResult is the out-of-sample loss"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#cross-validation",
    "href": "slides/raw_slides/slides_03.html#cross-validation",
    "title": "STAT 479: Lecture 3",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nProblem: we don’t have access to a second dataset\n\n\n\nSolution: create random training/testing split\n\nTrain on a random subset containing 75% of the data\nEvaluate using the remaining held-out portion"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#the-trainingtesting-paradigm",
    "href": "slides/raw_slides/slides_03.html#the-trainingtesting-paradigm",
    "title": "STAT 479: Lecture 3",
    "section": "The Training/Testing Paradigm",
    "text": "The Training/Testing Paradigm\n\nCreate SplitFit ModelsEvaluate Models\n\n\n\nn &lt;- nrow(wi_shots)\nn_train &lt;- floor(0.75 * n)\nn_test &lt;- n - n_train\n\nwi_shots &lt;- wi_shots |&gt; dplyr::mutate(id = 1:n)\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train) \ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\")\n\n\n\n\nmodel1 &lt;- \n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nmodel2 &lt;-\n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\ntrain_preds &lt;-\n  train_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\n\n\n\ntest_preds &lt;-\n  test_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\nlogloss(train_preds$Y, train_preds$XG1)\nlogloss(test_preds$Y, test_preds$XG1)\n\nlogloss(train_preds$Y, train_preds$XG2)\nlogloss(test_preds$Y, test_preds$XG2)\n\n. . .\n\n\nBodyPart train log-loss: 0.357 test log-loss: 0.334 \n\n\nBodyPart+Technique train log-loss: 0.355 test log-loss: 0.351"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#multiple-splits",
    "href": "slides/raw_slides/slides_03.html#multiple-splits",
    "title": "STAT 479: Lecture 3",
    "section": "Multiple Splits",
    "text": "Multiple Splits\n\nRecommend averaging over many train/test splits (e.g., 100)\n\n\n\nModel 1 training logloss: 0.351 \nModel 2 training logloss: 0.348 \nModel 1 test logloss: 0.352 \nModel 2 test logloss: 0.356 \n\n\n\n\nSimpler model appaers to have slightly smaller out-of-sample log-loss!"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#motivation-accounting-for-distance",
    "href": "slides/raw_slides/slides_03.html#motivation-accounting-for-distance",
    "title": "STAT 479: Lecture 3",
    "section": "Motivation: Accounting for Distance",
    "text": "Motivation: Accounting for Distance\n\nModel 1 gives same prediction for\n\nA header from 1m away\nA header from 15m away\n\nHow to account for continuous feature like DistToGoal?\n\n\n\nIdea: Divide into discrete bins and then average within bins\n\nProblem: sensitivity to bin sizes (1m , 3m, 10m)\nProblem: potential small sample issues"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#logistic-regression-1",
    "href": "slides/raw_slides/slides_03.html#logistic-regression-1",
    "title": "STAT 479: Lecture 3",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nBinary outcome \\(Y\\) and numerical predictors \\(X_{1}, \\ldots, X_{p}\\): \\[\n\\log\\left(\\frac{\\mathbb{P}(Y= 1 \\vert \\boldsymbol{\\mathbf{X}})}{\\mathbb{P}(Y = 0 \\vert \\boldsymbol{\\mathbf{X}})}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p}.\n\\]\nKeeping all other predictors constant, a one unit change in \\(X_{j}\\) associated with a \\(\\beta_{j}\\) change in the log-odds\nSay \\(\\beta_{j} = 1\\). Increasing \\(X_{j}\\) by 1 unit moves \\(\\mathbb{P}(Y = 1)\\)\n\nFrom 4.7% to 11.2% (log-odds from -3 to -2)\nFrom 37.8% to 62.2% (log-odds from -0.5 to 0.5)\nFrom 73.1% to 88.1% (log-odds from 1 to 2)"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#logistic-inverse-logistic-functions",
    "href": "slides/raw_slides/slides_03.html#logistic-inverse-logistic-functions",
    "title": "STAT 479: Lecture 3",
    "section": "Logistic & Inverse Logistic Functions",
    "text": "Logistic & Inverse Logistic Functions\n\nLogistic function: \\(f(t) = [1 + e^{-t}]^{-1}\\)\nInverse logistic: \\(f^{-1}(p) = \\log{p/(1-p)}\\)\n\n\n\n\n\n\n\n\n\nFigure 1: Logistic and inverse logistic functions"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#an-initial-model",
    "href": "slides/raw_slides/slides_03.html#an-initial-model",
    "title": "STAT 479: Lecture 3",
    "section": "An Initial Model",
    "text": "An Initial Model\n\n\\(X_{1}\\): distance from shot to goal (DistToGoal)\n\\(\\log\\left(\\frac{\\mathbb{P}(Y = 1)}{\\mathbb{P}(Y = 0)} \\right) = \\beta_{0} + \\beta_{1} X_{1}\\)\n\n\nModel FittingModel Summary\n\n\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit1 &lt;- glm(formula = Y~DistToGoal, data = train_data,  \n            family = binomial(\"logit\"))\n\n\n\n\nsummary(fit1)\n\n\nCall:\nglm(formula = Y ~ DistToGoal, family = binomial(\"logit\"), data = train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.134174   0.128410  -1.045    0.296    \nDistToGoal  -0.127023   0.009115 -13.935   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2549.7  on 3569  degrees of freedom\nResidual deviance: 2290.7  on 3568  degrees of freedom\nAIC: 2294.7\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#assessing-model-performance",
    "href": "slides/raw_slides/slides_03.html#assessing-model-performance",
    "title": "STAT 479: Lecture 3",
    "section": "Assessing Model Performance",
    "text": "Assessing Model Performance\n\nUse predict() to make test set predictions\n\n\ntrain_pred1 &lt;- \n  predict(object = fit1,\n          newdata = train_data,\n          type = \"response\") \ntest_pred1 &lt;-\n  predict(object = fit1,\n          newdata = test_data,\n          type = \"response\")\n\nlogloss(train_data$Y, train_pred1)\nlogloss(test_data$Y, test_pred1)\n\n\n\n\nDist training logloss: 0.321 \n\n\nDist testing logloss: 0.305 \n\n\n\n\n\nAveraging across 100 train/test splits: distance-based model better than body-part-based model\n\n\n\nDist*BodyPart training logloss: 0.3167 \n\n\nDist*BodyPart testing logloss: 0.3173"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#including-multiple-predictors",
    "href": "slides/raw_slides/slides_03.html#including-multiple-predictors",
    "title": "STAT 479: Lecture 3",
    "section": "Including Multiple Predictors",
    "text": "Including Multiple Predictors\n\nDistance-based model is more accurate than body-part based model\nWhat if we account for body part and distance?\n\n\n\nBody part is a categorical predictor\n\n\n\n\n      Head  Left Foot Right Foot \n       920       1280       2560 \n\n\n\nConvert to factor type and one-hot encode\n\n\nwi_shots &lt;- wi_shots |&gt;\n  dplyr::mutate(shot.body_part.name = factor(shot.body_part.name))"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#model-specification",
    "href": "slides/raw_slides/slides_03.html#model-specification",
    "title": "STAT 479: Lecture 3",
    "section": "Model Specification",
    "text": "Model Specification\n\\[\n\\beta_{0} + \\beta_{1}\\times \\textrm{DistToGoal} + \\\\ \\beta_{\\textrm{LeftFoot}}\\times \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} \\times \\mathbb{I}(\\textrm{RightFoot})\n\\]\n\nDifferent predictions based on the body part used to attempt the shot\nFor a shot taken at distance \\(d\\):\n\nHeader: log-odds $ = {0} + {1}d$\nLeft-footed shot: $ = {0} + {1}d + _{}$\nRight-footed shot: $ = {0} + {1}d + _{}$"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#fitted-model",
    "href": "slides/raw_slides/slides_03.html#fitted-model",
    "title": "STAT 479: Lecture 3",
    "section": "Fitted Model",
    "text": "Fitted Model\n\nfit &lt;- glm(formula = Y~DistToGoal + shot.body_part.name, \n           data = train_data, family = binomial(\"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal + shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -0.49847    0.15504  -3.215   0.0013 ** \nDistToGoal                    -0.17501    0.01081 -16.187  &lt; 2e-16 ***\nshot.body_part.nameLeft Foot   1.28091    0.17286   7.410 1.26e-13 ***\nshot.body_part.nameRight Foot  1.30351    0.15711   8.297  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2533.3  on 3569  degrees of freedom\nResidual deviance: 2167.4  on 3566  degrees of freedom\nAIC: 2175.4\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#interactions",
    "href": "slides/raw_slides/slides_03.html#interactions",
    "title": "STAT 479: Lecture 3",
    "section": "Interactions",
    "text": "Interactions\n\nModel assumes effect of distance is the same regardless of body part\nInteractions allow the effect of one factor to vary based on the value of another. \\[\n\\begin{align}\n&\\beta_{0} + \\beta_{\\textrm{LeftFoot}} \\times \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} * \\mathbb{I}(\\textrm{RightFoot}) +  \\\\\n&+[\\beta_{\\textrm{Dist}} + \\beta_{\\textrm{Dist:LeftFoot}}*\\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{Dist:RightFoot}}\\mathbb{I}(\\textrm{RightFoot})] \\times \\textrm{Dist}   \n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#fitting-interactive-model",
    "href": "slides/raw_slides/slides_03.html#fitting-interactive-model",
    "title": "STAT 479: Lecture 3",
    "section": "Fitting Interactive Model",
    "text": "Fitting Interactive Model\n\nfit &lt;- \n  glm(formula = Y~DistToGoal * shot.body_part.name, \n      data = train_data, family = binomial(\"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal * shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                               0.87652    0.43891   1.997 0.045822\nDistToGoal                               -0.34477    0.05423  -6.358 2.05e-10\nshot.body_part.nameLeft Foot              0.04336    0.52416   0.083 0.934070\nshot.body_part.nameRight Foot            -0.30481    0.48128  -0.633 0.526517\nDistToGoal:shot.body_part.nameLeft Foot   0.15967    0.05769   2.768 0.005645\nDistToGoal:shot.body_part.nameRight Foot  0.18662    0.05576   3.347 0.000817\n                                            \n(Intercept)                              *  \nDistToGoal                               ***\nshot.body_part.nameLeft Foot                \nshot.body_part.nameRight Foot               \nDistToGoal:shot.body_part.nameLeft Foot  ** \nDistToGoal:shot.body_part.nameRight Foot ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2533.3  on 3569  degrees of freedom\nResidual deviance: 2154.5  on 3564  degrees of freedom\nAIC: 2166.5\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#cross-validation-1",
    "href": "slides/raw_slides/slides_03.html#cross-validation-1",
    "title": "STAT 479: Lecture 3",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nFit & assess models w/ 100 train/test splits\nModel w/ interactions is slightly better than others\n\n\nSimple ModelsDistance OnlyDistance + BodyPartDistance*Body Part\n\n\n\n\nBodyPart training logloss: 0.351 \nBodyPart+Technique training logloss: 0.348 \nBodyPart test logloss: 0.352 \nBodyPart+Technique test logloss: 0.356 \n\n\n\n\n\n\nDist*BodyPart training logloss: 0.3167 \n\n\nDist*BodyPart testing logloss: 0.3173 \n\n\n\n\n\n\nDist*BodyPart training logloss: 0.3069 \n\n\nDist*BodyPart testing logloss: 0.308 \n\n\n\n\n\n\nDist*BodyPart training logloss: 0.3047 \n\n\nDist*BodyPart testing logloss: 0.3066"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#including-even-more-features",
    "href": "slides/raw_slides/slides_03.html#including-even-more-features",
    "title": "STAT 479: Lecture 3",
    "section": "Including Even More Features",
    "text": "Including Even More Features\n\nStatsBomb records many potentially important features\n\n\n\n [1] \"shot.type.name\"      \"shot.technique.name\" \"shot.body_part.name\"\n [4] \"DistToGoal\"          \"DistToKeeper\"        \"AngleToGoal\"        \n [7] \"AngleToKeeper\"       \"AngleDeviation\"      \"avevelocity\"        \n[10] \"density\"             \"density.incone\"      \"distance.ToD1\"      \n[13] \"distance.ToD2\"       \"AttackersBehindBall\" \"DefendersBehindBall\"\n[16] \"DefendersInCone\"     \"InCone.GK\"           \"DefArea\"            \n\n\n\n\nHow much more predictive accuracy can we gain by accounting for these?\nChallenge: hard to specify nonlinearities & interactions correctly in glm()"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#regression-trees-i",
    "href": "slides/raw_slides/slides_03.html#regression-trees-i",
    "title": "STAT 479: Lecture 3",
    "section": "Regression Trees I",
    "text": "Regression Trees I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression trees can elegantly model interactions and non-linearities\nRegression trees are just step-functions"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#regression-trees-ii",
    "href": "slides/raw_slides/slides_03.html#regression-trees-ii",
    "title": "STAT 479: Lecture 3",
    "section": "Regression Trees II",
    "text": "Regression Trees II\n\n\nRegression trees can approximate functions arbitrarily well\nBut often need very deep and complicated trees"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#tree-ensembles",
    "href": "slides/raw_slides/slides_03.html#tree-ensembles",
    "title": "STAT 479: Lecture 3",
    "section": "Tree Ensembles",
    "text": "Tree Ensembles\n\n\nComplicated trees can be written as sums of shallow trees!"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#random-forests-1",
    "href": "slides/raw_slides/slides_03.html#random-forests-1",
    "title": "STAT 479: Lecture 3",
    "section": "Random Forests",
    "text": "Random Forests\n\nApproximate \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}]\\) w/ tree ensemble\nNo need to pre-specify functional form or interactions\nScales nicely even if \\(\\boldsymbol{\\mathbf{X}}\\) is high-dimensional\nWe will use implementation in ranger package"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#fitting-a-random-forests-model",
    "href": "slides/raw_slides/slides_03.html#fitting-a-random-forests-model",
    "title": "STAT 479: Lecture 3",
    "section": "Fitting a Random Forests Model",
    "text": "Fitting a Random Forests Model\n\nData PreparationModel FittingAssessing Predictions\n\n\n\nshot_vars &lt;-\n  c(\"Y\",\n    \"shot.type.name\", \n    \"shot.technique.name\", \"shot.body_part.name\",\n    \"DistToGoal\", \"DistToKeeper\", # dist. to keeper is distance from GK to goal\n    \"AngleToGoal\", \"AngleToKeeper\",\n    \"AngleDeviation\", \n    \"avevelocity\",\"density\", \"density.incone\",\n    \"distance.ToD1\", \"distance.ToD2\",\n    \"AttackersBehindBall\", \"DefendersBehindBall\",\n    \"DefendersInCone\", \"InCone.GK\", \"DefArea\")\n\nwi_shots &lt;-\n  wi_shots |&gt;\n  dplyr::mutate(\n    shot.type.name = factor(shot.type.name),\n    shot.body_part.name = shot.body_part.name,\n    shot.technique.name = shot.technique.name)\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train) |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\", shot_vars)))\n\ny_train &lt;- train_data$Y\ny_test &lt;- test_data$Y\n\ntrain_data &lt;-\n  train_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\ntest_data &lt;-\n  test_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\n\n\n\n\nlibrary(ranger)\nfit &lt;- ranger(formula = Y~., \n  data = train_data, probability = TRUE)\n\n\n\n\ntrain_preds &lt;- \n  predict(object = fit,\n          data = train_data)$predictions[,2] \n\ntest_preds &lt;- \n  predict(object = fit,\n          data = test_data)$predictions[,2]\n\nlogloss(y_train, train_preds)\n\n[1] 0.1135258\n\nlogloss(y_test, test_preds)\n\n[1] 0.2522654"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#lectures-office-hours",
    "href": "slides/raw_slides/slides_01.html#lectures-office-hours",
    "title": "STAT 479 Lecture 1",
    "section": "Lectures & Office Hours",
    "text": "Lectures & Office Hours\n\nLectures: Tuesdays & Thursday 11am-12:15pm (Morgridge Hall 1524)\nInstructor: Sameer Deshpande\n\nMondays: 11am - 12pm (Morgridge Hall 5586)\nWednesdays: 3pm - 4pm (Morgridge Hall 5586)\nFridays: 3pm - 4:30pm (Morgridge Hall 5618)\n\nTA: Zhexuan Liu\n\nTuesdays & Thursdays 9:15am-10:45am (Morgridge Hall 2515)"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#what-this-course-is-not",
    "href": "slides/raw_slides/slides_01.html#what-this-course-is-not",
    "title": "STAT 479 Lecture 1",
    "section": "What This Course is NOT",
    "text": "What This Course is NOT\n\nNOT a statistical methods course w/ sports applications\nMethods introduced as needed\n\nBut will not cover all technical details\nI’ll point to relevant resources & classes where relevant\nMain goal: answering substantive sports questions\n\n\n\n\nNOT a course on sports betting or fantasy sports\n\nSome material may be relevant …\n… but we won’t explicitly discuss these topics\n\n\n\n\n\nNOT a way to discuss last night’s game for credit\n\nThis is a serious statistics & data science course\n\nGoal: practice skills needed to work w/ sports data (e.g. for a team)\n\nCourse design reflects input from leaders in the industry"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#course-learning-outcomes",
    "href": "slides/raw_slides/slides_01.html#course-learning-outcomes",
    "title": "STAT 479 Lecture 1",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\n\n\nImplement appropriate statistical methods to assess player and team performance\nWork with play-by-play and high-resolution tracking data\nProvide constructive and actionable feedback on your peers’ analytic reports\nBuild a personal portfolio of sports data analyses\n\n\n\n\nAsking and answering substantive sports questions w/ data\n\nWe’ll go well beyond “who’s the best at…”"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#statistics-vs-analytics",
    "href": "slides/raw_slides/slides_01.html#statistics-vs-analytics",
    "title": "STAT 479 Lecture 1",
    "section": "Statistics vs Analytics",
    "text": "Statistics vs Analytics\n\nSports statistics refer to counts and rates that summarize performance:\n\nNumber of rebounds, wins, games played\nFree throw percentage, batting average\n\n\n\n\nAnalytics refers to the use of statistical modeling to help gain competitive advantage\nAnalytics makes use of basic statistics & creates new ones\nAnalytics uses data to answer substantive sports questions\nMany questions can framed in terms of prediction"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#bestnba-shooting-performances",
    "href": "slides/raw_slides/slides_01.html#bestnba-shooting-performances",
    "title": "STAT 479 Lecture 1",
    "section": "“Best”NBA Shooting Performances",
    "text": "“Best”NBA Shooting Performances\n\nWho is the best shooter in NBA history?\n\nWhat do we mean by “best”?\nIt’s not clear that this can be answered w/ data\n\n\n\n\nA more precise question: which player made the most shots in a single season?\n\nCan be answered by data\nWe’ll assess whether “best” = “made most shots” is satisfactory later\n\n\n\n\n\nData: box scores from each game since 2002-03\nAvailable from the hoopR package\n\n\nraw_box &lt;- hoopR::load_nba_player_box(seasons = 2002:(hoopR::most_recent_nba_season()))"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#data-snapshot",
    "href": "slides/raw_slides/slides_01.html#data-snapshot",
    "title": "STAT 479 Lecture 1",
    "section": "Data Snapshot",
    "text": "Data Snapshot\n\nColumnsSingle Game\n\n\n\n\n [1] \"game_id\"                           \"season\"                           \n [3] \"season_type\"                       \"game_date\"                        \n [5] \"game_date_time\"                    \"athlete_id\"                       \n [7] \"athlete_display_name\"              \"team_id\"                          \n [9] \"team_name\"                         \"team_location\"                    \n[11] \"team_short_display_name\"           \"minutes\"                          \n[13] \"field_goals_made\"                  \"field_goals_attempted\"            \n[15] \"three_point_field_goals_made\"      \"three_point_field_goals_attempted\"\n[17] \"free_throws_made\"                  \"free_throws_attempted\"            \n[19] \"offensive_rebounds\"                \"defensive_rebounds\"               \n[21] \"rebounds\"                          \"assists\"                          \n[23] \"steals\"                            \"blocks\"                           \n[25] \"turnovers\"                         \"fouls\"                            \n[27] \"plus_minus\"                        \"points\"                           \n[29] \"starter\"                           \"ejected\"                          \n[31] \"did_not_play\"                      \"active\"                           \n[33] \"athlete_jersey\"                    \"athlete_short_name\"               \n[35] \"athlete_headshot_href\"             \"athlete_position_name\"            \n[37] \"athlete_position_abbreviation\"     \"team_display_name\"                \n[39] \"team_uid\"                          \"team_slug\"                        \n[41] \"team_logo\"                         \"team_abbreviation\"                \n[43] \"team_color\"                        \"team_alternate_color\"             \n[45] \"home_away\"                         \"team_winner\"                      \n[47] \"team_score\"                        \"opponent_team_id\"                 \n[49] \"opponent_team_name\"                \"opponent_team_location\"           \n[51] \"opponent_team_display_name\"        \"opponent_team_abbreviation\"       \n[53] \"opponent_team_logo\"                \"opponent_team_color\"              \n[55] \"opponent_team_alternate_color\"     \"opponent_team_score\"              \n[57] \"reason\"                           \n\n\n\n\n\nraw_box |&gt; dplyr::filter(game_date == \"2011-06-12\") |&gt;\n  dplyr::select(athlete_display_name, \n         field_goals_made, field_goals_attempted)\n\n# A tibble: 30 × 3\n   athlete_display_name field_goals_made field_goals_attempted\n   &lt;chr&gt;                           &lt;int&gt;                 &lt;int&gt;\n 1 Dirk Nowitzki                       9                    27\n 2 Tyson Chandler                      2                     4\n 3 Jason Kidd                          2                     4\n 4 Shawn Marion                        4                    10\n 5 J.J. Barea                          7                    12\n 6 Brian Cardinal                      1                     1\n 7 Caron Butler                       NA                    NA\n 8 Ian Mahinmi                         2                     3\n 9 Rodrigue Beaubois                  NA                    NA\n10 DeShawn Stevenson                   3                     5\n# ℹ 20 more rows"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#extract-regular-season-data",
    "href": "slides/raw_slides/slides_01.html#extract-regular-season-data",
    "title": "STAT 479 Lecture 1",
    "section": "Extract Regular Season Data",
    "text": "Extract Regular Season Data\n\nallstar_dates &lt;- lubridate::date(c(\"2002-02-10\", \"2003-02-09\", \n    \"2004-02-15\",\"2005-02-20\", \"2006-02-19\", \"2007-02-18\", \n    \"2008-02-17\", \"2009-02-15\", \"2010-02-14\",\"2011-02-20\", \n    \"2012-02-26\", \"2013-02-17\", \"2014-02-16\", \"2015-02-15\", \n    \"2016-02-14\",\"2017-02-19\", \"2018-02-18\", \"2019-02-17\",\n    \"2020-02-16\", \"2021-03-07\", \"2022-02-20\",\"2023-02-19\", \n    \"2024-02-18\", \"2025-02-16\"))\nreg_box &lt;- raw_box |&gt;\n  dplyr::filter(season_type == 2 & !did_not_play & !game_date %in% allstar_dates)"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#preprocessing",
    "href": "slides/raw_slides/slides_01.html#preprocessing",
    "title": "STAT 479 Lecture 1",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nRename columns & set defaults for players with no minutes\n\n\nreg_box &lt;-\n  reg_box |&gt;\n  dplyr::rename(\n    Player = athlete_display_name, \n    FGM = field_goals_made, FGA = field_goals_attempted,\n    TPM = three_point_field_goals_made,TPA = three_point_field_goals_attempted,\n    FTM = free_throws_made, FTA = free_throws_attempted) |&gt;\n  dplyr::mutate(\n    FGM = ifelse(is.na(minutes), 0, FGM), FGA = ifelse(is.na(minutes), 0, FGA),\n    TPM = ifelse(is.na(minutes), 0, TPM),TPA = ifelse(is.na(minutes), 0, TPA),\n    FTM = ifelse(is.na(minutes), 0, FTM),FTA = ifelse(is.na(minutes), 0, FTA)) |&gt;\n  tidyr::replace_na(list(minutes = 0))"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#season-totals",
    "href": "slides/raw_slides/slides_01.html#season-totals",
    "title": "STAT 479 Lecture 1",
    "section": "Season Totals",
    "text": "Season Totals\n\nseason_box &lt;-\n  reg_box |&gt;\n  dplyr::group_by(Player, season) |&gt;\n  dplyr::summarise(FGM = sum(FGM),FGA = sum(FGA),\n    TPM = sum(TPM),TPA = sum(TPA),FTM = sum(FTM),FTA = sum(FTA),\n    minutes = sum(minutes), n_games = dplyr::n(),.groups = \"drop\")\nseason_box |&gt; dplyr::filter(Player == \"Dirk Nowitzki\") \n\n\n\n\n# A tibble: 18 × 10\n   Player        season   FGM   FGA   TPM   TPA   FTM   FTA minutes n_games\n   &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n 1 Dirk Nowitzki   2002   600  1258   139   350   440   516    2891      76\n 2 Dirk Nowitzki   2003   690  1489   148   390   483   548    3117      80\n 3 Dirk Nowitzki   2004   605  1310    99   290   371   423    2915      77\n 4 Dirk Nowitzki   2005   663  1445    91   228   615   708    3020      78\n 5 Dirk Nowitzki   2006   751  1564   110   271   539   598    3086      81\n 6 Dirk Nowitzki   2007   673  1341    72   173   498   551    2819      80\n 7 Dirk Nowitzki   2008   630  1314    79   220   478   544    2769      81\n 8 Dirk Nowitzki   2009   774  1616    61   170   485   545    3051      82\n 9 Dirk Nowitzki   2010   720  1496    51   121   536   586    3041      82\n10 Dirk Nowitzki   2011   610  1179    66   168   395   443    2505      82\n11 Dirk Nowitzki   2012   473  1034    78   212   318   355    2078      66\n12 Dirk Nowitzki   2013   332   707    63   151   164   191    1628      52\n13 Dirk Nowitzki   2014   633  1273   131   329   338   376    2625      80\n14 Dirk Nowitzki   2015   487  1062   104   274   255   289    2285      77\n15 Dirk Nowitzki   2016   498  1112   126   342   250   280    2362      75\n16 Dirk Nowitzki   2017   296   678    79   209    98   112    1421      54\n17 Dirk Nowitzki   2018   346   758   138   337    97   108    1901      77\n18 Dirk Nowitzki   2019   135   376    64   205    39    50     794      51"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#who-made-the-most-shots",
    "href": "slides/raw_slides/slides_01.html#who-made-the-most-shots",
    "title": "STAT 479 Lecture 1",
    "section": "Who Made the Most Shots?",
    "text": "Who Made the Most Shots?\n\nseason_box |&gt;\n  dplyr::arrange(dplyr::desc(FGM)) |&gt;\n  dplyr::select(Player, season, FGM, FGA, minutes, n_games) |&gt; \n  dplyr::slice_head(n=5)\n\n# A tibble: 5 × 6\n  Player                  season   FGM   FGA minutes n_games\n  &lt;chr&gt;                    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n1 Kobe Bryant               2006   949  2109    3184      78\n2 LeBron James              2006   875  1823    3361      82\n3 Kobe Bryant               2003   868  1924    3401      82\n4 Shai Gilgeous-Alexander   2025   868  1680    2633      77\n5 LeBron James              2018   857  1580    3024      82\n\n\n\n\nIs Kobe’s 2002-03 really the same as SGA’s 2024-25???\n\n\n\n\nArguably “best” should account for efficiency"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#field-goal-percentage",
    "href": "slides/raw_slides/slides_01.html#field-goal-percentage",
    "title": "STAT 479 Lecture 1",
    "section": "Field Goal Percentage",
    "text": "Field Goal Percentage\n\nseason_box &lt;-\n  season_box |&gt;\n  dplyr::mutate(FGP = ifelse(FGA &gt; 0, FGM/FGA, NA_real_))\n\n\n\nFGP LeadersAnother LookRestricted Leaders\n\n\n\nseason_box |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::slice_head(n=5) |&gt;\n  dplyr::select(Player, season, FGP)\n\n# A tibble: 5 × 3\n  Player           season   FGP\n  &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;\n1 Ahmad Caver        2022     1\n2 Alondes Williams   2025     1\n3 Andris Biedrins    2014     1\n4 Anthony Brown      2018     1\n5 Braxton Key        2023     1\n\n\n\n\n\nseason_box |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::slice_head(n=5) |&gt;\n  dplyr::select(Player, season, FGP, FGA)\n\n# A tibble: 5 × 4\n  Player           season   FGP   FGA\n  &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Ahmad Caver        2022     1     1\n2 Alondes Williams   2025     1     2\n3 Andris Biedrins    2014     1     1\n4 Anthony Brown      2018     1     1\n5 Braxton Key        2023     1     1\n\n\n\n\n\nseason_box |&gt; \n  dplyr::filter(FGA &gt;= 400) |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, FGP, FGA) |&gt;\n  dplyr::slice_head(n = 5)\n\n# A tibble: 5 × 4\n  Player         season   FGP   FGA\n  &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Daniel Gafford   2024 0.725   480\n2 Walker Kessler   2023 0.720   414\n3 DeAndre Jordan   2017 0.714   577\n4 Rudy Gobert      2022 0.713   508\n5 DeAndre Jordan   2015 0.710   534"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#effective-field-goal-percentage",
    "href": "slides/raw_slides/slides_01.html#effective-field-goal-percentage",
    "title": "STAT 479 Lecture 1",
    "section": "Effective Field Goal Percentage",
    "text": "Effective Field Goal Percentage\n\nFGP is arguably a better measure of skill than FGM but\n\nExtreme values for players w/ few attempts\nDoesn’t distinguish 2- and 3-point shots\n\n\n\n\n\\(\\textrm{eFGP} = (\\textrm{FGM} + 0.5 \\times \\textrm{TPM})/\\textrm{FGA}\\)\n\n\nseason_box &lt;- \n  season_box |&gt; \n  dplyr::mutate(eFGP = (FGM + 0.5 * TPM)/FGA) \n\n\n\n\nRaw eFGP LeadersRestricted Leaders\n\n\n\neFGP leaders are mostly centers who don’t shoot 3’s\n\n\n\n# A tibble: 5 × 6\n  Player         season  eFGP   FGP   TPA n_games\n  &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 Daniel Gafford   2024 0.725 0.725     0      74\n2 Walker Kessler   2023 0.721 0.720     3      74\n3 DeAndre Jordan   2017 0.714 0.714     2      81\n4 Rudy Gobert      2022 0.713 0.713     4      66\n5 DeAndre Jordan   2015 0.711 0.710     4      82\n\n\n\n\n\nWe can restrict to players with \\(\\textrm{FGA} &gt; 400\\) and \\(\\textrm{TPA} &gt; 100\\)\n\n\n\n# A tibble: 5 × 6\n  Player          season  eFGP   FGP   TPA n_games\n  &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 Kyle Korver       2015 0.671 0.487   449      75\n2 Duncan Robinson   2020 0.667 0.470   606      73\n3 Obi Toppin        2024 0.660 0.571   260      83\n4 Nikola Jokic      2023 0.660 0.632   149      69\n5 Joe Harris        2021 0.655 0.502   427      69"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#reflection",
    "href": "slides/raw_slides/slides_01.html#reflection",
    "title": "STAT 479 Lecture 1",
    "section": "Reflection",
    "text": "Reflection\n\neFGP is arguably better than FGP and FGM\nBut it is still highly variable\n\n\n\nRefined question: instead of “who’s the best”, we can ask\n\n“what is the probability that a player makes a shot”\n\nLater: methods to estimate these probs. that\n\nAccount for contextual factors (e.g., shot location)\nProduce stable estimates even w/ small sample sizes\nAvoid imposing arbitrary cut-offs (e.g., \\(\\textrm{FGA} &gt; 400\\))\nCalibrate performance against transparent baselines (e.g., “above replacement”)\n\nAnalytics involves motivating and justifying choices"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#requisites",
    "href": "slides/raw_slides/slides_01.html#requisites",
    "title": "STAT 479 Lecture 1",
    "section": "Requisites",
    "text": "Requisites\n\nSTAT 333 or 340\n\nRandom variables, expectations, probability\nFitting linear models & interpreting outputs\n\n\n\n\nPrior experience with R\n\nAssignment, scripting, loops, control flow\nSaving objects, installing & loading packages.\nData manipulation with dplyr and other tidyverse packages.\nCreating visualizations"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#course-website",
    "href": "slides/raw_slides/slides_01.html#course-website",
    "title": "STAT 479 Lecture 1",
    "section": "Course Website",
    "text": "Course Website\n\n\n\n\n\nAlso Canvas & Piazza"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#assignments-grading",
    "href": "slides/raw_slides/slides_01.html#assignments-grading",
    "title": "STAT 479 Lecture 1",
    "section": "Assignments & Grading",
    "text": "Assignments & Grading\n\n3 Group Projects (900 pts): Due on 10/10, 11/7, and 12/5\n\nWritten Report (100 pts) & Presentation (100 pts)\nPeer Reviews (60 pts)\nTeam Accountability Survey (40 pts)\n\n\n\n\nParticipation (100 pts): Assessed holistically\n\nRegularly attend lectures & office hours\nContribute to Piazza discussions\n\n\n\n\n\nFinal grade based on how many of the 1000 points earned"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#projects",
    "href": "slides/raw_slides/slides_01.html#projects",
    "title": "STAT 479 Lecture 1",
    "section": "Projects",
    "text": "Projects\n\nWork in groups of up to 4\n\nSame groups for Projects 1 & 2; can change for Project 3\nForm groups by Friday September 12\nUse Piazza to find teammates & sign up on Canvas\n\nCourse projects can\n\nModify or extend analysis from lecture (e.g., to new sport)\nAnswer a new question not covered in lecture\n…\n\nIdeally jump-start a portfolio that you can show teams"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-report",
    "href": "slides/raw_slides/slides_01.html#project-report",
    "title": "STAT 479 Lecture 1",
    "section": "Project Report",
    "text": "Project Report\n\nExecutive Summary:\n\nNon-technical overview of goals, methods, and results\nAudience: front office executive, coach, or player\n\nTechnical Report:\n\nInclude all code needed to reproduce findings\nTightly integrate code & output w/ written exposition\nAudience: fellow data scientists\n\n\n\nI highly recommend using Quarto or RMarkdown"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-presentation",
    "href": "slides/raw_slides/slides_01.html#project-presentation",
    "title": "STAT 479 Lecture 1",
    "section": "Project Presentation",
    "text": "Project Presentation\n\nRecord & upload a 8–10 minute presentation\nEvery group member must speak\n\n\n\nI’ll select 5-6 groups to present in-class on the last day (12/9)\n\nDetails to be announced\nPresenting on the last day unrelated to course grades\nBut there’ll be prizes"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-peer-review",
    "href": "slides/raw_slides/slides_01.html#project-peer-review",
    "title": "STAT 479 Lecture 1",
    "section": "Project Peer Review",
    "text": "Project Peer Review\n\nProvide feedback on 3 presentations & 3 reports\n\nRubrics will be available\nFill out rubric + leave constructive comments\n\nDue 1 weeks after projects: 10/17, 11/14, and 12/12"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#team-accountability-survey",
    "href": "slides/raw_slides/slides_01.html#team-accountability-survey",
    "title": "STAT 479 Lecture 1",
    "section": "Team Accountability Survey",
    "text": "Team Accountability Survey\n\nAssign score from 0 (least) to 10 (most) for\n\nParticipation, Preparation, and Respectfulness\n\nRate every group member (including yourself)\nRatings and comments will be kept anonymous\nCourse staff may give warnings for low peer score"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#unit-1-quantifying-performances",
    "href": "slides/raw_slides/slides_01.html#unit-1-quantifying-performances",
    "title": "STAT 479 Lecture 1",
    "section": "Unit 1: Quantifying Performances",
    "text": "Unit 1: Quantifying Performances\n\nExpected vs actual performance\nValue of a game state\nPerformance above “replacement” level\nCase Studies:\n\nExpected goals in soccer (Lectures 2 & 3)\nAdjusted plus/minus in the NBA (Lectures 4 & 5)\nRun expectancy & WAR in baseball (Lectures 6–8)\nWAR for the NFL (Lectures 9 & 10)\nPitch Framing in baseball (Lecture 11)"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-1",
    "href": "slides/raw_slides/slides_01.html#project-1",
    "title": "STAT 479 Lecture 1",
    "section": "Project 1",
    "text": "Project 1\n\nUse box-score or play-by-play data\nIntroduce a new metric & show it has favorable properties\n\nSeason-to-season stability\nAbility to predict match- or season-level outcomes\nReveal new insight about player valuation\n\nEvaluate individual or team performance in a new sport\n\nWAR for volleyball or college football?\nExpected goals in hockey?\n\nExtend case study from lecture\n\nConstruct analogs for another sport\nExamine season-to-season variability\n\n…"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#unit-2-rankings-simulation",
    "href": "slides/raw_slides/slides_01.html#unit-2-rankings-simulation",
    "title": "STAT 479 Lecture 1",
    "section": "Unit 2: Rankings & Simulation",
    "text": "Unit 2: Rankings & Simulation\n\nUse models to simulate games, tournaments, drafts, etc.\nCase Studies:\n\nNCAA Volleyball & Hockey Tournaments (Lectures 12 & 13)\nMarkov chain simulations (Lecture 14 & 15)\nBuilding a consensus mock draft (Lecture 16)\nEstimating impact of a rule change (Lecture 17)"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-2",
    "href": "slides/raw_slides/slides_01.html#project-2",
    "title": "STAT 479 Lecture 1",
    "section": "Project 2",
    "text": "Project 2\n\nFit a model to estimate latent team- or player- strength\nUse model estimates to simulate plays, games, tournaments, drafts, etc.\nCompute probabilities based on simulation\n\nProb. of winning tournament or making it past 1st round\nProb. football drive ends in a touchdown\nProb. of winning a cricket test\nChance player available in 2nd round of draft"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#unit-3-tracking-data",
    "href": "slides/raw_slides/slides_01.html#unit-3-tracking-data",
    "title": "STAT 479 Lecture 1",
    "section": "Unit 3: Tracking Data",
    "text": "Unit 3: Tracking Data\n\nCurrently the hottest area of sports analytics\n\nTrackman + Statcast in baseball\nNFL: player positions 10 times per second\nHawkeye for tennis, Hudl for soccer, …\n\nTracking data opens up many possibilities\n\nIncorporating spatial info. into prediction models\nCreating new metrics using tracking data\nSpace ownership & predicting trajectories"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-3",
    "href": "slides/raw_slides/slides_01.html#project-3",
    "title": "STAT 479 Lecture 1",
    "section": "Project 3",
    "text": "Project 3\n\nAnalyze tracking data\nNFL’s Big Data Bowl is a great opportunity\n\nBest way into the field (even for non-football sports)\nCash prizes & chance to present at Combine\n\nI highly encourage turning Project 3 into a BDB submission\n\nMore info will be posted on Piazza\nI’ll provide additional feedback/input for teams that do submit"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#course-expectations",
    "href": "slides/raw_slides/slides_01.html#course-expectations",
    "title": "STAT 479 Lecture 1",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nRespect diverse backgrounds\n\nNot everyone may know as much about a sport/method as You\nYou may not know as much about a sport/method as someone else\n\nDon’t hesitate to ask for and to provide help!\nTake care of yourself & each other"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#generative-ai-expectations-1",
    "href": "slides/raw_slides/slides_01.html#generative-ai-expectations-1",
    "title": "STAT 479 Lecture 1",
    "section": "Generative AI Expectations 1",
    "text": "Generative AI Expectations 1\n\nYou have the right to the full benefit of my expertise and engagement in this course.\nI will therefore never use AI to\n\nTo provide feedback on assignments\nTo prepare any course content (e.g., slides, code, assignment flavor-text, etc.)\nTo mediate or assist any communications with you\n\nEverything you see in the course was created by me without the aid of generative AI"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#generative-ai-expectations-2",
    "href": "slides/raw_slides/slides_01.html#generative-ai-expectations-2",
    "title": "STAT 479 Lecture 1",
    "section": "Generative AI Expectations 2",
    "text": "Generative AI Expectations 2\n\nA core theme of this class is practice\n\nIt is OK to make mistakes or not know an answer\nProcess is more important than results\n\nGenerative AI short-circuits the intended learning process\nIts use is expressly prohibited\n\n\n\n\n\n\n\nPeer Review\n\n\nRespect your classmates enough to review their projects yourself. Uploading someone else’s project report or presentation to a generative AI tool (e.g., for creating summaries) is forbidden and will result in a failing grade."
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#looking-ahead",
    "href": "slides/raw_slides/slides_01.html#looking-ahead",
    "title": "STAT 479 Lecture 1",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nLectures 2 & 3: Expected Goals in Soccer\nWill use public data provided by Hudl\nBe sure to install the StatsBombR & ranger packages\n\n\ndevtools::install_github(\"statsbomb/StatsBombR\")\ninstall.packages(\"ranger\")"
  },
  {
    "objectID": "slides/lecture04.html",
    "href": "slides/lecture04.html",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "slides/lecture02.html",
    "href": "slides/lecture02.html",
    "title": "Lecture 2: Expected Goals",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture11.html",
    "href": "lectures/lecture11.html",
    "title": "Lecture 11: Pitch Framing",
    "section": "",
    "text": "In October 2015, the Houston Astros played the New York Yankees in the American League Wild Card game. During and after the game, several Yankees fans took to social media to complain about inconsistencies in the strike zone being called by home plate umpire Eric Cooper. They argued that Cooper was not calling balls and strikes consistently for both teams, putting the Yankees at a distinct disadvantage. Beyond simple fan reaction, individual players took exception to Cooper’s strike zone: after striking out, Yankees catcher Brian McCann complained to Cooper that on similar pitches, Cooper was calling strikes when the Astros were pitching but balls when the Yankees were pitching. Figure 1 show two such pitches.\n\n\n\n\n\n\n\n\n\n\n\n(a) Pitch called a ball\n\n\n\n\n\n\n\n\n\n\n\n(b) Pitch called a strike\n\n\n\n\n\n\n\nFigure 1\n\n\n\nBoth pitches were thrown low and outside1, near the bottom left corner of the strike zone2 shown in the figure. Because no part of the ball passed through the strike zone, by rule, both pitches should have been called balls. But umpire Cooper called the pitch in Figure 1 (a), thrown by Yankees pitcher Masahiro Tanaka, a ball and called the pitch in Figure 1 (b), thrown by Astros pitcher Dallas Keuchel, a strike. During the television broadcast of the game and on social media after the game3, many speculated many speculated that the reason for this discrepancy in strike zones is due Astros catcher Jason Castro’s ability to frame pitches, catching them in a way so as to increase the chances of the umpire calling a strike.\nPitch framing, which has been studied by the sabermetrics community since at least 2008, received lots of coverage in the popular press between 2014 and 2016. A good example is this article about Jonathan Lucroy from ESPN The Magazine, which estimated that Lucroy’s framing accounted for a total of two wins for the Brewers in 2014 and was worth around $14 million. That article went on to say that claim that the most impactful player in baseball today is the game’s 17th highest-paid catcher.”\nIn this lecture, we will estimate how many more runs a catcher saves his team through his framing compared to a replacement-level catcher. Like we did in Lectures 6, 7, and 8, we will work with pitch tracking data scraped from StatCast. We will use the data from 2023 to establish historical strike probabilities (Section 3.1) based on pitch location, batter handedness, and pitcher handedness. We will then include these historical baselines as fixed effects in a multilevel model that also includes random intercepts for the catcher, pitcher, and batter involved in pitches from the 2024 season (Section 3). Using the estimated deviations between catcher intercepts and the grand mean from our multilevel model, we conclude by estimating how many more expected runs catchers save than a replacement-level catcher (Section 4).",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#overview",
    "href": "lectures/lecture11.html#overview",
    "title": "Lecture 11: Pitch Framing",
    "section": "",
    "text": "In October 2015, the Houston Astros played the New York Yankees in the American League Wild Card game. During and after the game, several Yankees fans took to social media to complain about inconsistencies in the strike zone being called by home plate umpire Eric Cooper. They argued that Cooper was not calling balls and strikes consistently for both teams, putting the Yankees at a distinct disadvantage. Beyond simple fan reaction, individual players took exception to Cooper’s strike zone: after striking out, Yankees catcher Brian McCann complained to Cooper that on similar pitches, Cooper was calling strikes when the Astros were pitching but balls when the Yankees were pitching. Figure 1 show two such pitches.\n\n\n\n\n\n\n\n\n\n\n\n(a) Pitch called a ball\n\n\n\n\n\n\n\n\n\n\n\n(b) Pitch called a strike\n\n\n\n\n\n\n\nFigure 1\n\n\n\nBoth pitches were thrown low and outside1, near the bottom left corner of the strike zone2 shown in the figure. Because no part of the ball passed through the strike zone, by rule, both pitches should have been called balls. But umpire Cooper called the pitch in Figure 1 (a), thrown by Yankees pitcher Masahiro Tanaka, a ball and called the pitch in Figure 1 (b), thrown by Astros pitcher Dallas Keuchel, a strike. During the television broadcast of the game and on social media after the game3, many speculated many speculated that the reason for this discrepancy in strike zones is due Astros catcher Jason Castro’s ability to frame pitches, catching them in a way so as to increase the chances of the umpire calling a strike.\nPitch framing, which has been studied by the sabermetrics community since at least 2008, received lots of coverage in the popular press between 2014 and 2016. A good example is this article about Jonathan Lucroy from ESPN The Magazine, which estimated that Lucroy’s framing accounted for a total of two wins for the Brewers in 2014 and was worth around $14 million. That article went on to say that claim that the most impactful player in baseball today is the game’s 17th highest-paid catcher.”\nIn this lecture, we will estimate how many more runs a catcher saves his team through his framing compared to a replacement-level catcher. Like we did in Lectures 6, 7, and 8, we will work with pitch tracking data scraped from StatCast. We will use the data from 2023 to establish historical strike probabilities (Section 3.1) based on pitch location, batter handedness, and pitcher handedness. We will then include these historical baselines as fixed effects in a multilevel model that also includes random intercepts for the catcher, pitcher, and batter involved in pitches from the 2024 season (Section 3). Using the estimated deviations between catcher intercepts and the grand mean from our multilevel model, we conclude by estimating how many more expected runs catchers save than a replacement-level catcher (Section 4).",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#data",
    "href": "lectures/lecture11.html#data",
    "title": "Lecture 11: Pitch Framing",
    "section": "Data",
    "text": "Data\n\nValue of a Called Strike\nLater, in Section 4, we will compute how many expected runs catchers save their teams To do this, we must first determine the value of a called strike using a similar framework as in Lecture 6. Specifically, we will compute the number of runs scored by the batting the team in the remainder of the half-inning following every pitch. Then, for every combination of the count4 and the number of outs, we will compute the average number of runs scored in the remainder of the half-inning after a called ball and after a called strike on taken pitches5. The difference between these quantities captures the value of a called strike in that particular count-out state.\nTo compute this, we start by loading the data frame statcast2024 that we prepared and saved in Lecture 6.\n\nload(\"statcast2024.RData\")\n\nRecall that this data table contains a row for every pitch and that the column RunsRemaining recorded how many runs the batting team scored in the remainder of the half-inning following each pitch. We determine whether a pitch was taken using the description field\n\ntable(statcast2024$description)\n\nball_descriptions &lt;- \n  c(\"ball\", \"blocked_ball\", \"pitchout\", \"hit_by_pitch\")\nswing_descriptions &lt;- \n  c(\"bunt_foul_tip\", \"foul\", \"foul_bunt\", \"foul_tip\",\n    \"hit_into_play\", \"missed_bunt\", \"swinging_strike\",\n    \"swinging_strike_block\")\n\ntaken2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::filter(!description %in% swing_descriptions) |&gt;\n  dplyr::mutate(\n2    Y = ifelse(description == \"called_strike\", 1, 0),\n    Count = paste(balls, strikes, sep = \"-\")) |&gt;\n  dplyr::select(Y, plate_x, plate_z, \n                Count, Outs,\n3                batter, pitcher, fielder_2,\n4                stand, p_throws,\n                RunsRemaining, sz_top, sz_bot) |&gt; \n  dplyr::mutate(\n    Count = factor(Count),\n    batter = factor(batter),\n    pitcher = factor(pitcher),\n    fielder_2 = factor(fielder_2),\n    stand = factor(stand),\n    p_throws = factor(p_throws))\n\n\n1\n\nExtract only the taken pitches\n\n2\n\nAdd columns recording whether taken pitch was called a strike (Y = 1) or a ball (Y = 0) and the count.\n\n3\n\nfielder_2 contains the MLB Advanced Media ID for the catcher\n\n4\n\nstand and p_throws record the handedness of the batter and pitcher.\n\n\n\n\n\n                   ball            blocked_ball           bunt_foul_tip \n                 231032                   14717                      15 \n          called_strike                    foul               foul_bunt \n                 113912                  126012                    1208 \n               foul_tip            hit_by_pitch           hit_into_play \n                   7218                    1979                  121751 \n            missed_bunt                pitchout         swinging_strike \n                    196                      52                   73209 \nswinging_strike_blocked \n                   3834 \n\n\nWe can now compute the expected numbers of runs scored after a called ball or strike for every combination of out and count.\n\ner_balls &lt;-\n  taken2024 |&gt;\n  dplyr::filter(Y == 0) |&gt;\n  dplyr::group_by(Count, Outs) |&gt;\n  dplyr::summarise(er_ball = mean(RunsRemaining), .groups = 'drop')\n\ner_strikes &lt;-\n  taken2024 |&gt;\n  dplyr::filter(Y == 1) |&gt;\n  dplyr::group_by(Count, Outs) |&gt;\n  dplyr::summarise(er_strike = mean(RunsRemaining), .groups = 'drop')\n\ner_taken &lt;-\n  er_balls |&gt;\n  dplyr::left_join(y = er_strikes, by = c(\"Count\", \"Outs\")) |&gt;\n  dplyr::mutate(value = er_ball - er_strike)\n\ner_taken |&gt; dplyr::slice_head(n=5)\n\n# A tibble: 5 × 5\n  Count  Outs er_ball er_strike  value\n  &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 0-0       0   0.731     0.605 0.126 \n2 0-0       1   0.547     0.425 0.122 \n3 0-0       2   0.283     0.193 0.0902\n4 0-1       0   0.660     0.525 0.135 \n5 0-1       1   0.464     0.365 0.0990\n\n\nLooking at er_taken, we find that batting teams score\n\nAbout 0.731 runs following a called ball on a 0-0 pitch with 0 outs\nAbout 0.605 runs following a called strike on a 0-0 pitch with 0 outs So, if a good pitch framer can get a 0-0 pitch with 0 outs called a strike instead of ball, he saves the fielding team about 0.126 runs, on average. From the standpoint of the fielding team, a called strike is most valuable on a 3-2 pitch with 0 outs (when they can expect to save almost 0.8 runs on average) and least value on a 0-1 pitch with 2 outs (when they can expect to save about 0.068 runs on average).\n\n\ner_taken |&gt; dplyr::arrange(dplyr::desc(value)) |&gt; dplyr::slice(c(1, dplyr::n()))\n\n# A tibble: 2 × 5\n  Count  Outs er_ball er_strike  value\n  &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 3-2       0   1.12      0.321 0.800 \n2 0-1       2   0.234     0.166 0.0684\n\n\nSo that we can use it later in Section 4, we will append a column to taken2024 containing the value of a called strike (from the fielding team’s perspective).\n\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::left_join(y = er_taken |&gt; dplyr::select(Count, Outs, value), by = c(\"Count\", \"Outs\"))",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#sec-multilevel",
    "href": "lectures/lecture11.html#sec-multilevel",
    "title": "Lecture 11: Pitch Framing",
    "section": "A Multilevel Model",
    "text": "A Multilevel Model\nWe are ultimately interested in understanding how individual players (i.e., batters, catchers, and pitchers) can influence the called strike probability. Any credible model for these probabilities must account for location. After all, if a pitch is thrown several feet away from the batter, no amount of pitch framing will change the call from a ball to a strike. Similarly, catcher skill is likely n The StatCast variables plate_x and plate_z respectively record the horizontal and vertical coordinate of each pitch as it crosses the front edge of home plate. These variables are measured from the catcher’s perspective so pitches thrown to the left of home plate have negative plate_x values and pitches thrown to the right of home plate have positive plate_x values. Both plate_x and plate_z are measured in feet.\nA natural starting point is to fit a multilevel model with player-specific random intercepts that adjusts for the fixed effects of plate_x and plate_z. Specifically, because we are dealing with a binary outcome, we might start with the model \\[\n\\log \\left( \\frac{\\mathbb{P}(Y_{i} = 1)}{\\mathbb{P}(Y_{i} = 0)} \\right) = B_{b[i]} + C_{c[i]} + P_{p[i]} + \\beta_{x} x_{i} + \\beta_{z}z_{i}\n\\] where \\(b[i], c[i],\\) and \\(p[i]\\) record the identities of the batter, catcher, and pitcher involved in taken pitch \\(i;\\) \\(x_{i}\\) and \\(z_{i}\\) are the plate_x and plate_z measurements; and the \\(B_{b}\\)’s, \\(C_{c}\\)’s, and \\(P_{p}\\)’s are random intercepts for the batters, catchers, and pitchers with \\(B_{b} \\sim N(\\mu_{B}, \\sigma^{2}_{B}),\\) \\(C_{c} \\sim N(\\mu_{C}, \\sigma^{2}_{C})\\), and \\(P_{p} \\sim N(\\mu_{P})\\). This simple model assumes that the log-odds of a called strike is monotonic in plate_x and plate_z. Due to the monotonicity of the inverse logistic transformation6, a positive (resp. negative) \\(\\beta_{x}\\) would imply that the probability of a called strike increases (resp. decreases) as the pitch moves from left to right. Similarly, a positive (resp. negative) \\(\\beta_{z}\\) implies that the probability of a called strike increases (resp. decreases) the higher up a pitch is thrown.\nA cursory glance at the data reveals such monotonicity is not realistic. Specifically, if called strike probability was monotonic in plate_x and plate_z, then we should see progressively more strikes in one of the four corners of the plot7. In the plot, we overlay the “average” rule book strike zone8.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\",\n1     xlim = c(-2.5, 2.5), ylim = c(0, 6),\n     xlab = \"\", ylab = \"\",\n     main = \"Called pitches (2024)\",\n     xaxt = \"n\", yaxt = \"n\", bty = \"n\")\npoints(x = taken2024$plate_x, y = taken2024$plate_z,\n       pch = 16, cex = 0.25,\n2       col = ifelse(taken2024$Y == 1, rgb(1, 0, 0, 0.25), rgb(0,0,1,0.25)))\nrect(xleft = -8.5/12, \n     ybottom = mean(taken2024$sz_bot, na.rm = TRUE),\n     xright = 8.5/12,\n     ytop = mean(taken2024$sz_top, na.rm = TRUE))\n\n\n1\n\nRestrict attention to pitches that are not in the dirt (i.e., plate_z &gt; 0) but not thrown too high (i.e. plate_z &lt; 6), are within 2.5 feet of the center of home plate in either direction.\n\n2\n\nColor strikes (Y = 1) in red and balls in blue (Y = 0). The 0.25 sets the transparency\n\n\n\n\n\n\n\n\n\n\n\nAll called balls and strikes\n\n\n\n\nLetting \\(n_{B}, n_{C},\\) and \\(n_{P}\\) be the numbers of batters, catchers, and pitchers, we will instead model \\[\n\\log \\left( \\frac{\\mathbb{P}(Y_{i} = 1)}{\\mathbb{P}(Y_{i} = 0)} \\right) = B_{b[i]} + C_{c[i]} + P_{p[i]} + \\beta_{p} \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})}\\right)\n\\] where \\(\\hat{p}(x,z)\\) is a baseline called strike probability estimated using data from the previous season9 and the random player intercepts satisfy \\[\n\\begin{align}\nB_{b} &= \\mu_{B} + u_{b}^{(B)}; \\quad u^{(B)}_{b} \\sim N(0, \\sigma^{2}_{B}) \\quad \\text{for each}~b = 1, \\ldots, n_{B} \\\\\nC_{c} &= \\mu_{C} + u_{c}^{(C)}; \\quad u^{(C)}_{c} \\sim N(0, \\sigma^{2}_{C}) \\quad \\text{for each}~c = 1, \\ldots, n_{C} \\\\\nP_{p} &= \\mu_{P} + u_{p}^{(P)}; \\quad u^{(P)}_{p} \\sim N(0, \\sigma^{2}_{P}) \\quad \\text{for each}~p = 1, \\ldots, n_{P}. \\\\\n\\end{align}\n\\] Our model accounts for pitch location by including a suitably-transformed baseline called strike probability estimate as a fixed effect. The random player intercepts capture how much each player contributions to the log-odds of a called strike over and above what is determined by pitch location.\n\nHistorical Strike Probabilities\nTo fit our proposed multilevel model, we must first compute the baseline called strike probabilities using data from the 2023 season. We first scrape all StatCast data fr om2023 using the function annual_statcast_query, which we defined in Lecture 6 and is available here.\n\n1source(\"annual_statcast_query.R\")\nraw_statcast2023 &lt;- annual_statcast_query(season = 2023)\nsave(raw_statcast2023, file = \"raw_statcast2023.RData\")\n\n\n1\n\nThis will only work if the file “annual_statcast_query.R” is saved in your R working directory\n\n\n\n\nWe can now apply the same basic pre-processing to raw_statcast2023 that we did to raw_statcast2024 back in Lecture 6.\n\nstatcast2023 &lt;-\n  raw_statcast2023 |&gt; \n  dplyr::filter(game_type == \"R\") |&gt;\n  dplyr::filter(\n    strikes &gt;= 0 & strikes &lt; 3 & \n      balls &gt;= 0 & balls &lt; 4 & \n      outs_when_up &gt;= 0 & outs_when_up &lt; 3) |&gt;\n  dplyr::arrange(game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n    BaseRunner = \n      paste0(1*(!is.na(on_1b)),1*(!is.na(on_2b)),1*(!is.na(on_3b)))) |&gt;\n  dplyr::rename(Outs = outs_when_up)\n\nNow that we have pre-processed the 2023 StatCast data, we will extract all called pitches. We will fit our generalized additive model only to pitches with plate_x values between -1.5 and 1.5 and plate_z values between 1 and 6. Beyond this region there are virtually no called strikes. Including pitches that are too far away from the strike zone — i.e., pitches for which the called strike probability is likely exactly zero — can lead to some numerical instabilities when fitting the GAM.\n\ntaken2023 &lt;-\n  statcast2023 |&gt;\n  dplyr::filter(!description %in% swing_descriptions) |&gt; \n  dplyr::mutate(\n    Y = ifelse(description == \"called_strike\", 1, 0)) |&gt; \n  dplyr::select(Y, plate_x, plate_z, stand, p_throws) |&gt; \n  dplyr::filter(!is.na(plate_x) & !is.na(plate_z)) |&gt;\n  dplyr::filter(abs(plate_x) &lt;= 1.5 & plate_z &gt;= 1 & plate_z &lt;= 6) |&gt;\n  dplyr::mutate(\n    stand = factor(stand),\n    p_throws = factor(p_throws))\n\nWe now fit our generalized additive model.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code takes several minutes to run.\n\n\n\nlibrary(mgcv)\n1hgam_fit &lt;-\n  bam(formula = Y ~ stand + p_throws + s(plate_x, plate_z),\n      family = binomial(link=\"logit\"), \n      data = taken2023)\n\n\n1\n\nThe “h” serves as a reminder that we’re getting historical called strike probabilities\n\n\n\n\nLike we did with our model for the probability of making an out as a function of fielding location in Lecture 8, we can visualize the fitted probabilities along a fine grid of locations. The following code plots the fitted called strike probabilities as a function of location for a right-handed pitcher facing a right-handed batter.\n\ngrid_sep &lt;- 0.05\nx_grid &lt;- seq(-1.5, 1.5, by = grid_sep)\nz_grid &lt;- seq(0, 6, by = grid_sep)\ncol_list &lt;- colorBlindness::Blue2DarkRed18Steps \nplate_grid &lt;- \n  expand.grid(plate_x = x_grid, plate_z = z_grid) |&gt;\n1  dplyr::mutate(stand = \"R\", p_throws = \"R\") |&gt;\n2  dplyr::mutate(stand = factor(stand, levels = c(\"L\", \"R\")),\n                p_throws = factor(p_throws, levels = c(\"L\", \"R\")))\n\ngrid_preds &lt;-\n  predict(object = hgam_fit,\n          newdata = plate_grid,\n          type = \"response\")\ngrid_prob_cols &lt;- \n  rgb(colorRamp(col_list,bias=1)(grid_preds)/255)\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", \n     xlim = c(-1.5, 1.5), ylim = c(0,6),\n     main = \"Historical called strike probabilities\",\n     xlab = \"\", ylab = \"\", \n     xaxt = \"n\", yaxt = \"n\")\nfor(i in 1:nrow(plate_grid)){\n  rect(xleft = plate_grid$plate_x[i] - grid_sep/2, \n       ybot = plate_grid$plate_z[i] - grid_sep/2,\n       xright = plate_grid$plate_x[i] + grid_sep/2,\n       ytop = plate_grid$plate_z[i] + grid_sep/2,\n       col = adjustcolor(grid_prob_cols[i], alpha.f = 0.5),\n       border = NA)\n}\nrect(xleft = -8.5/12, \n     ybottom = mean(taken2024$sz_bot, na.rm = TRUE),\n     xright = 8.5/12,\n     ytop = mean(taken2024$sz_top, na.rm = TRUE))\n\n\n1\n\nChange this line to visualize the fitted probabilities for different combinations of batter and pitcher handedness\n\n2\n\nWe will eventually pass plate_grid to the predict function, which will expect both stand and p_throws to be factor variables\n\n\n\n\n\n\n\n\n\n\nFigure 2: Historical called strike probabilities for a right-handed batter facing a right-handed pitcher.\n\n\n\n\n\n\n\nFitting Our Multilevel Model\nNow that we have fit a GAM model to the historical data, we are ready to compute the baseline log-odds for a called strike for each pitch from 2024. We can do this using the predict function with the argument type = \"link\" instead of type  = \"response\".\n\nbaseline &lt;- \n  predict(object = hgam_fit,\n          newdata = taken2024, \n          type = \"link\")\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::mutate(baseline = baseline) |&gt;\n1  dplyr::filter(abs(plate_x) &lt;= 1.5 & plate_z &gt;= 1 & plate_z &lt;= 6)\n\n\n1\n\nWe focus only on “frameable” pitches which are not throw too far away from home plate in the horizontal direction and are not thrown too high or too low in the vertical direction.\n\n\n\n\nWe’re finally ready to fit our multilevel model\n\nlibrary(lme4)\nmultilevel_fit &lt;-\n1  glmer(formula = Y ~ 1 + (1 | fielder_2) + (1 | batter)  + (1 | pitcher) +  baseline,\n        family = binomial(link = \"logit\"),\n        data = taken2024)\n\n\n1\n\nBecause the outcome is binary, we need to use the function glmer() instead of lmer(). We moreover need to specify the family argument.\n\n\n\n\nAlthough we cannot estimate the actual player-specific random intercepts \\(B_{b}, C_{c}\\) and \\(P_{p},\\) we can estimate the deviations between these the overall global means \\(\\mu_{B}, \\mu_{C},\\) and \\(\\mu_{P}.\\) The following code pulls out the deviations for the catchers (i.e., the deviations \\(u_{c}^{(C)}\\)) and appends these values to\n\ntmp &lt;- ranef(multilevel_fit)\ncatcher_u &lt;- \n  data.frame(\n1    fielder_2 = as.integer(rownames(tmp[[\"fielder_2\"]])),\n    catcher_u = tmp[[\"fielder_2\"]][,1])\n\n\n1\n\nThe MLB Advanced Media IDs are saved as integers but rownames() returns them as a string.",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#sec-frame-war",
    "href": "lectures/lecture11.html#sec-frame-war",
    "title": "Lecture 11: Pitch Framing",
    "section": "Runs Saved Above Replacement",
    "text": "Runs Saved Above Replacement\nThe data table catcher_u estimates of the amount each catcher adds to log-odds of a called strike, after adjusting for the historical baseline and the other players, relative to a global average across a super-population of catchers. As we argued in Lecture 10, it is arguably more useful to compare each individual catcher’s performance to a well-defined replacement level rather than to a nebulously defined global average. Noting that most MLB teams carry two catchers on their active roster, we will sort catchers based on the total number of pitches received in the 2024 season. We define the top-60 as non-replacement level and the remaining catchers as replacement-level.\nThe following code determines the pitch count threshold for defining replacement-level. It then computes the average of the estimated deviations \\(u^{(C)}_{c}\\)’s across all replacement-level catchers. We will denote this average by \\(\\overline{u}^{(C)}_{R}.\\)\n\ncatcher_counts &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarise(count = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(count))\n\ncatcher_threshold &lt;- \n  catcher_counts |&gt;\n  dplyr::slice(60) |&gt;\n  dplyr::pull(count)\n\ncatcher_u &lt;-\n  catcher_u |&gt;\n  dplyr::left_join(y = catcher_counts, by = \"fielder_2\")\n\nrepl_u &lt;-\n  catcher_u |&gt;\n  dplyr::filter(count &lt; catcher_threshold) |&gt;\n  dplyr::pull(catcher_u) |&gt;\n  mean()\n\nFor every pitch thrown in 2024, we can estimate how the called strike probability changes when the original catcher is replaced with a replacement-level catcher. First, observe that the log-odds of a called strike on pitch \\(i\\) is given by \\[\n\\mu_{C} + u_{c[i]}^{(C)} + B_{b[i]} + P_{p[i]} + \\hat{\\beta}_{p} \\times \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})} \\right)\n\\] We can compute these predicted log-odds using the predict() function and append them to the data table taken2024:\n\nml_preds &lt;-\n  predict(object = multilevel_fit,\n          newdata = taken2024,\n1          type = \"link\")\ntaken2024 &lt;- \n  taken2024 |&gt;\n  dplyr::mutate(fitted_log_odds = ml_preds)\n\n\n1\n\nSince we want the fitted log-odds and not the fitted probabilities, we specify type = \"link\".\n\n\n\n\nIf we replace the original catcher \\(c[i]\\) with a replacement-level catcher, then the log-odds of a called strike is now \\[\n\\mu_{C} + \\overline{u}_{R}^{(C)} + B_{b[i]} + P_{p[i]} + \\hat{\\beta}_{p} \\times \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})} \\right)\n\\] To compute the counter-factual log-odds, we need to take the fitted log-odds, subtract the original catcher’s deviation \\(u_{c[i]}^{(C)}\\), and add the average deviation across all replacement-level catchers (i.e., \\(\\overline{u}_{R}^{(C)}\\)). The following code does this by first appending the estimated \\(u^{(C)}_{c[i]}\\) to each row of taken2024 and then computing the counter-factual log-odds\n\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::left_join(y = catcher_u |&gt; \n                     dplyr::select(fielder_2, catcher_u) |&gt;\n1                     dplyr::mutate(fielder_2 = factor(fielder_2)),\n                   by = \"fielder_2\") |&gt;\n  dplyr::mutate(repl_log_odds = fitted_log_odds - catcher_u + repl_u) \n\n\n1\n\nfielder_2 is a factor variable in taken2024 but a numeric variable in catcher_u. By converting it to a factor, we ensure that the join is successful\n\n\n\n\nThe number of expected runs a catcher saves through his framing relative to a replacement-level catcher is the product of the value of a called strike multiplied by the actual and counter-factual called strike probability.\n\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::mutate(\n    fitted_prob = 1/(1 + exp(-1 * fitted_log_odds)),\n    repl_prob = 1/(1 + exp(-1 * repl_log_odds)),\n    rsar = value * (fitted_prob - repl_prob))\n\nSumming over all pitches received by a catcher over the season, we can compute his total number of runs saved above replacement. In the following code, we load in the player look-up table we created in Lecture 6\n\nload(\"player2024_lookup.RData\")\n\nrsar &lt;-\n  taken2024 |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarise(rsar = sum(rsar), n = dplyr::n()) |&gt; \n  dplyr::rename(key_mlbam = fielder_2) |&gt;\n  dplyr::left_join(player2024_lookup |&gt;\n                     dplyr::select(key_mlbam, Name) |&gt;\n                     dplyr::mutate(key_mlbam = factor(key_mlbam)), by = \"key_mlbam\")\n\nThere is considerable overlap between the top-10 catchers according to our framing runs saved above replacement metric and the top-10 catchers according to rankings produced by Baseball Savant. Both models, for instance, identify Patrick Bailey, who won a Golden Glove award in 2024, as a particularly strong framer.\n\nrsar |&gt;\n  dplyr::arrange(dplyr::desc(rsar)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::select(Name, rsar, n)\n\n# A tibble: 10 × 3\n   Name               rsar     n\n   &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n 1 Patrick Bailey     30.1  6118\n 2 Cal Raleigh        19.9  6449\n 3 Austin Wells       17.9  5540\n 4 Alejandro Kirk     17.3  4761\n 5 Jake Rogers        16.7  4417\n 6 Christian Vazquez  15.2  4541\n 7 Jose Trevino       14.6  3639\n 8 Francisco Alvarez  13.6  4629\n 9 Bo Naylor          11.6  5639\n10 Yasmani Grandal    11.0  3535",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#footnotes",
    "href": "lectures/lecture11.html#footnotes",
    "title": "Lecture 11: Pitch Framing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThat is, away from the batter.↩︎\nThe strike zone is defined as “the area over home plate from the midpoint between a batter’s shoulders and the top of the uniform pants – when the batter is in his stance and prepared to swing at a pitched ball – and a point just below the kneecap.” See here.↩︎\nSee this blogpost for a compilation of posts.↩︎\nThe numbers of balls and strikes previously called in the at-bat.↩︎\nThese are pitches in which the batter doesn’t swing.↩︎\nIf \\(u = \\log{\\frac{p}{1-p}}\\) for some \\(p \\in [0,1],\\) then \\(p = 1/[1 + e^{-u}]\\), which is monotonically increasing as↩︎\nTry to prove yourself by considering four cases, one for each combination of the sign of \\(\\beta_{x}\\) and \\(\\beta_{z}\\) in the model above.↩︎\nHome plate measures 17 inches across, so we set the horizontal limits at -8.5/12 and 8.5/12. To set the vertical limits, we take the average values of the StatCast variable sz_top and sz_bot, which are manually determined by the StatCast operator on every pitch↩︎\nThe strategy to include a suitably-transformed baseline probability estimates was first used by Judge, Pavlidis, and Brooks in a 2015 Baseball Prospectus article. It was also used by Deshpande and Wyner (2017) (see Section 2.2 of that paper).↩︎",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture09.html",
    "href": "lectures/lecture09.html",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "",
    "text": "Which of the two touchdown plays is more impressive?\n\nThe 86-yard touchdown pass from Justin Herbert to Ladd McConckey (video link)\nThe 64-yard touchdown pass from Cooper Rush to KaVontae Turpin (video link)\n\nWhile these two plays share many similarities — they were both thrown on 3rd down, out of the shotgun formation with no running back, against the Houston Texans, when the team on offense was trailing, and resulted in a score — there are some big differences: Herbert and the Chargers needed 26 yards to gain a first down while the Cowboys need only 10; Herber threw his pass from a collapsing pocket while Rush threw with essentially no pressure; McConckey broke at least one tackle after catching the pass. Based on these qualitative differences, we might view the first touchdown pass as more impressive. In this lecture, we introduce expected points (EP; Section 3) and expected points added (EPA), which together facilitate a more nuanced, quantitative comparison of plays. Then, using a multilevel model (Section 5), we will identify the players whom we would expect to generate the most expected points per passing attempt.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-intro",
    "href": "lectures/lecture09.html#sec-intro",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "",
    "text": "Which of the two touchdown plays is more impressive?\n\nThe 86-yard touchdown pass from Justin Herbert to Ladd McConckey (video link)\nThe 64-yard touchdown pass from Cooper Rush to KaVontae Turpin (video link)\n\nWhile these two plays share many similarities — they were both thrown on 3rd down, out of the shotgun formation with no running back, against the Houston Texans, when the team on offense was trailing, and resulted in a score — there are some big differences: Herbert and the Chargers needed 26 yards to gain a first down while the Cowboys need only 10; Herber threw his pass from a collapsing pocket while Rush threw with essentially no pressure; McConckey broke at least one tackle after catching the pass. Based on these qualitative differences, we might view the first touchdown pass as more impressive. In this lecture, we introduce expected points (EP; Section 3) and expected points added (EPA), which together facilitate a more nuanced, quantitative comparison of plays. Then, using a multilevel model (Section 5), we will identify the players whom we would expect to generate the most expected points per passing attempt.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-nflfastr-intro",
    "href": "lectures/lecture09.html#sec-nflfastr-intro",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Play-By-Play Football Data",
    "text": "Play-By-Play Football Data\nWe will use play-by-play data from the NFL available from the nflfastR package, which can be installed using the code:\n\ninstall.packages(\"nflfastR\")\n\nThe package contains several functions for efficiently scraping NFL data and for pre-processing the play-by-play data. In this lecture and in Lecture 10, we will work with pre-processed play-by-play data, which we can load into our R session using the function nflfastR::load_pbp().\n\npbp2024 &lt;- nflfastR::load_pbp(season = 2024)\n\nEach row of the table pbp2024 corresponds to an individual play from the 2024-25 NFL season. The data table contains information about 49492 distinct plays. Some columns of pbp2024 contain game-level information including\n\na unique identifier for the game (game_id);\nthe week in which the game was played (week);\nwhether the play took place in a regular season game (season_type = \"REG\") or play-off game (season_type=\"POST\");\nthe identities of the home and away team (home_team and away_team).\n\nSeveral columns record information about the state of the game just before the start of the play including\n\nwhich team has possession of the ball (posteam) and which team is on defense (defteam)\nthe scores of the two teams (posteam_score and defteam_score)\nthe amount of time left in the quarter (time)\nthe location of the ball at the start of the play (side_of_field and yardline_100)\nthe down and distance to the first down marker (down, ydstogo).\n\nThe table also contains columns recording information about the state of the game as soon as the play ends including the scores of both teams (posteam_score_post and defteam_score_post) and the number of yards gained on the play (ydsgained) It finally contains a large number of columns that record exactly what happened during the play including\n\nthe type of play (e.g., run, pass, kickoff, punt, etc.; play_type)\na narrative description of the play (desc)\nseveral self-explanatory indicators of what happened (e.g., fumble, complete_pass, and passing_yards).\n\nYou can find a full listing of the variableshere.\nnflfastR identifies players using a unique 9-digit identifier provided by Game Statistics and Information Systems (GSIS). We can load a table containing roster information, including player identifier, name, position, and team using the function nflfastR::fast_scraper_roster()\n\nroster2024 &lt;-\n  nflfastR::fast_scraper_roster(seasons = 2024)",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-ep",
    "href": "lectures/lecture09.html#sec-ep",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Expected Points",
    "text": "Expected Points\nParaphrasing Yurko, Ventura, and Horowitz (2019), the expected points framework uses historical data to estimate the average number of points eventually scored by teams in similar situations. Similar to what we did in Lecture 6 with expected runs at the start and end of at-bats in baseball, the difference between the post-snap and pre-snap expected points is known as expected points added (EPA). Generally speaking, plays that result in positive EPA are considered successful while plays resulting in negative EPA are not.\nYurko, Ventura, and Horowitz (2019) introduced an expected points model that focused on predicting the next scoring event in the half after each play. The “next score” responses, ignoring points after touchdown (PAT) attempts, are, with respect to the team in possession:\n\nTouchdown (TD), which is worth 7 points\nField goal (FG), which is worth 3 points\nSafety (SAF), which is worth 2 points\nNo score (NS), which worth 0 points\nOpponent safety (oSAF), which is worth -2 points\nOpponent field goal (oFG), which is worth -3 points\nOpponent touch down (oTD), which is worth -7 points\n\nFormally, for every play, we can collect features summarizing the state of the game in the \\(\\boldsymbol{\\mathbf{Z}}.\\) Typically, these features will include things like the time left in the half, the current score, the down and distance to the first down line, etc.\n\n\n\n\n\n\nDefinition: Drive Outcome Probabilities\n\n\n\nGiven the state of the game at the beginning of a play \\(\\boldsymbol{\\mathbf{z}},\\) within a drive, for each possible outcome \\(k \\in \\left\\{\\textrm{TD}, \\textrm{FG}, \\ldots, \\textrm{oTD}\\right\\}\\) let \\(\\pi_{k}(\\boldsymbol{\\mathbf{z}})\\) conditional probability that the drives containing plays characterized by \\(\\boldsymbol{\\mathbf{z}}\\) end in outcome \\(k.\\) We collect these probabilities into a vector \\(\\boldsymbol{\\pi}(\\boldsymbol{\\mathbf{z}}) = (\\pi_{\\textrm{TD}}(\\boldsymbol{\\mathbf{z}}), \\ldots, \\pi_{\\textrm{oppFG}}(\\boldsymbol{\\mathbf{z}})).\\)\n\n\n\n\n\n\n\n\nDefinition: Expected Points\n\n\n\nGiven a game state feature vector \\(\\boldsymbol{\\mathbf{z}}\\) and vector of drive outcome probabilities \\(\\boldsymbol{\\pi}(\\boldsymbol{\\mathbf{z}}),\\) the expected points \\(\\textrm{EP}(\\boldsymbol{\\mathbf{z}})\\) is \\[\n\\begin{align}\n\\textrm{EP}(\\boldsymbol{\\mathbf{z}}) &=  7\\times\\pi_{\\textrm{TD}}(\\boldsymbol{\\mathbf{z}}) +\n3\\times\\pi_{\\textrm{FG}}(\\boldsymbol{\\mathbf{z}}) +\n2\\times\\pi_{\\textrm{SAF}}(\\boldsymbol{\\mathbf{z}}) \\\\\n~&~~-2\\times\\pi_{\\textrm{oSAF}}(\\boldsymbol{\\mathbf{z}}) -\n3\\times\\pi_{\\textrm{oFG}}(\\boldsymbol{\\mathbf{z}})\n- 7\\times\\pi_{\\textrm{oTD}}(\\boldsymbol{\\mathbf{z}})\n\\end{align}\n\\]\n\n\n\nEstimating \\(\\textrm{EP}(\\boldsymbol{\\mathbf{z}})\\)\nnflfastR implements a version of Yurko, Ventura, and Horowitz (2019)’s original EP model. At a high-level, it is a multinomial classification model built using extreme gradient boosting (xgboost). Like random forests, xgboost builds an ensemble of decision trees, each of which map a vector of game state feature \\(\\boldsymbol{\\mathbf{x}}\\) to one of the next score outcomes \\(k.\\) The final prediction is formed using the proportions of trees outputting a certain outcome. More details are about nflfastR’s EP model are available here.\n\n\nBasic EPA Uses\nFor each play in pbp2024, the column epa records the expected points added on the play. It turns out that the Herbert-to-McConckey touchdown introduced in Section 1 was the play with the largest EPA across the entire 2024-25 regular season. At the start of the play, the Chargers were facing 3rd and 26 from their 14 yard line. According to nflfastR’s EP model, teams starting from the same position score about -1.54 points. The Chargers ended the play with a touchdown, which corresponds to a terminal game state with an EP of 7. Thus, their EPA on the play was 7-(-1.54) = 8.54.\n\npbp2024 |&gt;\n1  dplyr::slice_max(epa) |&gt;\n  dplyr::select(ep, desc)\n\n\n1\n\nslice_max() extracts the row with the largest value in a column\n\n\n\n\n# A tibble: 1 × 2\n     ep desc                                                                    \n  &lt;dbl&gt; &lt;chr&gt;                                                                   \n1 -1.54 (10:50) (Shotgun) 10-J.Herbert pass deep middle to 15-L.McConkey for 86…\n\n\nIt turns out that Turpin’s 64-yard touchdown was Dallas Cowboys’ passing play with the largest EPA across the whole season.\n\npbp2024 |&gt;\n  dplyr::filter(posteam==\"DAL\" & play_type == \"pass\") |&gt;\n  dplyr::slice_max(epa) |&gt;\n  dplyr::select(ep, desc)\n\n# A tibble: 1 × 2\n     ep desc                                                                    \n  &lt;dbl&gt; &lt;chr&gt;                                                                   \n1 0.775 (15:00) (Shotgun) 10-C.Rush pass short right to 9-K.Turpin for 64 yards…\n\n\nFacing 3rd down with 10 yards to go from their own 36 yard line, the Cowboys were expected to score about 0.77 points at the start of the play. By scoring a touchdown, they gained 7-0.77 = 6.23. Because the Chargers scored from a much less favorable starting point than the Cowboys in terms of EP, one could argue that the 86-yard touchdown is more impressive than the 64-yard touchdown.\nBeyond comparing individual plays, we can rank different teams’ offenses by their average EPA per play\n\npbp2024 |&gt;\n1  dplyr::group_by(posteam) |&gt;\n2  dplyr::summarize(epa = mean(epa, na.rm = TRUE)) |&gt;\n  dplyr::arrange(desc(epa)) |&gt;\n3  dplyr::slice(c(1:5, (dplyr::n()-4):(dplyr::n())))\n\n\n1\n\nDivide plays by offensive team\n\n2\n\nCompute the average EPA for each team’s offense\n\n3\n\nShow only the top-5 and bottom-5 teams in terms of average EPA per play\n\n\n\n\n# A tibble: 10 × 2\n   posteam     epa\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 BAL      0.143 \n 2 BUF      0.141 \n 3 DET      0.135 \n 4 WAS      0.115 \n 5 TB       0.103 \n 6 NE      -0.0643\n 7 NYG     -0.0829\n 8 TEN     -0.0896\n 9 LV      -0.107 \n10 CLE     -0.151 \n\n\nInterestingly, the top three offenses in terms of average EPA (the Baltimore Ravens, Buffalo Bills, and Detroit Lions) were also the three highest scoring offenses in the 2024 season, each scoring over 30 points per game, on average.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-newpass",
    "href": "lectures/lecture09.html#sec-newpass",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Predicting EPA On A New Pass",
    "text": "Predicting EPA On A New Pass\nBased on the 2024-25 regular season data, how much EPA would we expect to see on a hypothetical future passing play? Without any additional information about the play — i.e., who threw the pass, how long the pass was, whether it was thrown out of the shotgun formation, etc. — it seems reasonable to use the average EPA across all passing plays in our data set. But, if we knew the identity of the passers, we might reasonably expect the average EPA across their passes would yield better predictions.\nTo compute the league-wide and player-specific average EPAs, we first create a data table containing only the passing plays from the regular season. We can do this by filtering on the variables play_type and season_type. We see that there are about 19,000 pass plays in the regular season.\n\ntable(pbp2024[,c(\"play_type\", \"season_type\")])\n\n             season_type\nplay_type      POST   REG\n  extra_point    61  1241\n  field_goal     51  1115\n  kickoff       146  2803\n  no_play       205  4729\n  pass          853 19153\n  punt           73  2046\n  qb_kneel       32   405\n  qb_spike        4    71\n  run           726 14318\n\n\nIn addition to EPA, we also include the unique identifier of the passer (passer_player_id) and a narrative description of the play (desc).\n\npass2024 &lt;- \npass2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(play_type == \"pass\" & season_type == \"REG\") |&gt; \n1  dplyr::filter(!grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc) &\n                  !grepl(\"sacked\", desc)) |&gt;\n  dplyr::select(epa, passer_player_id, desc)\npass2024 |&gt; dplyr::slice_head(n = 2)\n\n\n1\n\nFor now, we will ignore 2-point conversion passing attempts. We also ignore sacks, which nflfastR sometimes treat as passing plays\n\n\n\n\n── nflverse play by play data ──────────────────────────────────────────────────\n\n\nℹ Data updated: 2025-08-11 15:43:02 EDT\n\n\n# A tibble: 2 × 3\n    epa passer_player_id desc                                                   \n  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;                                                  \n1 2.03  00-0035228       (14:27) 1-K.Murray pass short left to 6-J.Conner to BU…\n2 0.754 00-0035228       (13:43) (Shotgun) 1-K.Murray pass short middle to 6-J.…\n\n\nAfter excluding passes on two-point conversion attempts and sacks, we have data for 17727 passes thrown by 103 players. Across all passes and passers, the average EPA per pass is about 0.159.\n\nmean(pass2024$epa)\n\n[1] 0.1589677\n\n\n\nPlayer-Specific EPA/Pass\nGiven the vast spread of EPAs in our dataset (roughly -12.7 to 8.5), using the league-wide average of 0.159 to predict the EPA on a future pass is rather unsatisfactory. One avenue for improvement is to condition on additional information like the identity of the passer, which is recorded in the column passer_player_id. Intuitively, we might expect a superstar quarterback like Patrick Mahomes to generate more EPA per pass than Daniel Jones, who was benched and released by his team mid-way through the season.\nWe will model the observed EPA on each passing play as a noisy observation of an underlying average EPA specific to each passer. So, the EPA’s on passes thrown by Dak Prescott, for instance, are treated as noisy measurements of his average EPA. We can then predict the EPA on a future pass thrown by Prescott using an estimate of his per-pass EPA. We can further rank passers by their per-pass EPAs.\nFormally, suppose that we have data for \\(I\\) unique passers1 and for each passer \\(i = 1, \\ldots, I,\\) let \\(n_{i}\\) be the total number of passes thrown by passer \\(i.\\) Further, for each \\(j = 1, \\ldots, n_{i},\\) let \\(Y_{ij}\\) be the EPA on the the \\(j\\)-th pass thrown by passer \\(i.\\) Let \\(\\alpha_{i}\\) be the underlying — and as yet unknown — average EPA for passer \\(i.\\) We model \\(Y_{ij} = \\alpha_{i} + \\epsilon_{ij}\\) where the random errors \\(\\epsilon_{ij}\\) have mean zero and are assumed to be independent across passers and passes.\nWhile we can certainly run this calculation by grouping plays in pass2024 by passer_player_id and then computing the average EPA of the plays involving each passer, for reasons that will hopefully become clear soon, we will instead estimate the \\(\\alpha_{i}\\)’s by fitting a linear regression model with ordinary least squares. Specifically, we will regress epa onto the categorical variable passer_player_id, which we will represent as a factor variable. When run with a single categorical predictor, function lm() does not directly estimate the average outcome within each level of the predictor. Instead, it returns the average outcome within one reference level of the predictor and the average difference in outcomes between the non-reference levels and the reference. We will, rather arbitrarily, use Aaron Rodgers as the reference player.\n\nrodgers_id &lt;- \n  roster2024 |&gt; \n  dplyr::filter(full_name == \"Aaron Rodgers\") |&gt; \n1  dplyr::pull(gsis_id)\n\npass2024 &lt;-\n  pass2024 |&gt;\n  dplyr::mutate(\n    passer_player_id = factor(passer_player_id),\n2    passer_player_id = relevel(passer_player_id, ref = rodgers_id))\n\n\n1\n\nPull out Aaron Rodgers’ id number\n\n2\n\nSet the reference level to Rodgers\n\n\n\n\nWe can now fit our linear model and collect the estimated parameters in the vector ols_betas. Notice that the names of all but the first element of ols_betas contain the string “passer_player_id” and a player identifier.\n\nols_fit &lt;-\n1  lm(formula = epa ~ 1 + passer_player_id,\n2     data = pass2024)\nols_betas &lt;- coefficients(ols_fit)\n3ols_betas[1:5]\n\n\n1\n\nThe 1+ is not strictly necessary (since R usually includes an intercept by default).\n\n2\n\nBecause the formula argument only involves epa and passer_player_id, none of the other variables in pass2024 are included as outcomes or predictors in the model\n\n3\n\nPrint out a handful of parameter estimates\n\n\n\n\n               (Intercept) passer_player_id00-0026158 \n                0.11303340                 0.04427535 \npasser_player_id00-0026300 passer_player_id00-0026498 \n               -0.32316791                 0.05621013 \npasser_player_id00-0027973 \n               -0.19838613 \n\n\nIn our simple EPA model, the estimated intercept is exactly the average EPA on all passes thrown by Aaron Rodgers.\n\nmean( pass2024$epa[pass2024$passer_player_id == rodgers_id])\n\n[1] 0.1130334\n\n\nTo get the average EPA on all passes thrown by a different quarterback (e.g., Dak Prescott whose id is 00-0033077), we need to add the corresponding entry in ols_beta to the estimated intercept2.\n\nmean(pass2024$epa[pass2024$passer_player_id == \"00-0033077\"]) \n\n[1] 0.05691811\n\nols_betas[\"passer_player_id00-0033077\"] + ols_betas[\"(Intercept)\"]\n\npasser_player_id00-0033077 \n                0.05691811 \n\n\nWe will repeat this calculation for each of the 101 unique passers in our data set and build a table alphas with columns containing the player’s identifier, name, and average EPA on passing plays. In the following code, we strip out the string “passer_player_id” from the names of ols_beta.\n\n1names(ols_betas)[1] &lt;- paste0(\"passser_player_id\", rodgers_id)\nnames(ols_betas) &lt;- \n2  stringr::str_remove(string = names(ols_betas), pattern = \"passer_player_id\")\nalphas &lt;- \n3  data.frame(gsis_id = names(ols_betas), ols = ols_betas) |&gt;\n4  dplyr::mutate(ols = ifelse(gsis_id == rodgers_id, ols, ols + dplyr::first(ols))) |&gt;\n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\")\n\n\n1\n\nRename first element of ols_beta so its name follows the same format as the names of all other elements\n\n2\n\nRemove the string “passer_player_id” from all names of ols_beta\n\n3\n\nFacilitate a later join with roster2024, which records player identifiers as gsis_id\n\n4\n\nSince the first row of alphas corresponds to Aaron Rodgers, we need to add the first element in the column ols to all other elements in the column.\n\n\n\n\nSomewhat surprisingly, the top- and bottom-five passers based on average EPA per play are not quarterbacks but include kickers. What’s going on??\n\nalphas |&gt;\n  dplyr::arrange(dplyr::desc(ols)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n())) |&gt;\n  dplyr::select(full_name, ols)\n\n          full_name       ols\n1           AJ Cole  4.333918\n2  Courtland Sutton  3.745369\n3          Jack Fox  3.637429\n4  Justin Jefferson  3.050907\n5      Stefon Diggs  2.762919\n6   Miles Killebrew -2.677720\n7        J.K. Scott -2.706606\n8       Bryan Anger -2.843511\n9     Johnny Hekker -2.856017\n10     Keenan Allen -5.911163\n\n\n\n\nIssues\nBy this point in the course, you probably already suspect what’s going on: the extreme average EPA estimates are artifacts of small sample sizes. To verify that this is so, we’ll add a column to alphas recording the number of passes thrown by each player.\n\nn_passes &lt;-\n  pass2024 |&gt;\n  dplyr::group_by(passer_player_id) |&gt; \n1  dplyr::summarise(n = dplyr::n()) |&gt;\n2  dplyr::rename(gsis_id = passer_player_id)\n\nalphas &lt;-\n  alphas |&gt;\n  dplyr::inner_join(y = n_passes, by = \"gsis_id\")\n\nalphas |&gt;\n  dplyr::arrange(dplyr::desc(ols)) |&gt;\n3  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n\n1\n\nCounts the number of passes thrown by each player in our dataset\n\n2\n\nSo that we can join the counts to alphas, we need to rename the column recording the player identifiers\n\n3\n\nLook at the top-5 and bottom-5 rows\n\n\n\n\n      gsis_id       ols        full_name n\n1  00-0035190  4.333918          AJ Cole 1\n2  00-0034348  3.745369 Courtland Sutton 2\n3  00-0035156  3.637429         Jack Fox 1\n4  00-0036322  3.050907 Justin Jefferson 1\n5  00-0031588  2.762919     Stefon Diggs 1\n6  00-0032399 -2.677720  Miles Killebrew 1\n7  00-0034162 -2.706606       J.K. Scott 1\n8  00-0029692 -2.843511      Bryan Anger 2\n9  00-0028872 -2.856017    Johnny Hekker 2\n10 00-0030279 -5.911163     Keenan Allen 1\n\n\nAs anticipated, the passers with the top- and bottom-5 per-pass EPA all threw 1 or 2 passes. For instance, AJ Cole is a punter who threw a single pass that ultimately gained 34 yards and resulted in a very large EPA of around 4.33.\n\npass2024 |&gt;\n  dplyr::filter(passer_player_id == \"00-0035190\") |&gt;\n  dplyr::select(desc, epa)\n\n── nflverse play by play data ──────────────────────────────────────────────────\n\n\nℹ Data updated: 2025-04-30 02:39:06 EDT\n\n\n# A tibble: 1 × 2\n  desc                                                                       epa\n  &lt;chr&gt;                                                                    &lt;dbl&gt;\n1 (8:33) (Punt formation) 6-A.Cole pass short right to 5-D.Deablo to DEN …  4.33\n\n\nPlotting each player’s average EPA against the number of pass attempts reveals that there is a lot of (resp. very little) variation in EPA-per-pass among passers who threw a very small (resp. very large) number of passes.\n\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(alphas$n, alphas$ols, \n     xlab = \"Number of passes\",\n     ylab = \"Avg. EPA per pass\",\n     main = \"EPA per pass\",\n     pch = 16, cex = 0.5, col = oi_colors[1])\nabline(h = mean(pass2024$epa), col = oi_colors[3])\n\n\n\n\n\n\n\nFigure 1: The variability in per-pass decreases dramatically as the number of passes increases.\n\n\n\n\n\n\n\nPartial Pooling\nLet’s return to the problem of forecasting EPA on a single new pass based solely on the identity of the passer. One option is to ignore the passer identity and just use the overall mean of about 0.159. Doing so completely ignores differences between passers, which is fairly unreasonable given the large gaps in talent between NFL quarterbacks. At the other extreme, we could use the player-specific average EPA estimates. While this approach acknowledges differences between players, it can sometimes lead to very extreme estimates. Neither option seems particularly compelling.\nAs with most things, an ideal solution lies somewhere between the two extremes. For players who threw a large number of passes, we may be more comfortable using the their average EPA value since we are more certain about its value3. But for players who threw a very small number of passes, it’s probably safe to assume that they’ll perform closer to the league-average rather than to rely on their potentially very noisy average EPA estimate4. And for players who threw a moderate number of passes, we might prefer a value somewhere between the league-wide average and the player-specific estimate.\nFormally, we can make such predictions with a weighted average: letting \\(\\overline{y}\\) be the overall league-wide average, we can predict the EPA on a new pass thrown by player \\(i\\) using \\[\nw_{i}\\times \\hat{\\alpha}_{i} + (1 - w_{i}) \\times \\overline{y},\n\\] where the weight \\(w_{i}\\) depends on the number of passes thrown by player \\(i\\) and how far \\(\\hat{\\alpha}_{i}\\) differs from \\(\\overline{y}.\\) Ideally, if a player \\(i\\) throws a large number of passes the weight \\(w_{i}\\) assigned to his empirical average EPA would be close to 1 and and if he threw very few passes, the weight would be set much closer to 0. The hope is, in other words, that we could differentially shrink every player’s raw average per-pass EPA towards the grand mean.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-multilevel-model",
    "href": "lectures/lecture09.html#sec-multilevel-model",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Multilevel Models",
    "text": "Multilevel Models\nMultilevel models provide a principled way to compute these weights automatically. Like in our initial analysis, our first multilevel will model the observed EPA’s as noisy measurements of some underlying player-specific parameter. However, we make an additional model assumption: that the \\(\\alpha_{i}\\)’s are themselves normally distributed around some grand mean \\(\\alpha_{0}\\) with standard deviation \\(\\sigma_{\\alpha}.\\) We can specify our model in two levels: \\[\n\\begin{align}\n\\textrm{Level 1}&: &\\quad  Y_{ij} &= \\alpha_{i} + \\epsilon_{ij}; \\epsilon_{ij} \\sim N(0, \\sigma^{2}) \\quad \\textrm{for all}\\ j = 1, \\ldots, n_{i},\\ i = 1, \\ldots, I \\\\\n\\textrm{Level 2}&: &\\quad \\alpha_{i} &= \\alpha_{0} + u_{i}; u_{i} \\sim N(0, \\sigma^{2}_{\\alpha}) \\quad \\textrm{for all}\\ i = 1, \\ldots, I\n\\end{align}\n\\]\nThe Level 1 model says that for each passer \\(i,\\) the observed EPAs on his passing plays are normally distributed around his latent averge EPA per play \\(\\alpha_{i}.\\) The parameter \\(\\sigma\\) quantifies the amount by which we expect each passer’s EPAs to deviate from his latent average. In our model, we assume that this “within-passer” variability is the same across passers.\nIn Level 2 of our model, the responses (\\(\\alpha_{i}\\)) are not observed quantities but rather unknown regression coefficients that appear in the Level 1 model. Level 2 of our model assumes that the passers constitute a random sample from some super-population of passers in which latent per-pass EPAs follow a normal distribution. The parameter \\(\\alpha_{0}\\) represents the global average per-pass EPA value across this super-population. The parameter \\(\\sigma_{\\alpha}\\) captures the average deviation between each passer’s latent EPA per-play and the global average.\nTo see why a multilevel model might make sense here, imagine trying to make a prediction about the EPA on a pass thrown by a new player not already in our original dataset. If we fit the usual multiple linear regression model (i.e., only the Level 1 model and ignored the grouping structure & Level 2), there is really no way for us to make a principled prediction about his latent EPA per play. But with a multilevel model, if we’re willing to assume that the player is also drawn from the same super-population of passers, we can predict his latent EPA per play by drawing from a \\(N(\\hat{\\alpha}_{0}, \\hat{\\sigma}^{2}_{\\alpha})\\) distribution. Effectively, multilevel models allow us to “borrow information” across different groups.\nIn general, multilevel models are used when observations in a dataset display hierarchical grouping structure (e.g., passes grouped by quarterbacks, students in classrooms in schools, etc.). Often in these settings, the grouping variables are believed to be relevant to the outcome of interest and can induce correlation between observations within the same group. Moreover, we might expect that outcomes from the same group to be more tightly clustered around a group-level average than around the overall grand mean. For instance, if a particular medical practice uses a new, highly effective treatment strategy, knowing the outcome of one patient from that practice who received the treatment provides a lot of information about the potential outcome of another patient from that practice who also received the treatment. As we will soon see, the standard multiple linear regression model with which you are already familiar tends not to perform well in the presence of grouping structure, largely because it assumes observations are independent.\n\n\n\n\n\n\nWarning\n\n\n\nThe above is by no means a comprehensive review of multilevel models, which is beyond the scope of this course. I highly recommend Chapter 8 of the book Beyond Multiple Linear Regression. I also recommend this blog post, which discusses how the R syntax maps onto the multilevel model notation.\n\n\n\nFitting Multilevel Models\nWe can fit multilevel models using the lme4 package, which can be installed using the code\n\ninstall.packages(\"lme4\")\n\nThis cheatsheet provides a really nice overview of the underlying concepts and the relevant R syntax.\n\nlibrary(lme4)\nmultilevel_fit &lt;-\n1  lmer(formula = epa ~ 1 + (1|passer_player_id),\n       data = pass2024)\n\n\n1\n\nThe term (1 | passer_play_id) signals to lmer() that there should be a separate random intercept for every passer\n\n\n\n\nThe model summary contains a lot of useful information about our model\n\nsummary(multilevel_fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: epa ~ 1 + (1 | passer_player_id)\n   Data: pass2024\n\nREML criterion at convergence: 65351.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.3587 -0.5309 -0.1400  0.5547  5.4923 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n passer_player_id (Intercept) 0.01518  0.1232  \n Residual                     2.33009  1.5265  \nNumber of obs: 17723, groups:  passer_player_id, 101\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.1297     0.0207   6.265\n\n\nFirst, we find the estimate of the overall grand mean EPA per play in the “Fixed effects” section: the average EPA across all passers and passes is estimated to be \\(\\hat{\\alpha}_{0} \\approx 0.1294.\\) The “Random effects” section reports our estimates \\(\\hat{\\sigma} \\approx 1.526\\) and \\(\\hat{\\sigma}_{\\alpha} = 0.123\\) of, respectively, the variability in EPA that we’d expect to see between passes thrown by the same player and the variability in latent EPA per pass across the different passers. The fact that \\(\\hat{\\sigma}_{\\alpha}\\) is so much smaller than \\(\\hat{\\sigma}\\) suggests that most of the variability in observed passing play EPA is driven more by noise than differences in latent player ability.\nWe can use the function ranef() to extract the estimates of the \\(u_{i}\\)’s, the function fixef to extract the estimate of \\(\\alpha_{0},\\) and the function coef() to extract the estimates of \\(\\alpha_{i} = \\alpha_{0} + u_{i}.\\) The function coef() returns a named list with one element per grouping variable5. Each element of coef()’s output is a data frame with named rows\n\ntmp &lt;- coef(multilevel_fit)\n1tmp[[\"passer_player_id\"]] |&gt; dplyr::slice_head(n = 5)\n\n\n1\n\nFor brevity, we only display the first 5 rows\n\n\n\n\n           (Intercept)\n00-0023459   0.1164973\n00-0026158   0.1467385\n00-0026300   0.1231416\n00-0026498   0.1601689\n00-0027973   0.0199101\n\n\nWe will append our multilevel estimates to our data table alpha\n\n1lmer_alpha &lt;-\n  data.frame( \n2    lmer = tmp[[\"passer_player_id\"]][,1],\n3    gsis_id = rownames(tmp[[\"passer_player_id\"]]))\n\nalphas &lt;-\n  alphas |&gt;\n  dplyr::inner_join(y = lmer_alpha, by = \"gsis_id\")\n\n\n1\n\nWe’ll create a data frame with columns containing the player identifier and their corresponding parameter estimate\n\n2\n\nIn this case, the output of coef is a matrix with 1 column whose row names are just the levels of the grouping variable\n\n3\n\nFor our eventual join, we will store the player’s identifier as gsis_id\n\n\n\n\nOur multilevel model returns very high per-pass EPA estimates for several top quarterbacks and very low estimates for quarterbacks widely regarded to have struggled in 2024-25.\n\nalphas |&gt;\n  dplyr::arrange(dplyr::desc(lmer)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n())) |&gt;\n  dplyr::select(full_name, lmer, n)\n\n                  full_name        lmer   n\n1             Lamar Jackson  0.36827643 469\n2                Jared Goff  0.33318327 536\n3                Josh Allen  0.27346854 482\n4                Joe Burrow  0.27175401 651\n5            Baker Mayfield  0.27066068 569\n6               Andy Dalton  0.01991010 160\n7                 Drew Lock  0.01613596 180\n8        Anthony Richardson  0.01212565 261\n9           Spencer Rattler -0.04063512 227\n10 Dorian Thompson-Robinson -0.13685136 118\n\n\nWe see that our multilevel estimates for players who threw very few passes are aggressively shrunk towards to the overall grand-mean.\n\n1alpha0_hat &lt;- fixef(multilevel_fit)[\"(Intercept)\"]\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(alphas$n, alphas$ols, \n     ylim = c(-4.5, 4.5),\n     xlab = \"Number of passes\",\n     ylab = \"Avg. EPA per pass\",\n     main = \"EPA per pass\",\n     pch = 16, cex = 0.5, col = oi_colors[1])\npoints(alphas$n, alphas$lmer, pch = 15, col = oi_colors[2], cex = 0.5)\nabline(h = alpha0_hat, col = oi_colors[3])\n\n\n1\n\nThe function fixef() returns all the fixed effects (see below) in a named vector. As this is an intercept-only model, we use the name “(Intercept)” to access the parameter estimate.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Although our multilevel model heavily shrink the raw per-pass EPA values (black dots) towards the overall sample mean (blue line), there is still player-to-player variability in the resulting estimates (orange).",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-covariates",
    "href": "lectures/lecture09.html#sec-covariates",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Adjusting For Additional Covariates",
    "text": "Adjusting For Additional Covariates\nUp to this point, we’ve worked with a rather toy problem, predicting EPA given only the identity of the passer. But what if we include additional information about the pass (e.g., whether it was throw by the home or away team, whether it was thrown out of the shotgun formation, etc.)? Do we still run into the small sample size issues when determining how much EPA an individual player adds, after accounting for this other information?\nTo answer these questions, we re-build our data table pass2024 but now include:\n\nair_yards: the distance traveled by the pass through the air6\nshotgun: whether the pass was thrown out of the shotgun formation\nqb_hit: whether the quarterback was hit while throwing\nno_huddle: whether the play was run without a huddle\nposteam_type: whether the home team had possession (i.e., was on offense)\npass_location: whether the pass was thrown to the left, right, or center of the field\n\n\npass2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(play_type == \"pass\" & season_type == \"REG\") |&gt; \n  dplyr::filter(!grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc) &\n                  !grepl(\"sacked\", desc)) |&gt;\n  dplyr::select(epa, passer_player_id, \n                air_yards, \n                posteam_type, shotgun, no_huddle, qb_hit,\n                pass_location,\n                desc)\npass2024 |&gt; dplyr::slice_head(n = 2)\n\n── nflverse play by play data ──────────────────────────────────────────────────\n\n\nℹ Data updated: 2025-09-01 05:28:06 EDT\n\n\n# A tibble: 2 × 9\n    epa passer_player_id air_yards posteam_type shotgun no_huddle qb_hit\n  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 2.03  00-0035228              -3 away               0         0      0\n2 0.754 00-0035228               2 away               1         0      0\n# ℹ 2 more variables: pass_location &lt;chr&gt;, desc &lt;chr&gt;\n\n\nFor pass \\(j\\) thrown by passer \\(i,\\) let \\(\\boldsymbol{\\mathbf{x}}_{ij}\\) be a vector containing the features air_yards, posteam_type, shotgun, no_huddle, qb_hit, and pass_location. We can elaborate our initial single-level regression model, which doesn’t account for the grouping structure, to adjust for the effects of these factors: \\[\nY_{ij} = \\alpha_{i} + \\boldsymbol{\\mathbf{x}}_{ij}^{\\top}\\boldsymbol{\\beta} + \\epsilon_{ij}\n\\] Under this model, \\(\\alpha_{i}\\) represents the amount of EPA that can be attributable to the passer over and above what can be explained by these factors. When we fit this single-level linear model using lm, we find that the estimated passer-specific intercepts display the exact same small sample size artifacts as our initial intercept-only model.\n\nols_fit_full &lt;- \n1  lm(epa ~ passer_player_id + air_yards + posteam_type + shotgun + no_huddle + qb_hit + pass_location,\n     data = pass2024)\nols_betas_full &lt;- coefficients(ols_fit_full)\n\nnames(ols_betas_full)[1] &lt;- paste0(\"passser_player_id\", rodgers_id) \nnames(ols_betas_full) &lt;- \n  stringr::str_remove(string = names(ols_betas_full), pattern = \"passer_player_id\") \nalphas_full &lt;- \n  data.frame(gsis_id = names(ols_betas_full), ols = ols_betas_full) |&gt; \n  dplyr::mutate(ols = ifelse(gsis_id == rodgers_id, ols, ols + dplyr::first(ols))) |&gt; \n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::inner_join(y = n_passes, by = \"gsis_id\")\n\nalphas_full |&gt;\n  dplyr::arrange(dplyr::desc(ols)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n())) |&gt;\n  dplyr::select(full_name, ols, n)\n\n\n1\n\nUse all the available information, including passer identity, and the features to predict epa\n\n\n\n\n          full_name       ols n\n1           AJ Cole  4.337054 1\n2  Courtland Sutton  3.702885 2\n3  Justin Jefferson  3.316711 1\n4          Jack Fox  3.264767 1\n5      Stefon Diggs  2.994487 1\n6        J.K. Scott -2.586283 1\n7   Miles Killebrew -2.913268 1\n8       Bryan Anger -2.969681 2\n9     Johnny Hekker -3.151710 2\n10     Keenan Allen -6.093688 1\n\n\nLuckily, we can similarly elaborate our multilevel model to account for the covariate effects. \\[\n\\begin{align}\n\\textrm{Level 1}&: &\\quad  Y_{ij} &= \\alpha_{i} + \\boldsymbol{\\mathbf{x}}_{ij}^{\\top}\\boldsymbol{\\beta} +  \\epsilon_{ij}; \\quad \\epsilon_{ij} \\sim N(0, \\sigma^{2}) \\quad \\textrm{for all}\\ j = 1, \\ldots, n_{i},\\ i = 1, \\ldots, I \\\\\n\\textrm{Level 2}&: &\\quad \\alpha_{i} &= \\alpha_{0} + u_{i}; \\quad u_{i} \\sim N(0, \\sigma^{2}_{\\alpha}) \\quad \\textrm{for all}\\ i = 1, \\ldots, I\n\\end{align}\n\\] Like our original multilevel model, Level 2 of our new model assumes that each player’s latent per-pass EPA is normally distributed around some grand mean \\(\\alpha_{0}.\\) However, it no longer assumes that the EPA on all passes thrown by passer \\(i\\) are clustered around the latent \\(\\alpha_{i}.\\) Instead, the average EPA on a pass thrown by passer \\(i\\) depends on several covariates and the player’s latent intercept. In our new model, the covariate effects are assumed to be constant across passers. So, for instance, keeping all other covariates the same, the difference in average EPA on 5 yard and 10 yard passes is the same whether the pass was thrown by Patrick Mahomes or Daniel Jones. Within the multilevel modeling literature, these effects are often called “fixed effects” while effects that can vary across groups (e.g., passers) are termed “random effects.” See Section 8.5.2 of Beyond Multiple Linear Regression and here for some discussion about the distinction.7\nIt is fairly easy to fit our elaborated multilevel model that accounts for fixed covariate effects:\n\nml_fit_full &lt;-\n  lme4::lmer(formula = epa ~ 1 + (1|passer_player_id) + \n               air_yards + posteam_type + shotgun +\n               no_huddle + qb_hit + pass_location, data = pass2024)\n\nLike with our initial random intercept model, we can read off several important parameter estimates using the model summary.\n\nsummary(ml_fit_full)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: epa ~ 1 + (1 | passer_player_id) + air_yards + posteam_type +  \n    shotgun + no_huddle + qb_hit + pass_location\n   Data: pass2024\n\nREML criterion at convergence: 65016.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.5225 -0.5478 -0.0841  0.5538  5.4631 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n passer_player_id (Intercept) 0.01344  0.1159  \n Residual                     2.28043  1.5101  \nNumber of obs: 17727, groups:  passer_player_id, 103\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)          0.133559   0.039314   3.397\nair_yards            0.016728   0.001132  14.772\nposteam_typehome    -0.010065   0.022883  -0.440\nshotgun             -0.109698   0.032192  -3.408\nno_huddle            0.021179   0.034703   0.610\nqb_hit              -0.470851   0.039760 -11.842\npass_locationmiddle  0.135755   0.030724   4.418\npass_locationright  -0.053875   0.025769  -2.091\n\nCorrelation of Fixed Effects:\n            (Intr) ar_yrd pstm_t shotgn n_hddl qb_hit pss_lctnm\nair_yards   -0.207                                             \npostm_typhm -0.280  0.012                                      \nshotgun     -0.676 -0.005 -0.011                               \nno_huddle   -0.039 -0.009 -0.024 -0.105                        \nqb_hit      -0.078 -0.067  0.008  0.005  0.011                 \npss_lctnmdd -0.240 -0.060 -0.013 -0.041 -0.002 -0.019          \npss_lctnrgh -0.348  0.007 -0.010  0.010 -0.003 -0.019  0.441   \n\n\nLooking first at the “Random effects” panel, we that our estimated residual standard deviation is \\(\\hat{\\sigma} \\approx 1.5101.\\) The estimate \\(\\hat{\\sigma}_{\\alpha} \\approx 0.1159\\) indicates that even after adjusting for constant covariate effects, there is little variability in the passer-to-passer latent per-pass EPA. The “Fixed effects” panel contains estimates for the global mean intercept \\(\\alpha_{0}\\) as well as the fixed covariate effects. We see, for instance, that passes thrown from the shotgun formation are expected to generate about 0.11 fewer expected points than passes thrown from under center, all else being equal.\nThe following code extracts the estimates of the player-specific latent per-pass EPAs and appends them to the data table alphas_full.\n\ntmp &lt;- coef(ml_fit_full)\nlmer_alpha_full &lt;- \n  data.frame( \n    lmer = tmp[[\"passer_player_id\"]][,1],\n    gsis_id = rownames(tmp[[\"passer_player_id\"]])) \nalphas_full &lt;-\n  alphas_full |&gt;\n  dplyr::inner_join(y = lmer_alpha_full, by = \"gsis_id\")\nalphas_full |&gt;\n  dplyr::arrange(dplyr::desc(lmer)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))  |&gt;\n  dplyr::select(full_name, lmer, n)\n\n                  full_name        lmer   n\n1             Lamar Jackson  0.34106738 469\n2                Jared Goff  0.32096595 536\n3                Joe Burrow  0.29762396 651\n4            Tua Tagovailoa  0.26988653 397\n5                Josh Allen  0.26577678 482\n6              Bailey Zappe  0.03321274  31\n7               Andy Dalton  0.02969282 160\n8        Anthony Richardson -0.01605454 261\n9           Spencer Rattler -0.03511495 227\n10 Dorian Thompson-Robinson -0.08925603 118\n\n\nWe can further visualize the shrinkage effect by comparing the multiple linear regression a multilevel model estimates of the passer-specific per-pass EPAs to the number of passing attempts.\n\nalpha0_hat &lt;- fixef(ml_fit_full)[\"(Intercept)\"]\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(alphas_full$n, alphas_full$ols, ylim = c(-0.75, 0.75), \n     xlab = \"Number of passes\",\n     ylab = \"EPA\",\n     main = \"EPA per pass\",\n     pch = 16, cex = 0.5, col = oi_colors[1])\npoints(alphas_full$n, alphas_full$lmer, pch = 15, col = oi_colors[2], cex = 0.5)\nabline(h = alpha0_hat, col = oi_colors[3])\n\n\n\n\n\n\n\nFigure 3: Even after adjusting for covariate effects, players with fewer passing attempts display much more variation in per-pass EPA than players with more passing attempts.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-next-time",
    "href": "lectures/lecture09.html#sec-next-time",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nUsing Expected Points Added (EPA), we can assess individual plays based on how the probability of different scoring outcomes changed over the course of the play. Armed with EPA, it is very tempting to rank passers in terms of the average EPA on their passing plays. But, as we have seen many times in the course, doing so can lead to extreme conclusions. Multilevel models offer a way to overcome these issues while also accounting for the inherent grouping structure.\nUsing a multilevel model, we estimated the average EP per pass each passer provides over and above what could be explained by covariates like air_yards and the formation from which the pass was thrown. In Lecture 10, we will fit a series of multilevel models to divide the EPA on a play between the different offensive players. We will use the model estimates to compute a version of wins above replacement for offensive players in the NFL.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#footnotes",
    "href": "lectures/lecture09.html#footnotes",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn our dataset \\(I=\\) 103.↩︎\nYou can prove that computing group-level averages is mathematically equivalent to fitting a linear regression model with a single categorical predictor encoding group membership. Try doing this yourself or come by office hours if you’d like to see the derivation.↩︎\nMathematically, we know that the average of 100 independent samples from some population has much more variance than the average of 10,000 independent samples from the same population.↩︎\nIndeed, if AJ Cole, who is a punter, were to throw many more passes, do we really think he’d consistently add around 4 expected points every time?↩︎\nFor now, we are only using one grouping variable. But later in Lecture 10, we will have multiple grouping variables.↩︎\nYurko, Ventura, and Horowitz (2019) defines air yards as “the perpendicular distance in yards from the line of scrimmage to the yard line at which the receiver was targeted or caught the ball.↩︎\nPersonally, I find the “fixed effects” and “random effects” terminology unhelpful. For one thing, the terms mean different things to different people! And, since I mostly work within the Bayesian paradigm, in which we use probability to model all of our uncertainty and not just to random noise or sampling variable, I could argue that everything is a “random.” I find it much more useful to think carefully about whether a covariate’s effect might vary across groups or whether it could be assumed to be constant. But, since you will likely see this terms in future courses or later in your careers, we’ll use them.↩︎",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture07.html",
    "href": "lectures/lecture07.html",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "",
    "text": "In Lecture 6, we computed the run value created by the offensive team in each at-bat of the 2024 MLB regular season. Run value is the sum of (i) the number of runs scored in the at-bat and (ii) the change in the number of runs the batting team is expected to score in the remainder of the half-inning. This change in expected runs is driven by the change in the combination of the number of outs and baserunner configuration. We then ranked players based on their run value totals, aggregating over all their at-bats. While the resulting rankings did appear to pass the “eye test” — both Aaron Judge and Shohei Ohtani created some of the largest run values — the metric implicitly gives batters all the credit for creating run value.\nOver the next two lectures, we will develop our own version of wins above replacement. Our development largely follows that of Baumer, Jensen, and Matthews (2015) but with some important differences.\n\n\nThe central idea — what (Baumer, Jensen, and Matthews 2015) call the “conservation of runs” framework — is that if the batting team gains \\(\\delta_{i}\\) units of run value during an at-bat, the fielding team gains \\(-\\delta_{i}\\) units of run value during that same at-bat. In this lecture, we will apportion \\(\\delta_{i}\\) between the batters (Section 4) and the baserunners involved in at-bat \\(i\\) (Section 5) In Lecture 8, we will apportion \\(-\\delta_{i}\\) between the pitcher and fielders involved in at-bat \\(i.\\)",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#overview",
    "href": "lectures/lecture07.html#overview",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "",
    "text": "In Lecture 6, we computed the run value created by the offensive team in each at-bat of the 2024 MLB regular season. Run value is the sum of (i) the number of runs scored in the at-bat and (ii) the change in the number of runs the batting team is expected to score in the remainder of the half-inning. This change in expected runs is driven by the change in the combination of the number of outs and baserunner configuration. We then ranked players based on their run value totals, aggregating over all their at-bats. While the resulting rankings did appear to pass the “eye test” — both Aaron Judge and Shohei Ohtani created some of the largest run values — the metric implicitly gives batters all the credit for creating run value.\nOver the next two lectures, we will develop our own version of wins above replacement. Our development largely follows that of Baumer, Jensen, and Matthews (2015) but with some important differences.\n\n\nThe central idea — what (Baumer, Jensen, and Matthews 2015) call the “conservation of runs” framework — is that if the batting team gains \\(\\delta_{i}\\) units of run value during an at-bat, the fielding team gains \\(-\\delta_{i}\\) units of run value during that same at-bat. In this lecture, we will apportion \\(\\delta_{i}\\) between the batters (Section 4) and the baserunners involved in at-bat \\(i\\) (Section 5) In Lecture 8, we will apportion \\(-\\delta_{i}\\) between the pitcher and fielders involved in at-bat \\(i.\\)",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-data-prep",
    "href": "lectures/lecture07.html#sec-data-prep",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo divide up offensive run value, we need to create a data table whose rows correspond to individual at-bats. This data table must, at a minimum, contain the starting and ending outs and baserunner configurations as well as the identities of the baserunners at the start and end of the at-bat. We will also want to include the columns event and des, which record the events and a narrative description of what happened in the at-bat.\n\nload(\"statcast2024.RData\")\nload(\"runValue2024.RData\")\nraw_atbat2024 &lt;- \n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n2    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner),\n    next_on_1b = dplyr::lead(on_1b),\n    next_on_2b = dplyr::lead(on_2b),\n    next_on_3b = dplyr::lead(on_3b)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n3    end_Outs = dplyr::last(next_Outs),\n    end_BaseRunner = dplyr::last(next_BaseRunner), # \n4    end_on_1b = dplyr::last(next_on_1b),\n    end_on_2b = dplyr::last(next_on_2b), \n    end_on_3b = dplyr::last(next_on_3b),  \n5    end_events = dplyr::last(events)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(end_bat_score = bat_score + RunsScored, end_fld_score = fld_score,\n         end_Outs = ifelse(is.na(end_Outs), 3, end_Outs)) |&gt;\n  dplyr::select(game_date, game_pk, at_bat_number, inning, inning_topbot,\n         Outs, BaseRunner, batter, on_1b, on_2b, on_3b, bat_score, fld_score, \n         end_Outs, end_BaseRunner, end_on_1b, end_on_2b, end_on_3b, end_bat_score, end_fld_score,\n         end_events, des) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\"))\n\n\n1\n\nDivide by game and half-inning\n\n2\n\nGets the value of several variables from the next pitch in the half-inning\n\n3\n\nGoes to last pitch of each at bat and gets next value of variable. That is, the starting value of the first pitch in the next at-bat.\n\n4\n\nFor instance, this looks up who’s on first at end of the current at-bat/start of the next at-bat.\n\n5\n\nThe variable events tells us what happened during the plate-appearance\n\n\n\n\n\nDealing with Missing Events\nThe column end_events in our data table raw_atbat2024 records what happened as a result of the at-bat. There are 308 rows with a missing entry.\n\ntable(raw_atbat2024$end_events)\n\n\n                                     catcher_interf                    double \n                      308                        97                      7608 \n              double_play               field_error                 field_out \n                      336                      1093                     72233 \n          fielders_choice       fielders_choice_out                 force_out \n                      373                       306                      3408 \ngrounded_into_double_play              hit_by_pitch                  home_run \n                     3152                      1977                      5326 \n                 sac_bunt                   sac_fly       sac_fly_double_play \n                      446                      1222                        13 \n                   single                 strikeout     strikeout_double_play \n                    25363                     40145                       107 \n                   triple               triple_play              truncated_pa \n                      685                         1                       304 \n                     walk \n                    14029 \n\n\nThe column des includes a much more detailed description of what happened during the plate appearance. A cursory look through the values of des corresponding to rows with missing end_events reveals that several of these at-bats ended with a walk, involved an automatic strike1, or an inning-ending pick off2\n\nraw_atbat2024 |&gt;\n  dplyr::filter(end_events == \"\") |&gt;\n  dplyr::slice_head(n = 15) |&gt;\n  dplyr::select(Outs, end_Outs, des)\n\n# A tibble: 15 × 3\n    Outs end_Outs des                                                           \n   &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                                                         \n 1     0        0 Mookie Betts walks.                                           \n 2     2        2 Freddie Freeman walks.                                        \n 3     2        3 Xander Bogaerts strikes out on automatic strike.              \n 4     2        2 Héctor Neris intentionally walks Wyatt Langford.              \n 5     2        2 Logan Webb intentionally walks Ha-Seong Kim.                  \n 6     2        2 Cole Ragans intentionally walks Carlos Santana.               \n 7     1        2 Andrew Vaughn strikes out on automatic strike.                \n 8     2        3 Oneil Cruz strikes out on automatic strike.                   \n 9     2        3 Pitcher Bryce Miller picks off Wilyer Abreu at on throw to sh…\n10     1        1 Yohan Ramírez intentionally walks Christian Yelich.           \n11     1        2 Alex Kirilloff strikes out on automatic strike.               \n12     1        1 Tony Kemp walks. James McCann to 2nd.                         \n13     2        3 With Anthony Rendon batting, Zach Neto picked off and caught …\n14     2        3 Miguel Sanó strikes out on automatic strike.                  \n15     2        3 With Vinnie Pasquantino batting, Bobby Witt Jr. picked off an…\n\n\nThe following code manually corrects the missing values for end_events\n\natbat2024 &lt;-\n  raw_atbat2024 |&gt;\n  dplyr::mutate(\n    end_events = dplyr::case_when(\n      end_events == \"\" & grepl(\"walk\", des) ~ \"walk\",\n      end_events == \"\" & grepl(\"strikes out\", des) ~ \"strikeout\",\n1      end_events == \"\" & end_Outs == 3 ~ \"truncated_pa\",\n2      end_events == \"\" & grepl(\"flies out\", des) ~ \"field_out\",\n      .default = end_events))\n\n\n1\n\nAfter accounting for the walks and strike outs on automatic strikes, all but one of the at-bats that still had a missing end_events value involved a pick-off that ended the inning\n\n2\n\nThe remaining at-bat involved a fly out that was caught in foul territory.",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-adjusted-rv",
    "href": "lectures/lecture07.html#sec-adjusted-rv",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Adjusted Run Values",
    "text": "Adjusted Run Values\nWe want to give credit to the batter and base runners for creating value over and above what would been expected given the game state and the actual outcome of the at-bat. More precisely, recall that \\(\\delta_{i}\\) is the run value created in at-bat \\(i.\\) We will denote the game state at the beginning of the at-bat with \\(\\textrm{g}_{i}\\) and the ending event with \\(\\textrm{e}_{i}.\\) We form the game state variable \\(\\textrm{g}\\) by concatenating the Outs and BaseRunners and separating them with a period so that \\(\\textrm{g} = \"0.101\"\\) corresponds to a situation with no outs and runners on first and third base.\n\natbat2024 &lt;-\n  atbat2024 |&gt;\n  dplyr::mutate(GameState = paste(Outs, BaseRunner, sep = \".\"))\n\nWe will assume that the run value created in each at-bat beginning in state \\(\\textrm{g}\\) and ending with event \\(\\textrm{e}\\) is equal to the average run value created in all at-bats with the same beginning and end plus some mean-zero error. That is, for each at-bat \\(i\\), \\[\n\\delta_{i} = \\mathbb{E}[\\delta \\vert \\textrm{g} = \\textrm{g}_{i}, \\textrm{e} = \\textrm{e}_{i}] + \\varepsilon_{i},\n\\] The average run value \\(\\mu:=\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}]\\) represents the average run value created in at-bats that begin in state \\(\\textrm{g}\\) and end with the event \\(\\textrm{e}.\\)\nIt is tempting to compute the expectation \\(\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}]\\) using the “binning-and-averaging” approach we took when developing our initial XG models back in Lecture 2. Unfortunately, such a procedure is liable to yield extreme and erratic answers as the number of bins is quite large. To wit, there are 24 distinct game states (i.e., combinations of outs and base runners) and 21 different events.\nThe 2024 dataset contains only 373 of the 504 total combinations of game state and ending event. Of the observed combinations, there is a huge disparity in the relative frequencies. Some combinations (e.g., triples with no outs and runners on second and third) occurred just once while others (e.g., striking out with no outs and nobody on) occurred close to 10,000 times.\n\natbat2024 |&gt;\n  dplyr::count(Outs, BaseRunner, end_events) |&gt;\n  dplyr::arrange(n) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n# A tibble: 10 × 4\n    Outs BaseRunner end_events                n\n   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;\n 1     0 001        catcher_interf            1\n 2     0 001        fielders_choice_out       1\n 3     0 011        triple                    1\n 4     0 101        strikeout_double_play     1\n 5     0 101        triple_play               1\n 6     1 000        strikeout              7490\n 7     0 000        strikeout              9814\n 8     2 000        field_out             11563\n 9     1 000        field_out             14573\n10     0 000        field_out             20148\n\n\nInstead of “binning and averaging”, like we did with our distance-based XG models in Lecture 3, we will fit a statistical model. A natural starting model asserts that there are numbers \\(\\alpha_{0.000}, \\ldots, \\alpha_{2.111}\\) and \\(\\alpha_{\\textrm{catcher\\_interf}}, \\ldots, \\alpha_{\\textrm{walk}}\\) such that for all game states \\(\\textrm{g}\\) and ending events \\(\\textrm{e},\\) \\[\n\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}] = \\alpha_{\\textrm{g}} + \\alpha_{\\textrm{e}}.\n\\]\nUnder the assumed model, the average run value created by hitting a single when there are two outs and no runners on is \\(\\alpha_{\\textrm{2.000}} + \\alpha_{\\textrm{single}}\\) while the average run value created by hitting a single when there are no outs and runners on first and second is \\(\\alpha_{\\textrm{0.110}} + \\alpha_{\\textrm{single}}.\\)\nBecause we do not know the exact values of the \\(\\alpha_{g}\\)’s and \\(\\alpha_{e}\\)’s, we need to estimate them using our data. Perhaps the simplest way is by solving a least squares minimization problem \\[\n\\hat{\\boldsymbol{\\alpha}} = \\textrm{argmin} \\sum_{i = 1}^{n}{(\\delta_{i} - \\alpha_{g_{i}} - \\alpha_{e_{i}})^2},\n\\] where \\(g_{i}\\) and \\(e_{i}\\) record the game state and event of at-bat \\(i.\\)\nSolving this problem is equivalent to fitting a linear regression model without an intercept3. We can do this in R using the lm() function and including -1 in the formula argument4. In the following code, we create a temporary data frame that extracts just the run values \\(\\delta\\), game states \\(\\textrm{g}\\), and ending events \\(\\textrm{e}\\) from atbats2024 and convert the game state and event variables into factors.\n\ntmp_df &lt;-\n  atbat2024 |&gt;\n  dplyr::select(RunValue, GameState, end_events) |&gt;\n  dplyr::mutate(\n    GameState = factor(GameState),\n    end_events = factor(end_events))\n\nstate_event_fit &lt;-\n  lm(RunValue ~ -1 + GameState + end_events, data = tmp_df)\n\nUsing the summary() function, we can take a quick look at the \\(\\alpha\\)’s.\n\nsummary(state_event_fit)\n\n\nCall:\nlm(formula = RunValue ~ -1 + GameState + end_events, data = tmp_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.65924 -0.10891  0.01957  0.08867  2.24020 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \nGameState0.000                       0.344318   0.024078  14.300  &lt; 2e-16 ***\nGameState0.001                       0.240733   0.027701   8.690  &lt; 2e-16 ***\nGameState0.010                       0.392981   0.024448  16.074  &lt; 2e-16 ***\nGameState0.011                       0.272952   0.026280  10.386  &lt; 2e-16 ***\nGameState0.100                       0.391419   0.024160  16.201  &lt; 2e-16 ***\nGameState0.101                       0.237505   0.025347   9.370  &lt; 2e-16 ***\nGameState0.110                       0.407975   0.024500  16.652  &lt; 2e-16 ***\nGameState0.111                       0.369993   0.025731  14.379  &lt; 2e-16 ***\nGameState1.000                       0.348466   0.024094  14.463  &lt; 2e-16 ***\nGameState1.001                       0.253997   0.024902  10.200  &lt; 2e-16 ***\nGameState1.010                       0.354195   0.024327  14.560  &lt; 2e-16 ***\nGameState1.011                       0.265455   0.025049  10.597  &lt; 2e-16 ***\nGameState1.100                       0.396792   0.024121  16.450  &lt; 2e-16 ***\nGameState1.101                       0.332078   0.024655  13.469  &lt; 2e-16 ***\nGameState1.110                       0.412645   0.024311  16.974  &lt; 2e-16 ***\nGameState1.111                       0.359217   0.024860  14.449  &lt; 2e-16 ***\nGameState2.000                       0.356264   0.024096  14.785  &lt; 2e-16 ***\nGameState2.001                       0.346744   0.024549  14.124  &lt; 2e-16 ***\nGameState2.010                       0.318496   0.024256  13.131  &lt; 2e-16 ***\nGameState2.011                       0.321323   0.024888  12.911  &lt; 2e-16 ***\nGameState2.100                       0.351540   0.024132  14.567  &lt; 2e-16 ***\nGameState2.101                       0.363065   0.024474  14.835  &lt; 2e-16 ***\nGameState2.110                       0.354515   0.024269  14.608  &lt; 2e-16 ***\nGameState2.111                       0.338946   0.024651  13.750  &lt; 2e-16 ***\nend_eventsdouble                     0.398473   0.024206  16.461  &lt; 2e-16 ***\nend_eventsdouble_play               -1.293011   0.027320 -47.328  &lt; 2e-16 ***\nend_eventsfield_error                0.097087   0.025100   3.868  0.00011 ***\nend_eventsfield_out                 -0.589994   0.024071 -24.510  &lt; 2e-16 ***\nend_eventsfielders_choice            0.342555   0.027015  12.680  &lt; 2e-16 ***\nend_eventsfielders_choice_out       -0.963020   0.027678 -34.794  &lt; 2e-16 ***\nend_eventsforce_out                 -0.713014   0.024403 -29.219  &lt; 2e-16 ***\nend_eventsgrounded_into_double_play -1.195243   0.024442 -48.901  &lt; 2e-16 ***\nend_eventshit_by_pitch              -0.001119   0.024636  -0.045  0.96376    \nend_eventshome_run                   1.043303   0.024271  42.985  &lt; 2e-16 ***\nend_eventssac_bunt                  -0.443274   0.026600 -16.664  &lt; 2e-16 ***\nend_eventssac_fly                   -0.341349   0.025120 -13.589  &lt; 2e-16 ***\nend_eventssac_fly_double_play       -0.887624   0.070048 -12.672  &lt; 2e-16 ***\nend_eventssingle                     0.115000   0.024099   4.772 1.83e-06 ***\nend_eventsstrikeout                 -0.618027   0.024083 -25.662  &lt; 2e-16 ***\nend_eventsstrikeout_double_play     -1.041349   0.033231 -31.337  &lt; 2e-16 ***\nend_eventstriple                     0.704276   0.025700  27.404  &lt; 2e-16 ***\nend_eventstriple_play               -2.133826   0.238222  -8.957  &lt; 2e-16 ***\nend_eventstruncated_pa              -0.625189   0.026806 -23.323  &lt; 2e-16 ***\nend_eventswalk                      -0.024163   0.024135  -1.001  0.31675    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2369 on 178488 degrees of freedom\nMultiple R-squared:  0.7682,    Adjusted R-squared:  0.7682 \nF-statistic: 1.344e+04 on 44 and 178488 DF,  p-value: &lt; 2.2e-16\n\n\nWe estimate \\(\\hat{\\alpha}_{2.000} \\approx 0.356\\) and \\(\\hat{\\alpha}_{single} \\approx 0.115.\\) So, according to our fitted model the average run value created by singling when there are two outs and no runners on is about \\(0.471.\\)\n\n\n\n\n\n\nStatistical Significance & Model Assumptions\n\n\n\nYou’ll notice that summary() returns a lot of inferential output (e.g., standard errors, p-values). These are computed under an additional assumption that the true errors \\(\\varepsilon_{i}\\) are independent and following a mean-zero normal distribution with constant variance. Since our main interest is prediction, we’re really not interested in checking whether, say, \\(\\alpha_{0:010}\\) is statistically significantly different than zero. So, we will not check whether the usual multiple linear model assumptions nor will we attempt. If you did want to make inferential statements about our model parameters, you would need to first check that the multiple linear multiple assumptions are not grossly violated.\n\n\nEquipped with our estimated model parameters, for each at-bat \\(i,\\) let \\(\\hat{\\mu}_{i} = \\hat{\\alpha}_{\\textrm{g}_{i}} + \\hat{\\alpha}_{\\textrm{e}_{i}}\\) and let \\(\\eta_{i} = \\delta_{i} - \\hat{\\mu}_{i}.\\) In terms of dividing credit between the batter and the base runner, we will follow Baumer, Jensen, and Matthews (2015) and attribute \\(\\hat{\\mu}_{i}\\) to the batter’s hitting in at-bat \\(i\\) and \\(\\eta_{i}\\) to the base running in that at-bat. We will add columns to atbat2024 holding the values of \\(\\hat{\\mu}\\) (mu) and \\(\\eta\\) (eta).\n\natbat2024$eta &lt;- state_event_fit$residuals\natbat2024$mu &lt;- state_event_fit$fitted.values\nsave(atbat2024, file = \"atbat2024.RData\")",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-baserunner",
    "href": "lectures/lecture07.html#sec-baserunner",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Baserunning Run Value",
    "text": "Baserunning Run Value\nOhtani’s second hit against the Padres on March 20, 2024 was a single with runners on first and second and no outs. While Ohtani and the runner originally on first advanced one base, the runner originally on second scored. Because this latter runner advanced more than what might have been otherwise expected, it makes sense to give him a larger share of the \\(\\eta_{i}\\) than to the first two runners, who only advanced one base on a single. Following (Baumer, Jensen, and Matthews 2015, sec. 3.2), the amount of base running run value \\(\\eta_{i}\\) that we assign to base runner \\(j\\) in at-bat \\(i\\) will be proportional to \\(\\kappa_{ij} = \\mathbb{P}(K &lt; k_{ij} \\vert \\textrm{e}_{i}),\\) where \\(k_{ij}\\) is the number of bases actually advanced by the base runner.\nEssentially, \\(\\kappa_{ij}\\) is the probability that a typical base runner advanced at most the \\(k_{ij}\\) bases advanced by base runner \\(j\\) in at-bat \\(i\\) following event \\(\\textrm{e}_{i}.\\) If the base runner does worse than expected (e.g., not advancing from second on a single), then \\(\\kappa_{ij}\\) will be very small. But if the base runner does better than expected (e.g., scoring from second on a single), then \\(\\kappa_{ij}\\) will be larger. When computing \\(\\kappa_{ij}\\) it is crucial that we condition on the actual ending event \\(\\textrm{e}_{i}.\\) After all, while we may want to penalize a runner for not advancing from second on a single, we definitely don’t want to penalize a runner for not advancing from second following a strike out!\n\nBaserunner Advancement\nUnfortunately, StatCast does not compute the number of bases that each runner advances during each at-bat. The following code implements a function that determines the number of bases advanced by the runner on first (if any). It works by first checking whether there is anyone on 1b at the start of the at-bat. If so, it checks whether that player is on first, second, or third base at the end of the at-bat. If not, it parses the at-bat description contained in des and looks for a sentence containing the player’s name. If that sentence contains the words “out” or “caught stealing”, it sets the number of bases advanced to 0. But, if the sentnce contains the word “score”, it sets the number of bases advanced to 3, since the runner scored from first.\n\nload(\"player2024_lookup.RData\")\n#| label: mvt-1b-function\nmvt_1b &lt;- function(on_1b, Outs, bat_score,\n                   end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_1b)){\n    # there was someone on 1st base at the start of the at-bat\n1    if(!is.na(end_on_1b) & on_1b == end_on_1b) mvt &lt;- 0\n2    if(!is.na(end_on_2b) & on_1b == end_on_2b) mvt &lt;- 1\n3    if(!is.na(end_on_3b) & on_1b == end_on_3b) mvt &lt;- 2\n\n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on first\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_1b)]\n      # Start by splitting it a string\n      play_split &lt;- \n        stringr::str_split_1(\n          string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n          pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      \n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 3 # player scored from 1st\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      } \n    }\n  } \n  return(mvt)\n}\n\n\n1\n\nRunner remained on first, so they advanced 0 bases\n\n2\n\nRunner advanced 1 base (first to second)\n\n3\n\nRunner advanced 2 bases (first to third)\n\n\n\n\nWe similarly define functions to track the number of bases advanced by the runners on second and third base and by the batter. For brevity, we have folded the code.\n\n\nShow code for computing the number of bases advanced by the batter and the runners on 2nd and 3rd base.\nmvt_2b &lt;- function(on_2b, Outs, bat_score,\n                   end_on_2b, end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_2b)){\n    # there was someone on 2nd base at the start of the at-bat\n    if(!is.na(end_on_2b) & on_2b == end_on_2b) mvt &lt;- 0 # runner remained on 2nd\n    if(!is.na(end_on_3b) & on_2b == end_on_3b) mvt &lt;- 1 # runner advanced to 3rd\n    \n    #if(end_Outs == 3) mvt &lt;- 0 # inning ended ; there may be some edge cases here\n    # e.g., in last at-bat there may be a wild pitch\n    # https://www.espn.com/mlb/playbyplay/_/gameId/401568474 where runner scores and then batter gets out to end the inning\n    \n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on second\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_2b)]\n      # Start by splitting it a string\n      play_split &lt;- \n        stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                    pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      \n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 2 # player scored from 2nd\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      } \n    }\n  } \n  return(mvt)\n}\n\n\nmvt_3b &lt;- function(on_3b, Outs, bat_score,\n                   end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_3b)){\n    if(!is.na(end_on_3b) & on_3b == end_on_3b) mvt &lt;- 0 # runner remained on 3rd\n    \n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on second\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_3b)]\n      play_split &lt;- \n        stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                    pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 1 # player scored from 3rd\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      }\n    }\n  } \n  return(mvt)\n}\n\n\nmvt_batter &lt;- function(batter, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score, des)\n{\n  mvt &lt;- NA\n  if(!is.na(end_on_1b) & batter == end_on_1b) mvt &lt;- 1 # batter advanced to 1st\n  else if(!is.na(end_on_2b) & batter == end_on_2b) mvt &lt;- 2 # batter advanced to 2nd\n  else if(!is.na(end_on_3b) & batter == end_on_3b) mvt &lt;- 3 # batter advanced to 3rd\n  else{\n    # batter is not on base\n    # look up player name\n    player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == batter)]\n    \n    play_split &lt;-\n      stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                           pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n    \n    check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n    if(any(check)){\n      # found something with player name in it\n      play &lt;- play_split[check]\n      if( any(grepl(pattern = \"out\", x = play))) mvt &lt;- 0 # player got out\n      else if(any(grepl(pattern = \"score\", x = play) | grepl(pattern = \"home\", x = play))) mvt &lt;- 4 # batter scored\n      else if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      else mvt &lt;- NA\n    }\n  }\n  return(mvt)\n}\n\n\nWe can now apply these functions to every row of our data frame.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code takes a few minutes to run.\n\n\n\nbaserunning &lt;-\n  atbat2024 |&gt;\n  dplyr::rowwise() |&gt;\n  dplyr::mutate(\n    mvt_batter = mvt_batter(batter, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score, des),\n    mvt_1b = mvt_1b(on_1b, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b,end_Outs, end_bat_score, des),\n    mvt_2b = mvt_2b(on_2b, Outs, bat_score, end_on_2b, end_on_3b, end_Outs, end_bat_score, des),\n    mvt_3b = mvt_3b(on_3b, Outs, bat_score, end_on_3b, end_Outs, end_bat_score, des)) |&gt;\n  dplyr::ungroup() \n\n\n\nCumulative Baserunning Probabilities\nNow that we have computed the \\(k_{ij}\\)’s — that is, the number of bases each base runner advanced in each at-bat — we are ready to compute the cumulative base running probabilities \\(\\mathbb{P}(K \\leq k  \\vert \\textrm{e}).\\) In the following code, we first group at-bats by the ending event and then compute the proportion of times that the baserunner advances at most \\(k\\) bases. We also set the cumulative probability to zero for situations when there isn’t a runner on a particular base.\n\nbr_batter_probs &lt;-\n  baserunning |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_batter &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_batter &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_batter &lt;= 2, na.rm = TRUE),\n    kappa_3 = mean(mvt_batter &lt;= 3, na.rm = TRUE),\n    kappa_4 = mean(mvt_batter &lt;= 4, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_batter\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_batter\") |&gt;\n  dplyr::mutate(\n    mvt_batter = ifelse(mvt_batter == \"NA\", NA, mvt_batter),\n    mvt_batter = as.numeric(mvt_batter))\nbr_1b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_1b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_1b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_1b &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_1b &lt;= 2, na.rm = TRUE),\n    kappa_3 = mean(mvt_1b &lt;= 3, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_1b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_1b\") |&gt;\n  dplyr::mutate(mvt_1b = ifelse(mvt_1b == \"NA\", NA, mvt_1b),\n                mvt_1b = as.numeric(mvt_1b))\n\nbr_2b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_2b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_2b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_2b &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_2b &lt;= 2, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_2b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_2b\") |&gt;\n  dplyr::mutate(mvt_2b = ifelse(mvt_2b == \"NA\", NA, mvt_2b),\n                mvt_2b = as.numeric(mvt_2b))\n\nbr_3b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_3b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_3b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_3b &lt;= 1, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_3b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_3b\") |&gt;\n  dplyr::mutate(mvt_3b = ifelse(mvt_3b == \"NA\", NA, mvt_3b),\n                mvt_3b = as.numeric(mvt_3b))\n\nThe table br_1b_probs contains the cumulative base running probabilities for runners who start on first base broken down by ending event. We find that in about 64.7% of singles, the runner on first advances one base or fewer while in 95.7% of singles, the runner on first advances two bases or fewer.\n\nbr_1b_probs |&gt;\n  dplyr::filter(end_events == \"single\")\n\n# A tibble: 5 × 3\n  end_events mvt_1b kappa_1b\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 single          0   0.0194\n2 single          1   0.647 \n3 single          2   0.957 \n4 single          3   1     \n5 single         NA   0     \n\n\n\n\nBaserunning Runs Above Average\nNow that we have the cumulative base running probabilities, we’re (finally) ready to compute \\(\\kappa_{ij}.\\) To do so, we will use inner_join()’s to add columns to our baserunning data table with columns for the batter and runners on first, second, and third. Note, whenever there is no baserunner on first base (i.e., on_1b = NA), we will set the corresponding \\(\\kappa\\) to 0. Because we want to divide all of \\(\\eta_{i}\\) amongst the base runners, we need to normalize the \\(\\kappa_{ij}\\) values to sum to 1 within each at-bat. The columns norm_batter, norm_1b, norm_2b, and norm_3b contain these normalized weights.\n\nbaserunning &lt;-\n  baserunning |&gt;\n  dplyr::inner_join(y = br_batter_probs, by = c(\"end_events\", \"mvt_batter\")) |&gt;\n  dplyr::inner_join(y = br_1b_probs, by = c(\"end_events\", \"mvt_1b\")) |&gt;\n  dplyr::inner_join(y = br_2b_probs, by = c(\"end_events\", \"mvt_2b\")) |&gt;\n  dplyr::inner_join(y = br_3b_probs, by = c(\"end_events\", \"mvt_3b\")) |&gt;\n  dplyr::mutate(\n    total_kappa = kappa_batter + kappa_1b + kappa_2b + kappa_3b,\n    norm_batter = kappa_batter/total_kappa,\n    norm_1b = kappa_1b/total_kappa,\n    norm_2b = kappa_2b/total_kappa,\n    norm_3b = kappa_3b/total_kappa)\n\nTo illustrate our calculations so far, let’s look at Ohtani’s at-bats from that game against the Padres. First, we see that Ohtani reached first base in all except his fourth at-bat. So, for these four at-bats, his mvt_batter value is 1.\n\nload(\"player2024_lookup.RData\")\nohtani_id &lt;- \n  player2024_lookup |&gt;\n  dplyr::filter(FullName == \"Shohei Ohtani\") |&gt;\n  dplyr::pull(key_mlbam)\n\nbaserunning |&gt;\n  dplyr::filter(game_pk == 745444 & batter == ohtani_id) |&gt;\n  dplyr::select(at_bat_number, mvt_batter, des)\n\n# A tibble: 5 × 3\n  at_bat_number mvt_batter des                                                  \n          &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                                                \n1             2          1 Shohei Ohtani grounds into a force out, shortstop Ha…\n2            18          1 Shohei Ohtani singles on a sharp line drive to right…\n3            37          1 Shohei Ohtani grounds into a force out, third basema…\n4            52          0 Shohei Ohtani grounds out softly, pitcher Wandy Pera…\n5            65          1 Shohei Ohtani singles on a line drive to left fielde…\n\n\nIn his second at-bat, Ohtani singled with no runners on. So, he should get credit for creating all the base running run value above average on that at-bat. In contrast, we argued earlier that when he drove in a run in his fifth at-bat, the runner who scored from second should get a bit more credit than Ohtani and the runner on first, who only advanced one base. Looking at the weights norm_1b, norm_2b, and norm_batter for this at-bat, we see that indeed, we’re assigning a bit more weight to the runner on second than the runner on first.\n\nbaserunning |&gt;\n  dplyr::filter(game_pk == 745444 & batter == ohtani_id) |&gt;\n  dplyr::select(at_bat_number, mvt_1b, mvt_2b, mvt_3b, mvt_batter, norm_1b, norm_2b, norm_3b, norm_batter)\n\n# A tibble: 5 × 9\n  at_bat_number mvt_1b mvt_2b mvt_3b mvt_batter norm_1b norm_2b norm_3b\n          &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1             2      0     NA     NA          1   0.491   0           0\n2            18     NA     NA     NA          1   0       0           0\n3            37      0     NA     NA          1   0.491   0           0\n4            52     NA     NA     NA          0   0       0           0\n5            65      1      2     NA          1   0.247   0.382       0\n# ℹ 1 more variable: norm_batter &lt;dbl&gt;\n\n\nRecall that \\(\\eta_{i}\\) represents the run value above average generated in at-bat \\(i\\) due to base running. Whenever there is a runner on first at the start of the at-bat, the quantity \\(\\kappa_{i,\\textrm{1b}}/\\sum_{j}{\\kappa_{ij}} \\times \\eta_{i}\\) reflects the run value above average generated in the at-bat due to the base running of the runner initially on first. For each player, we can aggregate these values across all at-bats in which they are on first.\n\nraa_1b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_1b)) |&gt;\n  dplyr::mutate(RAA_1b = kappa_1b * eta) |&gt;\n  dplyr::group_by(on_1b) |&gt;\n  dplyr::summarise(RAA_1b = sum(RAA_1b)) |&gt;\n  dplyr::rename(key_mlbam = on_1b)\n\nWe can similarly compute the run value above average created by the runners on second base and third base as well as by the batter.\n\n\nCompute the baserunning runs above average created by batter and the runners on second and third bases\nraa_2b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_2b)) |&gt;\n  dplyr::mutate(RAA_2b = kappa_2b * eta) |&gt;\n  dplyr::group_by(on_2b) |&gt;\n  dplyr::summarise(RAA_2b = sum(RAA_2b)) |&gt;\n  dplyr::rename(key_mlbam = on_2b)\n\nraa_3b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_3b)) |&gt;\n  dplyr::mutate(RAA_3b = kappa_3b * eta) |&gt;\n  dplyr::group_by(on_3b) |&gt;\n  dplyr::summarise(RAA_3b = sum(RAA_3b)) |&gt;\n  dplyr::rename(key_mlbam = on_3b)\n\nraa_batter &lt;-\n  baserunning |&gt;\n  dplyr::mutate(RAA_batter = kappa_batter * eta) |&gt;\n  dplyr::group_by(batter) |&gt;\n  dplyr::summarise(RAA_batter = sum(RAA_batter)) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\n\nFinally, we can aggregate the total run value above average that each player creates from their base running, which we can \\(\\textrm{RAA}^{\\textrm{br}}.\\)\n\nraa_br &lt;-\n  raa_batter |&gt;\n  dplyr::full_join(y = raa_1b, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_2b, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_3b, by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(list(RAA_batter = 0, RAA_1b = 0, RAA_2b = 0, RAA_3b = 0)) |&gt;\n  dplyr::mutate(RAA_br = RAA_batter + RAA_1b + RAA_2b + RAA_3b) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_br, RAA_batter, RAA_1b, RAA_2b, RAA_3b)\n\nInterestingly, the player with the largest \\(\\textrm{RAA}^{\\textrm{br}}\\), Jose Ramirez, is known for his aggressive baserunning5.",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-batting",
    "href": "lectures/lecture07.html#sec-batting",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Crediting Batters",
    "text": "Crediting Batters\nWe now need to distribute \\(\\hat{\\mu}_{i}\\) to each batter. As noted in (Baumer, Jensen, and Matthews 2015, sec. 3.3), we need to take care to calibrate each batter’s hitting performance with the expected performance by position. \n\nAverage Run Value by Position\n\nload(\"positions2024.RData\")\nbatting &lt;-\n  atbat2024 |&gt;\n  dplyr::select(batter, mu) |&gt;\n  dplyr::rename(key_mlbam = batter) |&gt;\n  dplyr::left_join(y = positions2024, by = \"key_mlbam\")\n\navg_mu_position &lt;-\n  batting |&gt;\n  dplyr::group_by(position) |&gt;\n  dplyr::summarise(avg_mu = mean(mu))\n\nWe can now go through and subtract the position average from each \\(\\hat{\\mu}_{i}\\) in batting. This represents the run value above average created by the batter’s hittign in the at-bat. In the following code, we aggregate these at-bat level run values above average for each player. We denote the total \\(\\textrm{RAA}^{\\textrm{b}}.\\)\n\nraa_b &lt;-\n  batting |&gt;\n  dplyr::left_join(y = avg_mu_position, by = \"position\") |&gt;\n  dplyr::mutate(mu_aa = mu - avg_mu) |&gt;\n  dplyr::group_by(key_mlbam) |&gt;\n  dplyr::summarise(RAA_b = sum(mu)) |&gt;\n  dplyr::left_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_b)\n\nWe find a lot of very good batters among the players with highest \\(\\textrm{RAA}^{\\textrm{b}}\\)’s\n\nraa_b |&gt;\n  dplyr::arrange(dplyr::desc(RAA_b)) |&gt;\n  dplyr::slice_head(n=10)\n\n# A tibble: 10 × 2\n   Name              RAA_b\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Aaron Judge        84.6\n 2 Juan Soto          64.3\n 3 Shohei Ohtani      59.6\n 4 Bobby Witt         59.0\n 5 Vladimir Guerrero  45.0\n 6 Gunnar Henderson   42.7\n 7 Ketel Marte        41.3\n 8 Jose Ramirez       35.9\n 9 Brent Rooker       34.8\n10 William Contreras  33.5",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-looking-ahead",
    "href": "lectures/lecture07.html#sec-looking-ahead",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nWe’ve distributed the run value \\(\\delta_{i}\\) created in each at-bat between the batter and base runners and computed season total runs values above average based on batting \\(\\textrm{RAA}^{\\textrm{b}}\\) and base running \\(\\textrm{RAA}^{\\textrm{br}}.\\) Next lecture, we will distribute \\(-\\delta_{i}\\) between the pitcher and fielders involved in each at-bat. So that we don’t have to repeat our earlier calculations, we will save raa_br and raa_b\n\nsave(raa_br, raa_b, file = \"raa_offensive2024.RData\")",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#footnotes",
    "href": "lectures/lecture07.html#footnotes",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStarting in 2023, Major League Baseball implemented a pitch timer. Batters who were not in the batter’s box and alert to the pitcher by the 8-second mark of the timer are penalized with an automatic strike. See the rules here.↩︎\nWhen this happens, Statcast usually records it as a truncated plate appearance (truncated_pa).↩︎\nTry proving this mathematically!↩︎\nCheck out the documentation for R’s formula interface here. Specifically, look at the bullet point about the - operations under “Details”↩︎\nSee this article about his base running from earlier this year.↩︎",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture05.html",
    "href": "lectures/lecture05.html",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "",
    "text": "In Lecture 4, we introduced adjusted plus/minus, which attempts to estimate how much value an NBA player creates after adjusting for the quality of his teammates and opponents. As noted in that lecture, using the method of least squares to estimate APM values is problematic because it assumes that all baseline players have the exact same partial effect on their team’s average point differential per 100 possessions. In this lecture, we solve a penalized version of the least squares problem (Section 2) to fit a regularized adjusted plus/minus model that avoids the need to specify baseline players (Section 3). We then use the bootstrap (Section 4) to quantify uncertainty about differences between players and the overall RAPM rankings.\n\n\nWe begin by reviewing the notation and set-up from Lecture 4. First, let \\(n\\) denote the number of stints and \\(p\\) the total number of players. For each stint \\(i = 1, \\ldots, n\\) and player \\(j = 1, \\ldots, p,\\) let \\(x_{ij}\\) be the signed on-court indicator for player \\(j\\) during stint \\(i.\\) That is, \\(x_{ij} = 1\\) (resp. \\(x_{ij} = -1)\\) if player \\(j\\) is on the court and playing at home (resp. on the road) during stint \\(i\\) and \\(x_{ij} = 0\\) if player \\(j\\) is not on the court during stint \\(i.\\) We can arrange these indicators into a large \\(n \\times p\\) matrix \\(\\boldsymbol{\\mathbf{X}}\\) whose rows (resp. columns) correspond to stints (resp. players). Finally, let \\(Y_{i}\\) be the point differential per 100 possessions observed during shift \\(i.\\)\nThe original APM model introduced a single number \\(\\alpha_{j}\\) for each player \\(j = 1, \\ldots, p,\\) and modeled \\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\] where \\(h_{1}(i), \\ldots, h_{5}(i)\\) and \\(a_{1}(i), \\ldots, a_{5}(i)\\) are, respectively, the indices of the homes and away team players on the court during stint \\(i\\); \\(\\alpha_{0}\\) captures the average home court advantage in terms of point-differential per 100 possessions across all teams; and the \\(\\epsilon_{i}\\)’s are independent random errors drawn from a distribution with mean zero.\nIn Lecture 4, we formed the \\(n \\times (p+1)\\) matrix \\(\\boldsymbol{\\mathbf{Z}}\\) by appending a column of 1’s to the matrix \\(\\boldsymbol{\\mathbf{X}}\\) and let \\(\\boldsymbol{\\alpha} = (\\alpha_{0}, \\alpha_{1}, \\ldots, \\alpha_{p})^{\\top}\\) be the vector of length \\(p+1\\) containing the home court advantage \\(\\alpha_{0}\\) and all the player-specific parameters \\(\\alpha_{j}.\\) Letting \\(\\boldsymbol{\\mathbf{z}}_{i}\\) be the \\(i\\)-th row of \\(\\boldsymbol{\\mathbf{Z}},\\) we can write the APM model in a more compact form. \\[\nY_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}.\n\\]\nAt the end of Lecture 4, we saved a copy of the full matrix \\(\\boldsymbol{\\mathbf{X}}\\) (which we called X_full) of signed on-court indicators, a vector of point differentials per 100 possessions \\(\\boldsymbol{Y}\\) (Y), and a look-up table containing player id numbers and names. We load those objects into our environment.\n\nload(\"lecture04_05_data.RData\")",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#sec-overview",
    "href": "lectures/lecture05.html#sec-overview",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "",
    "text": "In Lecture 4, we introduced adjusted plus/minus, which attempts to estimate how much value an NBA player creates after adjusting for the quality of his teammates and opponents. As noted in that lecture, using the method of least squares to estimate APM values is problematic because it assumes that all baseline players have the exact same partial effect on their team’s average point differential per 100 possessions. In this lecture, we solve a penalized version of the least squares problem (Section 2) to fit a regularized adjusted plus/minus model that avoids the need to specify baseline players (Section 3). We then use the bootstrap (Section 4) to quantify uncertainty about differences between players and the overall RAPM rankings.\n\n\nWe begin by reviewing the notation and set-up from Lecture 4. First, let \\(n\\) denote the number of stints and \\(p\\) the total number of players. For each stint \\(i = 1, \\ldots, n\\) and player \\(j = 1, \\ldots, p,\\) let \\(x_{ij}\\) be the signed on-court indicator for player \\(j\\) during stint \\(i.\\) That is, \\(x_{ij} = 1\\) (resp. \\(x_{ij} = -1)\\) if player \\(j\\) is on the court and playing at home (resp. on the road) during stint \\(i\\) and \\(x_{ij} = 0\\) if player \\(j\\) is not on the court during stint \\(i.\\) We can arrange these indicators into a large \\(n \\times p\\) matrix \\(\\boldsymbol{\\mathbf{X}}\\) whose rows (resp. columns) correspond to stints (resp. players). Finally, let \\(Y_{i}\\) be the point differential per 100 possessions observed during shift \\(i.\\)\nThe original APM model introduced a single number \\(\\alpha_{j}\\) for each player \\(j = 1, \\ldots, p,\\) and modeled \\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\] where \\(h_{1}(i), \\ldots, h_{5}(i)\\) and \\(a_{1}(i), \\ldots, a_{5}(i)\\) are, respectively, the indices of the homes and away team players on the court during stint \\(i\\); \\(\\alpha_{0}\\) captures the average home court advantage in terms of point-differential per 100 possessions across all teams; and the \\(\\epsilon_{i}\\)’s are independent random errors drawn from a distribution with mean zero.\nIn Lecture 4, we formed the \\(n \\times (p+1)\\) matrix \\(\\boldsymbol{\\mathbf{Z}}\\) by appending a column of 1’s to the matrix \\(\\boldsymbol{\\mathbf{X}}\\) and let \\(\\boldsymbol{\\alpha} = (\\alpha_{0}, \\alpha_{1}, \\ldots, \\alpha_{p})^{\\top}\\) be the vector of length \\(p+1\\) containing the home court advantage \\(\\alpha_{0}\\) and all the player-specific parameters \\(\\alpha_{j}.\\) Letting \\(\\boldsymbol{\\mathbf{z}}_{i}\\) be the \\(i\\)-th row of \\(\\boldsymbol{\\mathbf{Z}},\\) we can write the APM model in a more compact form. \\[\nY_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}.\n\\]\nAt the end of Lecture 4, we saved a copy of the full matrix \\(\\boldsymbol{\\mathbf{X}}\\) (which we called X_full) of signed on-court indicators, a vector of point differentials per 100 possessions \\(\\boldsymbol{Y}\\) (Y), and a look-up table containing player id numbers and names. We load those objects into our environment.\n\nload(\"lecture04_05_data.RData\")",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#sec-ridge",
    "href": "lectures/lecture05.html#sec-ridge",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nUnfortunately, a unique least squares estimator of \\(\\boldsymbol{\\alpha}\\) does not exist. This is because the matrix \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) does not have a unique inverse[^rankdeficient]. Instead, there are infinitely many different vector \\(\\boldsymbol{\\alpha}\\) that minimize the quantity \\[\n\\sum_{i = 1}^{n}{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}},\n\\] Complicating matters further, the data do not provide any guidance from picking between these minimizers as they all yield the same sum of squared errors. The original developers of adjusted plus/minus overcame this issue by (i) specifying a group of baseline-level players; (ii)\nRather than minimize the sum of the squared errors, we will instead try to minimize the quantity \\[\n\\sum_{i = 1}^{n}{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}} + \\lambda \\times \\sum_{j = 0}^{p}{\\alpha_{j}^{2}},\n\\] where \\(\\lambda &gt; 0\\) is a fixed number1.\nThe first term is the familiar sum of squared errors \\(\\sum{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}}.\\) The second term \\(\\sum_{j}{\\alpha_{j}^{2}}\\) is known as the shrinkage penalty because it is small whenever all the \\(\\alpha_{j}\\)’s are close to zero (i.e., when they are shrunk towards zero). Minimizing the penalized sum of squares trades off finding \\(\\alpha_{j}\\)’s that best fit the data (i.e., minimize the squared error) and finding \\(\\alpha_{j}\\)’s that are not too far away from zero. When \\(\\lambda\\) is very close to zero, the shrinkage penalty tends to exert little influence but as \\(\\lambda \\rightarrow \\infty,\\) the penalty dominates and the minimizer converges to the vector of all zeros. Consequently, it is imperative that we select a good value of \\(\\lambda.\\)\nBefore discussing how we set the tuning parameter \\(\\lambda\\) in practice, it is worth pointing out that for every \\(\\lambda &gt; 0,\\) a unique solution to the penalized sum of squares problems exists and is available in closed-form: \\[\n\\hat{\\boldsymbol{\\alpha}}(\\lambda) = \\left(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}} + \\lambda I \\right)^{-1}\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Y}},\n\\] Looking at this formula, we notice that the expression is very nearly identical to the general formula for ordinary least squares. The only difference is that we’ve added \\(\\lambda\\) to the diagonals of \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\), which guarantees that the matrix \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}} + \\lambda I\\) has a unique inverse. This technique of adding a small perturbation to \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) in the least squares formula — which is equivalent to solving the penalized sum of squares problem — is known as ridge regression.2\n\nPicking \\(\\lambda\\) with Cross-Validation\nIn practice, we typically pick a value of \\(\\lambda\\) using cross-validation The basic idea is to select the value of \\(\\lambda\\) that results in the smallest out-of-sample prediction error. Since we don’t have any out-of-sample data, we will instead approximate it using the technique from Lecture 3 in which we repeatedly held out a subset of the data when training our model and then assessed the error on the held-out data. This process is known as “cross-validation.”\nMore specifically, we begin by specifying a large grid of potential \\(\\lambda\\) values. Then, for each \\(\\lambda\\) in the grid, we repeatedly iterate through the following steps. 1. Form a random training/testing split 2. Fit a ridge regression model with the current value of \\(\\lambda\\) using only the training subset to obtain an estimate \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda)\\) 3. Compute the average of the squared errors \\(\\left(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\hat{\\boldsymbol{\\alpha}}(\\lambda) \\right)\\) in the testing subset. After iterating through these steps many times for a single \\(\\lambda,\\) we compute the average out-of-sample errors (across the different training/testing splits) before moving onto the next \\(\\lambda\\) value in the grid.\nAt the end of this process, we have an estimate for the out-of-sample mean square error for every \\(\\lambda\\) and can pick the \\(\\lambda\\) that yields the smallest one. Then, we can fit a ridge regression model using all of the data to obtain our final solution.",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#sec-rapm",
    "href": "lectures/lecture05.html#sec-rapm",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Regularized Adjusted Plus/Minus",
    "text": "Regularized Adjusted Plus/Minus\nTo recap, by fitting a ridge regression model instead of an ordinary linear model, we are able to estimate an adjusted plus/minus value for all players, thereby avoiding having to specify an arbitrary baseline-level. Although this is conceptually easy, finding a suitable \\(\\lambda\\) via cross-validation is quite an involved process. Luckily for us, the function cv.glmnet() from the glmnet package automates the process (including specifying the initial grid).\nThe following code shows how to perform the cross-validation, plots the cross-validation error for each value of \\(\\lambda\\), and selects the \\(\\lambda\\) with smallest error.\n\n1set.seed(479)\n2library(glmnet)\ncv_fit &lt;-\n3  cv.glmnet(x = X_full, y = Y,\n4            alpha = 0,\n5            standardize = FALSE)\n6lambda_min &lt;- cv_fit$lambda.min\n\n\n1\n\nBecause cross-validation involves forming random training/testing splits, setting the randomization seed ensures our results are reproducible.\n\n2\n\nLoad the glmnet package into our R environment\n\n3\n\nUnlike lm, glmnet functions do not use a formula argument. Instead, you must manually pass the design matrix (without intercept) and output vector.\n\n4\n\nThe argument alpha = 0 tells glmnet to use a squared error penalty.\n\n5\n\nIt is extremely important to set standardize = FALSE. If you don’t, you end up over-shrinking the APM estimates of players with lots of playing time and under-shrinking APM estimates for players with little playing time\n\n6\n\nExtracts the value of \\(\\lambda\\) with smallest out-of-sample error\n\n\n\n\nWe can plot the out-of-sample error as a function of \\(\\lambda\\):\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(cv_fit)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nPlots the out-of-sample error as a function of \\(\\lambda.\\)\n\nWe can now fit a ridge regression model with the cross-validated choice of \\(\\lambda\\) using data from all our stints. Because we do not need to run cross-validation again, we use the function glmnet instead of cv.glmnet.\nIn the following code, we actually fit a ridge regression model using all values of \\(\\lambda\\) from the grid constructed by cv.glmnet(). This is because the function glmnet can behave a little strangely when only one value of \\(\\lambda\\) is specified3. Then, we pull out the estimated coefficients corresponding to the optimal \\(\\lambda\\) value.\n\n1lambda_index &lt;- which(cv_fit$lambda == lambda_min)\nfit &lt;-\n  glmnet(x = X_full, y = Y,\n         alpha = 0,\n2         lambda = cv_fit$lambda,\n         standardize = FALSE)\n\n3alpha_hat &lt;- fit$beta[,lambda_index]\n\n4rapm &lt;-\n  data.frame(\n    id = names(alpha_hat),\n    rapm = alpha_hat) |&gt;\n  dplyr::inner_join(y = player_table |&gt; dplyr::select(id,Name), by = \"id\")\n\n\n1\n\nDetermines where in the grid of \\(\\lambda\\) values specified by cv_glmnet() the optimal one lies.\n\n2\n\nSolves the ridge regression problem at all values of \\(\\lambda\\) along the grid specified by the initial cv_glmnet() fit.\n\n3\n\nglmnet() returns a matrix of parameter estimates where the columns correspond to the different \\(\\lambda\\) values. Here, we pull out the estimates for the optimal \\(\\lambda\\), which was the second value provided.\n\n4\n\nLike we did in Lecture 4, we create a data frame with every players’ regularized adjusted plus/minus value and name\n\n\n\n\nWe recognize many super-star-level players (e.g., Gilgeous-Alexander, Dončić, Jokić, Giannis) among those with the highest regularized adjusted plus/minus values.\n\nrapm |&gt;\n  dplyr::arrange(dplyr::desc(rapm)) |&gt;\n  dplyr::slice_head(n = 10)\n\n        id     rapm                    Name\n1  1628983 3.602143 Shai Gilgeous-Alexander\n2  1627827 3.472249     Dorian Finney-Smith\n3  1630596 3.415232             Evan Mobley\n4  1629029 3.352736             Luka Doncic\n5   202699 3.183716           Tobias Harris\n6   203999 2.846327            Nikola Jokic\n7  1631128 2.829463         Christian Braun\n8  1628384 2.813794              OG Anunoby\n9   203507 2.646488   Giannis Antetokounmpo\n10 1626157 2.641075      Karl-Anthony Towns\n\n\nRecall that in our regularized adjusted plus/minus model, the individual \\(\\alpha_{j}\\)’s correspond to a somewhat absurd counter-factual. Specifically, \\(\\alpha_{j}\\) represents the amount by which a team’s point differential per 100 possession increases if they go from playing 4-on-5 to playing 5-on-5 with player \\(j\\) on the court. Differences of the form \\(\\alpha_{j} - \\alpha_{j'}\\) are much more useful than the single \\(\\alpha_{j}\\) values. By replacing player \\(j'\\) with player \\(j,\\) a team can expect to score \\(\\alpha_{j} - \\alpha_{j'}\\) more points per 100 possessions.\nFor instance, by replacing Luka Dončić with Anthony Davis, teams are expected to score an additional -3.41 points per possession4.\n\nluka_rapm &lt;- rapm |&gt; dplyr::filter(Name == \"Luka Doncic\") |&gt; dplyr::pull(rapm)\nad_rapm &lt;- rapm |&gt; dplyr::filter(Name == \"Anthony Davis\") |&gt; dplyr::pull(rapm)\n\nad_rapm - luka_rapm\n\n[1] -3.417006",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#sec-bootstrap",
    "href": "lectures/lecture05.html#sec-bootstrap",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Uncertainty Quantification with the Bootstrap",
    "text": "Uncertainty Quantification with the Bootstrap\nSo far, we have only focused on obtaining point estimates. What is the range of our uncertainty about the relative impact of players.\nThe bootstrap is an elegant way to quantify uncertainty about the output particular statistical method. At a high-level, the bootstrap works by repeatedly sampling observations from the original dataset and applying the method to the re-sampled dataset. In the context of our RAPM analysis, we will repeat the following steps many, many times (e.g., \\(B = 100\\) times):\n\nRandomly draw a sample of \\(n\\) stints with replacement\nUsing the random re-sample, compute an estimate of the RAPM parameters \\(\\hat{\\boldsymbol{\\alpha}}(\\hat{\\lambda})\\) by fitting a ridge regression model with the optimal \\(\\lambda\\) found by our initial cross-validation.\nSave the estimates in an array\n\nRunning this procedure, we obtain \\(B\\) different estimates \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda)^{(1)}, \\ldots, \\hat{\\boldsymbol{\\alpha}}(\\lambda)^{(B)}\\) where \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda)^{(b)}\\) is the estimate obtained during the \\(b\\)-th time through the loop. Importantly, each bootstrap estimate of the whole vector \\(\\boldsymbol{\\alpha}\\) yields a bootstrap estimate of the constrast \\(\\alpha_{j} - \\alpha_{j'}.\\) By examining the\nAs a concrete illustration, the following code computes 500 bootstrap estimates of the RAPM parameter vector. It saves the bootstrap estimates of \\(\\boldsymbol{\\alpha}\\) in a large matrix whose rows correspond to bootstrap samples and whose columns correspond to players.\n\n\n\n\n\n\nImportant\n\n\n\n##Warning The following code takes several minutes to run!\n\n\n\n1B &lt;- 500\n2n &lt;- nrow(X_full)\np &lt;- ncol(X_full)\n3player_names &lt;- colnames(X_full)\nboot_rapm &lt;- \n4  matrix(nrow = B, ncol = p,\n5         dimnames = list(c(), player_names))\n\nfor(b in 1:B){\n  if(b %% 10 == 0) cat(b, \" \")\n6  set.seed(479+b)\n7  boot_index &lt;- sample(1:n, size = n, replace = TRUE)\n  \n  fit &lt;- \n8    glmnet(x = X_full[boot_index,], y = Y[boot_index],\n           alpha = 0,\n           lambda = cv_fit$lambda,\n           standardize = FALSE)\n  tmp_alpha &lt;- fit$beta[,lambda_index]\n9  boot_rapm[b, names(tmp_alpha)] &lt;- tmp_alpha\n}\n\n\n1\n\nSet number of bootstrap iterations\n\n2\n\nGet the number of stints and players\n\n3\n\nGet the list of player id’s, which are also the column names of X_full\n\n4\n\nBuild a \\(B \\times p\\) matrix to store bootstrap estimates of \\(\\boldsymbol{\\alpha}.\\)\n\n5\n\nUse the players’ IDs as column names for boot_rapm\n\n6\n\nWe set the seed for reproducibility. But we need to vary the seed with every bootstrap re-sample. Otherwise, we’d be drawing the exact same sample every time!\n\n7\n\nThis is a list of row-indices for our re-sampled stints.\n\n8\n\nTakes advantage of R’s indexing capabilities to create our re-sampled design matrix and outcome vector\n\n9\n\nSaves the bootstrap estimate of \\(\\boldsymbol{\\alpha}\\) in the \\(b\\)-th row of boot_ramp\n\n\n\n\n10  20  30  40  50  60  70  80  90  100  110  120  130  140  150  160  170  180  190  200  210  220  230  240  250  260  270  280  290  300  310  320  330  340  350  360  370  380  390  400  410  420  430  440  450  460  470  480  490  500  \n\n\nWe can now compute the difference between Dončić’s and Davis’ estimated RAPM values for every bootstrap estimate. We find that the overwhelming majority of these samples are negative, suggesting that that we can be virtually certain that a team is expected to score score fewer points per 100 possessions by replacing Dončić by Davis.\n\n1doncic_id &lt;- player_table |&gt; dplyr::filter(Name == \"Luka Doncic\") |&gt; dplyr::pull(id)\ndavis_id &lt;- player_table |&gt; dplyr::filter(Name == \"Anthony Davis\") |&gt; dplyr::pull(id) \n\n2boot_diff &lt;- boot_rapm[,davis_id] - boot_rapm[,doncic_id]\n\n3hist(boot_diff,\n4     breaks = 20,\n     main = \"Bootstrap Distribution of Davis-Doncic\", \n     xlab = \"Difference\")\n\n\n1\n\nGet Dončić’s and Davis’ ids\n\n2\n\nGet the vectors of Dončić’s and Davis’ bootstrapped RAPM values by extracting the relevant columns from boot_rapm and compute the difference\n\n3\n\nMake a histogram of the bootstrapped differences between Davis’ and Dončić’s RAPM values\n\n4\n\nSet the number of bins in the histogram to 20\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bootstrap distribution of difference between Doncic & Davis\n\n\n\n\n\nWe can form an uncertainty interval using the sample quantiles of the bootstrapped differences between Davis’ and Dončić’s RAPM values.\n\n1quantile(boot_diff, probs = c(0.025, 0.975))\n\n\n1\n\nA 95% uncertainty interval can be formed using the 2.5% and 97.5% quantiles of the bootstrapped estimates\n\n\n\n\n      2.5%      97.5% \n-6.3776674 -0.4839716 \n\n\nWe conclude with 95% confidence that by replacing Dončić by Davis, the Mavericks are expected to score between 0.48 and 6.3 fewer points per 100 possessions.\n\nBootstrap Intervals for Ranks\nRecall that in our original analysis, Shai Gilgeous-Alexander had the highest RAPM value. How certain should we be about his overall ranking?\nJust like we computed the difference between Dončić’s and Davis’ RAPM estimates for every bootstrap re-sample, we can compute the rank of SGA’s RAPM estimate in every bootstrap sample. Then, we can look at the bootstrap distribution of these ranks to get an idea about the uncertainty in the rank.\nThe rank() function returns the sample ranks of the values in vector. For instance,\n\nx &lt;- c(100, -1, 3, 2.5, -2)\nrank(x)\n\n[1] 5 2 4 3 1\n\n\nNotice that the smallest value in x (-2) is ranked 1 and the largest value in x (100) is ranked 5. This demonstrates that rank() assigns ranks in increasing order. If we wanted the largest value to be ranked 1, the second largest ranked 2, etc., then we need to call rank on the negative values:\n\nrank(-x)\n\n[1] 1 4 2 3 5\n\n\nThe following code confirms that SGA ranked first in terms of the original RAPM estimate.\n\nrapm &lt;-\n  rapm |&gt;\n  dplyr::mutate(rank_rapm = rank(-1 * rapm))\n\nrapm |&gt; dplyr::filter(Name == \"Shai Gilgeous-Alexander\")\n\n       id     rapm                    Name rank_rapm\n1 1628983 3.602143 Shai Gilgeous-Alexander         1\n\n\nTo quantify the uncertainty in SGA’s RAPM ranking, we need to rank the values within every bootstrap estimate of \\(\\boldsymbol{\\alpha}.\\) In other words, we need to rank the values within every row of boot_rapm. We can do this using the apply() function. For our purposes, apply() runs a function (specified using the FUN argument) on every row or column of a two-dimensional array. To run the function along every row, one specifies the argument MAR = 1 and to run the function along every column, one specifies the argument MAR = 2. Notice that the output has \\(p\\) rows (one for every player) and \\(B\\) columns (one for every bootstrap iteration).\n\nboot_rank &lt;- \n  apply(-1*boot_rapm, MARGIN = 1, FUN = rank)\ndim(boot_rank)\n\n[1] 569 500\n\n\nTo get the bootstrap distribution of SGA’s ranking, we need to extract the values from the corresponding row of boot_rank. In the following code, we first look up SGA’s player ID and then get the values from the row of boot_rank that is named with SGA’s ID.\n\nsga_id &lt;- \n  player_table |&gt;\n  dplyr::filter(Name == \"Shai Gilgeous-Alexander\") |&gt;\n  dplyr::pull(id)\n\nsga_ranks &lt;- boot_rank[sga_id,]\ntable(sga_ranks)\n\nsga_ranks\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18  19  20  21 \n 58  59  46  55  36  21  19  12  17  17  13  10   8  12   8   5   2   9   6   5 \n 22  23  24  25  26  27  28  29  30  31  32  33  34  36  37  38  39  40  41  44 \n  4   5   4   5   5   2   2   4   5   2   2   5   4   3   2   1   2   1   1   2 \n 45  47  48  51  52  56  57  66  70  84  85  86  91 105 106 176 \n  5   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1 \n\n\nIt turns out that SGA had the highest RAPM value in just 58 of our 500 bootstrap samples and the second-highest RAPM value in 59 of our bootstrap samples. We can form a 95% bootstrap uncertainty interval around his overall ranking by computing the sample quantiles of these rankings.\n\nquantile(sga_ranks, probs = c(0.025, 0.975))\n\n  2.5%  97.5% \n 1.000 51.525 \n\n\nOur analysis reveals quite a bit of uncertainty about SGA’s overall rank in terms of RAPM!",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#exercises",
    "href": "lectures/lecture05.html#exercises",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Exercises",
    "text": "Exercises\n\nPick several players and compare the bootstrap distribution of the difference in their RAPM estimates.\nUsing the same 100 training/testing splits constructed in Exercise 4.5, estimate the out-of-sample mean square error for RAPM based on the 2024-25 data. Instead of computing an optimal \\(\\lambda\\) for each training split, you may use the optimal \\(\\lambda\\) value computed on the whole dataset. Is RAPM a better predictive model than APM?\nFit RAPM models to data from (i) the 2023-24 and 2024-25 regular seasons and (ii) the 2022-23, 2023-24, and 2024-25 regular season. Use the bootstrap to quantify how uncertainty about the difference in player RAPM values and the overall rankings changes as you include data from more seasons.\nJust like lm, the functions cv.glmnet() and glmnet() accept a weights argument. Using the weighting schemes you considered in Exercise 4.2, fit regularized weighted adjusted plus/minus models.",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#footnotes",
    "href": "lectures/lecture05.html#footnotes",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn practice, \\(\\lambda\\) is often determined using the data. See Section 2.1.↩︎\nSee Section 6.2 of An Introduction to Statistical Learning for a good review of ridge regression.↩︎\nIn fact, the glmnet() documentation explicitly warns users from running with only a single value of \\(\\lambda.\\)↩︎\nOnce again: Fire Nico.↩︎",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture03.html",
    "href": "lectures/lecture03.html",
    "title": "Lecture 3: Estimating XG",
    "section": "",
    "text": "In Lecture 2, we fit two very basic models for XG. The first used only body part to predict the probability of the shot resulting in a goal while the second used both body part and shot technique. By looking at a few of Beth Mead’s shots from EURO 2022, we decided that the second model was a better because it assigned a higher XG to her one-on-one lob than the shot through multiple defenders.\nIn this lecture, we review several quantitative measures of predictive performance (Section 2) and introduce a principled way to estimate how model’s perform out-of-sample (Section 3). Then, we fit introduce a logistic regression model that accounts for continuous features like distance to the goal (Section 4) before fitting a more flexible, nonparametric model that can simultaneously account for many more features (Section 5).\n\n\nWe will continue to work with all shot event data from women’s international competitions that StatsBomb makes publicly available. Using code from Lecture 2, we can load in all the shot event data and create a binary outcome recording whether the shot resulted in a goal.\n\nwi_shots &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international)|&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam() |&gt;\n6  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n7  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\n1\n\nGets the table of all available competitions\n\n2\n\nIdentifies all women’s international competitions\n\n3\n\nGets match-level data from all women’s international competitions\n\n4\n\nGets event-level data\n\n5\n\nRuns StatsBomb’s recommend pre-processing\n\n6\n\nSubsets to shot-event data taken with either the foot or head\n\n7\n\nCreates a column Y\n\n\n\n\nWe will also re-construct the two simple XG models, one that conditions only on body part and the other that conditions on body part and technique. We then create a data table with columns containing the body part, technique, and outcome of the shot as well as the XG predictions from both models.\n\nxg_model1 &lt;- \n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\") \n\nsimple_preds &lt;-\n  wi_shots |&gt;\n  dplyr::select(Y, shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::left_join(y = xg_model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::left_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-overview",
    "href": "lectures/lecture03.html#sec-overview",
    "title": "Lecture 3: Estimating XG",
    "section": "",
    "text": "In Lecture 2, we fit two very basic models for XG. The first used only body part to predict the probability of the shot resulting in a goal while the second used both body part and shot technique. By looking at a few of Beth Mead’s shots from EURO 2022, we decided that the second model was a better because it assigned a higher XG to her one-on-one lob than the shot through multiple defenders.\nIn this lecture, we review several quantitative measures of predictive performance (Section 2) and introduce a principled way to estimate how model’s perform out-of-sample (Section 3). Then, we fit introduce a logistic regression model that accounts for continuous features like distance to the goal (Section 4) before fitting a more flexible, nonparametric model that can simultaneously account for many more features (Section 5).\n\n\nWe will continue to work with all shot event data from women’s international competitions that StatsBomb makes publicly available. Using code from Lecture 2, we can load in all the shot event data and create a binary outcome recording whether the shot resulted in a goal.\n\nwi_shots &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international)|&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam() |&gt;\n6  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n7  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\n1\n\nGets the table of all available competitions\n\n2\n\nIdentifies all women’s international competitions\n\n3\n\nGets match-level data from all women’s international competitions\n\n4\n\nGets event-level data\n\n5\n\nRuns StatsBomb’s recommend pre-processing\n\n6\n\nSubsets to shot-event data taken with either the foot or head\n\n7\n\nCreates a column Y\n\n\n\n\nWe will also re-construct the two simple XG models, one that conditions only on body part and the other that conditions on body part and technique. We then create a data table with columns containing the body part, technique, and outcome of the shot as well as the XG predictions from both models.\n\nxg_model1 &lt;- \n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\") \n\nsimple_preds &lt;-\n  wi_shots |&gt;\n  dplyr::select(Y, shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::left_join(y = xg_model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::left_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-model-comparison",
    "href": "lectures/lecture03.html#sec-model-comparison",
    "title": "Lecture 3: Estimating XG",
    "section": "Model Comparison",
    "text": "Model Comparison\nOur first XG model conditions only on the body part used to take the shot while our second model additionally conditions on the technique. Consequently, the first model returns exactly the same predicted XG for a right-footed half-volley (e.g., this Beth Mead goal against Sweden in EURO 2022) as it does for a right-footed backheel shot (e.g., this Alessia Russo goal from the same game). In contrast, our second model, which accounts for both the body part and the shot technique, can distinguish between these two shots and return different XG predictions. Intuitively, we might expect the second model’s predictions to be more accurate because it leverages more information.\nTo make this more concrete, suppose we have observed a shots dataset of pairs \\((\\boldsymbol{\\mathbf{x}}_{1}, y_{1}), \\ldots (\\boldsymbol{\\mathbf{x}}_{n}, y_{n})\\) of feature vectors \\(\\boldsymbol{\\mathbf{X}}\\) and binary indicators \\(Y\\) recording whether the shot resulted in a goal or not. Recall from Lecture 2 that a key assumption of XG models is the observed dataset comprises a sample from some infinite super-population of shots. For each feature combination \\(\\boldsymbol{\\mathbf{x}},\\) the corresponding \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is defined to be the conditional expectation \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) := \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}].\\) Because \\(Y\\) is a binary indicator, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) can be interpreted as the probability that a shot with features \\(\\boldsymbol{\\mathbf{x}}\\) results in a goal.\nNow, suppose we have used our data to fit an XG model. We can go back to every shot \\(i\\) in our dataset and use each fitted model to obtain the predicted XG, which we denote1 as \\(\\hat{p}_{i}.\\) We are ultimately interested in assessing how close \\(\\hat{p}_{i}\\) is to the actual observed \\(Y_{i}.\\)\nIn Statistics and Machine Leaning, there are three common ways to assess the discrepancy between a probability forecast \\(\\hat{p}_{i}\\) and a binary outcome: misclassification rate, Brier score, and log-loss.\n\nMisclassification Rate\nMisclassification rate is the coarsest measure of discrepancy between a predicted probability and a binary outcome. A forecast \\(\\hat{p}_{i} &gt; 0.5\\) (resp. \\(\\hat{p}_{i} &lt; 0.5\\)) reflects the fact that we believe it more likely than not that \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\)). So, intuitively, we should expect a highly accurate model to return forecasts \\(\\hat{p}_{i}\\) that were greater (resp. less) than 0.5 whenever \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\)). A model’s misclassification rate is the proportion of times that the model deviates from this expectation (i.e., when it returns forecasts greater than 50% when \\(y = 0\\) and returns forecasts less than 50% when \\(y = 1.\\))\nFormally, given a binary observations \\(y_{1}, \\ldots, y_{n}\\) and predicted probabilities \\(\\hat{p}_{1}, \\ldots, \\hat{p}_{n},\\) the misclassification rate is defined as \\[\n\\textrm{MISS} = n^{-1}\\sum_{i = 1}^{n}{\\mathbb{I}(y_{i} \\neq \\mathbb{I}(\\hat{p}_{i} \\geq 0.5))},\n\\] where \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) equals 1 when \\(\\hat{p}_{i} \\geq 0.5\\) and equals 0 otherwise and \\(\\mathbb{I}(y_{i} \\neq \\mathbb{I}(\\hat{p}_{i} \\geq 0.5))\\) equals 1 when \\(y_{i}\\) is not equal to \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) and equals 0 otherwise. On the view that misclassification rate captures the number of times our predictions are on the wrong side of the 50% boundary, we prefer models with smaller misclassification rate.\nIn the context of XG, misclassification rate measures the proportion of times we predict an XG higher than 0.5 for a non-goal or an XG lower than 0.5 for a goal.\n\n1misclass &lt;- function(y, phat){\n  return( mean( (y != 1*(phat &gt;= 0.5))))\n}\n\n2cat(\"BodyPart misclassification\",\n    round(misclass(simple_preds$Y, simple_preds$XG1), digits = 3), \"\\n\")\ncat(\"BodyPart+Technique misclassificaiton\", \n    round(misclass(simple_preds$Y, simple_preds$XG2), digits = 3), \"\\n\")\n\n\n1\n\nA helper function that computes misclassification rate.\n\n2\n\ncat() prints output to the R console and it is usually good to round numeric output to 2 or 3 decimal points.\n\n\n\n\nBodyPart misclassification 0.112 \nBodyPart+Technique misclassificaiton 0.112 \n\n\nOur two simple models have identical misclassification rates of about 10.7%. At first glance this is a bit surprising because the two models return different predicted XG values\n\ncat(\"Unique XG1 values:\", sort(unique(simple_preds$XG1)), \"\\n\")\n\nUnique XG1 values: 0.1113281 0.1119565 0.1140625 \n\ncat(\"Unique XG2 values:\", sort(unique(simple_preds$XG2)), \"\\n\")\n\nUnique XG2 values: 0 0.06372549 0.06756757 0.07142857 0.08920188 0.1034483 0.1131868 0.1207729 0.1214361 0.1632653 0.2083333 \n\n\nOn closer inspection, we find that the two simple models both output predicted XG values that are all less than 0.5. So, the thresholded values \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) are identical for both models’ \\(\\hat{p}_{i}\\)’s. Just for comparison’s sake, let’s compute the misclassification rate of the proprietary StatsBomb XG model\n\ncat(\"StatsBomb misclassificaiton\", \n    round(misclass(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 3), \"\\n\")\n\nStatsBomb misclassificaiton 0.095 \n\n\nWe see that StatsBomb’s model has a slightly smaller misclassification rate than our two simple models, but the gap is not very large.\n\n\nBrier Score & Log-Loss\nA major drawback of misclassification rate is that it only penalizes predictions for being on the wrong side of 50% but does not penalize predictions based on how far away they are from 0 or 1. For instance, if one model returns \\(\\hat{p} = 0.999\\) and another model returns \\(\\hat{p} = 0.501,\\) the thresholded forecasts \\(\\mathbb{I}(\\hat{p} &gt; 0.5)\\) are identical.\nBrier score and log-loss both take into account how far \\(\\hat{p}\\) is from \\(Y,\\) albeit in slightly different ways. The Brier score is defined as \\[\n\\text{Brier} = n^{-1}\\sum_{i = 1}^{n}{(y_{i} - \\hat{p}_{i})^2}.\n\\] The quantity \\((y_{i} - \\hat{p}_{i})^{2}\\) is small whenever (i) \\(y_{i} = 1\\) and \\(\\hat{p}_{i}\\) is very close to 1 or (ii) \\(y_{i} = 0\\) and \\(\\hat{p}_{i}\\) is very close to 0. The quantity is very large when \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\) and \\(\\hat{p}\\) is very close to 0 (resp. very closer to 1).\nLog-loss, which is also known as cross-entropy loss in the Machine Learning literature, more aggressively penalizes wrong forecasts. It is defined as \\[\n\\textrm{LogLoss} = -1 \\times \\sum_{i = 1}^{n}{\\left[ y_{i} \\times \\log(\\hat{p}_{i}) + (1 - y_{i})\\times\\log(1-\\hat{p}_{i})\\right]}.\n\\] Notice that if \\(y_{i} = 1\\) as \\(\\hat{p}_{i} \\rightarrow 0,\\) the term \\(y_{i} \\times \\log{\\hat{p}_{i}} \\rightarrow -\\infty.\\) So, while the Brier score is mathematically bounded between 0 and 1, the average log-loss can be arbitrarily large. Essentially, log-loss heavily penalizes predictions that are confident (i.e., very close to 0 or 1) but wrong (i.e., different than \\(y_{i}\\).) Like we did for the misclassification rate, we define a helper function for computing the Brier score and then apply that function to predictions from our two simple XG models and StatsBomb’s XG model.\n\nbrier &lt;- function(y, phat){\n  return(mean( (y - phat)^2 ))\n}\n\ncat(\"BodyPart Brier\", \n    round(brier(simple_preds$Y, simple_preds$XG1), digits = 4), \"\\n\")\n\nBodyPart Brier 0.0996 \n\ncat(\"BodyPart+Technique Brier\", \n    round(brier(simple_preds$Y, simple_preds$XG2) , digits = 4), \"\\n\")\n\nBodyPart+Technique Brier 0.0991 \n\ncat(\"StatsBomb Brier\",\n    round(brier(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 4), \"\\n\")\n\nStatsBomb Brier 0.0753 \n\n\nWhen computing log-loss in practice, it’s common to truncate probabilities close to 0 or 1, which ensures the final answer is finite (i.e., avoids trying to compute log(0)).\n\nlogloss &lt;- function(y, phat){\n  \n1  if(any(phat &lt; 1e-12)) phat[phat &lt; 1e-12] &lt;- 1e-12\n  if(any(phat &gt; 1-1e-12)) phat[phat &gt; 1-1e-12] &lt;- 1-1e-12\n  return(-1 * mean( y * log(phat) + (1-y) * log(1-phat)))\n}\n\ncat(\"BodyPart LogLoss:\", \n    round(logloss(simple_preds$Y, simple_preds$XG1), digits = 3), \"\\n\")\ncat(\"BodyPart+Technique LogLoss:\", \n    round(logloss(simple_preds$Y, simple_preds$XG2) , digits = 3), \"\\n\")\ncat(\"StatsBomb LogLoss:\",\n    round(logloss(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 3), \"\\n\")\n\n\n1\n\nSince \\(\\log(0) = -\\infty,\\) we often truncate very small or very large predictions when computing average log-loss\n\n\n\n\nBodyPart LogLoss: 0.351 \nBodyPart+Technique LogLoss: 0.348 \nStatsBomb LogLoss: 0.264",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-train-test",
    "href": "lectures/lecture03.html#sec-train-test",
    "title": "Lecture 3: Estimating XG",
    "section": "Estimating Out-of-Sample Performance",
    "text": "Estimating Out-of-Sample Performance\nBased on the preceding calculations, it is tempting to conclude that our second, more complex model, is more accurate than the first model. After all, it achieved a slightly smaller Brier score and log-loss. There is, however, a small catch: we assessed the model’s performance using exactly the same data that we used to fit the model.\nGenerally speaking, if we used the data \\((\\boldsymbol{\\mathbf{x}}_{1}, y_{1}), \\ldots, (\\boldsymbol{\\mathbf{x}}_{n}, y_{n})\\) to produce an estimate \\(\\hat{f}\\) of the conditional expectation function \\(f(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}].\\) We can use this estimate to compute predictions \\(\\hat{y}_{i} = \\hat{f}(\\boldsymbol{\\mathbf{x}})\\) of the originally observed outcomes \\(y_{i}.\\) While we would like each \\(\\hat{y}_{i} \\approx y_{i},\\) we are often more interested in determining whether \\(\\hat{f}(\\boldsymbol{\\mathbf{x}}^{\\star})\\) is approximately equal to \\(y^{\\star}\\) for some new pair \\((\\boldsymbol{\\mathbf{x}}^{\\star}, y^{\\star})\\) not included in the original dataset. That is, we are often interested in knowing how well the fitted model can predict previously unseen data from the same infinite super-population. Moreover, between two models, we tend to prefer the one that yields smaller out-of-sample or testing error. That is, in the context of our XG models, we would prefer the one that yielded smaller misclassification rate, Brier score, and/or log-loss on shots not used to fit our initial models.\nIn practice, we rarely have access to a separate set of data that we can hold-out of training. Instead, we often (i) divide our data into two parts; (ii) fit our model on one part (the training data); and (iii) assess the predictive performance on the second part (the testing data). Commonly, we use 75% or 80% of the data to train a model and set the remaining part aside as testing data. Moreover, we often create the training/testing split randomly.\nThe next codeblock illustrates this workflow. To help create training/testing splits, we start by giving every row its own unique ID. Then, we randomly select a fixed number of rows to form our training dataset.\n\nn &lt;- nrow(wi_shots)\n1n_train &lt;- floor(0.75 * n)\nn_test &lt;- n - n_train\n\nwi_shots &lt;-\n  wi_shots |&gt;\n2  dplyr::mutate(id = 1:n)\n\n3set.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n4  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n5  dplyr::anti_join(y = train_data, by = \"id\")\n\n\n1\n\nWe will use about 75% of the data for training and the remainder for testing\n\n2\n\nAssign a unique ID number to each row in wi_shots\n\n3\n\nWe often manually the randomization seed, which dictates how R generates (pseudo)-random numbers (see here), to ensure full reproducibility of our analyses. Another user running this code should get the same results.\n\n4\n\nThe function dplyr::slice_sample() returns a random subset of rows.\n\n5\n\nThe function dplyr::anti_join() extracts the rows in wi_shots whose IDs are not contained in train_data\n\n\n\n\nAs a sanity check, we can check whether any of the id’s in test_data are also in train_data:\n\nany(train_data$id %in% test_data$id)\n\n[1] FALSE\n\n\nNow that we have a single training and testing split, let’s fit our two simple XG models only using the training data and then computing the average training and testing losses:\n\nmodel1 &lt;- \n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nmodel2 &lt;-\n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\ntrain_preds &lt;-\n  train_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ntest_preds &lt;-\n  test_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ncat(\"BodyPart train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG1), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG1), digits = 3), \"\\n\")\n\nBodyPart train log-loss: 0.357 test log-loss: 0.334 \n\ncat(\"BodyPart+Technique train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG2), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG2), digits = 3), \"\\n\")\n\nBodyPart+Technique train log-loss: 0.355 test log-loss: 0.351 \n\n\nFor this training/testing split, we see that the more complex model, which conditioned on both body-part and technique, produced slightly smaller log-loss than the simple model, which only conditioned on body-part. But this may not necessarily always hold. For instance, if we drew a different training/testing split (e.g., by initially setting a different randomization seed), we might observe the opposite:\n\nset.seed(478)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\")\n\nmodel1 &lt;- \n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nmodel2 &lt;-\n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\ntrain_preds &lt;-\n  train_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ntest_preds &lt;-\n  test_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ncat(\"BodyPart train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG1), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG1), digits = 3), \"\\n\")\n\nBodyPart train log-loss: 0.359 test log-loss: 0.327 \n\ncat(\"BodyPart+Technique train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG2), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG2), digits = 3), \"\\n\")\n\nBodyPart+Technique train log-loss: 0.356 test log-loss: 0.346 \n\n\nOn this second random split, the simpler model had better in-sample and out-of-sample performance.\nInstead of relying on a single training/testing split, we generally average the in- and out-of-sample losses across several different random splits. In the code below, we repeat the above exercise using 100 random training/testing splits. Notice that in our for loop, we manually set the seed in a predictable way to ensure reproducibility.\n\nn_sims &lt;- 100\n1train_logloss &lt;-\n  matrix(nrow = 2, ncol = n_sims,\n         dimnames = list(c(\"XG1\",\"XG2\"), c()))\ntest_logloss &lt;- \n  matrix(nrow = 2, ncol = n_sims,\n         dimnames = list(c(\"XG1\",\"XG2\"), c()))\n\nfor(r in 1:n_sims){\n2  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train) \n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n  \n  model1 &lt;- \n    train_data |&gt;\n    dplyr::group_by(shot.body_part.name) |&gt;\n    dplyr::summarise(XG1 = mean(Y))\n  \n  model2 &lt;-\n    train_data |&gt;\n    dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n    dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\n  train_preds &lt;-\n    train_data |&gt;\n    dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n    dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\n  test_preds &lt;-\n    test_data |&gt;\n    dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n    dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n  \n3  train_logloss[\"XG1\", r] &lt;- logloss(train_preds$Y, train_preds$XG1)\n  train_logloss[\"XG2\",r] &lt;- logloss(train_preds$Y, train_preds$XG2)\n  \n  test_logloss[\"XG1\", r] &lt;- logloss(test_preds$Y, test_preds$XG1)\n  test_logloss[\"XG2\",r] &lt;- logloss(test_preds$Y, test_preds$XG2)\n}\n\n4cat(\"XG1 training logloss:\", round(mean(train_logloss[\"XG1\",]), digits = 3), \"\\n\")\ncat(\"XG2 training logloss:\", round(mean(train_logloss[\"XG2\",]), digits = 3), \"\\n\")\n\ncat(\"XG1 test logloss:\", round(mean(test_logloss[\"XG1\",]), digits = 3), \"\\n\")\ncat(\"XG2 test logloss:\", round(mean(test_logloss[\"XG2\",]), digits = 3), \"\\n\")\n\n\n1\n\nWe will save the training and testing log-loss for each training/testing split in a matrix\n\n2\n\nWe create each split using a different randomization seed. Many other choices are possible but simply adding the fold number r to some base seed (479 in this case) is perhaps the simplest approach.\n\n3\n\nWe write the training and testing log-losses for the r-th training/testing split to the r-th column of the matrices train_logloss and test_logloss\n\n4\n\nGet the average loss within each row\n\n\n\n\nXG1 training logloss: 0.351 \nXG2 training logloss: 0.348 \nXG1 test logloss: 0.352 \nXG2 test logloss: 0.356 \n\n\nBased on this analysis, we conclude that the simpler model provides better out-of-sample predictions than the more complex model. In other words, the more complex model appears to over-fit the data!",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-logistic",
    "href": "lectures/lecture03.html#sec-logistic",
    "title": "Lecture 3: Estimating XG",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nAlthough the model that conditions only on body-part has smaller out-of-sample loss than the model that additionally accounts for shot technique, the model is still not wholly satisfactory. This is because it fails to account for important spatial information that we intuitively believe affects goal probability. For instance, we might intuitively expect a model that accounts for the distance between the shot and goal to be more accurate. Unfortunately, it is not easy to apply the same “binning and averaging” approach that we used for our first two models to condition on distance. This is because, with finite data, we will not have much data (if any) for all possible values of distance. As we saw in Lecture 2, averaging outcomes within bins with very small sample sizes can lead to extreme and erratic estimates. One way to overcome this challenge is to fit a statistical model.\nLogistic regression is the canonical statistical model for predicting a binary outcome \\(Y\\) using a vector \\(\\boldsymbol{\\mathbf{X}}\\) of \\(p\\) numerical features \\(X_{1}, \\ldots, X_{p}\\)2.\nThe model asserts that the log-odds that \\(Y = 1\\) is a linear function of the features. That is, \\[\n\\log\\left(\\frac{\\mathbb{P}(Y= 1 \\vert \\boldsymbol{\\mathbf{X}})}{\\mathbb{P}(Y = 0 \\vert \\boldsymbol{\\mathbf{X}})}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p}.\n\\]\n\nLogistic Regression with One Feature\nWe begin by fitting a model that only conditions on distance to the goal using the function glm(). This function works a lot like lm in that we specify the model using R’s formula interface. Whereas lm() is used only to fit linear models via the method least squares, glm() can be used to fit a much wider range of models3. So, we use the family argument to tell glm() exactly what model we want to fit.\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\n1fit1 &lt;- glm(formula = Y~DistToGoal,\n2            data = train_data,\n3            family = binomial(\"logit\"))\n\n\n1\n\nIn the formula argument, the outcome goes on the left-hand side of the ~ and predictors go on the right-hand side\n\n2\n\nLike lm, the function glm takes a data argument. Every term appearing in the formula must appear as a column in the data frame passed via data.\n\n3\n\nThe specification binomial(\"logit\") signals to glm that we want to fit a logistic regression model.\n\n\n\n\nThe summary function provides a snapshot about the fitted model, showing the individual parameter estimates, their associated standard errors, and whether or not they are statistically significantly different than zero.\n\nsummary(fit1)\n\n\nCall:\nglm(formula = Y ~ DistToGoal, family = binomial(\"logit\"), data = train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.134174   0.128410  -1.045    0.296    \nDistToGoal  -0.127023   0.009115 -13.935   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2549.7  on 3569  degrees of freedom\nResidual deviance: 2290.7  on 3568  degrees of freedom\nAIC: 2294.7\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe see that the estimate of \\(\\beta_{1},\\) the coefficient capturing DistToGoal’s effect on the log-odds of a goal, is negative (-0.11) and that this is statistically significantly different than 0 (the associated p-value is &lt;2e-16). This is reassuring as we might intuitively expect the probability of a goal to decrease the further away a shot is from the goal. Recall that the intercept term \\(\\beta_{0}\\) quantifies the log-odds of a goal when DistToGoal = 0 (i.e., when a shot is taken from the middle of the goal line). Our model estimates this intercept to about -0.43, which, on the probability scale is about 39%. On the face of it, this doesn’t make a ton of sense: surely shots taken essentially from the middle of the goal line should go in much more frequently.\nWe can resolve this apparent contradiction by taking a closer look at the range of DistToGoal values in our dataset:\n\nrange(train_data$DistToGoal)\n\n[1]  1.860108 92.800862\n\n\nSince our model was trained on shots that came between 1.8 and 92 yards away, the suspiciously low 39% forecast for goal line shots represents a fairly significant extrapolation.\n\n\n\n\n\n\nExtrapolation\n\n\n\nExercise caution when using predictions at inputs that are far away, systematically different, or otherwise not well-represented in the training data in any downstream analysis.\n\n\nCrucially, we can use our fitted model to make predictions about shots not seen in our training data. In R, we make predictions using the predict() function. The function takes three arguments:\n\nobject: this is the output from our glm call (i.e., the fitted model object), which we have here saved as fit1\nnewdata: this is a data table containing the features of the observations for which we would like predictions. The table that we pass here needs to have columns for every feature used in the model.\ntype: When object is the result from fitting a logistic regression model, predict will return predictions on the log-odds scale (i.e, \\(\\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_[1} + \\cdots + \\hat{\\beta}_{p}X_{p}\\)) by default. To get predictions on the probability scale, we need to set type = \"response\"\n\nIn the code below, we obtain predictions for both the training and testing observations and then compute the average log-loss for this training/testing split.\n\ntrain_pred1 &lt;- \n  predict(object = fit1,\n          newdata = train_data,\n          type = \"response\") \ntest_pred1 &lt;-\n  predict(object = fit1,\n          newdata = test_data,\n          type = \"response\")\n\ncat(\"Dist training logloss:\", round(logloss(train_data$Y, train_pred1), digits = 3), \"\\n\")\n\nDist training logloss: 0.321 \n\ncat(\"Dist testing logloss:\", round(logloss(test_data$Y, test_pred1), digits = 3), \"\\n\")\n\nDist testing logloss: 0.305 \n\n\nAt least for this training/testing split, the logistic regression model that conditions on distance is a bit more accurate than the simpler models. Before concluding that this model indeed is more accurate than the body-part based model, we will repeat the above calculations using 100 training/testing splits.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal, data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist training logloss:\", round(mean(train_logloss), digits = 3), \"\\n\")\n\nDist training logloss: 0.317 \n\ncat(\"Dist testing logloss:\", round(mean(test_logloss), digits = 3), \"\\n\")\n\nDist testing logloss: 0.317 \n\n\nSo, accounting for the shot distance appears to yield more accurate predictions than account for just body-part. This immediately raises the question “how much more accurate would predictions be if we accounted for shot distance, body-part, and technique?”\n\n\nAccounting for Multiple Predictors\nIt turns out to be relatively straightforward to answer this question with a logistic regression model fit using glm(). Specifically, we can include more variables in the formula argument. Note that we first convert the variables shot.body_part.name to a factor.\n\nwi_shots &lt;-\n  wi_shots |&gt;\n  dplyr::mutate(\n    shot.body_part.name = factor(shot.body_part.name))\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit &lt;- \n  glm(formula = Y~DistToGoal + shot.body_part.name, \n      data = train_data, family = binomial(\"logit\"))\n\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal + shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -0.48741    0.14831  -3.287  0.00101 ** \nDistToGoal                    -0.15839    0.01032 -15.348  &lt; 2e-16 ***\nshot.body_part.nameLeft Foot   1.02210    0.16747   6.103 1.04e-09 ***\nshot.body_part.nameRight Foot  1.03793    0.15106   6.871 6.38e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2549.7  on 3569  degrees of freedom\nResidual deviance: 2234.3  on 3566  degrees of freedom\nAIC: 2242.3\n\nNumber of Fisher Scoring iterations: 6\n\n\nLooking at the output of summary(fit), we see that the model includes several more parameters. There is a slope associated with left-footed and right-footed shots. Internally, glm() has one-hot-encoded the categorical predictor shot.body_part.name and decomposed the log-odds of a goal as \\[\n\\beta_{0} + \\beta_{1}\\times \\textrm{DistToGoal} + \\beta_{\\textrm{LeftFoot}}\\times \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} \\times \\mathbb{I}(\\textrm{RightFoot})\n\\]\nTo understand what’s going on, suppose that a shot is taken from distance \\(d.\\) The model makes different predictions based on the body part used to attempt the shot:\n\nIf the shot was a header then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d\\)\nIf the shot was taken with the left foot, then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d + \\beta_{\\textrm{LeftFoot}}\\)\nIf the shot was taken with the right foot, then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d + \\beta_{\\textrm{RightFoot}}\\)\n\nReassuringly, the estimates for \\(\\beta_{\\textrm{RightFoot}}\\) and \\(\\beta_{\\textrm{LeftFoot}}\\) are positive, indicating that for a fixed distance, the log-odds of scoring a goal on a left- or right-footed shot is higher than headers.\nBy repeatedly fitting and assessing our new model using 100 random training/testing splits, we discover that accounting for both body part and shot distance results in even more accurate predictions.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal + shot.body_part.name, \n             data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist+BodyPart training logloss:\", round(mean(train_logloss), digits = 3), \"\\n\")\n\nDist+BodyPart training logloss: 0.307 \n\ncat(\"Dist+BodyPart testing logloss:\", round(mean(test_logloss), digits = 3), \"\\n\")\n\nDist+BodyPart testing logloss: 0.308 \n\n\n\n\nInteractions\nOur most accurate model so far accounts for both shot distance and the body part used to attempt the shot. Taking a closer look at the assumed log-odds of a goal, however, reveals a potentially important limitation: the effect of distance is assumed to be the same for all shot-types. That is, the model assume that moving one yard further away decreases the log-odds of a goal of a right-footed shot by exactly the same amount as it would a header.\nInteractions between features in a regression model allow the effect of one factor to vary based on the value of another features. Using R’s formula interface, we can introduce interactions between two variables using *.\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit &lt;- \n  glm(formula = Y~DistToGoal * shot.body_part.name, \n      data = train_data, family = binomial(\"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal * shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                               1.04701    0.41889   2.499  0.01244\nDistToGoal                               -0.34422    0.05092  -6.761 1.37e-11\nshot.body_part.nameLeft Foot             -0.21195    0.50881  -0.417  0.67700\nshot.body_part.nameRight Foot            -0.82193    0.46059  -1.785  0.07434\nDistToGoal:shot.body_part.nameLeft Foot   0.16418    0.05457   3.009  0.00263\nDistToGoal:shot.body_part.nameRight Foot  0.20871    0.05233   3.988 6.66e-05\n                                            \n(Intercept)                              *  \nDistToGoal                               ***\nshot.body_part.nameLeft Foot                \nshot.body_part.nameRight Foot            .  \nDistToGoal:shot.body_part.nameLeft Foot  ** \nDistToGoal:shot.body_part.nameRight Foot ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2549.7  on 3569  degrees of freedom\nResidual deviance: 2214.5  on 3564  degrees of freedom\nAIC: 2226.5\n\nNumber of Fisher Scoring iterations: 6\n\n\nMathematically, the formula specification Y ~ DistToGoal * shot.body_part.name tells R to fit the model that expresses the log-odds of a goal as \\[\n\\begin{align}\n\\beta_{0} + \\beta_{\\textrm{LeftFoot}} * \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} * \\mathbb{I}(\\textrm{RightFoot}) &+ ~ \\\\\n[\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:LeftFoot}}*\\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{DistToGoal:RightFoot}}\\mathbb{I}(\\textrm{RightFoot})] \\times \\textrm{DistToGoal} &  \n\\end{align}\n\\] where \\(\\mathbb{I}(RightFoot)\\) is an indicator of whether or not the shot was taken with the right foot.\nNow for a given shot taken at distance \\(d\\), the log-odds of a goal are:\n\n\\(\\beta_{0} + \\beta_{\\textrm{DistToGoal} * d\\) is the shot was a header.\n\\(\\beta_{0} + \\beta_{\\textrm{LeftFoot}} + (\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:LeftFoot}}) * d\\) if the shot was taken with the left foot.\n\\(\\beta_{0} + \\beta_{\\textrm{RightFoot}} + (\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:RightFoot}}) * d\\) if the shot was taken with the right foot.\n\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal*shot.body_part.name, \n             data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist*BodyPart training logloss:\", round(mean(train_logloss), digits = 4), \"\\n\")\n\nDist*BodyPart training logloss: 0.3047 \n\ncat(\"Dist*BodyPart testing logloss:\", round(mean(test_logloss), digits = 4), \"\\n\")\n\nDist*BodyPart testing logloss: 0.3066 \n\n\nInterestingly, adding the interaction between distance and body part did not seem to improve the test-set log-loss by much.",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-rf",
    "href": "lectures/lecture03.html#sec-rf",
    "title": "Lecture 3: Estimating XG",
    "section": "Random Forests",
    "text": "Random Forests\nRemember how we used the function StatsBombR::allclean() to apply some of StatsBomb’s suggested pre-processing? It turns out that this function creates several features related to shot attempt including4:\n\nDistances from the shot the center of the goal line (DistToGoal); from the shot to the goal keeper (DistSGK); from the goalkeeper to the center of the goal line (DistToGK)\nThe horizontal angles between the shot and goalkeeper to the center of the goal line (AngleToGoal & AngleToKeeper) and the difference between these angles (AngleDeviation)\nThe distances between the shot and the two nearest defenders (distance.ToD1 and distance.ToD2)\nThe number of defenders and whether the goal keeper is in the triangular area defined by the shot location and the two goal posts (the “cone”;DefendersInCone and InCone.GK)\nThe sum of the inverse distances between the shot location the locations of all defenders (density) and those defenders in the cone. A small density indicates that defenders are very far from the shot location.\nThe area of the smallest square that covers the locations of all center-backs and full-backs (DefArea)\n\nThe code below pulls out several of these features and creates a training/testing split\n\nshot_vars &lt;-\n  c(\"Y\",\n    \"shot.type.name\", \n    \"shot.technique.name\", \"shot.body_part.name\",\n    \"DistToGoal\", \"DistToKeeper\", # dist. to keeper is distance from GK to goal\n    \"AngleToGoal\", \"AngleToKeeper\",\n    \"AngleDeviation\", \n    \"avevelocity\",\"density\", \"density.incone\",\n    \"distance.ToD1\", \"distance.ToD2\",\n    \"AttackersBehindBall\", \"DefendersBehindBall\",\n    \"DefendersInCone\", \"InCone.GK\", \"DefArea\")\n\nwi_shots &lt;-\n  wi_shots |&gt;\n  dplyr::mutate(\n    shot.type.name = factor(shot.type.name),\n    shot.body_part.name = shot.body_part.name,\n    shot.technique.name = shot.technique.name)\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train) |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\", shot_vars)))\n\ny_train &lt;- train_data$Y\ny_test &lt;- test_data$Y\n\ntrain_data &lt;-\n  train_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\ntest_data &lt;-\n  test_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\n\nHow much more predictive accuracy can we squeeze out of an XG model when we account for all these variables? While it is tempting to continue fitting logistic regression models, these models make fairly strong assumptions about how the log-odds of a goal change as we vary the features. Moreover, it becomes increasingly difficult to specify interactions, especially between continuous/numerical factors.\nRandom forests is a powerful regression approach that avoids specifying functional forms and allows for high-order interactions. At a high-level, a random forest model approximates a regression function \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}}]\\) using the sum of several regression trees (which are just piece-wise constant step functions).\n\nRegression Trees & Ensembles\n?@fig-tree shows an example of a regression tree defined over the space \\([0,1]^{2}\\). Regression trees represent piece-wise constant step-functions (?@fig-partition). To see this, imagine taking a point \\(\\boldsymbol{\\mathbf{x}} = (x_{1}, \\ldots, x_{p})\\) and tracing a path from the root of the tree — that is, the node at the very top — down to one of the terminal leaf nodes by following the decision rules. Specifically, when the path encounters an interior node associated with the rule \\(\\{X_{j} &lt; c\\}\\), the path proceeds to the left child if \\(x_{j} &lt; c\\) and the right child otherwise. When the path reaches a leaf \\(\\ell\\), we output the associated scalar \\(\\mu_{\\ell}.\\) For instance, in the case of ?@fig-partition, the path associated with every point in the yellow rectangle terminates in the yellow leaf node in ?@fig-tree\n\n\n\n\n\n\n\n\n\n\nFigure 1: Regression trees are piecewise-constant step-functions\n\n\n\nIt turns out that regression trees are universal function approximators: for just about every function with \\(p\\) inputs, there is a regression tree that can approximate the function to an arbitrarily small tolerance. ?@fig-tree-approx illustrates this: the true function is shown in pink and various tree approximations are shown in green.\n So, at least conceptually, this makes regression trees a very powerful tool in Statistics: rather than trying to pre-specify the functional form of \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}]\\) — that is, manually specifying non-linearities or high-order interactions in a (generalized) linear model — we can instead focus on learning a regression trees5 that fits the data well.\nAs suggested by ?@fig-tree-approx, we often need a very deep tree with lots of leaves to get a good approximation. Luckily, it turns out that very complicated trees can be decomposed into sums of simpler trees. In ?@fig-sum-trees, we write the somewhat complex tree on the left as a sum of three simpler trees on the right6. We typically refer to a collection of simple regression trees as an ensemble. Methods that make predictions using ensembles of regression trees often out-perform more complicated deep-learning and neural-network models7\n\n\n\nComplex trees can be represented as sums of simpler trees\n\n\n\n\nFitting Random Forests Models\nBreiman (2001) introduced the random forests algorithm for estimating a collection of regression trees that, when added up, provided a good approximation to an unknown regression function8. The technical details of this algorithm are beyond the score of this course9\nIn this course, we will fit random forests models using the ranger package. The main function of that package is called ranger(). Its syntax is somewhat similar to glm() insofar as both functions require us to specify a formula and pass our data in as a data frame (or tibble). Unlike glm(), which required us to specify the argument family = binomial(\"logit\") to fit binary outcomes, ranger() requires us to specify the argument probability=TRUE. This signals to the function that we want predictions on the probability scale and not the \\(\\{0,1\\}\\)-outcome scale.\nWe can also use predict() to get predictions from a model fitted with ranger(). But this also involves slightly different syntax: instead of using the argument newdata, we need to use the argument data. Additionally, when making predictions based on a ranger model with probability = TRUE, the function predict returns a named list. One element of that list is called predictions and it is a matrix whose rows correspond to the rows of data and whose columns contain the probabilities \\(\\mathbb{P}(Y = 0)\\) and \\(\\mathbb{P}(Y = 1).\\) So, to get the predicting XG values, we need to look at the 2nd column of this matrix.\n\nfit &lt;-\n1  ranger::ranger(formula = Y~.,\n                 data = train_data, \n                 probability = TRUE)\ntrain_preds &lt;- \n  predict(object = fit,\n2          data = train_data)$predictions[,2]\n\ntest_preds &lt;- \n  predict(object = fit,\n3          data = test_data)$predictions[,2]\n\nlogloss(y_train, train_preds)\nlogloss(y_test, test_preds)\n\n\n1\n\nThe . is short-hand for “all of the columns in data except for what’s on the left-hand side of the ~”\n\n2\n\nWhen making a prediction using the output of ranger, we pass the data in using the data argument instead of newdata.\n\n3\n\nAs noted earlier, predict returns a list, one of whose elements is a two-column matrix containing the fitted probability. This matrix is stored in the list element predictions. The second column of this matrix contains the fitted probabilities of a goal (\\(Y = 1\\)).\n\n\n\n\n[1] 0.1135258\n[1] 0.2522654\n\n\nAt least for this training/testing split, our in-sample log-loss is much smaller than our out-of-sample log-loss. This is not at all surprising: flexible regression models like random forests are trained to predict the in-sample data and so we should expect to see smaller training errors than testing errors.\nRepeating these calculations over 100 training/testing splits, we can conclude that our more flexible random forest model, which accounts for many more features and can accommodate many more interaction terms than our simple logistic regression model, out-performs the logistic regression models.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train) |&gt;\n    dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") |&gt;\n    dplyr::select(dplyr::all_of(c(\"id\", shot_vars)))\n\n  y_train &lt;- train_data$Y\n  y_test &lt;- test_data$Y\n\n  train_data &lt;-\n    train_data |&gt;\n    dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n    dplyr::select(-id)\n  test_data &lt;-\n    test_data |&gt;\n    dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n    dplyr::select(-id)\n  \n  fit &lt;- ranger::ranger(\n    formula = Y~.,\n    data = train_data, \n    probability = TRUE)\n  train_preds &lt;- \n    predict(object = fit, data = train_data)$predictions[,2]\n\n  test_preds &lt;- \n    predict(object = fit, data = test_data)$predictions[,2]\n\n  train_logloss[r] &lt;- logloss(y_train, train_preds)\n  test_logloss[r] &lt;- logloss(y_test, test_preds)\n}\ncat(\"RandomForest training logloss:\", round(mean(train_logloss), digits = 4), \"\\n\")\n\nRandomForest training logloss: 0.1135 \n\ncat(\"RandomForest testing logloss:\", round(mean(test_logloss), digits = 4), \"\\n\")\n\nRandomForest testing logloss: 0.2523 \n\n\n\n\nComparison to StatsBomb’s XG Model\nNow that we have a much more predictive model, we can compare our model estimates to those from StatsBomb’s proprietary model. In the code below, we re-fit a random forests model to the whole dataset. We then plot our XG estimates against StatsBomb’s.\n\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\nfull_rf_fit &lt;-\n  ranger::ranger(formula = Y~.,data = train_data, probability = TRUE)\n\npreds &lt;- predict(object = fit, data = train_data)$predictions[,2]\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(wi_shots$shot.statsbomb_xg, preds,\n     pch = 16, cex = 0.5, col = rgb(0,0,0, 0.25),\n     xlim = c(0, 1), ylim = c(0,1),\n     xlab = \"StatsBomb's XG\", ylab = \"Our XG\", main = 'Comparison of XG estimates')\n\n\n\n\n\n\n\n\nIf our model perfectly reproduced StatsBomb’s, we would expect to see all the points in the figure line up on the 45-degree diagonal. In fact, we observe some fairly substantial deviations from StatsBomb’s estimates. In particular, there are some shots for which StatsBomb’s model returns an XG of about 0.8. Our random forests model, on the other hand, returns a range of XG values for those shots. We also see that there are some shots for which StatsBomb’s model returns very small XG;s but our model returns XG’s closer to 0.5\nUltimately, the fact that our model estimates do not match StatsBomb’s is not especially concerning. For one thing, we trained our model using only about 3800 shots from women’s international matches contained in the public data. StatsBomb, on the other hand, trained their model using their full corpus of match data.\nMore substantively, StatsBomb’s model also conditions on many more features than ours. If our model estimates exactly matched StatsBomb’s, that would indicate that these extra features offered no additional predictive value. Considering that StatsBomb’s XG models condition on the height of the ball when the shot was attempted, the differences seem less surprising.",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#exercises",
    "href": "lectures/lecture03.html#exercises",
    "title": "Lecture 3: Estimating XG",
    "section": "Exercises",
    "text": "Exercises\n\nCreate a new feature ConeAngle that measures the angle formed by connecting the shot location to the goal posts (whose coordinates are (120,36) and c(120,44)). What sort of relationship would you expect between ConeAngle and XG?\nCreate a new feature ConeArea that measures the area of the triangle formed by connecting the shot location to the goal posts. What sort of relationship would you expect between ConeArea and XG?\nUsing 100 random training/testing splits, how much more predictive accuracy is gained (if any) by including ConeAngle, ConeArea, and both ConeAngle & ConeArea in our random forests model.\nIn lecture, we trained our XG models using data only from women’s international competitions. Train an XG model using data from at least 5 different competitions/seasons and assess how similar its predications are to the ones provided by StatsBomb",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#footnotes",
    "href": "lectures/lecture03.html#footnotes",
    "title": "Lecture 3: Estimating XG",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOur notation nods to the fact that predicted XG is just a predicted probability↩︎\nWe often convert categorical features like shot.body_part.name or shot.technique.name into several numerical features using one-hot encoding.↩︎\nThese are known as genearlized linear models and include things like logistic regression and Poisson regression. See Chatper 5 of Beyond Multiple Linear Regression for a nice overview.↩︎\nsee this issue in the StatsBomb’s opendata GitHub repo for more information about the variable definitions. The code to compute these values is available here. Note that in the StatsBomb coordinate system, the center of the goal line is at (120,4).↩︎\nThat is, learn the number and arrangement of the interior and leaf nodes as well as the decision rules and leaf outputs↩︎\nTo verify this decomposition is correct, imagine stacking the three partitions on top of each other.↩︎\nThe canonical reference is Grinsztajn, Oyallaon, and Varoquaux (2022) (link).↩︎\nYou can read the full paper here↩︎\nIf you want to learn more about regression trees, consider taking STAT 443.↩︎",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture00.html",
    "href": "lectures/lecture00.html",
    "title": "Lecture 0: Boxscore Metrics",
    "section": "",
    "text": "Who is the best shooter in the NBA? How do we determine this using data?\nIn this note, we will practice using functions from the tidyverse suite of packages (especially dplyr) to manipulate tables of NBA box score data. Hopefully, much of the functionality we encounter in this lecture will be familiar to you. But, if you need a high-level refresher, I highly recommend the following resources:\n\nChapter 3 and Chapter 5 of R for Data Science.\nSection 1.9 and Chapter 3 of *Data Science: A First Introduction.",
    "crumbs": [
      "Lecture 0: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture00.html#motivation-the-best-shooting-season-in-the-nba",
    "href": "lectures/lecture00.html#motivation-the-best-shooting-season-in-the-nba",
    "title": "Lecture 0: Boxscore Metrics",
    "section": "",
    "text": "Who is the best shooter in the NBA? How do we determine this using data?\nIn this note, we will practice using functions from the tidyverse suite of packages (especially dplyr) to manipulate tables of NBA box score data. Hopefully, much of the functionality we encounter in this lecture will be familiar to you. But, if you need a high-level refresher, I highly recommend the following resources:\n\nChapter 3 and Chapter 5 of R for Data Science.\nSection 1.9 and Chapter 3 of *Data Science: A First Introduction.",
    "crumbs": [
      "Lecture 0: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture00.html#sec-basic-box-score",
    "href": "lectures/lecture00.html#sec-basic-box-score",
    "title": "Lecture 0: Boxscore Metrics",
    "section": "Basic Box Score Statistics",
    "text": "Basic Box Score Statistics\nWe will use the package hoopR to scrape NBA boxscore data. You should install the package using the code\n\ninstall.packages(\"hoopR\")\n\nThe function hoopr::load_nba_player_box() loads season-level box-score data:\n\nraw_box &lt;-\n  hoopR::load_nba_player_box(seasons = 2002:(hoopR::most_recent_nba_season()))\n\nThe data table raw_box contains 813246 rows and 57 columns. Checking the column names, we see that there are columns for the numbers of field goals, three point shots, and free throws made and attempted.\n\ncolnames(raw_box)\n\n [1] \"game_id\"                           \"season\"                           \n [3] \"season_type\"                       \"game_date\"                        \n [5] \"game_date_time\"                    \"athlete_id\"                       \n [7] \"athlete_display_name\"              \"team_id\"                          \n [9] \"team_name\"                         \"team_location\"                    \n[11] \"team_short_display_name\"           \"minutes\"                          \n[13] \"field_goals_made\"                  \"field_goals_attempted\"            \n[15] \"three_point_field_goals_made\"      \"three_point_field_goals_attempted\"\n[17] \"free_throws_made\"                  \"free_throws_attempted\"            \n[19] \"offensive_rebounds\"                \"defensive_rebounds\"               \n[21] \"rebounds\"                          \"assists\"                          \n[23] \"steals\"                            \"blocks\"                           \n[25] \"turnovers\"                         \"fouls\"                            \n[27] \"plus_minus\"                        \"points\"                           \n[29] \"starter\"                           \"ejected\"                          \n[31] \"did_not_play\"                      \"active\"                           \n[33] \"athlete_jersey\"                    \"athlete_short_name\"               \n[35] \"athlete_headshot_href\"             \"athlete_position_name\"            \n[37] \"athlete_position_abbreviation\"     \"team_display_name\"                \n[39] \"team_uid\"                          \"team_slug\"                        \n[41] \"team_logo\"                         \"team_abbreviation\"                \n[43] \"team_color\"                        \"team_alternate_color\"             \n[45] \"home_away\"                         \"team_winner\"                      \n[47] \"team_score\"                        \"opponent_team_id\"                 \n[49] \"opponent_team_name\"                \"opponent_team_location\"           \n[51] \"opponent_team_display_name\"        \"opponent_team_abbreviation\"       \n[53] \"opponent_team_logo\"                \"opponent_team_color\"              \n[55] \"opponent_team_alternate_color\"     \"opponent_team_score\"              \n[57] \"reason\"                           \n\n\nNotice as well that there are columns for the game date (game_date), game id (game_id), and player (e.g., athlete_display_name). This suggests that each row corresponds to a unique combination of game and player and records the players individual statistics in that game.\nFor instance, here are the box score statistics for several players from a single game in 2011.\n\nraw_box |&gt;\n  dplyr::filter(game_date == \"2011-06-12\") |&gt;\n  dplyr::select(athlete_display_name, \n         field_goals_made, field_goals_attempted,\n         three_point_field_goals_made, three_point_field_goals_attempted,\n         free_throws_made, free_throws_attempted)\n\n── ESPN NBA Player Boxscores from hoopR data repository ───────── hoopR 2.1.0 ──\n\n\nℹ Data updated: 2025-07-31 06:39:25 CDT\n\n\n# A tibble: 30 × 7\n   athlete_display_name field_goals_made field_goals_attempted\n   &lt;chr&gt;                           &lt;int&gt;                 &lt;int&gt;\n 1 Dirk Nowitzki                       9                    27\n 2 Tyson Chandler                      2                     4\n 3 Jason Kidd                          2                     4\n 4 Shawn Marion                        4                    10\n 5 J.J. Barea                          7                    12\n 6 Brian Cardinal                      1                     1\n 7 Caron Butler                       NA                    NA\n 8 Ian Mahinmi                         2                     3\n 9 Rodrigue Beaubois                  NA                    NA\n10 DeShawn Stevenson                   3                     5\n# ℹ 20 more rows\n# ℹ 4 more variables: three_point_field_goals_made &lt;int&gt;,\n#   three_point_field_goals_attempted &lt;int&gt;, free_throws_made &lt;int&gt;,\n#   free_throws_attempted &lt;int&gt;\n\n\nAs a sanity check, we can cross-reference the data in our table with the box score from ESPN. Luckily, these numbers match up!\nIt turns out that raw_box contains much more data than we need. Specifically, it includes statistics from play-in and play-off games as well as data from some (but not all) All-Star games. Since we’re ultimately interested in identifying the best player-seasons in terms of shooting performance, we need to remove all play-off, play-in, and All-Star games from the dataset. Additionally, the column did_not_play contains a Boolean (i.e., logical) variable that is TRUE is the player did not play in the game and is FALSE if the player did not play in the game\n\nallstar_dates &lt;-\n  lubridate::date(c(\"2002-02-10\", \"2003-02-09\", \"2004-02-15\",\n    \"2005-02-20\", \"2006-02-19\", \"2007-02-18\", \n    \"2008-02-17\", \"2009-02-15\", \"2010-02-14\",\n    \"2011-02-20\", \"2012-02-26\", \"2013-02-17\", \n    \"2014-02-16\", \"2015-02-15\", \"2016-02-14\",\n    \"2017-02-19\", \"2018-02-18\", \"2019-02-17\",\n    \"2020-02-16\", \"2021-03-07\", \"2022-02-20\",\n    \"2023-02-19\", \"2024-02-18\", \"2025-02-16\"))\nreg_box &lt;-\n  raw_box |&gt;\n  dplyr::filter(\n    season_type == 2 & \n      !did_not_play & \n      !game_date %in% allstar_dates)\n\nLooking at the data table reg_box, we see that in about 9% of rows, the number of minutes played is missing. These likely correspond to players who were active but did not play or logged only a few seconds (generally at the end of games). We will replace these NA values with 0’s and, while doing so, rename some of the columns in reg_box.\n\nreg_box &lt;-\n  reg_box |&gt;\n1  dplyr::rename(\n    Player = athlete_display_name,\n    FGM = field_goals_made,\n    FGA = field_goals_attempted,\n    TPM = three_point_field_goals_made,\n    TPA = three_point_field_goals_attempted,\n    FTM = free_throws_made, \n    FTA = free_throws_attempted) |&gt;\n  dplyr::mutate(\n2    FGM = ifelse(is.na(minutes), 0, FGM),\n    FGA = ifelse(is.na(minutes), 0, FGA),\n    TPM = ifelse(is.na(minutes), 0, TPM),\n    TPA = ifelse(is.na(minutes), 0, TPA),\n    FTM = ifelse(is.na(minutes), 0, FTM),\n    FTA = ifelse(is.na(minutes), 0, FTA)) |&gt;\n3  tidyr::replace_na(list(minutes = 0))\n\n\n1\n\nRename several variables\n\n2\n\nFor those rows where minutes is NA, set the numbers of makes and attempts to 0\n\n3\n\nReplace missing minutes values with 0\n\n\n\n\nAt this point, every row of reg_box corresponds to a player-game combination. We ultimately wish to sum up the number of makes and misses of each shot type across an entire season for each player. To illustrate this, let’s focus on Dirk Nowitzki’s performance in the 2006-07 season when he won the league MVP award. Conceptually, we can accomplish this by first dividing the full data table into several smaller tables, one for each combination of player and season. Then, we can sum the number of field goals, three point shots, and free throws attempted and made by each player in each of their season. This is an example of the split-apply-combine strategy in which you “break up a big problem into manageable pieces, operate on each piece independently, and then put all the pieces back together” (Wickham 2011). This functionality is implemented using dplyr::group_by()\n\nseason_box &lt;-\n  reg_box |&gt;\n  dplyr::group_by(Player, season) |&gt;\n  dplyr::summarise(\n    FGM = sum(FGM),\n    FGA = sum(FGA),\n    TPM = sum(TPM),\n    TPA = sum(TPA),\n    FTM = sum(FTM),\n    FTA = sum(FTA),\n    minutes = sum(minutes),\n    n_games = dplyr::n(),\n    .groups = \"drop\")\n\nThe data table season_box contains 11920 rows, each of corresponds to a single player-season combination. Here is a quick snapshot of some of the data for Dirk Nowitzki\n\nseason_box |&gt;\n  dplyr::filter(Player == \"Dirk Nowitzki\") |&gt;\n  dplyr::select(season, FGM, FGA, TPM, TPA, FTM, FTA)\n\n# A tibble: 18 × 7\n   season   FGM   FGA   TPM   TPA   FTM   FTA\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   2002   600  1258   139   350   440   516\n 2   2003   690  1489   148   390   483   548\n 3   2004   605  1310    99   290   371   423\n 4   2005   663  1445    91   228   615   708\n 5   2006   751  1564   110   271   539   598\n 6   2007   673  1341    72   173   498   551\n 7   2008   630  1314    79   220   478   544\n 8   2009   774  1616    61   170   485   545\n 9   2010   720  1496    51   121   536   586\n10   2011   610  1179    66   168   395   443\n11   2012   473  1034    78   212   318   355\n12   2013   332   707    63   151   164   191\n13   2014   633  1273   131   329   338   376\n14   2015   487  1062   104   274   255   289\n15   2016   498  1112   126   342   250   280\n16   2017   296   678    79   209    98   112\n17   2018   346   758   138   337    97   108\n18   2019   135   376    64   205    39    50",
    "crumbs": [
      "Lecture 0: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture00.html#sec-percentages",
    "href": "lectures/lecture00.html#sec-percentages",
    "title": "Lecture 0: Boxscore Metrics",
    "section": "From Totals to Percentages",
    "text": "From Totals to Percentages\nIn order to determine which player-season was the best in terms of shooting, we need to first define “best”. Perhaps the simplest definition is to find the player-season with the most made shots. We can identify this player-season by sorting the data in season_box by FGM in descending order with the dplyr::arrange() function\n\nseason_box |&gt;\n  dplyr::arrange(dplyr::desc(FGM))\n\n# A tibble: 11,920 × 10\n   Player             season   FGM   FGA   TPM   TPA   FTM   FTA minutes n_games\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n 1 Kobe Bryant          2006   949  2109   179   506   675   788    3184      78\n 2 LeBron James         2006   875  1823   127   379   601   814    3361      82\n 3 Kobe Bryant          2003   868  1924   124   324   601   713    3401      82\n 4 Shai Gilgeous-Ale…   2025   868  1680   165   444   604   673    2633      77\n 5 LeBron James         2018   857  1580   149   406   388   531    3024      82\n 6 Dwyane Wade          2009   854  1739    88   278   590   771    3048      82\n 7 Kevin Durant         2014   849  1688   192   491   703   805    3118      81\n 8 James Harden         2019   843  1909   378  1028   754   858    2870      78\n 9 Giannis Antetokou…   2024   837  1369    34   124   514   782    2573      73\n10 Tracy McGrady        2003   829  1813   173   448   576   726    2954      75\n# ℹ 11,910 more rows\n\n\nWhen we look at the ten “best” shooting seasons, we immediately recognize a lot of superstar players! On this basis, we might be satisfied evaluating shooting performances based only on the total number of shots. But taking a closer look, should we really consider Kobe Bryant’s 2002-03 and Shai Gilgeous-Alexander’s 2024-25 seasons to be equally impressive when Kobe took attempted 242 more shots than Shai in order to make 868 shots? Arguably, Shai’s 2024-25 season should rank higher than Kobe’s 2002-03 season because Shai was more efficient.\nThis motivates us to refine our definition of “best” by focusing on the percentage of field goals made rather than total number of field goals made.\n\nseason_box &lt;-\n  season_box |&gt;\n1  dplyr::mutate(FGP = ifelse(FGA &gt; 0, FGM/FGA, NA_real_))\nseason_box |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, FGP)\n\n\n1\n\nFor players who attempted no field goals (i.e., FGA = 0), their field goal percentage is undefined.\n\n\n\n\n# A tibble: 11,920 × 3\n   Player           season   FGP\n   &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;\n 1 Ahmad Caver        2022     1\n 2 Alondes Williams   2025     1\n 3 Andris Biedrins    2014     1\n 4 Anthony Brown      2018     1\n 5 Braxton Key        2023     1\n 6 Chris Silva        2023     1\n 7 Dajuan Wagner      2007     1\n 8 DeAndre Liggins    2014     1\n 9 Donnell Harvey     2005     1\n10 Eddy Curry         2009     1\n# ℹ 11,910 more rows\n\n\nSorting the players by their \\(\\textrm{FGP},\\) we find that several players made 100% of their field goals. But very few of these players are immediately recognizable — and, indeed, none of them have been in the MVP conversation, despite the fact that they made all their shots!\nTo understand what’s going on, let’s take a look at the number of attempts.\n\nseason_box |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, FGP, FGA)\n\n# A tibble: 11,920 × 4\n   Player           season   FGP   FGA\n   &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ahmad Caver        2022     1     1\n 2 Alondes Williams   2025     1     2\n 3 Andris Biedrins    2014     1     1\n 4 Anthony Brown      2018     1     1\n 5 Braxton Key        2023     1     1\n 6 Chris Silva        2023     1     1\n 7 Dajuan Wagner      2007     1     1\n 8 DeAndre Liggins    2014     1     1\n 9 Donnell Harvey     2005     1     2\n10 Eddy Curry         2009     1     2\n# ℹ 11,910 more rows\n\n\nGiven the very low number of shots attempted in any of these player-season, claiming that any of these player-seasons are among the best ever would strain credulity! So, in order to determine the best shooting performance, we will need to threshold our data to players who took a minimum number of shots. For simplicity, let’s focus our attention on those players who attempted at least 400 field goals in a season (i.e., they attempted, on average, at least 5 shots per game).\n\nseason_box |&gt;\n  dplyr::filter(FGA &gt;= 400) |&gt;\n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, FGP,FGA)\n\n# A tibble: 4,987 × 4\n   Player         season   FGP   FGA\n   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Daniel Gafford   2024 0.725   480\n 2 Walker Kessler   2023 0.720   414\n 3 DeAndre Jordan   2017 0.714   577\n 4 Rudy Gobert      2022 0.713   508\n 5 DeAndre Jordan   2015 0.710   534\n 6 Jarrett Allen    2025 0.706   640\n 7 Nic Claxton      2023 0.705   587\n 8 DeAndre Jordan   2016 0.703   508\n 9 Daniel Gafford   2025 0.702   403\n10 Daniel Gafford   2022 0.693   411\n# ℹ 4,977 more rows\n\n\nDo we really believe that these performances, all of which were made centers who mostly shoot at or near the rim, represent some of the best shooting performances of all time?\n\nEffective Field Goal Percentage\nA major limitation of FGP is that it treats 2-point shots the same as 3-point shots. As a result, the league-leader in FGP every season is usually a center whose shots mostly come from near the rim. Effective Field Goal Percentage (eFGP) adjusts FGP to account for the fact that a made 3-point shots is worth 50% more than a made 2-point shot. The formula for eFGP is \\[\n\\textrm{eFGP} = \\frac{\\textrm{FGM} + 0.5 \\times \\textrm{TPM}}{\\textrm{FGA}}\n\\]\n\nseason_box &lt;-\n  season_box |&gt;\n  dplyr::mutate(\n    TPP = ifelse(TPA &gt; 0, TPM/TPA,NA_real_),\n    eFGP = (FGM + 0.5 * TPM)/FGA) \nseason_box |&gt;\n  dplyr::filter(FGA &gt;= 400) |&gt;\n  dplyr::arrange(dplyr::desc(eFGP), dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, eFGP, FGP, TPP, TPA, n_games)\n\n# A tibble: 4,987 × 7\n   Player         season  eFGP   FGP    TPP   TPA n_games\n   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Daniel Gafford   2024 0.725 0.725 NA         0      74\n 2 Walker Kessler   2023 0.721 0.720  0.333     3      74\n 3 DeAndre Jordan   2017 0.714 0.714  0         2      81\n 4 Rudy Gobert      2022 0.713 0.713  0         4      66\n 5 DeAndre Jordan   2015 0.711 0.710  0.25      4      82\n 6 Jarrett Allen    2025 0.706 0.706  0         5      82\n 7 Nic Claxton      2023 0.705 0.705  0         2      76\n 8 DeAndre Jordan   2016 0.703 0.703  0         1      77\n 9 Daniel Gafford   2025 0.702 0.702 NA         0      57\n10 Daniel Gafford   2022 0.693 0.693  0         1      72\n# ℹ 4,977 more rows\n\n\nWe see again that some of the best seasons, according to eFGP, were from centers, many of whom attempt few few three point shots. When filter out players who took at least 100 three point shots, we start to see other positions in the top-10.\n\nseason_box |&gt;\n  dplyr::filter(FGA &gt;= 400 & TPA &gt;= 100) |&gt;\n  dplyr::arrange(dplyr::desc(eFGP), dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, eFGP, FGP, TPP, TPA, n_games)\n\n# A tibble: 3,658 × 7\n   Player             season  eFGP   FGP   TPP   TPA n_games\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Kyle Korver          2015 0.671 0.487 0.492   449      75\n 2 Duncan Robinson      2020 0.667 0.470 0.446   606      73\n 3 Obi Toppin           2024 0.660 0.571 0.404   260      83\n 4 Nikola Jokic         2023 0.660 0.632 0.383   149      69\n 5 Joe Harris           2021 0.655 0.502 0.478   427      69\n 6 Joe Ingles           2021 0.652 0.489 0.451   406      67\n 7 Grayson Allen        2024 0.649 0.499 0.461   445      75\n 8 Michael Porter Jr.   2021 0.646 0.541 0.447   374      61\n 9 Mikal Bridges        2021 0.643 0.543 0.425   315      72\n10 Al Horford           2024 0.640 0.511 0.419   258      65\n# ℹ 3,648 more rows\n\n\n\n\nTrue Shooting Percentage #{sec-tsp}\nBoth FGP and eFGP totally ignore free throws. Intuitively, we should expect the best shooter to be proficient at making two-and three-point shots as well as their free throws. One metric that accounts for all field goals, three pointers, and free throws is true shooting percentage (\\(\\textrm{TSP}\\)), whose formula is given by \\[\n\\textrm{TSP} = \\frac{\\textrm{PTS}}{2 \\times \\left(\\textrm{FGA} + (0.44 \\times \\textrm{FTA})\\right)},\n\\] where \\(\\textrm{PTS} =  \\textrm{FTM} + 2 \\times \\textrm{FGM} + \\textrm{TPM}\\) is the total number of points scored.\n\nseason_box &lt;-\n  season_box |&gt;\n  dplyr::mutate(\n    PTS = FTM + 2 * FGM + TPM,\n    TSP = PTS/(2 * (FGA + 0.44 * FTA)))\nseason_box |&gt;\n  dplyr::filter(FGA &gt;= 400 & TPA &gt;= 100) |&gt;\n  dplyr::arrange(dplyr::desc(TSP), dplyr::desc(eFGP), dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, TSP, eFGP, FGP, TPP, n_games)\n\n# A tibble: 3,658 × 7\n   Player          season   TSP  eFGP   FGP   TPP n_games\n   &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Nikola Jokic      2023 0.701 0.660 0.632 0.383      69\n 2 Kyle Korver       2015 0.699 0.671 0.487 0.492      75\n 3 Austin Reaves     2023 0.687 0.616 0.529 0.398      64\n 4 Duncan Robinson   2020 0.684 0.667 0.470 0.446      73\n 5 Dwight Powell     2019 0.682 0.637 0.597 0.307      77\n 6 Grayson Allen     2024 0.679 0.649 0.499 0.461      75\n 7 Kevin Durant      2023 0.677 0.614 0.560 0.404      47\n 8 Moritz Wagner     2024 0.676 0.636 0.601 0.330      80\n 9 Stephen Curry     2018 0.675 0.618 0.495 0.423      51\n10 Obi Toppin        2024 0.675 0.660 0.571 0.404      83\n# ℹ 3,648 more rows",
    "crumbs": [
      "Lecture 0: Boxscore Metrics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "",
    "text": "Welcome to STAT 479 (Special Topics in Statistics)! This iteration of the course will focus on sports analytics.\nAll course material including lecture slides, code to reproduce the analyses discussed in lecture, and information about the projects will be posted on this website. So, please bookmark this page and check it regularly throughout the course.\nA PDF copy of the course syllabus is available here and an HTML version of the syllabus is available here Below, you will find more details about course logistics and important information about setting up R."
  },
  {
    "objectID": "index.html#sec-course-logistics",
    "href": "index.html#sec-course-logistics",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nLectures\nLectures will be held in-person Tuesdays and Thursdays from 11:00am to 12:15pm in Morgridge Hall 1524. Slides and code for each lecture will be posted on this website. Lectures will not be recorded. You are not permitted to record the lectures either.\n\n\nCourse Staff & Office Hours\nInstructor: Sameer Deshpande (sameer.deshpande@wisc.edu). Instructor office hours will take place on:\n\nMondays from 11:00am to 12:00pm in Morgridge Hall 5586 (beginning 9/8/25).\nWednesdays from 3:00pm to 4:00pm in Morgridge Hall 5586 (beginning 9/3/25)\nFridays from 3:00pm to 4:30pm in Morgridge Hall 5618 (beginning 9/5/25)\n\nOffice hours on Mondays and Wednesdays are designed for one-on-one meetings with the instructor. While the main purpose of Monday and Wednesday office hours is to discuss your specific experience and learning in the course, feel free to stop by to chat about your educational and career goals and/or any personal sports analytic projects you might be pursuing.\nFriday office hours are intended for collaborative and small group work. During Friday office hours, small groups can step through analyses from lecture, work on the exercises, and on their team projects with the instructor and other students. Friday office hours are also a great way to brainstorm project ideas with and to get feedback from other teams.\nIf you have specific questions about the course content (e.g., parsing a particular bit of syntax or understanding a step in an analysis), I encourage you to ask the question on Piazza and to attend Friday office hours. If you cannot make the Monday or Wednesday office hours, please email me and suggest some times at which you are free.\nTeaching Assistant: Zhexuan Liu (zhexuan.liu2@wisc.edu). TA office hours will be held 9:15am - 10:45am on Tuesday and Thursdays in Morgridge Hall 2515. TA office hours will begin on 9/16/25."
  },
  {
    "objectID": "index.html#sec-R-setup",
    "href": "index.html#sec-R-setup",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "System Setup",
    "text": "System Setup\nAs noted in the syllabus, this course will make extensive use of R. Even if you have prior experience, I strongly recommend installing the latest version of both R and RStudio at the beginning of the course. As of the time of this writing, that is R version 4.5.1 and RStudio version 2025.05.\n\nInstalling R & RStudio\nYou can download a version of R specific to your operating system from this website. After installing R, you should download and re-install RStudio from this website.\n\n\n\n\n\n\nTip\n\n\n\nWhenever you update your version of R, you need to re-install the packages; this is a perennial source of frustration for many R users and some good-natured humor from others1.\n\n\n\n\nRequired Packages\nThroughout the course, we will make extensive use of several tidyverse packages, primarily for data loading, pre-processing, and manipulation. We will also make extensive use of the packages glmnet, lme4, mgcv, and ranger for model fitting. As the course progresses, we will introduce and install new package as required. For the most part, these packages will be specific to a particular sport. Every package that we will use in this class is available through either (i) the Comprehensive R Archive Network (CRAN) or (ii) a public GitHub repository maintained by the packager developer. We typically install CRAN packages using the install.packages() command. To install packages hosted on GitHub, we will use the install_github() function in the devtools package, which is itself available on CRAN.\nFor visualizations, we will try to use colorblind-friendly palettes2 as much as possible. The package colorBlindness contains several palettes for diverging and qualitative data. We will use the Blue2DarkRed18Steps palette from colorBlindness for making heatmaps (e.g., this one)\nPrior to Lecture 2, please make sure you install the required packages.\n\ninstall.packages(c(\"devtools\", \"tidyverse\", \"glmnet\", \"lme4\", \"mgcv\", \"ranger\", \"colorBlindness\"))\n\nI am also personally partial to the palette developed by Okabe & Ito, which is available using the palette.colors() function. We will often use this palette to annotate scatter plots (e.g., this one in Lecture 9). Figure 1 shows the 9 colors contained in the Okabe-Ito palette.\n\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n1par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\n2plot(1:9, 1:9, pch = 16, cex = 4, col = oi_colors)\n\n\n1\n\nThe mar argument reserves 3 margin lines for the bottom and left sides of the plot, 2 for the top, and 1 for the right. The mgp argument specifies how the axis title, label, and lines are placed.\n\n2\n\nPlots large, filled circles with the palette colors\n\n\n\n\n\n\n\n\n\n\nFigure 1: The 9 colors in the Okabe-Ito colorblind-friendly palette."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEven Hadley Wickham, who leads the tidyverse team at Posit, manually re-installs packages after every update↩︎\nSee these notes by Paul Tol for a lot of useful information about colorblindness and how different palettes are perceived by different people.↩︎"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Illustrates the use of statistical modeling and data science techniques to derive actionable insights from sports data. Emphasizes not only technical calculation of advanced metrics but also on written and oral communication to other data scientists and to non-technical audience. Topics may include: deriving team rankings from paired competitions; measuring an individual player’s contribution to their team’s overall success; assessing player performance and team strategy in terms of expected outcomes; forecasting the impact of new rule changes using simulation; and creating new metrics using high-resolution player tracking data."
  },
  {
    "objectID": "syllabus.html#sec-description",
    "href": "syllabus.html#sec-description",
    "title": "Syllabus",
    "section": "",
    "text": "Illustrates the use of statistical modeling and data science techniques to derive actionable insights from sports data. Emphasizes not only technical calculation of advanced metrics but also on written and oral communication to other data scientists and to non-technical audience. Topics may include: deriving team rankings from paired competitions; measuring an individual player’s contribution to their team’s overall success; assessing player performance and team strategy in terms of expected outcomes; forecasting the impact of new rule changes using simulation; and creating new metrics using high-resolution player tracking data."
  },
  {
    "objectID": "syllabus.html#sec-learning-outcomes",
    "href": "syllabus.html#sec-learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nThroughout the course you will\n\nImplement appropriate statistical methods to assess player and team performance\nWork with play-by-play and high-resolution tracking data\nProvide constructive and actionable feedback on your peers’ analytic reports\nBuild a personal portfolio of sports data analyses"
  },
  {
    "objectID": "syllabus.html#sec-course-requisites",
    "href": "syllabus.html#sec-course-requisites",
    "title": "Syllabus",
    "section": "Requisites",
    "text": "Requisites\nThis course will make extensive use of the R programming language through the RStudio integrated development environment (IDE). Because the formal pre-requisites for this course are STAT 333 or 340, you are expected to have previous experience using the R programming language.\n\n\n\n\n\n\nWarning\n\n\n\nIf you do not meet the formal course prerequisites and/or have not used R in a previous course, this is not the right course for you. Exceptions will not made to the formal course requisites.\n\n\nI will assume fluency with basic R functionality (e.g., assignment, writing and executing scripts, saving data objects, setting environments, installing and loading packages), data manipulation with dplyr and other tidyverse packages, and visualization using either base R graphics or ggplot2. I will additionally assume some familiarity with fitting statistical models in R and interpreting their output (e.g., using lm and glm). Here is an example of the type of R coding with which I will assume you are familiar. If you need a refresher on some of the data wrangling in that document, I strongly recommend reviewing your old course notes as well as\n\nChapter 3 and Chapter 5 of R for Data Science.\nSection 1.9 and Chapter 3 of Data Science: A First Introduction."
  },
  {
    "objectID": "syllabus.html#sec-course-staff",
    "href": "syllabus.html#sec-course-staff",
    "title": "Syllabus",
    "section": "Course Staff & Office Hours",
    "text": "Course Staff & Office Hours\nInstructor: Sameer Deshpande (sameer.deshpande@wisc.edu). Instructor office hours will take place on:\n\nMondays from 11:00am to 12:00pm in Morgridge Hall 5586 (beginning 9/8/25).\nWednesdays from 3:00pm to 4:00pm in Morgridge Hall 5586 (beginning 9/3/25)\nFridays from 3:00pm to 4:30pm in Morgridge Hall 5618 (beginning 9/5/25)\n\nOffice hours on Mondays and Wednesdays are designed for one-on-one meetings with the instructor. While the main purpose of Monday and Wednesday office hours is to discuss your specific experience and learning in the course, feel free to stop by to chat about your educational and career goals and/or any personal sports analytic projects you might be pursuing.\nFriday office hours are intended for collaborative and small group work. During Friday office hours, small groups can step through analyses from lecture, work on the exercises, and on their team projects with the instructor and other students. Friday office hours are also a great way to brainstorm project ideas with and to get feedback from other teams.\nIf you have specific questions about the course content (e.g., parsing a particular bit of syntax or understanding a step in an analysis), I encourage you to ask the question on Piazza and to attend Friday office hours. If you cannot make the Monday or Wednesday office hours, please email me and suggest some times at which you are free.\nTeaching Assistant: Zhexuan Liu (zhexuan.liu2@wisc.edu). TA office hours will be held 9:15am - 10:45am on Tuesday and Thursdays in Morgridge Hall 2515. TA office hours will begin on 9/16/25."
  },
  {
    "objectID": "syllabus.html#sec-assignments",
    "href": "syllabus.html#sec-assignments",
    "title": "Syllabus",
    "section": "Assignments & Grading",
    "text": "Assignments & Grading\nYour final grade will be based on your performance on three group projects and your overall participation. For each project, groups can earn up to 100 points for their written report and up to 100 points for their recorded presentation. After each project, every student will peer review three reports and three presentations and complete a team accountability survey. You can earn a total of 10 points for each peer review by (i) completing the provided rubric and (ii) leaving a constructive comment that identities strengths and areas for potential improvement. Students will earn up to 20 points per survey: 10 for completing the survey and 10 based on the feedback from their teammates and their own self-assessment.\nWe will use Canvas to handle report and presentation submissions, peer reviews, and team accountability surveys.\n\nGroup Projects\nThere will be three group projects. Each project consists of two parts, a written report and a group presentation. The project due dates are October 10, November 7, and December 5.\n\nWritten Report #{sec-written-report}\nThe written report consists of a non-technical executive summary and a technical report. The executive summary, which should not exceed 500 words, should describe the overall goals, analytic approach, and main conclusions in non-technical language. The executive summary should be free from jargon, code listings, figures, tables, and charts. It should be written to be read and understood by a front office executive, coach, player, or fan with little data science experience. The rest of written report should\n\nClearly state the problem being studied and provide sufficient background details and to motivate why the problem is important and interesting.\nDescribe the data and major steps of the analysis\nPresents the main results within the context of the relevant sport(s) and supports the results with figures, tables, charts, and other statistical software output as appropriate.\nDiscusses the limitations of the analysis and outlines concrete steps for further development.\n\nThe technical section of the report should contain enough detail and code that another data scientist could replicate your analysis verify its soundness. Code listings and output (e.g., figures, tables, charts, and numerical summaries) should be tightly integrated with the written exposition. The use of Quarto or RMarkdown is highly recommended for preparing the written report.\n\n\nPresentation\nEach team will also record an 8–10 minute presentation (e.g., using Zoom that provides an overview of their analysis. Each presentation should include the following elements\n\nBackground (2–4 slides): clearly motivate and state the main problem being studied. Explain why it is interesting and important. Present just enough background to motivate the problem, while taking care not to overwhelm the audience with extraneous details. If appropriate, comment on the limitations of existing solutions to the problem or closely-related problems\nAnalysis overview (2–4 slides): present only the main steps of your analysis. Be sure to explain why each step was necessary and how these steps contribute to the overall solution. Focus more on the high-level ideas and motivation for each step rather than the specific implementation or software syntax\nMain results (2–3 slides): distill your results into a few key points. Use figures, tables, charts, and other statistical software output to support your findings.\nConclusion (1 slide): briefly summarize your analysis and findings and outline between 1 and 3 specific directions for future development, improvement or refinement.\n\n\n\n\nPeer Reviews\nAfter every group project, every student is expected to peer-review the written report and presentations of 3 other teams. The primary purpose of this peer review is to practice providing constructive feedback on both technical and non-technical writing to your fellow data scientists. Peer reviews will be completed on Canvas and a structured evaluation rubric will be provided for each assignment. The peer review process will be single-blinded so teams will not know the identities of their reviewers. Students can earn a total of 20 points for each peer review by (i) completing the provided rubric and (ii) leaving a constructive comment that identities strengths and areas for potential improvement. Peer reviews are due one week after the project assignment due dates; that is, they are due on October 17, November 14, and December 12. A penalty will be assessed for submitting late reviews.\n\n\nTeam Accountability Survey\nAfter each project submission, you will fill out a team accountability survey. You can earn up to 40 points per project submission: 10 for completing the survey and 10 based on the feedback from their teammates and their own self-assessment. In total, students can earn up to 120 points based on their level of participation in their project groups.\n\n\nParticipation\nAn additional 100 points will be awarded based on participation during class, office hours, and online discussions on Piazza.\n\n\nFinal Grades\nYour final grade will be based on how many of the 1000 total points you earn during the semester. Over 925 points is at least an A, over 875 points is at least an AB, over 800 points is at least a B, over 700 is at least a BC, over 600 points is at least a C, and over 500 point is at least a D. Final grade boundaries will be announced no later than December 5, 2025."
  },
  {
    "objectID": "syllabus.html#sec-course-sites",
    "href": "syllabus.html#sec-course-sites",
    "title": "Syllabus",
    "section": "Canvas & Piazza",
    "text": "Canvas & Piazza\nWe will use Canvas for course announcement, assignment submission, and grading. The Canvas course page is accessible via this link. We will also use Piazza for specific discussions about course content (e.g., sharing additional information/resources related to material discussed in lecture; answering questions about data, method, and code; etc.) and more general discussions about sports analytics (e.g., sharing job postings, open data competition opportunities, popular press articles, and other analyses you might find online etc.) The Piazza page is accessible via this link.\n\n\n\n\n\n\nAccess to Canvas and Piazza\n\n\n\nThe Canvas and Piazza sites associated with this course are limited to students enrolled in the course and the course teaching staff. Although they can freely access the course notes and code, auditors, UW–Madison students not currently enrolled in the course, and anyone not affiliated with UW–Madison will not have access to Canvas and Piazza sites. Requests to access these sites will be ignored."
  },
  {
    "objectID": "syllabus.html#sec-resources",
    "href": "syllabus.html#sec-resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThe course will make extensive use of R. The following are excellent references and I highly encourage you to read and consult them as needed:\n\nR for Data Science by Wickham, Çentikaya-Rundel, & Grolemud\nThe Quarto guide\nRMarkdown: The Definitive Guide by Xie, Allaire, & Grolemud\nAn Introduction to Statistical Learning by James, Wittin, Hastie, and Tibshirani\nData Science: A First Introduction by Timbers, Campbell, and Lee\nBeyond Linear Modeling: Applied Generalized Linear Models and Multilevel Models in R by Roback and Leger\n\nYou may also find the following websites, blogs, and podcasts useful as sources of inspiration as you develop analyses of your own.\n\nThe online textbook Analyzing Baseball Data with R and the associated blog, which contains lots of helpful resources for analyzing tabular box score data to high-resolution ball tracking data and everything in between\nRon Yurko’s Substack [``Statistical Thinking in Sports Analytics’’] (https://statthinksportsanalytics.substack.com)\nThe : the premiere venue for academic sports analytics articles. You should have access to all articles through the UW Libraries. Each issue also features a publicly available ``Editor’s Choice’’ article.\nThe [Open Source Sports podcast] (https://open.spotify.com/show/3vTtH2JJXbjrzOtEfjrqc4): Each episode of this podcast focuses on a single academic research paper featuring authors as guests, with discussions about the statistical methodology, relevance and future directions of the research.\nHockey Graphs: a blog that’s developed really innovative public-facing hockey analytics.\nThe [Wharton Moneyball Post Game Podcast] (https://knowledge.wharton.upenn.edu/shows/moneyball-highlights/): a podcast version of the popular Sirius XM radio show hosted by several Wharton professors and featuring interviews with sport analytics thought leaders"
  },
  {
    "objectID": "syllabus.html#sec-academic-integrity",
    "href": "syllabus.html#sec-academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nI take academic integrity extremely seriously. While I encourage you to collaborate with your classmates on the problem sets, you are expected to write up your own solutions. You must acknowledge any and all sources that you consulted, whether it be a reference book, online resource, or another person (in the class or otherwise). You may not post, share, or upload any course material, including lecture slides, code, project reports, and presentation recordings, to websites like StackOverflow, Quora, Chegg, and Reddit or to ChatGPT or a similar generative AI service."
  },
  {
    "objectID": "syllabus.html#sec-ai",
    "href": "syllabus.html#sec-ai",
    "title": "Syllabus",
    "section": "Policy on the Use of Generative Artificial Intelligence Models",
    "text": "Policy on the Use of Generative Artificial Intelligence Models\nYou have the right to the full benefits of my expertise and engagement in this course. So, I will never use AI to - Provide feedback on assignments - Prepare any course content (e.g., slides, code, assignments, etc.) - Mediate or assist communications with you In other words, everything you see in this course was created by me without the aid of generative AI.\nWhile the Statistics Department recognizes the potential benefits of AI, its use in academic work can be problematic. Insofar as this course is really about process and not final results, I believe strongly that the use of generative AI is at odds with the learning goals. So, three rules regarding the use of ChatGPT and other generative AI models will be enforced:\n\nPassing off AI-generated responses as original student work constitutes plagiarism and is strictly prohibited. Any students found to be engaging in this practice will be cited for academic misconduct.\nUnless explicitly authorized by the instructor to do so, any use of AI-generated responses as sources of information, even with documentation and attribution, is prohibited.\nYou may not, under any circumstances, upload any course material to a generative AI model or agent. This includes lecture slides and notes and reports or videos uploaded by other student groups.\n\n\n\n\n\n\n\nPeer Review\n\n\n\nRespect your classmates enough to review their projects yourself. Uploading someone else’s project report or presentation to a generative AI tool (e.g., for creating summaries) is forbidden and will result in a failing grade."
  },
  {
    "objectID": "syllabus.html#additional-information",
    "href": "syllabus.html#additional-information",
    "title": "Syllabus",
    "section": "Additional Information",
    "text": "Additional Information\n\nHow credit hours are met\nThis class meets for two 75-minute class periods each week over the semester and carries the expectation that students will work on course learning activities (reading, writing, problem sets, studying, etc.) for about three hours out of the classroom for every class period.\n\n\nRegular and substantive student-instructor interaction.\nSubstantive interaction occurs via two channels: (i) regular class meetings in which students are encouraged to engage in discussion and (ii) weekly office hours.\n\n\nEthics\nThe members of the faculty of the Department of Statistics at UW–Madison uphold the highest ethical standards of teaching, data, and research. We expect our students to uphold the same standards of ethical conduct. The American Statistical Association’s standards for ethical conduct in data analysis and data privacy are available at and include:\n\nUse methodology and data that are relevant and appropriate; without favoritism or prejudice; and in a manner intended to produce valid, interpretable, and reproducible results.\nBe candid about any known or suspected limitations, defects, or biases in the data that may affect the integrity or the reliability of the analysis. Obviously, never modify or falsify data\nProtect the privacy and confidentiality of research subjects and data concerning them, whether obtained from the subjects directly, other persons, or existing records.\n\n\n\nAccommodations for students with disabilities.\nThe University of Wisconsin–Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute 36.12, and UW–Madison policy UW-855 require the university to provide reasonable accommodations to students with disabilities to access and participate in its academic programs and educational services. Faculty and students share responsibility in the accommodation process. Students are expected to inform faculty of their need for instructional accommodations during the beginning of the semester, or as soon as possible after being approved for accommodations. Faculty will work either directly with the student or in coordination with the McBurney Resource Center to provide reasonable instructional and course-related accommodations. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA.\n\n\nDiversity & inclusion\nDiversity is a source of strength, creativity, and innovation for UW–Madison. We value the contributions of each person and respect the profound ways their identity, culture, background, experience, status, abilities, and opinion enrich the university community. We commit ourselves to the pursuit of excellence in teaching, research, outreach, and diversity as inextricably linked goals. The University of Wisconsin–Madison fulfills its public mission by creating a welcoming and inclusive community for people from every background – people who as students, faculty, and staff serve Wisconsin and the world.\n\n\nPrivacy of student records & the use of audio recorded lectures.\nLecture materials and recordings for this course are protected intellectual property at UW–Madison. Students in courses may use the materials and recordings for their personal use related to participation in class. Students may also take notes solely for their personal use. If a lecture is not already recorded, students are not authorized to record lectures without permission unless they are considered by the university to be a qualified student with a disability who has an approved accommodation that includes recording ( Regents Policy Document 4-1).\nStudents may not copy or have lecture materials and recordings outside of class, including posting on internet sites or selling to commercial entities, with the exception of sharing copies of personal notes as a notetaker through the McBurney Disability Resource Center. Students are otherwise prohibited from providing or selling their personal notes to anyone else or being paid for taking notes by any person or commercial firm without the instructor’s express written permission. Unauthorized use of these copyrighted lecture materials and recordings constitutes copyright infringement and may be addressed under the university’s policies, UWS Chapters 14 and 17, governing student academic and non-academic misconduct.\n\n\nReligious observances\nStudents are responsible for notifying the instructor within the first two weeks of classes about any need for flexibility due to religious observances.\n\n\nFurther information and policies}.\nPlease visit this link for additional information about student privacy, course evaluations, and student rights and responsibilities."
  },
  {
    "objectID": "lectures/lecture02.html",
    "href": "lectures/lecture02.html",
    "title": "Lecture 2: Expected Goals",
    "section": "",
    "text": "During the EURO2022 football tournament, English player Beth Mead scored 6 goals. Below are links to videos for three of her goals. Which is most impressive to you?\n\nGoal against Austria (link)\nGoal against Norway link\nGoal against Sweden link\n\nThere several important qualitative differences between these shots that affect our subjective comparison. For instance, because Mead scored against Austria in a one-on-one situation but had to shoot through multiple defenders against Norway and Sweden, we might view the former goal as easier and less impressive. On the other hand, lobbing the ball so it does not go over the bar takes a considerable amount of skill.\nWe could argue endlessly about the qualitative differences between these shots. To make our discussion more precise, it is helpful to quantify these differences. One way — but certainly not the only way — is to ask what might happen if Mead were to repeat these three shots over and over again. In this thought experiment, we could compare the shots based on the relative proportion of times that the shots resulted in a goal. Comparing the these hypothetical long-run proportions to the actual observed shot outcomes allows us to how impressive the outcome was. For instance, the lob shot against Austria might look a lot less impressive if we knew that such a shot would very often result in a goal if repeated over and over again. Of course, Mead can’t actually repeat these shots over and over again. In this lecture, we will introduce the expected goals framework, which allows us to estimate those long-run goal frequencies.\n\n\n\nIn Section 2, we discuss how to access soccer event data. We then formally define expected goals and introduce some simple models for estimating XG (Section 3 and Section 5). Finally, we compare Mead’s actual performance in EURO 2022 to her expected performance (Section 6).",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#overview",
    "href": "lectures/lecture02.html#overview",
    "title": "Lecture 2: Expected Goals",
    "section": "",
    "text": "During the EURO2022 football tournament, English player Beth Mead scored 6 goals. Below are links to videos for three of her goals. Which is most impressive to you?\n\nGoal against Austria (link)\nGoal against Norway link\nGoal against Sweden link\n\nThere several important qualitative differences between these shots that affect our subjective comparison. For instance, because Mead scored against Austria in a one-on-one situation but had to shoot through multiple defenders against Norway and Sweden, we might view the former goal as easier and less impressive. On the other hand, lobbing the ball so it does not go over the bar takes a considerable amount of skill.\nWe could argue endlessly about the qualitative differences between these shots. To make our discussion more precise, it is helpful to quantify these differences. One way — but certainly not the only way — is to ask what might happen if Mead were to repeat these three shots over and over again. In this thought experiment, we could compare the shots based on the relative proportion of times that the shots resulted in a goal. Comparing the these hypothetical long-run proportions to the actual observed shot outcomes allows us to how impressive the outcome was. For instance, the lob shot against Austria might look a lot less impressive if we knew that such a shot would very often result in a goal if repeated over and over again. Of course, Mead can’t actually repeat these shots over and over again. In this lecture, we will introduce the expected goals framework, which allows us to estimate those long-run goal frequencies.\n\n\n\nIn Section 2, we discuss how to access soccer event data. We then formally define expected goals and introduce some simple models for estimating XG (Section 3 and Section 5). Finally, we compare Mead’s actual performance in EURO 2022 to her expected performance (Section 6).",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-statsbomb-data",
    "href": "lectures/lecture02.html#sec-statsbomb-data",
    "title": "Lecture 2: Expected Goals",
    "section": "Soccer Event Data",
    "text": "Soccer Event Data\nWe will make use of high-resolution tracking data provided by the company StatsBomb, which was recently acquired by Hudl. StatsBomb extracts player locations from game film using some pretty interesting computer vision techniques. To their great credit, StatsBomb releases a small snapshot of their data for public use1 We can access this data directly in R using the StatsBombR package.\nStatsBomb provides their public data with the package StatsBombR, which you can install using\n\ndevtools::install_github(\"statsbomb/StatsBombR\")\n\nStatsBomb organizes its free data by competition/tournament. The screenshot below shows a table of all the available competitions. We can load this table into our R environment using the function StatsBombR::FreeCompetitions(). Figure 1 shows a snapshot of the available competitions.\n\n\n\n\n\n\nFigure 1: Screenshot of the competitions covered in the public data\n\n\n\nEach competition and season have unique id and we can also see whether it was a men’s or women’s competition. To see which matches from selected competitions have publicly available data, we can pass the corresponding rows of this table to the function StatsBombR::FreeMatches(). For instance, here are the first few rows and some selected columns from the matches from EURO 2022. StatsBomb graciously provided data for all the matches in the tournament. Here is a table of matches from the 2022 EURO Competition; StatsBomb graciously provided data for all matches from the tournament, which can be obtained using the code below.\n\nStatsBombR::FreeCompetitions() |&gt;\n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt;\n  dplyr::select(match_id, home_team.home_team_name, away_team.away_team_name, home_score, away_score)\n\n[1] \"Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please credit StatsBomb as your data source when using the data and visit https://statsbomb.com/media-pack/ to obtain our logos for public use.\"\n[1] \"Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please credit StatsBomb as your data source when using the data and visit https://statsbomb.com/media-pack/ to obtain our logos for public use.\"\n\n\n# A tibble: 31 × 5\n   match_id home_team.home_team_n…¹ away_team.away_team_…² home_score away_score\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt;                       &lt;int&gt;      &lt;int&gt;\n 1  3835331 Sweden Women's          Switzerland Women's             2          1\n 2  3835324 Netherlands Women's     Sweden Women's                  1          1\n 3  3844384 England Women's         Spain Women's                   2          1\n 4  3847567 England Women's         Germany Women's                 2          1\n 5  3845506 England Women's         Sweden Women's                  4          0\n 6  3835335 Northern Ireland        England Women's                 0          5\n 7  3835323 Portugal Women's        Switzerland Women's             2          2\n 8  3835325 France Women's          Italy Women's                   5          1\n 9  3835320 Norway Women's          Northern Ireland                4          1\n10  3845507 Germany Women's         France Women's                  2          1\n# ℹ 21 more rows\n# ℹ abbreviated names: ¹​home_team.home_team_name, ²​away_team.away_team_name\n\n\nTo access the raw event-level data from a subset of matches, we need to pass the table above to the function StatsBombR::free_allevents(). StatsBomb also recommends running some basic pre-processing, all of which is nicely packaged together in the functions StatsBombR::allclean() and StatsBombR::get.opposingteam().\nAs an example, the code chunk below pulls out publicly available event data for every women’s international match.\n\nwi_events &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international) |&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam()\n\n\n1\n\nGet table of all available competitions\n\n2\n\nFind all women’s international competition\n\n3\n\nGet table of matches\n\n4\n\nGet all events\n\n5\n\nallclean() and get.opposingteam() run several pre-processing scripts that StatsBomb recommends.\n\n\n\n\n\n\n\n\n\n\nDeveloping Complex Pipelines\n\n\n\nIt is not easy to codepipelines like the above in a single attempt. In fact, I had to build the code line-by-line. For instance, I initially ran just the first line and manually inspected the table of free competitions (using View()) to figure out which variables I needed to filter() on in the second line. It is very helpful to develop pipelines incrementally and to check intermediate results before putting everything together in one block of code.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-init-xg",
    "href": "lectures/lecture02.html#sec-init-xg",
    "title": "Lecture 2: Expected Goals",
    "section": "Estimating Expected Goals",
    "text": "Estimating Expected Goals\nSuppose we observe a dataset consisting of \\(n\\) shots. For each shot \\(i = 1, \\ldots, n,\\) let \\(Y_{i}\\) be a binary indicator of whether the shot resulted in a goal (\\(Y_{i} = 1\\)) or not (\\(Y_{i} = 0\\)). From the high-resolution tracking data, we can extract a potentially huge number of features about the shot at the moment of it was taken. Possible features include, but are certainly not limited to, the player taking the shot, the body part and side of the body used, the positions of the defenders and goal keepers, and contextual information like the score. Mathematically, we can collect all these features into a (potentially large) vector \\(\\boldsymbol{\\mathbf{X}}_{i}.\\)\nExpected Goals (XG) models work by (i) positing an infinite super-population of shots represented by pairs \\((\\boldsymbol{\\mathbf{X}}, Y)\\) of feature vector \\(\\boldsymbol{\\mathbf{X}}\\) and binary outcome \\(Y\\); and (ii) assuming that the shots in our dataset constitute a random sample \\((\\boldsymbol{\\mathbf{X}}_{1}, Y_{1}), \\ldots, (\\boldsymbol{\\mathbf{X}}_{n}, Y_{n})\\) from that population.\n\n\n\n\n\n\nConditional Expectations\n\n\n\nFor each combination of features \\(\\boldsymbol{\\mathbf{x}}\\), the expect goals given \\(\\boldsymbol{\\mathbf{x}},\\) which we will denote by \\(\\textrm{XG}(\\boldsymbol{\\mathbf{X}})\\) is just the average value of \\(Y\\) among the (assumed infinite) sub-population of shots with features \\(\\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}.\\) Mathematically, XG is conditional expectation: \\[\n\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}],\n\\]\n\n\nBecause the shot outcome \\(Y\\) is binary, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is the proportion of goals scored within the sub-population of shots defined by the feature combinations \\(\\boldsymbol{\\mathbf{x}}.\\) In other words, it is the conditional probability of a goal given the shot features \\(\\boldsymbol{\\mathbf{x}}.\\) On this view, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) provides a quantitative answer to our motivating question “If we were to replay a particular shot over and over again, what fraction of the time does it result in a goal?”\nThe StatsBomb variable shot.body_part.name records the body part with which each shot was taken. Within our dataset of women’s international matches, we can see the breakdown of these body parts.\n\ntable(wi_events$shot.body_part.name)\n\n\n      Head  Left Foot      Other Right Foot \n       920       1280         27       2560 \n\n\nFor this analysis, we will focus on fitting XG models using data from shots taken with a player’s feet or head.\n\nwi_shots &lt;-\n  wi_events |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;  \n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\nLater, it will be useful for us to focus only on the shots from EURO2022, so we will also create a table euro2022_shots of all shots from that competition using similar code.\n\n\nCode\neuro2022_shots &lt;-\n  StatsBombR::FreeCompetitions() |&gt; \n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt; \n  StatsBombR::free_allevents() |&gt; \n  StatsBombR::allclean() |&gt; \n  StatsBombR::get.opposingteam() |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\nNow suppose we only include the body part in \\(\\boldsymbol{\\mathbf{X}}\\). If we had full access to the infinite super-population of women’s international shots, then we could compute \\[\\textrm{XG}(\\text{right-footed shot}) = \\mathbb{P}(\\text{goal} \\vert \\text{right-footed shot})\\] by (i) forming a sub-group containing only those right-footed shots and then (ii) calculating the proportion of goals scored within that sub-group. We could similarly compute \\(\\textrm{XG}(\\text{left-footed shot})\\) and \\(\\textrm{XG}(\\text{header})\\) by calculating the proportion of goals scored within the sub-groups containing, respectively, only left-footed shots and only headers.\nOf course, we don’t have access to the infinite super-population of shots. However, on the assumption that our observed data constitute a sample from that super-population, we can estimate \\(\\textrm{XG}\\) by mimicking the idealized calculations described above:\n\nBreak the dataset of all observed shots in women’s international matches into several groups based on the body part\nWithin these two groups, compute the proportion of goals\n\nTo keep things simple, we dropped the 23 shots that were taken with a body part other than the feet or the head.\n\nxg_model1 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y), n = dplyr::n())\nxg_model1\n\n# A tibble: 3 × 3\n  shot.body_part.name   XG1     n\n  &lt;chr&gt;               &lt;dbl&gt; &lt;int&gt;\n1 Head                0.112   920\n2 Left Foot           0.114  1280\n3 Right Foot          0.111  2560\n\n\n\n\n\n\n\n\nGeneralization\n\n\n\nA key assumption of all XG models is that the observed data is a random sample drawn from the super-population. The only women’s internationals matches for which StatsBomb data were from the 2019 and 2023 World Cup and the 2022 EURO tournaments. These matches are arguably not highly representative of all women’s international matches, meaning that we should exercise some caution when using models fitted to these data to analyze matches from other competitions (e.g., an international friendly or a match in a domestic league).\n\n\nWe can now create a table of just Beth Mead’s shots from EURO 2022 and add a column with the XG for each shot. To do this, we first filter our table wi_shots using the player name (note, StatsBomb uses her full name!). Then, for every left-footed shot Mead attempted, we want to copy over the corresponding value from the table xg_model1, which in this case is 0.114. Similarly, we want to copy over the corresponding values for right-footed shots and headers from xg_model1 into our table for Mead’s shots. We can do this using an left join. In the code below, we actually create a temporary version of xg_model1 that drops the column recording the overall counts of the body part used for the shots in wi_shots. This way, when we perform the join, we don’t create a new column with these counts.\n\nmead_shots &lt;-\n  euro2022_shots |&gt;\n  dplyr::filter(player.name == \"Bethany Mead\") |&gt;\n1  dplyr::left_join(y = xg_model1 |&gt; dplyr::select(shot.body_part.name, XG1),\n                   by = c(\"shot.body_part.name\"))\n\n\n1\n\nNo need to include n when joining the tables\n\n\n\n\nWe can now look at the what our model says about the three goals from above. The first, against Austria in the 15th minute; the second, against Norway in the 37th minute, and the third against Sweden in the 33rd minute. These turn out to be in rows 1, 4, and 14 of the table mead_shots\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, Y, XG1) |&gt;\n  dplyr::slice(c(1, 4, 14))\n\n# A tibble: 3 × 5\n  OpposingTeam    minute shot.body_part.name     Y   XG1\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Austria Women's     15 Right Foot              1 0.111\n2 Norway Women's      37 Left Foot               1 0.114\n3 Sweden Women's      33 Right Foot              1 0.111\n\n\nAccording to our first model, the “impressiveness” of these goals is pretty similar: our model put the respective chances of each shot resulting in a goal at about 11%. But, watching the videos a bit more closely, this conclusion is not especially satisfying: Mead scored the first goal in a one-on-one situation but had to shoot through several defenders on the second and third goal. The discrepancy between our qualitative comparisons and our quantitative modeling results stems from the fact that we only conditioned on the body part and did not account for the other ways that the shots are different. In other words, our initial XG model is much too coarse to quantify the differences between the three chances that we believe are important.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-addtl-features",
    "href": "lectures/lecture02.html#sec-addtl-features",
    "title": "Lecture 2: Expected Goals",
    "section": "Conditioning On Additional Features",
    "text": "Conditioning On Additional Features\nFor a more refined comparison, we need an XG model that conditions on more features, including ones that differ between the two shots. To this end, notice that Mead uses a different technique on the three shots: she lobs the ball into the net on the first goal; shoots the ball from the ground on the second goal; and scores the third goal off of a half-volley, striking the ball as it bounces up off the ground. The StatsBomb variable shot.technique.name records the technique of each shot type\n\ntable(wi_shots$shot.technique.name)\n\n\n     Backheel Diving Header   Half Volley           Lob        Normal \n           35            10           648            28          3720 \nOverhead Kick        Volley \n           17           302 \n\n\nBy conditioning on both body part and technique, we can begin to build a more refined XG model. The code to do this is almost identical to the code used in our first model. The only difference is that we now group by two variables shot.body_part.name and shot.technique_name. Because we are grouping by two variables, specify the argument .groups=\"drop\" argument when calling summarize; this prevents a (mostly innocuous) warning message2. We additionally append our new XG estimates to the table containing all of Mead’s shots.\n\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarize(XG2 = mean(Y), n = dplyr::n(), .groups = \"drop\")\n\nmead_shots &lt;-\n  mead_shots |&gt;\n  dplyr::inner_join(\n    y = xg_model2 |&gt; dplyr::select(-n), \n    by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y, XG2) |&gt;\n  dplyr::slice(c(1, 4, 14))\n\n# A tibble: 3 × 6\n  OpposingTeam    minute shot.body_part.name shot.technique.name     Y    XG2\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n1 Austria Women's     15 Right Foot          Lob                     1 0.208 \n2 Norway Women's      37 Left Foot           Normal                  1 0.121 \n3 Sweden Women's      33 Right Foot          Half Volley             1 0.0892",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-sophisticated-xg",
    "href": "lectures/lecture02.html#sec-sophisticated-xg",
    "title": "Lecture 2: Expected Goals",
    "section": "More Sophisticated XG Models",
    "text": "More Sophisticated XG Models\nAccording to our new XG model, the right-footed lob against Austria has a much higher XG than the other shots against Norway and Sweden, which seems much more reasonable than our previous model. But are we fully satisfied with this model?\nOne could credibly argue that even though our model returns somewhat more sensible XG estimates, it is still too coarse for to meaningfully compare the shots above. After all, because it does not condition on distance, our model would return exactly the same XG for right-footed volleys taken one meter and 15 meters away from the goal. Similarly, we could try to account for the number of defenders between the shot and the goal and the position of the keeper.\nIf we had access to the infinite super-population of shots, conditioning on even more features is conceptually straightforward: we look at the corresponding sub-group of the super-population defined by a particular combination of features and compute the average \\(Y.\\) Unfortunately, with finite data, trying to “bin-and-average” using lots of features can lead to erratic estimates. For instance, here are the five largest and five smallest XG estimates based on body-part and shot technique.\n\nxg_model2 |&gt; \n  dplyr::arrange(dplyr::desc(XG2)) |&gt; \n  dplyr::filter(dplyr::row_number() %in% c(1:5, (dplyr::n()-4):dplyr::n()))\n\n# A tibble: 10 × 4\n   shot.body_part.name shot.technique.name    XG2     n\n   &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt; &lt;int&gt;\n 1 Right Foot          Lob                 0.208     24\n 2 Left Foot           Volley              0.163     98\n 3 Left Foot           Normal              0.121    947\n 4 Right Foot          Normal              0.121   1863\n 5 Head                Normal              0.113    910\n 6 Right Foot          Volley              0.0637   204\n 7 Head                Diving Header       0         10\n 8 Left Foot           Backheel            0          6\n 9 Left Foot           Lob                 0          4\n10 Left Foot           Overhead Kick       0          3\n\n\nBecause none of the 3 left-footed lobs in our dataset led to goals, our model estimates \\(\\textrm{XG}(\\text{left-footed lob})\\) as 0. Similarly, the rather large \\(\\textrm{XG}(\\text{right-footed lob})\\) of 33% is based on only 12 shots. Attempting to condition on even more variables would result in estimates based on even smaller sample sizes3.\nSo, it would appear that we’re stuck between a rock and a hard place. On the one hand, our XG model with two features is still too coarse to quantify important differences between the motivating shots. But, on the other hand, binning and averaging with even more features carries the risk of producing highly erratic, extreme, and somewhat nonsensical estimates4.\nStatistical models offer a principled approach to overcome these issues. We will explore several such models in Lecture 3. But for now, we will rely on a model developed by StatsBomb that accounts for a large number of features based on player locations (in two dimensions), the ball location (in three dimension), and other factors like the body part, shot technique, and the actions leading up to the shot (e.g., whether shot was taken off dribble or first touch). You can read more about their model here. Luckily for us, they include XG estimates for each shot in the public data, under the column shot.statsbomb_xg.\nFor instance, here are the XG estimates from StatsBomb’s model for all of Beth Mead’s goals\n\nmead_shots |&gt;\n  dplyr::filter(Y == 1) |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, \n                shot.technique.name, Y, shot.statsbomb_xg)\n\n# A tibble: 6 × 6\n  OpposingTeam     minute shot.body_part.name shot.technique.name     Y\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;\n1 Austria Women's      15 Right Foot          Lob                     1\n2 Norway Women's       33 Head                Normal                  1\n3 Norway Women's       37 Left Foot           Normal                  1\n4 Norway Women's       80 Left Foot           Volley                  1\n5 Northern Ireland     43 Left Foot           Normal                  1\n6 Sweden Women's       33 Right Foot          Half Volley             1\n# ℹ 1 more variable: shot.statsbomb_xg &lt;dbl&gt;\n\n\nRecall that XG quantifies a certain hypothetical long-term frequency of scoring a goal: if the shot was replayed under exactly the conditions quantified by the feature vector \\(\\boldsymbol{\\mathbf{x}}\\), \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is the proportion of times a goal is scored. So, according to StatsBomb’s proprietary model, if Mead repeatedly attempted the three shots introduced in Section 1.1, we should expect them to result in goals 36%, 44%, and 9% of the time. In other words, according to StatsBomb’s model, Meads goal against Sweden is much more impressive than her goals against Austria and Norway. One could argue, further, that this goal was somewhat lucky.\nWe can also look at the XG’s of the shots Mead took that didn’t result in goals.\n\nmead_shots |&gt;\n  dplyr::filter(Y == 0) |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, \n                shot.technique.name, Y, shot.statsbomb_xg)\n\n# A tibble: 9 × 6\n  OpposingTeam     minute shot.body_part.name shot.technique.name     Y\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;\n1 Norway Women's       29 Right Foot          Normal                  0\n2 Norway Women's       52 Right Foot          Volley                  0\n3 Northern Ireland      5 Head                Normal                  0\n4 Northern Ireland     15 Right Foot          Half Volley             0\n5 Northern Ireland     56 Right Foot          Normal                  0\n6 Northern Ireland     83 Right Foot          Normal                  0\n7 Sweden Women's        4 Head                Normal                  0\n8 Sweden Women's       19 Left Foot           Normal                  0\n9 Sweden Women's       46 Left Foot           Normal                  0\n# ℹ 1 more variable: shot.statsbomb_xg &lt;dbl&gt;\n\n\nWe see that most of Mead’s misses were on shots with very low XG values, indicating that none of these misses were especially unlucky.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-goe",
    "href": "lectures/lecture02.html#sec-goe",
    "title": "Lecture 2: Expected Goals",
    "section": "Goals Over Expected",
    "text": "Goals Over Expected\nBy summing the differences \\(Y_{i} - \\textrm{XG}_{i}\\) across all of her shots, we can quantify the degree to which Mead under- or over-performed the model expectations.\n\nsum(mead_shots$Y - mead_shots$shot.statsbomb_xg)\n\n[1] 2.896323\n\n\nWe conclude that during EURO 2022, Beth Mead scored 2.9 more goals than what StatsBomb’s XG model expected based on the contexts in which she attempted shots. We can repeat this calculation — summing over the difference between shot outcome \\(Y\\) and \\(\\textrm{XG}\\) — for all players in EURO 2022 to find the players that most over-performed and most under-performed the model expectations.\n\ngoe &lt;- \n  euro2022_shots |&gt;\n  dplyr::mutate(diff = Y - shot.statsbomb_xg) |&gt;\n  dplyr::group_by(player.name) |&gt;\n  dplyr::summarise(GOE = sum(diff),n = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(GOE))\ngoe\n\n# A tibble: 200 × 3\n   player.name               GOE     n\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Alexandra Popp          3.34     16\n 2 Bethany Mead            2.90     15\n 3 Alessia Russo           1.79     12\n 4 Francesca Kirby         1.79      5\n 5 Lina Magull             1.70     14\n 6 Ingrid Filippa Angeldal 1.37     10\n 7 Romée Leuchter          1.19      2\n 8 Hanna Ulrika Bennison   0.952     1\n 9 Nicole Anyomi           0.896     1\n10 Julie Blakstad          0.879     1\n# ℹ 190 more rows\n\n\nIt turns out that Alexandra Popp, the German captain, outperformed StatsBomb’s XG model expectations by an even wider margin than Beth Mead. Like Mead, Popp scored 6 goals during the tournament off a similar number of shots (16 for Popp and 15 for Mead). Interestingly, Mead won the Golden Boot because she had one more assist…",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-exercises",
    "href": "lectures/lecture02.html#sec-exercises",
    "title": "Lecture 2: Expected Goals",
    "section": "Exercises",
    "text": "Exercises\n\nFor each team in EURO 2022, sum the \\(\\textrm{XG}_{i}\\) values and sum the residuals \\(Y_{i} - \\textrm{XG}_{i}.\\) Which teams most over- or under-performed expectations? What relationship, if any, do you observe between teams’ cumulative XG and their performance relative to the StatsBomb XG model’s expectations?\nHudl recently released data from EURO 2025. Repeat the analysis from Section 6 to identify the players with the most extreme goals over expected values from the tournament.\nRepeat Exercise 2.1 using data from EURO 2025\nHudl also release several full seasons worth of data from professional leagues. Pick one such season and explore the relationships between (i) the cumulative XG the team generates on offense; (ii) the cumulative XG the team gives up on defense; and (iii) the win-loss record across the season.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#footnotes",
    "href": "lectures/lecture02.html#footnotes",
    "title": "Lecture 2: Expected Goals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd props to Hudl for continuing to make this data available!↩︎\nSee this StackOverflow post and the documentation for summarise for details.↩︎\nTry to convince yourself why this is the case!↩︎\nIndeed, it seems absurd to claim that, at least in women’s international soccer, players will never score off left-footed lobs! As the Statistician Dennis Lindley put it, we must “never believe in anything absolutely” and we should “leave a little probability for the moon being made of green cheese; it can be as small as 1 in a million, but have it there since otherwise an army of astronauts returning with samples of the said cheese will leave you unmoved” (Lindley 1985, sec. 6.7, available here). Lindley termed this principle “Cromwell’s Rule”, a reference to Oliver Cromwell’s quote “I beseech you, in the bowels of Christ, think it possible that you may be mistaken” from his letter to the Church of Scotland.↩︎",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture04.html",
    "href": "lectures/lecture04.html",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "",
    "text": "Which NBA players do the most to help their teams win? And how might we quantify that contribution? One option is to rely on box score statistics like points, rebounds, assists, steals, blocks, and turnovers. Underlying this approach is the assumption that the “best” or players with the most impact are the ones that record extreme values of these statistics. That is, they are the ones who score the most points, grab the most rebounds, commit the fewest turnovers, etc?\nWhile this approach seems natural, it quickly runs into problems. As Ilardi 2007 notes, it is not obvious whether all of these statistics should count the same or whether some should be weighted more heavily than others. It is also difficult to compare players who play different roles: is a guard who records 10 assists per game more valuable than a center who grabs 10 rebounds per game? More subtley, players do many things during the course of a game (e.g., setting screens, rotating on defense, dive for loose balls, etc.) that impact the outcome of a play or the game but do not appear in the box score. How should those contributions be valued?\nIn today’s lecture, we introduce two now-classical approaches to player evaluation: plus/minus (Section 3) and adjusted plus/minus (Section 4). At a high-level, plus/minus is simply the total point differential that a player’s team accrues when he is on the court. Adjusted plus/minus measures how much more a player contributes to his team’s point differential per 100 after accounting than a baseline-level player, after accounting for the relative contributions of his teammates and opponents.",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-overview",
    "href": "lectures/lecture04.html#sec-overview",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "",
    "text": "Which NBA players do the most to help their teams win? And how might we quantify that contribution? One option is to rely on box score statistics like points, rebounds, assists, steals, blocks, and turnovers. Underlying this approach is the assumption that the “best” or players with the most impact are the ones that record extreme values of these statistics. That is, they are the ones who score the most points, grab the most rebounds, commit the fewest turnovers, etc?\nWhile this approach seems natural, it quickly runs into problems. As Ilardi 2007 notes, it is not obvious whether all of these statistics should count the same or whether some should be weighted more heavily than others. It is also difficult to compare players who play different roles: is a guard who records 10 assists per game more valuable than a center who grabs 10 rebounds per game? More subtley, players do many things during the course of a game (e.g., setting screens, rotating on defense, dive for loose balls, etc.) that impact the outcome of a play or the game but do not appear in the box score. How should those contributions be valued?\nIn today’s lecture, we introduce two now-classical approaches to player evaluation: plus/minus (Section 3) and adjusted plus/minus (Section 4). At a high-level, plus/minus is simply the total point differential that a player’s team accrues when he is on the court. Adjusted plus/minus measures how much more a player contributes to his team’s point differential per 100 after accounting than a baseline-level player, after accounting for the relative contributions of his teammates and opponents.",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-data-prep",
    "href": "lectures/lecture04.html#sec-data-prep",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Stint-Level NBA Data",
    "text": "Stint-Level NBA Data\n\n\n\n\n\n\nDefinition: Stint\n\n\n\nA stint is a period of play between substitutions during which the same 10 players remain on the court.\n\n\nTo compute plus/minus and adjusted plus/minus, we need to build a data table in which rows correspond to stints within individual games. In addition to recording the identities of the home and away team players on the court during each stint, the data table needs to include game-state information about each stint including both teams’ scores and the amount of time left at the start and end of each stint and the number of possessions in each stint. Such a table can be built using play-by-play data.\nThe NBA posts a play-by-play for every game containing the time-stamp and a short description of important events during a game. Basically, whenever a player does something tracked by the scorekeepers or reported in the box-score (e.g., attempt a shot, steal the ball, commit a foul, etc.), an entry is added to the play-by-play log. Here is an example from a game from a few years ago between the Dallas Mavericks and Minnesota Timberwolves1.\nWe can scrap play-by-play data into R using the [hoopR] package, which can be installed using the code:\n\ninstall.packages(\"hoopR\")\n\nThe following function, which is based on a script written by Ron Yurko, extracts stint information from a single game’s play-by-play.\n\n\n\n\n\n\nWarning\n\n\n\nIt takes about 30 minutes to download all single season’s worth of play-by-play data.\n\n\n\n\nShow code to get stint data\n1get_game_stint_data &lt;- function(game_i) {\n  \n  game_data &lt;- hoopR::nba_pbp(game_id = game_i)\n  \n2  game_data$home_score[1] &lt;- 0\n  game_data$away_score[1] &lt;- 0\n  game_data$score_margin[1] &lt;- 0\n  \n  game_data &lt;- \n    game_data |&gt;\n3    tidyr::fill(home_score, away_score, score_margin)\n  \n  home_lineups &lt;- \n    game_data |&gt;\n    dplyr::select(home_player1, home_player2, home_player3, home_player4, home_player5) |&gt;\n4    apply(MARGIN = 1, FUN = function(x){paste(sort(x), collapse = \"_\")})\n  away_lineups &lt;-\n    game_data |&gt;\n    dplyr::select(away_player1, away_player2,away_player3, away_player4, away_player5) |&gt;\n5    apply(MARGIN = 1, FUN = function(x){paste(sort(x), collapse = \"_\")})\n  \n  game_data$home_lineup &lt;- home_lineups\n6  game_data$away_lineup &lt;- away_lineups\n  \n  # Now there are a couple of ways to figure out the stints, but I think the\n  # best way is to use the substitution events - where a stint changes once\n  # a substitution takes places (but only if the previous event was NOT a substitution)\n  \n  # Start by making an indicator for the stint change:\n  game_data &lt;- \n    game_data |&gt;\n    dplyr::mutate(is_sub = ifelse(event_type == 8, 1, 0),\n                  new_stint_start = ifelse((is_sub == 1) & (dplyr::lead(is_sub) != 1),1, 0))\n  \n  # For substitutions that take place\n  # during a free throw window, then the new stint should start after the free throw\n  \n  # Easiest way to track if substitution takes places before final free throw:\n  game_data &lt;- \n    game_data |&gt;\n    dplyr::mutate(sub_during_free_throw = dplyr::case_when(\n      (stringr::str_detect(visitor_description, \"Free Throw 1 of 1\") |\n         stringr::str_detect(visitor_description, \"Free Throw 2 of 2\") |\n         stringr::str_detect(visitor_description, \"Free Throw 3 of 3\")) &\n        (dplyr::lag(is_sub) == 1) ~ 1,\n      (stringr::str_detect(home_description, \"Free Throw 1 of 1\") |\n         stringr::str_detect(home_description, \"Free Throw 2 of 2\") |\n         stringr::str_detect(home_description, \"Free Throw 3 of 3\")) &\n        (dplyr::lag(is_sub) == 1) ~ 1,\n      .default = 0),\n      # Now if the sub is followed by this, then set new_stint_start to 0, but\n      # if this set a new stint to start post the final free throw:\n      new_stint_start = ifelse(is_sub == 1 & dplyr::lead(sub_during_free_throw) == 1, 0, new_stint_start),\n      new_stint_start = ifelse(lag(sub_during_free_throw) == 1,1, new_stint_start),\n      new_stint_start = ifelse(is.na(new_stint_start), 0, new_stint_start)\n    )\n  \n  # Filter out subs that are not new stints, and then just use\n  # the cumulative sum of the new stint start to effectively create a stint ID:\n  game_data &lt;- game_data |&gt;\n    dplyr::filter(!(is_sub == 1 & new_stint_start == 0)) |&gt;\n    dplyr::mutate(stint_id = cumsum(new_stint_start) + 1)\n  \n  # Toughest part - need to count the number of possessions for each team during\n  # the stint... will rely on this for counting when a possession ends:\n  # https://squared2020.com/2017/09/18/deep-dive-on-regularized-adjusted-plus-minus-ii-basic-application-to-2017-nba-data-with-r/\n  # \"Recall that a possession is ended by a converted last free throw, made field goal, defensive rebound, turnover, or end of period\"\n  \n  game_data &lt;- game_data |&gt;\n    dplyr::mutate(pos_ends = dplyr::case_when(\n      stringr::str_detect(home_description, \" PTS\") &\n        stringr::str_detect(home_description, \"Free Throw 1 of 2\", negate = TRUE) &\n        stringr::str_detect(home_description, \"Free Throw 2 of 3\",negate = TRUE) ~ 1, # made field goals or free throws\n      stringr::str_detect(visitor_description, \" PTS\") & \n        stringr::str_detect(visitor_description, \"Free Throw 1 of 2\",negate = TRUE) & \n        stringr::str_detect(visitor_description, \"Free Throw 2 of 3\",negate = TRUE) ~ 1, \n      stringr::str_detect(tolower(visitor_description), \"rebound\") &\n        stringr::str_detect(tolower(lag(home_description)), \"miss \") ~ 1,\n      stringr::str_detect(tolower(home_description), \"rebound\") &\n        stringr::str_detect(tolower(lag(visitor_description)), \"miss \") ~ 1,\n      stringr::str_detect(tolower(home_description), \" turnover\") ~ 1,\n      stringr::str_detect(tolower(visitor_description), \" turnover\") ~ 1,\n      stringr::str_detect(neutral_description, \"End\") ~ 1,.default = 0))\n  \n  # Now the final part - compute the stint level summaries:\n  game_data |&gt;\n    dplyr::group_by(stint_id) |&gt;\n    dplyr::summarize(\n      home_lineup = dplyr::first(home_lineup),\n      away_lineup = dplyr::first(away_lineup),\n      n_home_lineups = length(unique(home_lineup)),\n      n_away_lineups = length(unique(away_lineup)),\n      start_home_score = dplyr::first(home_score),\n      end_home_score = dplyr::last(home_score),\n      start_away_score = dplyr::first(away_score),\n      end_away_score = dplyr::last(away_score),\n      start_minutes = dplyr::first(minute_game),\n      end_minutes = dplyr::last(minute_game),\n      n_pos = sum(pos_ends),\n      .groups = \"drop\") |&gt;\n    dplyr::mutate(\n7      home_points = end_home_score - start_home_score,\n      away_points = end_away_score - start_away_score,\n      minutes = end_minutes - start_minutes,\n      pts_diff = home_points - away_points,\n      margin = 100 * (pts_diff) / n_pos,\n      game_id = game_i) |&gt;\n    dplyr::select(game_id, stint_id, \n                  start_home_score, start_away_score,\n                  start_minutes, \n                  end_home_score, end_away_score, end_minutes,\n                  home_lineup, away_lineup, n_pos,\n                  home_points, away_points, minutes, pts_diff, margin) |&gt;\n8    dplyr::filter(n_pos != 0)\n}\n\nposs_get_game_stints &lt;- \n9  purrr::possibly(.f = get_game_stint_data, otherwise = NULL)\n\n\n\n1\n\nFunction to extract the stints from a single game’s play-by-play\n\n2\n\nInitialize the starting score and margin (i.e., point differential per 100 possessions) at 0\n\n3\n\ntidyr::fill() fills in missing values with the previous entry.\n\n4\n\nGet the 5 home team players and a create a string of the numeric IDs separated by underscores (“_“)\n\n5\n\nGet the 5 away team players and a create a string of the numeric IDs separated by underscores (“_“)\n\n6\n\nAdd the strings recording who is on the court to game_data\n\n7\n\nCompute number of points scored by each team and other game-state information\n\n8\n\nRemove stints with 0 possessions.\n\n9\n\nWe will loop over many games. To prevent an error from prematurely terminating the loop (e.g., from a game with missing or corrupted play-by-play), we create a version that returns NULL when it hits an error.\n\n\n\n\nThe following code downloads play-by-play logs for every game in the 2024-25 season and applies the function defined above to create our stint-level data table.\n\nraw_game_log &lt;- hoopR::nba_leaguegamelog(season = 2024)\nnba_game_log &lt;- raw_game_log$LeagueGameLog\n\nnba_season_games &lt;- \n  nba_game_log |&gt;\n  dplyr::pull(GAME_ID) |&gt;\n1  unique()\n\n\nseason_stint_data &lt;- \n2  purrr::map(nba_season_games, ~poss_get_game_stints(.x)) |&gt;\n3  dplyr::bind_rows()\n\n4game_stint_context &lt;-\n  season_stint_data |&gt;\n  dplyr::select(\n    game_id, stint_id, n_pos,\n    start_home_score, start_away_score,start_minutes, \n    end_home_score, end_away_score, end_minutes,\n    home_points, away_points, minutes, \n    pts_diff, margin)\n\n5home_players_data &lt;-\n  season_stint_data |&gt;\n  dplyr::select(game_id, stint_id, home_lineup)|&gt;\n  tidyr::separate_rows(home_lineup, sep = \"_\") |&gt;\n  dplyr::mutate(on_court = 1) |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(\"game_id\", \"stint_id\"),\n    names_from = home_lineup,\n    values_from = on_court,\n    values_fill = 0)\nhome_players_cols &lt;- colnames(home_players_data)[3:ncol(home_players_data)]\n\n6away_players_data &lt;-\n  season_stint_data |&gt;\n  dplyr::select(game_id, stint_id, away_lineup) |&gt;\n  tidyr::separate_rows(away_lineup, sep = \"_\") |&gt;\n  dplyr::mutate(on_court = -1) |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(\"game_id\", \"stint_id\"),\n    names_from = away_lineup,\n    values_from = on_court,\n    values_fill = 0)\naway_players_cols &lt;- colnames(away_players_data)[3:ncol(away_players_data)]\n\ngame_stint_players_data &lt;- \n  home_players_data |&gt;\n7  dplyr::bind_rows(away_players_data) |&gt;\n  dplyr::group_by(game_id, stint_id) |&gt;\n  dplyr::summarize(\n    dplyr::across(dplyr::everything(), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\")\n\n8rapm_data &lt;-\n  game_stint_context |&gt;\n  dplyr::left_join(game_stint_players_data,by = c(\"game_id\", \"stint_id\"))\n\n\n1\n\nGet the ID number for every game\n\n2\n\nLoop over all game IDs and extract stint information\n\n3\n\npurrr::map() stores the stint data table for each game as a separate element of a list. Here we concatenate each of these tables into a single table.\n\n4\n\nPulls out the game-state/contextual information about each stint\n\n5\n\nCreates a column for every home team player’s on-court indicator\n\n6\n\nCreates a column for every away team player’s on-court indicator\n\n7\n\nConcatenates the home team indicators and away team indicators\n\n8\n\nJoins the player on-court indicators with the contextual information about the stint\n\n\n\n\nThe first 14 columns of rapm_data record:\n\nNumeric identifiers for the game (game_id) and stint (stint_id).\nBoth teams’ scores and the amount of time remaining at the start of a stint: start_home_score, start_away_score, and start_minutes.\nBoth teams’ scores and the amount of time remaining at the end of each stint: end_home_score, end_away_score, and end_minutes.\nThe length of a stint (minutes), the number of points scored by the home (home_points) and away team (away_points), and the number of possessions (n_pos).\nThe score differential pts_diff = home_points - away_points.\nThe score differential per 100 possessions, margin = pts_diff/n_pos * 100.\n\nThe remaining columns record whether individual players are not on the court (0), on the court and playing at home (+1), or on the court and playing on the road (-1) in each stint. The names of these columns are just identifiers assigned by hoopR. The function hoopR::nba_commonallplayers() returns a table with all players identifiers and names. The code below uses the function hoopR::nba_commonallplayers() to make a look-up table containing player names and identifiers. It also creates a column where all accents and special characters have been removed, which makes it easier to look up player ids by name (i.e., we can type “Luka Doncic” instead of “Luka Dončić”).\n\nplayer_table &lt;-\n1  hoopR::nba_commonallplayers()[[\"CommonAllPlayers\"]] |&gt;\n  dplyr::select(PERSON_ID, DISPLAY_FIRST_LAST) |&gt;\n  dplyr::rename(id = PERSON_ID, FullName = DISPLAY_FIRST_LAST) |&gt;\n  dplyr::mutate(\n2    Name = stringi::stri_trans_general(FullName, \"Latin-ASCII\"))\n\n\n1\n\nThe function returns a list with a single element named “CommonAllPlayers”. The [[ ]] bit pulls out the table.\n\n2\n\nTransliterate player names to ASCII. Effectively this strips out the accents",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-pm",
    "href": "lectures/lecture04.html#sec-pm",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Plus/Minus",
    "text": "Plus/Minus\nIntuitively, we expect a team to perform better when a “good” player is on the court than when he is not on the court. Similarly, we expect a team to perform worse when a “bad” player is on the court than when he is not on the court. Plus/Minus (hereafter +/-) attempts to quantify this intuition by computing the total number of points by which a player’s team outscores their opponents when that player is on the floor. If a player has a large, positive +/- value, that means that the player’s team outscored its opponents by a large margin Similarly, a large, negative +/- value indicates that a player’s team was outscored by a wide margin when the player was on the court.\n\nComputing Individual +/-’s\nAs an example, let’s compute the plus/minus values for Shai Gilgeous-Alexander (SGA), who was recognized as the league’s Most Valuable Player (MVP) in the 2024-25 regular season. To compute SGA’s +/-, we need to\n\nSum the home team point differentials (i.e., the values in the column pts_diff) for all stints where SGA was on the court and playing at home.\nSum the negative of the home team point differentials for all stints where SGA was on the court and playing on the road.\nAdd the two totals from Steps 1 and 2.\n\nMathematically, for every stint \\(i\\) in our data table, let \\(\\Delta_{i}\\) be the home team’s point differential during that stint. Additionally, let \\(x_{i, \\textrm{SGA}}\\) be the signed on-court indicator that is equal to 1 if SGA is on the court and playing at home during stint \\(i\\); -1 if Shai is on the court and playing on the road during stint \\(i\\); and 0 if SGA is not on the court during stint \\(i.\\) Then, SGA’s +/- is equal to \\[\n\\sum_{i = 1}^{n}{x_{i,\\textrm{SGA}} \\times \\Delta_{i}}.\n\\] The data table rapm_data contains a column with the signed on-court indicators for every player. The names of these columns are just the hoopR identifiers, which were recorded in the id column of our look-up table player_table. The following code computes Shai’s +/- and uses the function [dplyr::pull()](https://dplyr.tidyverse.org/reference/pull.html) to extract values from specific columns of rapm_data.\n\nshai_id &lt;-\n  player_table |&gt;\n1  dplyr::filter(Name == \"Shai Gilgeous-Alexander\") |&gt;\n2  dplyr::pull(id)\n3shai_x &lt;- rapm_data |&gt; dplyr::pull(shai_id)\n4delta &lt;- rapm_data |&gt; dplyr::pull(pts_diff)\n\n5sum(shai_x * delta)\n\n\n1\n\nFind the row in our look-up table corresponding to SGA\n\n2\n\nSince there is only one row corresponding to SGA, this saves SGA’s identifier as shai_id\n\n3\n\nExtracts the vector of SGA’s signed on-court indicators from the corresponding column of rapm_data\n\n4\n\nExtracts the vector of home team point differentials across all stints from rapm_data\n\n5\n\nComputes SGA’s +/-\n\n\n\n\n[1] 888\n\n\nWe now repeat this calculation for Nikola Jokic, who finished second in the MVP voting in the 2024-25 regular season. His overall +/- is substantially smaller than SGA’s.\n\njokic_id &lt;-\n  player_table |&gt;\n  dplyr::filter(Name == \"Nikola Jokic\") |&gt;\n  dplyr::pull(id)\n\njokic_x &lt;- rapm_data |&gt; dplyr::pull(jokic_id) \nsum(jokic_x * delta) \n\n[1] 452\n\n\n\n\nMatrix-Based Computation\nRepeating this calculation for all players one at a time is exceptionally tedious. Luckily, with a bit of matrix algebra, we can compute the +/- for all players in one shot.\nTo this end, let \\(n\\) be the total number of stints in our data table and let \\(p\\) be the total number of players. Just like we did for SGA and Jokic in Section 3.1, for each stint \\(i = 1, \\ldots, n\\) and for each player \\(j = 1, \\ldots, p,\\) let \\(x_{ij}\\) be the signed on-court indicator for player \\(j\\) during stint \\(i.\\) So, \\(x_{ij}\\) is equal to 0 if player \\(j\\) is not on the court during stint \\(i\\) and is equal to 1 (resp. -1) if player \\(j\\) is on the court and playing at home (resp. on the road) during stint \\(i.\\) Player \\(j\\)’s overall +/- is given by \\[\n\\sum_{i = 1}^{n}{x_{ij}\\Delta_{i}}.\n\\] We can arrange the \\(x_{ij}\\) values into an \\(n \\times p\\) matrix \\(\\boldsymbol{\\mathbf{X}}\\) so that \\(x_{ij}\\) appears in in the \\(j-th\\) column of the \\(i\\)-th row (i.e., \\(x_{ij}\\) is the \\((i,j)\\) entry of \\(\\boldsymbol{\\mathbf{X}}\\)2). This way, the rows (resp. columns) of \\(\\boldsymbol{\\mathbf{X}}\\) correspond to stints (resp. players). Let \\(\\boldsymbol{\\Delta}\\) be the vector of length \\(n\\) whose \\(i\\)-th entry is \\(\\Delta_{i},\\) the home-team point differential in stint \\(i.\\)\nIt turns out3 that player \\(j\\)’s +/- is the \\(j\\)-th entry of the vector obtained when we multiply the transpose[^transpose] of \\(\\boldsymbol{\\mathbf{X}}\\) by the vector \\(\\boldsymbol{\\Delta}.\\) That is, if \\[\n\\boldsymbol{\\mathbf{pm}} = \\boldsymbol{\\mathbf{X}}^{\\top}\\boldsymbol{\\Delta},\n\\] then the vector \\(\\boldsymbol{\\mathbf{pm}}\\) contains \\(p\\) entries and its \\(j\\)-th entry is given by \\[\n\\textrm{pm}_{j} = \\sum_{i = 1}^{n}{x_{ij}\\Delta_{i}}.\n\\] [^transpose]: The transpose of a matrix \\(\\boldsymbol{A}\\), which is denoted as \\(\\boldsymbol{A}^{\\top}\\), is obtained by switching the rows and columns indices. For instance, \\(\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}^{\\top} = \\begin{pmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{pmatrix}.\\) See this Wikipedia article for more information.\nThe first 14 columns of the data table rapm_data contain contextual information about the stint. The remaining columns contain the values of each player’s signed on-court indicators. So, to form the matrix \\(\\boldsymbol{\\mathbf{X}},\\) we just need to drop the contextual columns.\n\n1context_vars &lt;-\n  c(\"game_id\", \"stint_id\", \"n_pos\", \n    \"start_home_score\", \"start_away_score\", \"start_minutes\",\n    \"end_home_score\", \"end_away_score\", \"end_minutes\",\n    \"home_points\", \"away_points\", \"minutes\",\n    \"pts_diff\", \"margin\")\n\n\nX_full &lt;-\n2  as.matrix(\n    rapm_data |&gt;\n3      dplyr::select(- tidyr::all_of(context_vars)))\n\n\n1\n\nVector of contextual variables contained in rapm_data\n\n2\n\nConverts from a tibble to a matrix\n\n3\n\nDrops the contextual variables\n\n\n\n\nIn R, we can compute the transpose of a matrix using the function t(). But, it turns out that when we want to compute a product like \\(\\boldsymbol{\\mathbf{X}}^{\\top}\\boldsymbol{\\Delta},\\) it is much more efficient to use the function crossprod().\nThe following code creates a data table pm with one column containing player id’s another column containing plus/minus values. It then uses an inner_join to append player names. The code also computes the number of possessions played by each player. To this end, let \\(\\textrm{pos}_{i}\\) be the number of possessions played during stint \\(i\\). Notice that \\(\\lvert x_{ij} \\rvert\\) is equal to 1 whenever player \\(j\\) is on the court and is equal to 0 otherwise. So, the number of possessions played by player \\(j\\) is simply \\(\\sum_{i = 1}^{n}{\\lvert x_{ij} \\rvert \\textrm{pos}_{i}}.\\)\n\npm &lt;-\n  data.frame( \n1    id = colnames(X_full),\n2    pm = crossprod(x = X_full, y = delta),\n3    n_pos = crossprod(abs(X_full), y = rapm_data |&gt; dplyr::pull(n_pos)),\n4    minutes = crossprod(abs(X_full), y = rapm_data |&gt; dplyr::pull(minutes))) |&gt;\n5  dplyr::inner_join(y = player_table |&gt; dplyr::select(id, Name), by = \"id\") |&gt;\n6  dplyr::select(id, Name, pm, n_pos, minutes) |&gt;\n  dplyr::arrange(dplyr::desc(pm))\n\n\n1\n\nCreates a column containing player ID’s\n\n2\n\nComputes each player’s +/-.\n\n3\n\nComputes the number of possessions played by each player. The function abs() computes absolute value. We used dplyr::pull to extract the number of possessions in each stint.\n\n4\n\nCompute the number of minutes played by each player.\n\n5\n\nAppend only the names of players from player_table whose ID appears as a column name of X_full\n\n6\n\nRe-arrange the columns\n\n\n\n\nLooking at the +/- values for a handful of selected players, we notice that SGA had a much higher +/- than many other prominent players in the league.\n\nselected_players &lt;- \n  c(\"Shai Gilgeous-Alexander\", \n    \"Jayson Tatum\",\n    \"Nikola Jokic\",\n    \"Giannis Antetokounmpo\",\n    \"Luka Doncic\",\n    \"Anthony Davis\", \n    \"LeBron James\")\npm |&gt; dplyr::filter(Name %in% selected_players) |&gt; dplyr::select(Name, pm)\n\n                     Name  pm\n1 Shai Gilgeous-Alexander 888\n2            Jayson Tatum 474\n3            Nikola Jokic 452\n4   Giannis Antetokounmpo 331\n5             Luka Doncic 276\n6           Anthony Davis -78\n7            LeBron James -88\n\n\nThe following code helps visualize just how much larger SGA’s +/- was compared to the rest of these selected players and the rest of the league.\n\n1oi_colors &lt;- palette.colors(palette = \"Okabe-Ito\")\nn &lt;- nrow(X_full)\n2p &lt;- ncol(X_full)\n3y_lim &lt;- max(abs(pm$pm)) * c(-1.01, 1.01)\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\n4plot(1, type = \"n\",\n5     xlim = c(0, p+1), ylim = y_lim,\n6     xlab = \"\", xaxt = \"n\",\n7     ylab = \"+/-\", main = \"Plus-Minus\")\n\nfor(i in 1:p){\n8  lines(x = c(p+1-i,p+1-i), y = c(0, pm$pm[i]),\n9        col = oi_colors[9], lwd = 0.25)\n  if(pm$Name[i] %in% selected_players){\n    points(x = p+1-i, y = pm$pm[i], pch = 16, cex = 0.7, col = oi_colors[3])\n  } else{\n    points(x = p+1-i, y = pm$pm[i], pch = 16, cex = 0.25, col = oi_colors[9])\n  }\n}\nabline(h = 0, col = oi_colors[8])\n\n\n1\n\nLoad a color-blind friendly palette\n\n2\n\nHandy to keep the number of stints and players in our environment\n\n3\n\nTo set symmetric vertical limits in our plot, we adjust the largest absolute +/- value by 1% in each direction.\n\n4\n\nTells R to set up the plot area but not to plot anything in it\n\n5\n\nSet the horizontal and vertical limits of the plot.\n\n6\n\nSuppresses the horizontal axis and its labels\n\n7\n\nLabel the vertical axis and add a title to the plot\n\n8\n\nThe data table pm is sorted with +/- in decreasing order. To visualize these values in increasing order, we plot the \\(i\\)-th largest +/- value at the horizontal coordinate \\(p+1-i\\).\n\n9\n\nThe argument lwd controls the thickness of the lines. Setting it to 0.25 prevents over-plotting in this case\n\n\n\n\n\n\n\n\n\n\n\n\n\nIssues with +/-\nThe gap SGA’s +/- and the +/- values for the rest of the league is striking. Is this gap really attributable to difference in skill alone? Or is the gap an artifact of some systematic factors? When we plot each player’s +/- against the number of possessions played, we find much more variation in the +/- values of players who played more possessions than those who played less.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(pm$n_pos, pm$pm, \n     pch = 16, cex = 0.5, col = oi_colors[9],\n     xlab = \"Possessions played\", ylab = \"+/-\",\n     main = \"Plus-Minus vs Possessions\")\nabline(h = 0, col = oi_colors[8])\n\n\n\n\n\n\n\nFigure 1: Relationship between Plus/Minus and number of possessions played\n\n\n\n\n\nBecause +/- is an aggregated statistic — that is, it is formed by adding up contributions over the course of a season — this is not wholly surprising. We see that SGA, in particular, played many more possessions than the other players with the ten highest +/- values. Consequently, if player A has a higher +/- than player B, we cannot immediately conclude that A is better or more valuable than B due to potential disparities in opportunities.\n\npm |&gt; dplyr::slice_head(n=10)\n\n        id                    Name  pm n_pos minutes\n1  1628983 Shai Gilgeous-Alexander 888  8159 2837.31\n2  1629652           Luguentz Dort 561  5584 1955.17\n3  1630198              Isaiah Joe 552  4896 1715.11\n4  1628401           Derrick White 509  6129 2327.72\n5  1630596             Evan Mobley 508  5944 2049.43\n6  1630598           Aaron Wiggins 507  4868 1683.08\n7  1628378        Donovan Mitchell 491  6101 2106.53\n8  1628369            Jayson Tatum 474  7498 2840.95\n9  1628386           Jarrett Allen 459  6163 2142.82\n10  203999            Nikola Jokic 452  7907 2707.97\n\n\nLooking at the top-10 plus/minus values reveals another potential issue: three of SGA’s teammates (Dort, Joe, Wiggins) appear in the top-10. Do we really believe that Lugentz Dort, who is known primarily for his defensive abilites, is the second-best player in the league? The player with the third-highest +/- is Isaiah Joe, who primarily came off the bench. Does this mean that the Oklahoma City Thunder under-utilitzed potentially the third-best player in the league?\nRecall that +/- is computed by adding up the point differential for every player when they are on the court. So, if a player is fortunate enough to play alongside extremely talented teammates, they might post a high +/- rating, regardless of their actual contribution. Moreover, players who consistently find themselves playing against the opponents’ best players may have a lower +/- rating than players who more often play against their opponents’ second units. Thus, even if player A and player B played a similar number of possessions, A having a higher +/- than B still does not necessarily mean that A is better than B due to potential differences in their teammates and opponents.",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-apm-intro",
    "href": "lectures/lecture04.html#sec-apm-intro",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Adjusted Plus/Minus",
    "text": "Adjusted Plus/Minus\nAn inherent weakness, then, of +/- is that a player’s +/- necessarily depends on the performances of his teammates and opponents, which can vary dramatically across players. Adjusted plus/minus, which was first introduced by Rosenbaum in 2004, attempts to take into account the quality of a player’s teammates and opponents. Paraphrasing Rosenbaum, adjusted plus/minus does not reward players for simply being fortunate enough to play with teammates better than their opponents.\nAs noted above, comparing totals runs the risk of favoring players with many more possessions. So, to facilitate fairer comparisons, APM first converts the point differential into rate, namely point differential per 100 possessions In our rapm_data data table, this quantity is saved in the column margin4. We will denote the margin for stint \\(i\\) by \\(Y_{i}.\\)\nFormally, APM associates a number \\(\\alpha_{j}\\) to every player \\(j = 1, \\ldots, p\\) and models \\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\] where \\(h_{1}(i), \\ldots, h_{5}(i)\\) and \\(a_{1}(i), \\ldots, a_{5}(i)\\) are, respectively, the indices of the homes and away team players on the court during stint \\(i\\); \\(\\alpha_{0}\\) captures the average home court advantage in terms of point-differential per 100 possessions across all teams; and the \\(\\epsilon_{i}\\)’s are independent random errors drawn from a distribution with mean zero. In other words, APM expresses the actually observed point differential per 100 possessions as a random deviation away from the expected point differential per 100 possessions when players \\(h_{1}(i), \\ldots, h_{5}(i)\\) play against \\(a_{1}(i), \\ldots, a_{5}(i).\\)\nTo better understand the model, consider the first stint from the December 23, 2024 game between the Dallas Mavericks (away) and the Golden State Warriors (home). The players on the court were:\n\nMavericks (away): Luka Doncic, Dereck Lively II, Kyrie Irving, P.J. Washington, and Klay Thompson\nWarriors (home): Stephen Curry, Buddy Hield, Andrew Wiggins, Jonathan Kuminga, and Kevon Looney.\n\nBecause the Mavericks were playing on the road, they would be expected to outscore the Warriors by \\[\n\\begin{align}\n&-1 \\times (\\alpha_{0} + \\alpha_{SC} + \\alpha_{BH} + \\alpha_{AW} + \\alpha_{JK} + \\alpha_{KL}) \\\\\n&~~~~~+(\\alpha_{LD} + \\alpha_{KI} + \\alpha_{DL} + \\alpha_{PW} + \\alpha_{KT}).\n\\end{align}\n\\] points per 100 possessions during this stint. Now imagine that Luka Doncic were replaced by Anthony Davis5 but the other 9 players remain on the court. Then, the Mavericks’ expected points differential per 100 possessions would be \\[\n\\begin{align}\n&-1 \\times (\\alpha_{0} + \\alpha_{SC} + \\alpha_{BH} + \\alpha_{AW} + \\alpha_{JK} + \\alpha_{KL}) \\\\\n&~~~~~+(\\alpha_{AD} + \\alpha_{KI} + \\alpha_{DL} + \\alpha_{PW} + \\alpha_{KT}).\n\\end{align}\n\\] Taking the difference between these two expectations, under the APM model, the Mavericks are expected to outscore the Warriors by \\(\\alpha_{\\textrm{AD}} - \\alpha_{\\textrm{LD}}\\) points per 100 possessions when they replace Dončić with Davis. More generally, the quantity \\(\\alpha_{j} - \\alpha_{j'}\\) represents how many more points per 100 possessions a team expects to score when player \\(j\\) is on the court than when he is replaced by player \\(j'\\).\n\nAPM As A Linear Model\nOf course, we don’t know the exact \\(\\alpha_{j}\\) values and must use our data to estimate them. To this end, let \\(\\boldsymbol{\\mathbf{Z}}\\) be the \\(n \\times (p+1)\\) matrix formed by appending a column of 1’s to the matrix \\(\\boldsymbol{\\mathbf{X}}\\) and let \\(\\boldsymbol{\\alpha} = (\\alpha_{0}, \\alpha_{1}, \\ldots, \\alpha_{p})^{\\top}\\) be the vector of length \\(p+1\\) containing the home court advantage \\(\\alpha_{0}\\) and all the player-specific parameters \\(\\alpha_{j}.\\) Then, letting \\(\\boldsymbol{\\mathbf{z}}_{i}\\) be the \\(i\\)-th row of \\(\\boldsymbol{\\mathbf{Z}},\\) the APM model asserts that \\[\nY_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}.\n\\] That is, the APM model is really just a multiple linear regression model6 where the predictors include the intercept and the signed on-court indicators for all players. Because it is just a linear model, it is tempting to estimate the unknown parameter vector \\(\\boldsymbol{\\alpha}\\) using ordinary least squares.\nThat is, we want to find \\(\\hat{\\boldsymbol{\\alpha}}\\) that minimizes the quantity \\[\n\\sum_{i = 1}^{n}{\\left( Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} \\right)^{2}} .\n\\]\nUnfortunately, this problem does not have a unique solution because the matrix \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) does not have a unique inverse. To see this, recall that each row of \\(\\boldsymbol{\\mathbf{Z}}}\\) contains exactly 11 non-zero entries: * The first element in each row is equal to 1, corresponding to the intercept in the APM model * 5 entries equal to 1, corresponding to the five home players on the court during the stint * 5 entries equal to -1, correspond to the five away players on the court during the stint So, each row of \\(\\boldsymbol{\\mathbf{Z}}\\) sums to zero, meaning that the matrix is not of full-rank. This in turn implies that \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) does not have a unique inverse and that the optimization problem does not have a unique solution.\n\n\nValue Relative to Baseline\nIn short, there are two main difficulties with our initial APM model. The first is practical: we simply cannot obtain player-specific estimates using the method of least squares. And even if we could, we face a more subtle problem: the individual parameters \\(\\alpha_{j}\\) are meaningless on their own and represent an absurd counter-factual situation.\nTo elaborate, earlier we considered a hypothetical scenario when Luka Dončić was replaced on the court by Anthony Davis. From that exercise, we saw that \\(\\alpha_{j} - \\alpha_{j'}\\) quantifies how much a team’s point differential per 100 possessions changes if you replace player \\(j'\\) by player \\(j\\) and leave everything else unchanged. Using the exact same logic, we can conclude that \\(\\alpha_{j}\\) represents the change in point differential per 100 possession when you remove player \\(j\\) from the court and don’t replace him with anybody else. Since teams never play 4-on-5, the absolute \\(\\alpha_{j}\\) values are meaningless.\nTo overcome these challenges, most analysts create a group of “baseline-level” players and, without losing any generality, they re-order the players \\(j = 1, \\ldots, p\\) so that the first \\(p'\\) are non-baseline and the last \\(p-p'\\) are baseline-level. Then, they assume that the \\(\\alpha_{j}\\)’s for all baseline-level players are identical and equal to some common value \\(\\mu.\\) For each non-baseline player \\(j = 1, \\ldots, \\tilde{p},\\) they introduce the parameter \\(\\beta_{j} = \\alpha_{j} - \\mu.\\) Unlike the individual \\(\\alpha_{j}\\) values, the \\(\\beta_{j}\\) parameters for non-baseline players have a sensible interpretation: a team should expect to score \\(\\beta_{j}\\) more points per 100 possessions if they replace a baseline-level player with non-baseline player \\(j\\), keeping all other players on the court the same7.\nThe \\(\\beta_{j}\\)’s can also be estimated using the method of least squares. Formally, let \\(\\beta_{0} = \\alpha_{0};\\) \\(\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p'})^{\\top}\\) be the vector of unknown model parameters; and let \\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\) be the \\(n \\times (p'+1)\\) sub-matrix of \\(\\boldsymbol{\\mathbf{Z}}\\) formed by removing the columns corresponding to baseline players. It turns out that[^prove2] \\[\n\\tilde{\\boldsymbol{\\mathbf{Z}}}\\boldsymbol{\\beta} = \\boldsymbol{\\mathbf{Z}}\\boldsymbol{\\alpha}\n\\] [^prove2]: Again, try to prove this yourself. If you run into any issues, ask on Piazza or stop by office hours.\nHowever, unlike the matrix \\(\\boldsymbol{\\mathbf{Z}},\\) the matrix \\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\) is full-rank, which means that the minimization problem \\[\n\\textrm{argmin}\\sum_{i = 1}^{n}{\\left(Y_{i} - \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta}\\right)^{2}}\n\\] has a unique solution: \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\tilde{\\boldsymbol{\\mathbf{Z}}}\\right)^{-1}\\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\boldsymbol{\\mathbf{Y}}.\n\\]\n\n\nA Re-parametrized Model\nIn practice, we rarely fit linear models like our re-parametrized APM model with manual matrix calculations. Instead, we rely on R’s built-in lm() function to do compute \\(\\hat{\\boldsymbol{\\beta}}.\\) To this end, it suffices to create a data table where each row corresponds to a stint and there are columns containing the margin for the stint and the signed on-court indicators for all non-baseline players.\nBefore proceeding, we need to define the set of baseline-level players. For our analysis, we will use a 250 minute threshold, classifying any player who played fewer than 250 minutes as baseline-level.\n\nnonbaseline_id &lt;-\n  pm |&gt;\n  dplyr::filter(minutes &gt;= 250) |&gt;\n  dplyr::pull(id)\n\nNext, we create a data table containing columns for the signed on-court indicators of all non-baseline players and the point differential per 100 possessions. Then, we fit the linear model and extract the estimated parameters.\n\napm_df &lt;-\n  rapm_data |&gt;\n  dplyr::select(tidyr::all_of(c(\"margin\", nonbaseline_id)))\n\n1apm_fit &lt;- lm(margin ~ ., data = apm_df)\n\n\n2beta0 &lt;- coefficients(apm_fit)[1]\n3beta &lt;- coefficients(apm_fit)[-1]\n\n\n1\n\nFit the linear model\n\n2\n\nExtract the intercept term, which captures the home-court advantage\n\n3\n\nExtract the estimates of \\(\\beta_{j}\\) for all non-baseline players\n\n\n\n\nAfter inspecting the first few elements, we see that the elements of beta are named and that there is an additional character back-tick in the element names.\n\nbeta[1:5]\n\n `1628983`  `1629652`  `1630198`  `1628401`  `1630596` \n10.2568750  1.8500194  4.0660224 -0.8287388  7.9423890 \n\n\nWe will build a data table similar to pm containing the id and estimated adjusted plus/minus value of every non-baseline player. To do this, we need to remove the back-tick from the names of beta.\n\n1names(beta) &lt;- stringr::str_remove_all(string = names(beta), pattern = \"`\")\napm &lt;-\n  data.frame(id = names(beta), apm = beta) |&gt;\n  dplyr::inner_join(y = player_table, by = \"id\")\n2rownames(apm) &lt;- NULL\n\n\n1\n\nRemoves all instances of the back-tick from the names of beta\n\n2\n\nBecause beta is named, the data table apm is created with rownames, which are unnecessary. The top-10 players based on adjusted plus/minus looks quite a bit different than the raw plus/minus!\n\n\n\n\n\napm |&gt;\n  dplyr::arrange(dplyr::desc(apm)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::select(Name, apm)\n\n                    Name      apm\n1          Tobias Harris 17.01331\n2         Mouhamed Gueye 16.79320\n3           Devin Carter 16.60148\n4             Trae Young 15.13287\n5  Giannis Antetokounmpo 14.61082\n6            Isaiah Wong 14.50091\n7           Nikola Jokic 14.40837\n8         Alperen Sengun 13.55544\n9        Quenton Jackson 13.13962\n10    Karl-Anthony Towns 13.13714\n\n\n\n\nWeighted adjusted plus/minus\nAlthough APM takes into account the quality of the teammates and opponents with whom each player plays, it fails to account fully for the context in which players play. Quoting Deshpande and Jensen (2016), as a result, metrics like APM &gt; “can artificially can artificially inflate the importance of performance in low-leverage situations, when the outcome of the game is essentially decided, while simultaneously deflating the importance of high-leverage performance, when the final outcome is still in question. For instance, point diﬀerential-based metrics model the home team’s lead dropping from 5 points to 0 points in the last minute of the first half in exactly the same way that they model the home team’s lead dropping from 30 points to 25 points in the last minute of the second half”.\nTo overcome this limitation, we might try to down-weight low-leverage stints and up-weight high-leverage stints. For instance, we might assign a weight \\(w_{i}\\) to stint \\(i\\) where * \\(w_{i} = 1\\) if, at the start of stint \\(i,\\) the teams are within 10 points of each other * \\(w_{i} = 0\\) if, at the start of the stint \\(i,\\) the difference in scores exceed 30 points * \\(w_{i} = 1 - (\\textrm{StartDiff} - 10)/20\\): if, at the start of stint \\(i,\\) the difference in scores is between 10 and 30 points. This weight smoothly interpolates between 0 (when the difference is 30) and 1 (when the difference is 10).\n\nwapm_df &lt;-\n  rapm_data |&gt;\n  dplyr::mutate(\n1    start_diff = abs(start_home_score - start_away_score),\n    w = dplyr::case_when(\n2      start_diff &lt; 10 ~ 1,\n3      start_diff &gt; 30 ~ 0,\n4      .default = 1 - (start_diff-10)/20)) |&gt;\n  dplyr::select(tidyr::all_of(c(\"margin\", \"w\", nonbaseline_id)))\n\n\n1\n\nCreate a variable recording the lead at the start of the stint\n\n2\n\nSet weight = 1 if the starting lead is less than 10 points\n\n3\n\nSet weight = 0 if starting lead exceeds 30 points\n\n4\n\nAssign a weight between 0 and 1 if the starting lead is between 10 and 30 points\n\n\n\n\nThen the weighted adjusted plus/minus coefficients \\(\\hat{\\boldsymbol{\\beta}}_{w}\\) is the unique minimizer of \\[\n\\sum_{i = 1}^{n}{w_{i}\\left(Y_{i} - \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta}\\right)^{2}}\n\\] and can be computed in closed form as \\[\n\\hat{\\boldsymbol{\\beta}}_{w} = \\left( \\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\tilde{\\boldsymbol{\\mathbf{Z}}}\\right)^{-1}\\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{Y}},\n\\] where \\(\\boldsymbol{\\mathbf{W}}\\) is a \\(n \\times n\\) diagonal matrix with the weights \\(w_{i}\\) along the diagonal.\nAs with regular APM, in practice, we can fit a weighted APM model using lm(). The main difference is that we must (i) include a vector of weights in the data frame that we pass; (ii) specify the weights using the weights argument; and (iii) exclude the weight variable from the linear predictor.\n\nwapm_fit &lt;- \n1  lm(formula = margin ~ . - w,\n2     weights = w,\n     data = wapm_df) \n\nwbeta0 &lt;- coefficients(wapm_fit)[1]\nwbeta &lt;- coefficients(wapm_fit)[-1]\n\nnames(wbeta) &lt;- stringr::str_remove_all(string = names(wbeta), pattern = \"`\")\nwapm &lt;-\n  data.frame(id = names(wbeta), wapm = wbeta) |&gt;\n  dplyr::inner_join(y = player_table, by = \"id\")\nrownames(wapm) &lt;- NULL \n\n\n1\n\nThe -w in the formula argument signals to R that it should not include the variable w as a linear predictor. See this cheatseet for more details about the formula interface\n\n2\n\nTells lm that it should weight observations by whatever is in the column w in wapm_df\n\n\n\n\nBecause we have now weight stints differently, we see that the top-10 players according to our weighted adjusted plus/minus metric is somewhat different than the original adjutsed plus/minus metric.\n\nwapm |&gt;\n  dplyr::arrange(dplyr::desc(wapm)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::select(Name, wapm)\n\n                Name     wapm\n1       Devin Carter 19.48318\n2      Tobias Harris 17.11044\n3    Lauri Markkanen 15.20344\n4     Mouhamed Gueye 14.76474\n5         Trae Young 14.52393\n6       Nikola Jokic 14.14573\n7     Alperen Sengun 14.03023\n8         OG Anunoby 13.96219\n9  Jordan McLaughlin 13.73760\n10      Jericho Sims 13.27850",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-looking-ahead",
    "href": "lectures/lecture04.html#sec-looking-ahead",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nTo fit our adjusted plus/minus model using the method of least squares, we specified a set of baseline-level players and assumed that all the baseline players had the exact same partial effect on their team’s average point differential per 100 possessions. This is a very strong and, frankly, unrealistic assumption! The use of baseline players and the assumed equality of their impact was motivated by a numerical challenge, viz. the inability to solve the least squares minimiziation problem when our design matrix \\(\\boldsymbol{\\mathbf{Z}}\\) had constant row sums. In Lecture 5, we will introduce an alternative approach to fitting adjusted plus/minus models that avoids the need to specify baseline players by solving a regularized version of the least squares problem. We save our player look-up table, the matrix X_full, and the vector of point differentials per 100 possessions so that we can load them next time.\n\nY &lt;- apm_df$margin\nsave(X_full, Y, player_table, file = \"lecture04_05_data.RData\")",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#exercises",
    "href": "lectures/lecture04.html#exercises",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Exercises",
    "text": "Exercises\n\nExplore the sensitivity of APM and our weighted APM to different choices of the minutes cut-off used to define the baseline player. How much do the top- and bottom-10 player rankings change?\nWe weighted stints using only on the lead at the start of the stint. One can reasonably argue that our weights should also account for the time left in the game. Propose your own weighting scheme and see how the player rankings change.\nWe estimated APM using data from a single season. Many analysts prefer to use data from multiple season when fitting APM models. Scrape data from the 2022-23 and 2023-24 regular seasons and fit APM models using (i) data from 2023-24 and 2024-25 and (iii) data from 2022-23, 2023-34, and 2024-25.\nOne can credibly argue that data from seasons further in the past should be weighted less than data from more recent seasons. Propose a weighting scheme that down-weights historical data and fit weighted versions of the models in Exercise 4.3. How do the relative player rankings change?\nEstimate the out-of-sample predictive mean square error of our basic APM model using 100 training/testing splits. That is, for each training and testing split, fit the APM model using the training data and then compute the average of the squared difference between the actual \\(Y\\)’s and the model predictions in the testing split. Is APM a good predictive model? Why or why not?",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#footnotes",
    "href": "lectures/lecture04.html#footnotes",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is the game in which Luka Dončić embarrassed Rudy Gobert hit a step-back 3 pointer over Rudy Gobert to win the game (link).↩︎\nSee this Wikipedia entry for background on on index notation for two-dimensional arrays.↩︎\nSee the Wikipedia entry on matrix-vector multiplication for the formula.↩︎\nAs best I can tell, the name margin refers back to the original article.↩︎\nFire Nico.↩︎\nMultiple linear regression is a focus of STAT 333. If you have taken that class, I encourage you to go back through your notes from it. And if you have not taken that class, I highly recommend reviewing Section 1.6 of the book Beyond Multiple Linear Regression and Sections 3.1 and 3.2 of An Introduction to Statistical Learning.↩︎\nTry proving this for yourself! If you find yourself getting stuck with this, ask your classmates on Piazza, or come to the instructor’s Friday office hours.↩︎",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture06.html",
    "href": "lectures/lecture06.html",
    "title": "Lecture 6: Run Expectancy",
    "section": "",
    "text": "The first game of the 2024 MLB Regular Season, between the Los Angeles Dodgers and the San Diego Padres, took place on March 20, 20241 During the game, Shohei Ohtani recorded two hits in his five plate appearances:\n\nIn the 3rd inning, with 2 outs and no runners on base, Ohtani singled into right field.\nIn the 8th inning, with 1 out and runners on first and second base, Ohtani singled into left center field, driving in one run.\n\nWhich of these singles was more valuable to Dodgers? And how much of that value should be specifically credited to Ohtani?\nBecause the second single directly resulted in a run scoring, it is tempting to conclude that the second single was more valuable than the first. However, although the Dodgers didn’t score as a result of the first single, Ohtani reached first base, putting his team in a position to potential score more runs than if he had not reached first2. In terms of allocating credit, it seems natural to give Ohtani all the credit for whatever value was created on his first single. But on his second single, at least some of the value created is attributable to a baserunner scoring from second base. How should we divide credit between Ohtani and that baserunner?\nOver the next several lectures, we will work with pitch-level tracking data from Major League Baseball to answer these questions. In this lecture, we begin by computing the number of runs that a team can expect to score in the remainder of a half-inning (Section 4). Then, we introduce run values (Section 5), a metric that combines changes in the number of a team expects to score and the number of runs actually scored in an at-bat. We conclude by determining which batters created the most run value for their teams (Section 6).\nIn Lecture 7, we will distribute the run value created in every at-bat between the batter and base runners involved in the at-bat. We will then begin Lecture 8 by distributing the negative of the run value created by th batting team between the pitcher and fielders involved in each at-bat. Finally, we will aggregate the total run value over average created by each player through their hitting, fielding, base running, and pitching and convert that aggregate into a measure of wins above replacement. Our development largely follows (Baumer, Jensen, and Matthews 2015), who created a transparent, open-source version of wins above replacement, a cornerstone of baseball analytics.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-motivation",
    "href": "lectures/lecture06.html#sec-motivation",
    "title": "Lecture 6: Run Expectancy",
    "section": "",
    "text": "The first game of the 2024 MLB Regular Season, between the Los Angeles Dodgers and the San Diego Padres, took place on March 20, 20241 During the game, Shohei Ohtani recorded two hits in his five plate appearances:\n\nIn the 3rd inning, with 2 outs and no runners on base, Ohtani singled into right field.\nIn the 8th inning, with 1 out and runners on first and second base, Ohtani singled into left center field, driving in one run.\n\nWhich of these singles was more valuable to Dodgers? And how much of that value should be specifically credited to Ohtani?\nBecause the second single directly resulted in a run scoring, it is tempting to conclude that the second single was more valuable than the first. However, although the Dodgers didn’t score as a result of the first single, Ohtani reached first base, putting his team in a position to potential score more runs than if he had not reached first2. In terms of allocating credit, it seems natural to give Ohtani all the credit for whatever value was created on his first single. But on his second single, at least some of the value created is attributable to a baserunner scoring from second base. How should we divide credit between Ohtani and that baserunner?\nOver the next several lectures, we will work with pitch-level tracking data from Major League Baseball to answer these questions. In this lecture, we begin by computing the number of runs that a team can expect to score in the remainder of a half-inning (Section 4). Then, we introduce run values (Section 5), a metric that combines changes in the number of a team expects to score and the number of runs actually scored in an at-bat. We conclude by determining which batters created the most run value for their teams (Section 6).\nIn Lecture 7, we will distribute the run value created in every at-bat between the batter and base runners involved in the at-bat. We will then begin Lecture 8 by distributing the negative of the run value created by th batting team between the pitcher and fielders involved in each at-bat. Finally, we will aggregate the total run value over average created by each player through their hitting, fielding, base running, and pitching and convert that aggregate into a measure of wins above replacement. Our development largely follows (Baumer, Jensen, and Matthews 2015), who created a transparent, open-source version of wins above replacement, a cornerstone of baseball analytics.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-statcast",
    "href": "lectures/lecture06.html#sec-statcast",
    "title": "Lecture 6: Run Expectancy",
    "section": "Tracking Data in Baseball",
    "text": "Tracking Data in Baseball\n\nA Brief History\nOn October 4, 2006, during an American League Division Series game between the Oakland Athletics and Minnesota Twins, the sports media company Sportvision3 debuted PitchF/X4, a system of cameras for tracking the position of every pitch as it traveled from the pitcher’s hand to the batter. After installing the system in every MLB ballpark, Sportvision began providing the data in real-time data to the MLB. These data, combined with additional information recorded by an MLB Advanced Media employee, powered the popular GameDay application and were publicly available through the GameDay application programming interface (API) for many years.\nIn 2017, PithcF/X system was phased out replaced by the radar-based Trackman system, which was originally developed to track the trajectories of golf balls. Trackman is part of the larger Statcast system, which additionally tracks the movement of all players on the field. See this New York Times article for more about the history of Statcast.\n\n\nAccessing Statcast Data in R\nMajor League Baseball hosts a public-facing web interface for accessing Statcast data. Using that interface, users can pull up data for individual players or about all pitches of a certain type. Powering this website is an API, which allows software applications to connect to the underlying Statcast database. It is through this API that the baseballr package acquires data. You can install the package with the code:\n\ndevtools::install_github(repo = \"BillPetti/baseballr\")\n\nThe function baseballr::statcast_search() allows users query all Statcast data by date, player, or player type. One of the original baseballr authors, Bill Petti, wrote a wrapper function that uses baseballr::statcast_search() to pull down an entire season’s worth of pitch-by-pitch data; see this blog post for the wrapper function code and this earlier post for details about its design. Since he published his original function, Statcast has added some new fields, necessitating a few changes to the original script. The code below defines a new scraper, which we will use in the course. An R script containing this code is available at this link. At a high level, the scraping function pulls data from Statcast on a week-by-week basis.\n\n\nShow the code for Statcast scraper\n## Code modified from Bill Petti's original annual Statcast scraper:\n## Main change is in the column names of the fielders\n\nannual_statcast_query &lt;- function(season) {\n  \n  data_base_column_types &lt;- \n    readr::read_csv(\"https://app.box.com/shared/static/q326nuker938n2nduy81au67s2pf9a3j.csv\")\n  \n  dates &lt;- \n    seq.Date(as.Date(paste0(season, '-03-01')),\n             as.Date(paste0(season, '-12-01')), \n             by = '4 days')\n  \n  date_grid &lt;- \n    tibble::tibble(start_date = dates, \n                   end_date = dates + 3)\n  \n  safe_savant &lt;- \n    purrr::safely(baseballr::scrape_statcast_savant)\n  \n  payload &lt;- \n    purrr::map(.x = seq_along(date_grid$start_date),\n               ~{message(paste0('\\nScraping week of ', date_grid$start_date[.x], '...\\n'))\n                 payload &lt;- \n                   safe_savant(start_date = date_grid$start_date[.x], \n                               end_date = date_grid$end_date[.x], \n                               type = 'pitcher')\n                 return(payload)\n               })\n  \n  payload_df &lt;- purrr::map(payload, 'result')\n  \n  number_rows &lt;- \n    purrr::map_df(.x = seq_along(payload_df),\n                  ~{number_rows &lt;- \n                    tibble::tibble(week = .x, \n                                   number_rows = length(payload_df[[.x]]$game_date))\n                  }) %&gt;%\n    dplyr::filter(number_rows &gt; 0) %&gt;%\n    dplyr::pull(week)\n  \n  payload_df_reduced &lt;- payload_df[number_rows]\n  \n  payload_df_reduced_formatted &lt;- \n    purrr::map(.x = seq_along(payload_df_reduced), \n               ~{cols_to_transform &lt;- \n                 c(\"pitcher\", \"fielder_2\", \"fielder_3\",\n                   \"fielder_4\", \"fielder_5\", \"fielder_6\", \"fielder_7\",\n                   \"fielder_8\", \"fielder_9\")\n               df &lt;- \n                 purrr::pluck(payload_df_reduced, .x) %&gt;%\n                 dplyr::mutate_at(.vars = cols_to_transform, as.numeric) %&gt;%\n                 dplyr::mutate_at(.vars = cols_to_transform, function(x) {ifelse(is.na(x), 999999999, x)})\n               character_columns &lt;- \n                 data_base_column_types %&gt;%\n                 dplyr::filter(class == \"character\") %&gt;%\n                 dplyr::pull(variable)\n               numeric_columns &lt;- \n                 data_base_column_types %&gt;%\n                 dplyr::filter(class == \"numeric\") %&gt;%\n                 dplyr::pull(variable)\n               integer_columns &lt;- \n                 data_base_column_types %&gt;%\n                 dplyr::filter(class == \"integer\") %&gt;%\n                 dplyr::pull(variable)\n               df &lt;- \n                 df %&gt;%\n                 dplyr::mutate_if(names(df) %in% character_columns, as.character) %&gt;%\n                 dplyr::mutate_if(names(df) %in% numeric_columns, as.numeric) %&gt;%\n                 dplyr::mutate_if(names(df) %in% integer_columns, as.integer)\n               return(df)\n               })\n  \n  combined &lt;- payload_df_reduced_formatted %&gt;%\n    dplyr::bind_rows()\n  \n  return(combined)\n}\n\n\nTo use this function, it is enough to run something like.\n\nraw_statcast2024 &lt;- annual_statcast_query(2024)\n\n\n\n\n\n\n\nWarning\n\n\n\nScraping a single season of Statcast data can take between 30 and 45 minutes. I highly recommend scraping the data for any season only once and saving the resulting data table in an .RData file that can be loaded into future R sessions.\n\nlibrary(tidyverse)\nraw_statcast2024 &lt;- annual_statcast_query(2024)\nsave(raw_statcast2024, file = \"raw_statcast2024.RData\")\n\nThese .RData files take between 75MB and 150MB of space. So, if you want to work with data from all available seasons (2008 to the present), you will need about 2.5GB of storage space on your computer.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-statcast-basics",
    "href": "lectures/lecture06.html#sec-statcast-basics",
    "title": "Lecture 6: Run Expectancy",
    "section": "Statcast Basics",
    "text": "Statcast Basics\nThe function annual_statcast_query actually scrapes data not only from the regular season but also from the pre-season and the play-offs. The column game_type records the type of game in which each pitch was thrown. Looking at the Statcast documentation, we see that regular season pitches have game_type==\"R\".\n\ntable(raw_statcast2024$game_type, useNA = 'always')\n\n\n     D      F      L      R      S      W   &lt;NA&gt; \n  5182   2488   3540 695136  77056   1576      0 \n\n\nFor our analyses, we will work only with data from regular season games. Below, we filter to those pitches with game_type==\"R\" and remove those with non-sensical values like 4 balls or 3 strikes.\n\nstatcast2024 &lt;-\n  raw_statcast2024 |&gt; \n  dplyr::filter(game_type == \"R\") |&gt;\n  dplyr::filter(\n    strikes &gt;= 0 & strikes &lt; 3 & \n      balls &gt;= 0 & balls &lt; 4 & \n      outs_when_up &gt;= 0 & outs_when_up &lt; 3) |&gt;\n  dplyr::arrange(game_pk, at_bat_number, pitch_number)\n\nWe’re now left with 695,135 regular season pitches.\n\nPitch- and At-Bat-level Descriptions\nStatcast records a ton of information about each pitch including the type of pitch (pitch_type), the velocity of the pitch when it was released (vx0, vy0, vz0), the acceleration of the pitch at about the half-way point between the pitcher and batter (ax, ay, and az), and horizontal and vertical coordinates of the pitch as it crosses the front edge of home plate (plate_x) and (plate_y). The column type also records whether the pitch resulted ball (type=B), a strike (type=S), or whether the ball was put in play (type=X). For balls in play, Statcast also records things like the launch speed and angle (launch_speed, launch_angle), the coordinates on the field where the ball landed (hc_x) and (hc_y), and the position of the first fielder to touch the ball (hit_location). In Lecture 8, we will work with hc_x and hc_y to build a model that predicts the probability that each fielder makes an out on balls hit to a specific part of the field. As a bit of a preview, here is a plot of all the hc_x and hc_y values.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x, statcast2024$hc_y, \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, col = rgb(0,0,0, 0.1))\n\n\n\n\n\n\n\n\n\n\nAdding Baserunner Information\nThe columns on_1b, on_2b, and on_3b record who is on first, second, or third base at the beginning of each pitch (NA values indicate that nobody is on base). For convenience, we will add a new column to the data table that records the baserunner configuration using a string of 3 binary digits. If there is a runner on first base, the first digit will be a 1 and if there is not a runner on first base, the first digit will be a 0. Similarly, the second and third digits respectively indicate whether there are runners on second and third base. So, the string \"101\" indicates that there are runners on first and third base but not on second. To create the 3-digit binary string encoding baserunner configuration, notice that 1*(!is.na(on_1b)) will return a 1 if there is someone on first base and 0 otherwise. So by pasting together the results of 1*(!is.na(on_1b)), 1*(!is.na(on_2b)), and 1*(!is.na(on_3b)), we can form the 3-digit binary string described above. In following code, we also rename the column outs_when_up to Outs.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::mutate(\n    BaseRunner = \n      paste0(1*(!is.na(on_1b)),1*(!is.na(on_2b)),1*(!is.na(on_3b)))) |&gt;\n  dplyr::rename(Outs = outs_when_up)\n\n\n\nGetting Player Names\nFor every pitch, the Statcast dataset records the identities of the batter (batter), pitcher (pitcher), and the other fielders (fielders_2, …, fielders_9). However, it does not identify them by name, instead using an ID number, which is assigned by MLB Advanced Media. We can look up the corresponding player names using a database maintained by the Chadwick Register. The function baseballr::chadwick_player_lu() downloads the Chadwick database and stores it as a data table in R. This database contains the names and MLB Advanced Media identifiers for players across all seasons, far more than we need for our purposes. So, in the code below, we first download the Chadwick player database and then extract only those players who appeared in the 2024 regular season. Like with the raw pitch-by-pitch data, I recommend that you download this player identity database once and save the table as an .RData object for future use.\n\n1player2024_id &lt;-\n  unique(\n    c(statcast2024$batter, statcast2024$pitcher,\n      statcast2024$on_1b, statcast2024$on_2b, statcast2024$on_3b,\n      statcast2024$fielder_2, statcast2024$fielder_3,\n      statcast2024$fielder_3, statcast2024$fielder_4,\n      statcast2024$fielder_5, statcast2024$fielder_6,\n      statcast2024$fielder_7, statcast2024$fielder_8,\n      statcast2024$fielder_9))\n\nchadwick_players &lt;- baseballr::chadwick_player_lu()\n2save(chadwick_players, file = \"chadwick_players.RData\")\n\nplayer2024_lookup &lt;-\n  chadwick_players |&gt;\n  dplyr::filter(!is.na(key_mlbam) & key_mlbam %in% player2024_id) |&gt;\n  dplyr::mutate(\n3    FullName = paste(name_first, name_last),\n4    Name = stringi::stri_trans_general(FullName, \"Latin-ASCII\"))\nsave(player2024_lookup, file = \"player2024_lookup.RData\")\n\n\n1\n\nThis vector contains the MLB Advanced Media ID for all players who appeared in 2024 as a batter, pitcher, fielder, or baserunner.\n\n2\n\nSave a local copy of the full Chadwick database.\n\n3\n\nCreates a column containing the player’s first and last name.\n\n4\n\nRemoves any accents or special characters, which we will need in Lecture 7.\n\n\n\n\n\n\nGetting Player Positions\nLater in Lecture 7, we will compare each batter’s hitting performance to the average performance of other batters who play the same position in the field. Unfortunately, the Chadwick database does not record the position of each player. Luckily, position information can be obtained using baseballr::mlb_batting_orders(), which retrieves the batting order for each MLB game. In this functions output, the column id contains the MLB Advanced Media id number for each player (i.e., key_mlbam in the data table player2024_lookup that we created above) and the column abbreviation contains the fielding position. For instance, Ohtani was listed as the Designated Hitter in that March 20, 2024 game between the Dodgers and the Padres.\n\nbaseballr::mlb_batting_orders(game_pk = 745444)\n\n── MLB Game Starting Batting Order data from MLB.com ─── baseballr 1.6.0.9002 ──\n\n\nℹ Data updated: 2025-07-30 20:58:12 CDT\n\n\n# A tibble: 18 × 8\n       id fullName         abbreviation batting_order batting_position_num team \n    &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;                &lt;chr&gt;\n 1 605141 Mookie Betts     SS           1             0                    away \n 2 660271 Shohei Ohtani    DH           2             0                    away \n 3 518692 Freddie Freeman  1B           3             0                    away \n 4 669257 Will Smith       C            4             0                    away \n 5 571970 Max Muncy        3B           5             0                    away \n 6 606192 Teoscar Hernánd… RF           6             0                    away \n 7 681546 James Outman     CF           7             0                    away \n 8 518792 Jason Heyward    RF           8             0                    away \n 9 666158 Gavin Lux        2B           9             0                    away \n10 593428 Xander Bogaerts  2B           1             0                    home \n11 665487 Fernando Tatis … RF           2             0                    home \n12 630105 Jake Cronenworth 1B           3             0                    home \n13 592518 Manny Machado    DH           4             0                    home \n14 673490 Ha-Seong Kim     SS           5             0                    home \n15 595777 Jurickson Profar LF           6             0                    home \n16 669134 Luis Campusano   C            7             0                    home \n17 642180 Tyler Wade       3B           8             0                    home \n18 701538 Jackson Merrill  CF           9             0                    home \n# ℹ 2 more variables: teamName &lt;chr&gt;, teamID &lt;int&gt;\n\n\nBecause baseballr::mlb_batting_orders() can be run using only one game identifier (i.e., game_pk) at a time, we need to loop over all of the unique game_pk values in statcast2024 to get all batting orders. In the code block below, we first create a wrapper function, which we call get_lineup around baseballr::mlb_batting_orders() that only retains the columns id and abbreviation and renames those columns.\n\nget_lineup &lt;- function(game_pk){\n  lineup &lt;- baseballr::mlb_batting_orders(game_pk = game_pk)\n  lineup &lt;-\n    lineup |&gt;\n    dplyr::mutate(game_pk = game_pk) |&gt;\n    dplyr::rename(key_mlbam = id, position = abbreviation) |&gt;\n    dplyr::select(game_pk, key_mlbam, position)\n  return(lineup)\n}\n\nNow, to get the batting orders for all regular season games from 2024, we could try writing a for loop that iterates over all unique game_pk values and writes the table to a list.\n\nall_lineups &lt;- list()\nunik_game_pk &lt;- unique(statcast2024$game_pk)\nfor(i in 1:length(unik_game_pk)){\n  all_lineups[[i]] &lt;- get_lineup(game_pk = unik_game_pk[i])\n}\n\nIf get_lineup() throws an error (e.g., because the batting orders for a particular game are not available), then the whole loop gets terminated. To avoid having to manually remove problematic games and re-start the loop, we will use purrr:possibly() to create a version of get_lineup() that returns NULL when it hits an error. We will also use purrr:map instead of writing an explict for loop.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code takes about an hour to run.\n\n\n\nposs_get_lineup &lt;- purrr::possibly(.f = get_lineup, otherwise = NULL) \nunik_game_pk &lt;- unique(statcast2024$game_pk)\n\n1block_starts &lt;- seq(1, length(unik_game_pk), by = 500)\nblock_ends &lt;- c(block_starts[-1], length(unik_game_pk))\n\nall_lineups &lt;- list()\nfor(b in 1:5){\n  tmp &lt;-\n    purrr::map(.x = unik_game_pk[block_starts[b]:block_ends[b]], \n               .f = poss_get_lineup, \n               .progress = TRUE)\n  all_lineups &lt;- c(all_lineups, tmp)\n}\n\nlineups2024 &lt;- \n2  dplyr::bind_rows(all_lineups) |&gt;\n  unique()\n3save(lineups2024, file = \"lineups2024.RData\")\n\n\n1\n\nI was not able to loop over the whole set of unique game_pk values at once. I found it useful to break the loop into blocks of about 500 games each.\n\n2\n\nEach element of all_lineup is a data table. This allows us to stack them on top of one another.\n\n3\n\nIt’s useful to save the table of batting orders just in case we need it again in the future\n\n\n\n\nThe table lineups2024 contains the starters and their position for all the games in our dataset. The following code determines the most commonly listed position for each player.\n\npositions2024 &lt;-\n  lineups2024 |&gt;\n  dplyr::group_by(key_mlbam, position) |&gt;\n1  dplyr::summarise(n = dplyr::n()) |&gt;\n2  dplyr::slice_max(order_by = n, with_ties = FALSE) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::select(key_mlbam, position)\nsave(positions2024, file = \"positions2024.RData\")\n\n\n1\n\nCounts the number of occurrences of each player-position combination\n\n2\n\nAfter we call dplyr::summarise(), the resulting data table is still grouped by batter. So, when we call dplyr::slice_max() it looks at all the rows corresponding to each player and extracts the one with highest count. This is how we determine the most commonly listed position.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-expected-runs",
    "href": "lectures/lecture06.html#sec-expected-runs",
    "title": "Lecture 6: Run Expectancy",
    "section": "Expected Runs",
    "text": "Expected Runs\n\n\n\n\n\n\nDefinition: Expected Runs\n\n\n\nFor each combination of the number of outs (\\(\\textrm{o} \\in \\{0,1,2\\}\\)) and base runner configurations (\\(\\textrm{br} \\in \\{\"000\", \"100\", \"010\", \"001\", \"110\", \"101\", \"011\", \"111\"\\}\\)), let \\(\\rho(\\textrm{o}, \\textrm{br})\\) be the average number of runs that a team scores in the remainder of the half-inning following a pitch thrown with \\(\\textrm{o}\\) outs and base runner configuration \\(\\textrm{br}.\\)\n\n\nComputing \\(\\rho(\\textrm{o}, \\textrm{br})\\) is conceptually straightforward: we need to divide our observed at-bats into 24 bins, one for each combination of \\((\\textrm{o}, \\textrm{br})\\) and then compute the average value of \\(R\\) within each bin. This is exactly the same “binning-and-averaging” procedure we used to fit our initial XG models in Lecture 2. We will do this using pitches taken in the first 8 innings of played in the 2024 regular season. We focus only on the first 8 innings because the 9th and extra innings are fundamentally different than the others. Specifically, the bottom half of the 9th (or later) innings is only played if the game is tied or the home team is trailing after the top of the 9th inning concludes. In those half-innings, if played, the game stops as soon as a winning run is scored. For instance, say that the home team is trailing by 1 runs in the bottom of the 9th and that there are runners on first and second base. If the batter hits a home run, the at-bat is recorded as resulting in only two runs (the tying run from second and the winning run from first). But the exact same scenario would result in 3 runs in an earlier inning.\n\nComputing Runs Scored in the Half-Inning\nSuppose that in a given at-bat \\(a\\) that there are \\(n_{a}\\) pitches. Within at-bat \\(a,\\) for each \\(i = 1, \\ldots, n_{a},\\) let \\(R_{i,a}\\) be the number of runs scored in the half-inning after that pitch (including any runs scored as a result of pitch \\(i\\)). So \\(R_{1,a}\\) is the number of runs scored in the half-inning after the first pitch, \\(R_{2,a}\\) is the number of runs scored subsequent to the second pitch, etc. Our first step towards building the necessary at-bat-level data set will be to append a column of \\(R_{i,a}\\) values to each season’s Statcast data.\nWe start by illustrating the computation using a single half-inning from the Dodgers-Padres game introduced earlier. The code below pulls out all pitches thrown in the top of the 8th inning of the game. During this inning, the Dodgers scored 4 runs.\n\ndodgers_inning &lt;-\n  statcast2024 |&gt;\n  dplyr::filter(game_pk == 745444 & inning == 8 & inning_topbot == \"Top\") |&gt;\n  dplyr::select(\n    at_bat_number, pitch_number, Outs, BaseRunner,\n    bat_score, post_bat_score, events, description, des,\n    type, on_1b, on_2b, on_3b, hc_x, hc_y, hit_location) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number)\n\nThe column bat_score records the batting team’s score before each pitch is thrown. The column post_bat_score records the batting team’s score after the the outcome of the pitch. For most of the 25 pitches, we find that bat_score is equal to post_bat_score; this is because only a few pitches result in scoring events.\n\nrbind(bat_score = dodgers_inning$bat_score, post_bat_score = dodgers_inning$post_bat_score)\n\n               [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nbat_score         1    1    1    1    1    1    1    1    1     1     1     1\npost_bat_score    1    1    1    1    1    1    1    1    1     1     1     1\n               [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nbat_score          1     1     2     3     3     3     4     5     5     5\npost_bat_score     1     2     3     3     3     4     5     5     5     5\n               [,23] [,24] [,25]\nbat_score          5     5     5\npost_bat_score     5     5     5\n\n\nCross-referencing the table above with the play-by-play data, we see that the Dodgers score their second run after the 14th pitch of the half-inning (on a Enrique Hernández sacrifice fly); their third run on the very next pitch (Gavin Lux grounding into a fielder’s choice); and their fourth and fifth runs on consecutive pitches (on singles by Mookie Betts and Shohei Ohtani).\n\n\n\n\n\n\nFigure 1: Play-by-Play from the March 20, 2024 Dodgers-Padres game\n\n\n\nWe can verify this by looking at the variable des, which stores a narrative description about what happened during the at-bat.\n\ndodgers_inning$des[c(14,15, 18, 19)]\n\n[1] \"Enrique Hernández out on a sacrifice fly to left fielder José Azocar. Max Muncy scores.\"                                                                                             \n[2] \"Gavin Lux reaches on a fielder's choice, fielded by first baseman Jake Cronenworth. Teoscar Hernández scores. James Outman to 2nd. Fielding error by first baseman Jake Cronenworth.\"\n[3] \"Mookie Betts singles on a ground ball to left fielder José Azocar. James Outman scores. Gavin Lux to 2nd.\"                                                                           \n[4] \"Shohei Ohtani singles on a line drive to left fielder José Azocar. Gavin Lux scores. Mookie Betts to 2nd.\"                                                                           \n\n\nNotice that, because we’ve sorted the pitches in ascending order by at-bat and pitch number, the very last row of the table corresponds to the last pitch of the inning. Accordingly, the last value in the post_bat_score column is the batting team’s score at the end of the inning. Thus, to compute \\(R_{i,a}\\) for all pitches in this inning, it is enough to subtract the bat_score in each row from the last post_bat_score in the table. To access this last value, we use the function last().\n\ndplyr::last(dodgers_inning$post_bat_score) - dodgers_inning$bat_score\n\n [1] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 2 2 2 1 0 0 0 0 0 0\n\n\nWe now append a column with these values to our data table dodgers_inning\n\ndodgers_inning &lt;-\n  dodgers_inning |&gt;\n  dplyr::mutate(RunsRemaining = dplyr::last(post_bat_score) - bat_score)\n\n\n\nPutting It All Together\nWe’re now ready to extend these calculation to every half-inning of every game in the season. To do this, we will take advantage of the dplyr::group_by() function to apply the same calculation to subsets defined by game and half-inning.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n2  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n3  dplyr::mutate(RunsRemaining = dplyr::last(post_bat_score) - bat_score) |&gt;\n  dplyr::ungroup()\n\n\n1\n\nDivide the data based on the combination of game and half-inning\n\n2\n\nArrange pitches in the appropriate temporal order\n\n3\n\nAdd column for how many runs were scored after each pitch\n\n\n\n\nNow we have the number of runs scored in the half-inning after each pitch. But to compute run expectancy, we need this quantity at the at-bat level and not at the pitch-level. Using our notation from before, note that \\(R_{1,a}\\) is the number of runs scored after the first pitch of at-bat \\(a.\\) So, to compute run expectancy, it is enough to pull out the first pitch from each at-bat (i.e., those pitches with pitch_number == 1) using the filter() function.\n\nexpected_runs &lt;-\n  statcast2024 |&gt;\n1  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(Outs, BaseRunner, RunsRemaining) |&gt;\n2  dplyr::group_by(Outs, BaseRunner) |&gt;\n3  dplyr::summarize(rho = mean(RunsRemaining), .groups = \"drop\")\n\n\n1\n\nGet the first pitch in every at-bat\n\n2\n\nSub-divide the data based on the 24 combinations of out and baserunner\n\n3\n\nCompute the mean of RunRemaining for each game state combination\n\n\n\n\nThe table expected_runs contains one row for every combination of outs and base-runner configuration. Traditionally, expected runs is reported using an \\(8\\times 3\\) matrix, with rows corresponding to base-runner configurations and columns corresponding to outs. We can re-format expected_runs to this matrix format using the tidyr::pivot_wider() function\n\nexpected_runs |&gt; \n  tidyr::pivot_wider(\n    names_from = Outs,\n    values_from = rho,\n    names_prefix=\"Outs: \")\n\n# A tibble: 8 × 4\n  BaseRunner `Outs: 0` `Outs: 1` `Outs: 2`\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 000            0.488     0.262    0.0980\n2 001            1.43      0.972    0.352 \n3 010            1.07      0.672    0.347 \n4 011            2.03      1.44     0.612 \n5 100            0.897     0.529    0.228 \n6 101            1.90      1.22     0.502 \n7 110            1.49      0.926    0.449 \n8 111            2.31      1.58     0.815 \n\n\nIn the March 20, 2024 game against the Padres, Ohtani recorded his first hit in the 3rd inning on a pitch with 2 outs and no runners on base. Based on the game state at the start of the at-bat (i.e., Outs=2 and BaseRunner='000'), his team can expect to score 0.1 runs in the remainder of the half-inning. As a result of Ohtani’s single, he changed the game state to Outs = 2 and BaseRunner=100, a state from which his team can expect to score 0.22 runs, on average. So, although his at-bat did not directly result in a run scoring, Ohtani increased his team’s run expectancy by about 0.12 runs.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-run-value",
    "href": "lectures/lecture06.html#sec-run-value",
    "title": "Lecture 6: Run Expectancy",
    "section": "Run Value",
    "text": "Run Value\nWhile expected runs is a really useful metric, for the purposes of allocating credit, we need to account not only for what we expect to happen but also what actually happened as a result of each at-bat.\n\n\n\n\n\n\nDefinition: Run Value\n\n\n\nThe run value of an at-bat is defined as the the number of runs scored in the at-bat plus the difference in expected runs from the starting to ending state. That is, denoting the number of runs scored in the at-bat as \\(\\textrm{RunsScored}\\) and the starting and ending states as \\((\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\) and \\((\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}),\\) then \\[\n\\textrm{RunValue} = \\textrm{RunsScored} + \\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}) - \\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\n\\]\n\n\nIn a sense, run value rewards batters and base runners for two things, actually scoring runs and putting their team in positions from which they could potentially score more runs.\nTocompute the run value of each at-bat in the 2024 season, we must compute\n\nThe number of runs scored during each at-bat\nThe game state (i.e., the number of outs and the base-runner configuration) at the start and end of each at-bat\nThe change in expected runs during the at-bat (i.e., \\(\\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}) - \\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\)).\n\nWe will first develop the necessary code using the data from Dodger’s 8th inning from their game against the Padres. Then, we will deploy that code to the whole statcast2024 table by grouping by game_pk and at_bat_number.\n\nCalculating Runs Score\nStatcast numbers every at-bat within the game and every pitch within each at-bat. To compute the number of runs scored within each at-bat, we will:\n\nSort the pitches by at-bat number and then by pitch number in ascending order\nTake the different between the last value of post_bat_score and first value of bat_score within each at-bat.\n\nLet’s try to verify this by looking at pitches from the third, fourth, and fifth at-bats of Dodgers’ 8th inning against the Padres5\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 61:63) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, bat_score, description, post_bat_score)\n\n# A tibble: 7 × 5\n  at_bat_number pitch_number bat_score description   post_bat_score\n          &lt;int&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n1            61            1         1 ball                       1\n2            61            2         1 ball                       1\n3            61            3         1 ball                       1\n4            61            4         1 ball                       1\n5            62            1         1 foul                       1\n6            62            2         1 hit_into_play              2\n7            63            1         2 hit_into_play              3\n\n\nBased on the description column, we see that the first pitch of at-bat 62 was a foul ball and the second pitch was hit into play. When we look at the corresponding row (row 6) of the table, we see that that Dodgers’ pre-pitch score was 1 (bat_score = 1) and that they scored 1 run as a result of the hit (post_bat_score = 2). Reassuringly, the difference between the value of post_bat_score in row 6 (the last row for at-bat 62) and the value of bat_score in row 5 (the first row for at-bat 62) is 1. We can similarly verify our procedure works in at-bat 61: the fourth value of post_bat_score and the first value of bat_score are equal and the Dodgers did not score in this at-bat.\nWe can apply our procedure to the entirety of the Dodgers’ half-inning\n\ndodgers_inning &lt;-\n  dodgers_inning |&gt;\n  dplyr::group_by(at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(RunsScored = dplyr::last(post_bat_score) - dplyr::first(bat_score)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::arrange(at_bat_number, pitch_number)\ndodgers_inning |&gt; dplyr::select(at_bat_number, pitch_number, bat_score, description, post_bat_score)\n\n# A tibble: 25 × 5\n   at_bat_number pitch_number bat_score description   post_bat_score\n           &lt;int&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1            59            1         1 called_strike              1\n 2            59            2         1 ball                       1\n 3            59            3         1 ball                       1\n 4            59            4         1 foul                       1\n 5            59            6         1 blocked_ball               1\n 6            60            1         1 ball                       1\n 7            60            2         1 foul_tip                   1\n 8            60            3         1 hit_into_play              1\n 9            61            1         1 ball                       1\n10            61            2         1 ball                       1\n# ℹ 15 more rows\n\n\nWe can now apply this formula to all pitches in statcast2024 by grouping by game_pk and at_bat_number. We will also save a copy of the data table statcast2024 so that we can load it into future R sessions.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, at_bat_number) |&gt;\n2  dplyr::arrange(pitch_number) |&gt;\n3  dplyr::mutate(RunsScored = dplyr::last(post_bat_score) - dplyr::first(bat_score)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number)\n\nsave(statcast2024, file = \"statcast2024.RData\")\n\n\n1\n\nSub-divide the data based on individual at-bats\n\n2\n\nPlace pitches within each at-bat in sequential order\n\n3\n\nAdd column recording the number of runs scored in the at-bat\n\n\n\n\n\n\nComputing Starting & Ending States\nExcept for the very last pitch in a team’s innings, the ending state of that pitch is, by definition, the starting state of the next pitch. In order to compute \\(\\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}),\\) and \\(\\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\) for each at-bat, we will first create a columns in statcast2024 that encode the game state at the beginning and end of the at-bat.\nTo build up our code, let’s continue with our running example of the Dodgers’ 8th inning, focusing on the at the second through fourth at-bats of the inning.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner)\n\n# A tibble: 9 × 4\n  at_bat_number pitch_number  Outs BaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;     \n1            60            1     0 100       \n2            60            2     0 100       \n3            60            3     0 100       \n4            61            1     0 110       \n5            61            2     0 110       \n6            61            3     0 110       \n7            61            4     0 110       \n8            62            1     0 111       \n9            62            2     0 111       \n\n\nWe start by creating new columns recording the Outs and BaseRunner values of the next pitch using dplyr::lead() function. 6.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner) |&gt;\n  dplyr::mutate(\n    next_Outs = dplyr::lead(Outs), next_BaseRunner = dplyr::lead(BaseRunner))\n\n# A tibble: 9 × 6\n  at_bat_number pitch_number  Outs BaseRunner next_Outs next_BaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;          \n1            60            1     0 100                0 100            \n2            60            2     0 100                0 100            \n3            60            3     0 100                0 110            \n4            61            1     0 110                0 110            \n5            61            2     0 110                0 110            \n6            61            3     0 110                0 110            \n7            61            4     0 110                0 111            \n8            62            1     0 111                0 111            \n9            62            2     0 111               NA &lt;NA&gt;           \n\n\nNow, within each at-bat, we can look at the last values of next_Outs and next_BaseRunner to figure out the ending state of the at-bat.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner) |&gt;\n  dplyr::mutate(\n    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner)) |&gt;\n  dplyr::group_by(at_bat_number) |&gt;\n  dplyr::mutate(\n    endOuts = dplyr::last(next_Outs),\n    endBaseRunner = dplyr::last(next_BaseRunner)) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner, endOuts, endBaseRunner) |&gt;\n  dplyr::ungroup()\n\n# A tibble: 9 × 6\n  at_bat_number pitch_number  Outs BaseRunner endOuts endBaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        \n1            60            1     0 100              0 110          \n2            60            2     0 100              0 110          \n3            60            3     0 100              0 110          \n4            61            1     0 110              0 111          \n5            61            2     0 110              0 111          \n6            61            3     0 110              0 111          \n7            61            4     0 110              0 111          \n8            62            1     0 111             NA &lt;NA&gt;         \n9            62            2     0 111             NA &lt;NA&gt;         \n\n\nWe can repeat this code for all at-bats.\n\nrunValue2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n2    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n3    end_Outs = dplyr::last(next_Outs),\n    end_BaseRunner = dplyr::last(next_BaseRunner)) |&gt; \n  dplyr::ungroup() |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n4  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(\n    game_pk, at_bat_number, \n    inning, inning_topbot, \n    Outs, BaseRunner, \n    RunsScored, RunsRemaining, \n    end_Outs, end_BaseRunner)\n\n\n1\n\nSub-divide into half-innings\n\n2\n\nGet values of Outs and BaseRunner for next pitch in the inning\n\n3\n\nGoes to last pitch of each at-bat and gets the next value of Outs and BaseRunner, which are the starting values of these variables in the next at-bat.\n\n4\n\nGets the first pitch from each at-bat\n\n\n\n\n\n\nComputing Run Values\nNow that we have a table runValue containing information about the starting and ending states of each at-bat, we are ready to compute run-values. In particular, we can use a join to add in the values of the starting and ending expected runs.\nBefore doing that, though, we need to deal with the NA’s introduced by lead(). Looking at the at-bats from the Dodger’s 8th inning from our running example, we see that those NA’s correspond to the very last at-bat of the half-inning.\n\nrunValue2024 |&gt;\n  dplyr::filter(game_pk == 745444 & inning == 8 & inning_topbot == \"Top\") |&gt;\n  dplyr::select(at_bat_number, Outs, BaseRunner, end_Outs, end_BaseRunner)\n\n# A tibble: 8 × 5\n  at_bat_number  Outs BaseRunner end_Outs end_BaseRunner\n          &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;         \n1            59     0 000               0 100           \n2            60     0 100               0 110           \n3            61     0 110               0 111           \n4            62     0 111               1 110           \n5            63     1 110               1 110           \n6            64     1 110               1 110           \n7            65     1 110               1 110           \n8            66     1 110              NA &lt;NA&gt;          \n\n\nBecause the “end of the inning” state is not one of the 24 combinations of outs and baserunner configurations in the expected_runs table, we’re going to row to that table with Outs=3, BaseRunners='000', and rho = 0 (since the team cannot score any more runs in the inning once it is over!).\n\nexpected_runs &lt;-\n  expected_runs |&gt;\n  tibble::add_row(Outs=3, BaseRunner=\"000\", rho = 0)\n\nrunValue2024 &lt;-\n  runValue2024 |&gt;\n  dplyr::mutate(\n    end_Outs = ifelse(is.na(end_Outs), 3, end_Outs),\n    end_BaseRunner = ifelse(is.na(end_BaseRunner), '000', end_BaseRunner))\n\nWe’re now ready to use a join to append the starting and ending expected runs.\n\nend_expected_runs &lt;- \n  expected_runs |&gt;\n  dplyr::rename(\n    end_Outs = Outs,\n    end_BaseRunner = BaseRunner,\n    end_rho = rho)\n\nrunValue2024 &lt;-\n  runValue2024 |&gt;\n  dplyr::left_join(y = expected_runs, by = c(\"Outs\", \"BaseRunner\")) |&gt;\n  dplyr::left_join(y = end_expected_runs, by = c(\"end_Outs\", \"end_BaseRunner\")) |&gt;\n  dplyr::mutate(RunValue = RunsScored + end_rho - rho) |&gt;\n  dplyr::select(game_pk, at_bat_number, RunValue)\n\nrm(end_expected_runs)\nsave(runValue2024, file = \"runValue2024.RData\")",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-batter-prod",
    "href": "lectures/lecture06.html#sec-batter-prod",
    "title": "Lecture 6: Run Expectancy",
    "section": "Assessing Batter Production",
    "text": "Assessing Batter Production\nEach row in runValue2024 corresponds to a single at-bat in the 2024 regular season and is uniquely determined by the game identifier (game_pk) and at-bat number (at_bat_number). We’re now in a position to quantify how run value Ohtani created during the Dodgers-Padres game introduced above?\nTo do this, we first look up Ohtani’s MLB Advanced Media ID number using our table player2024_lookup. Then, we can extract the rows of statcast2024 corresponding to the first pitch in each at-bat from the Dodgers-Padres game statcast2024. Then we can filter this subset to only those at-bats where Ohtani was the batter. Finally, we can join the associated run values.\n\nload(\"player2024_lookup.RData\")\n\n\nohtani_id &lt;- \n  player2024_lookup |&gt;\n  dplyr::filter(FullName == \"Shohei Ohtani\") |&gt;\n  dplyr::pull(key_mlbam)\n\nohtani_ab &lt;-\n  statcast2024 |&gt;\n  dplyr::filter(game_pk == 745444) |&gt;\n  dplyr::filter(pitch_number == 1 & batter == ohtani_id) |&gt;\n  dplyr::select(game_pk, at_bat_number, inning, des) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\")) |&gt;\n  dplyr::select(at_bat_number, RunValue, des)\nohtani_ab\n\n# A tibble: 5 × 3\n  at_bat_number RunValue des                                                    \n          &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                                                  \n1             2   -0.367 Shohei Ohtani grounds into a force out, shortstop Ha-S…\n2            18    0.130 Shohei Ohtani singles on a sharp line drive to right f…\n3            37   -0.367 Shohei Ohtani grounds into a force out, third baseman …\n4            52   -0.164 Shohei Ohtani grounds out softly, pitcher Wandy Peralt…\n5            65    1     Shohei Ohtani singles on a line drive to left fielder …\n\n\nOver the course of the entire game, Ohtani created a total of 0.2310438 in run value. The negative run value created in his first, third, and fourth at-bats (i.e., when he got out) is offset by the positive run value he created by singling in the top of the 3rd inning and driving in a run in the top of the 8th inning. Note the run value of his last at-at is exactly 1 because a single run scored but the base-runner configuration did not change as a result of the at-bat.\nBy repeating this calculation over the course of the entire 2024 regular season, we can identify those batters who created the most run value for their teams.\n\ntmp_lookup &lt;-\n  player2024_lookup |&gt;\n  dplyr::select(key_mlbam, Name) |&gt;\n  dplyr::rename(batter = key_mlbam)\n\nre24 &lt;-\n  statcast2024 |&gt; \n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(game_pk, at_bat_number, batter) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\")) |&gt;\n  dplyr::group_by(batter) |&gt;\n  dplyr::summarise(RE24 = sum(RunValue),N = dplyr::n()) |&gt;\n  dplyr::inner_join(y = tmp_lookup, by = \"batter\") |&gt;\n  dplyr::select(Name, RE24, N) |&gt;\n  dplyr::arrange(desc(RE24))\nre24\n\n# A tibble: 649 × 3\n   Name               RE24     N\n   &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n 1 Aaron Judge        89.0   675\n 2 Juan Soto          74.4   693\n 3 Shohei Ohtani      73.1   708\n 4 Bobby Witt         65.9   694\n 5 Brent Rooker       47.9   599\n 6 Vladimir Guerrero  45.0   671\n 7 Ketel Marte        41.2   562\n 8 Kyle Schwarber     40.9   672\n 9 Joc Pederson       39.2   433\n10 Jose Ramirez       39.1   657\n# ℹ 639 more rows\n\n\nWhen we compare our top-10 to FanGraph’s leaderboard for RE24, we see a lot of overlap. But there are some differences, especially with regards to the number of plate appearances and actual RE24 values. For the latter, FanGraph likely used a different expected run matrix. And the Statcast data is not complete; for instance, it is missing 3 games in which Judge played.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#exercises",
    "href": "lectures/lecture06.html#exercises",
    "title": "Lecture 6: Run Expectancy",
    "section": "Exercises",
    "text": "Exercises\n\nScrape data from the 2022 and 2023 regular seasons and determine which batters created the most run value.\nCompute a new version of expected runs that conditions on the number of outs, baserunner configuration, and ballpark. Using this new version of expected runs, determine which batters create the most run value for their team.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#footnotes",
    "href": "lectures/lecture06.html#footnotes",
    "title": "Lecture 6: Run Expectancy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis was also the first MLB game ever played in South Korea!↩︎\nIn fact, Ohtani went on to steal second base during the next at-bat, further increasing his team’s chances of scoring.↩︎\nSportvision is perhaps most famous for the yellow first down line that appears in American football broadcasts. They were eventually acquired by SMT.↩︎\nSee this article by Mike Fast for more background↩︎\nStatcast assigns each at-bat in a game a unique number. The third, fourth, and fifth at-bats during the Dodger’s 8th inning were the 61st, 62nd, and 63rd at-bats of the game.↩︎\nThe next value of a variable is undefined in the last row of a column, resulting in some NA’s. We’ll deal with those later on.↩︎",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture08.html",
    "href": "lectures/lecture08.html",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "",
    "text": "In Lecture 7, we distributed the run value created in each at-bat between the batter and base runners. Aggregating over all at-bats, we computed each player’s \\(\\textrm{RAA}^{\\textrm{br}},\\) which quantifies how much more run value a player created through his base running than would otherwise be expected based on the starting game states and ending events of the at-bats in which he was involved. We also computed \\(\\textrm{RAA}^{\\textrm{b}},\\) which quantifies how much more run value a player created through his hitting than would be expected based on his position.\nAccording to the conservation of runs framework introduced by Baumer, Jensen, and Matthews (2015), whenever the batting team creates \\(\\delta\\) units of run value in at-bat, the fielding team (necessarily) gives up \\(\\delta\\) units of run value. Equivalently, the fielding team creates \\(-\\delta\\) units of run value. In this lecture, we discuss how to divide this \\(-\\delta\\) run value between the pitchers (Section 6) and fielders (Section 5).\nFor at-bats \\(i\\) that end without the ball being put in play (i.e., a strikeout, walk, or a homerun), we will assign the entirety of the \\(-\\delta_{i}\\) run value to the pitcher. But for at-bats that result with balls put in play, we will divide \\(-\\delta_{i}\\) into two parts \\(\\delta^{(p)}_{i} = -\\delta \\times \\hat{p}_{i}\\) and \\(\\delta^{(f)} = -\\delta_{i} \\times (1 - \\hat{p}_{i})\\) where \\(\\hat{p}_{i}\\) is an estimate of the probability that play results in an out given the batted ball’s location (Section 3). We will specifically estimate these out probabilities using Generalized Additive Models (Section 4). Finally, we compute how more run value each player creates through their batting, baserunning, pitching, and fielding than a replacement level player (Section 7).",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-overview",
    "href": "lectures/lecture08.html#sec-overview",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "",
    "text": "In Lecture 7, we distributed the run value created in each at-bat between the batter and base runners. Aggregating over all at-bats, we computed each player’s \\(\\textrm{RAA}^{\\textrm{br}},\\) which quantifies how much more run value a player created through his base running than would otherwise be expected based on the starting game states and ending events of the at-bats in which he was involved. We also computed \\(\\textrm{RAA}^{\\textrm{b}},\\) which quantifies how much more run value a player created through his hitting than would be expected based on his position.\nAccording to the conservation of runs framework introduced by Baumer, Jensen, and Matthews (2015), whenever the batting team creates \\(\\delta\\) units of run value in at-bat, the fielding team (necessarily) gives up \\(\\delta\\) units of run value. Equivalently, the fielding team creates \\(-\\delta\\) units of run value. In this lecture, we discuss how to divide this \\(-\\delta\\) run value between the pitchers (Section 6) and fielders (Section 5).\nFor at-bats \\(i\\) that end without the ball being put in play (i.e., a strikeout, walk, or a homerun), we will assign the entirety of the \\(-\\delta_{i}\\) run value to the pitcher. But for at-bats that result with balls put in play, we will divide \\(-\\delta_{i}\\) into two parts \\(\\delta^{(p)}_{i} = -\\delta \\times \\hat{p}_{i}\\) and \\(\\delta^{(f)} = -\\delta_{i} \\times (1 - \\hat{p}_{i})\\) where \\(\\hat{p}_{i}\\) is an estimate of the probability that play results in an out given the batted ball’s location (Section 3). We will specifically estimate these out probabilities using Generalized Additive Models (Section 4). Finally, we compute how more run value each player creates through their batting, baserunning, pitching, and fielding than a replacement level player (Section 7).",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-hit-location-data",
    "href": "lectures/lecture08.html#sec-hit-location-data",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Hit Location Data",
    "text": "Hit Location Data\nWe begin by loading several of the data tables created in Lecture 6 and Lecture 7 including statcast2024, which contains pitch-level data for all regular season pitches in 2024, runValue2024, which contains the \\(\\delta_{i}\\)’s for each regular season at-bat, and player2024_lookup, which contains the names and MLB Advanced Media identifiers for each player.\n\nload(\"statcast2024.RData\")\nload(\"runValue2024.RData\")\nload(\"player2024_lookup.RData\")\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n\nRecall from Lecture 6 that the StatCast variables hc_x and hc_y record the coordinates where each batted ball is first fielded. When we plotted the available hc_x and hc_y values, we saw the outlines of the park, with home plate located around hc_x = 125 and hc_y = 200. We can also roughly make out the first and third base lines and the infield.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x, statcast2024$hc_y, \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\n\n\n\n\n\n\n\nFigure 1: Locations where all batted balls are first fielded\n\n\n\n\n\nIt is not immediately apparent, however, whether the first base line is on the left or right. Luckily, StatCast also contains the official fielding position of the player who first fields the ball1. When we plot just the pitches first fielded by first basemen (hit_location=3), we see that in the original (hc_x, hc_y) coordinate system, first base is on the right hand side of the plot.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x[statcast2024$hit_location==3], \n     statcast2024$hc_y[statcast2024$hit_location==3], \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[2], alpha.f = 0.1))\n\n\n\n\n\n\n\nFigure 2: Location of all batted balls initially fielded by the first baseman\n\n\n\n\n\nAlthough MLB Stadiums are equipped with lots of very cool camera and ball tracking technology, the variables hc_x and hc_y are not derived from those technologies. Instead a stringer manually marks the location of the batted ball on a tablet so the coordinates hc_x and hc_y are expressed in units of pixels on that tablet. Jim Albert, who is one of the founding fathers of Statistical research in baseball, suggested the following transformation from the original (hc_x, hc_y) coordinate system to one that places home plate at the bottom of the plot (with coordinates (0,0)) and measures distances in feet. \\[\n\\begin{align}\nx &= 2.5 \\times (\\texttt{hc_x} - 125.42) & y &= 2.5 \\times (198.27 - \\texttt{hc_y})\n\\end{align}\n\\]\nThe following code implements this transformation, storing the new coordinates as x and y and then plots the batted ball locations. It also overlays the first and third base lines, the base paths and places points at the base locations.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::mutate(\n    x = 2.5 * (hc_x - 125.42),\n    y = 2.5 * (198.27 - hc_y))\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$x, \n     statcast2024$y, \n     xlab = \"x\", ylab = \"y\",\n     pch = 16, cex = 0.2, \n     xlim = c(-300, 300),\n     ylim = c(-100, 500),\n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\n\n1abline(a = 0, b = 1, col = oi_colors[5], lwd = 1)\nabline(a = 0, b = -1, col = oi_colors[5], lwd = 1)\n\n2lines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 2)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nThe function abline(a,b) plots the line \\(y = ax + b\\). In our new coordinate system the first and third base lines correspond to the lines \\(y = x\\) and \\(y = -x\\).\n\n2\n\nIn our new coordinate system, home plate is at (0,0). First base and third base are 90 feet away from home plate along the 45 degree lines, meaning that their coordinates are (90/sqrt(2), 90/sqrt(2)) and (-90/sqrt(2), 90/sqrt(2))`.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Locations where batted balls are first fielded in our transformed coordinate system\n\n\n\n\n\nIn Lecture 7, we created a table atbat2024 that contained the beginning and ending states of each at-bat, run value, ending event, and identities of the batter and baserunners. Today, we will build a similar data frame that contains the ending event, identities of the pitcher and fielders, and, if the at-bat resulted in a ball being put into play, the batted ball locations. Like the code we used to build atbat2024, we need to identify the last entry in the column events in each at bat as well as the last entry of the columns x, y, and hit_location. We will also keep the run value.\n\ndef_atbat2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n    end_events = dplyr::last(events),\n    end_x = dplyr::last(x),\n    end_y = dplyr::last(y),\n    end_type = dplyr::last(type),\n    end_hit_location = dplyr::last(hit_location)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::select(\n    game_date, game_pk, at_bat_number,\n    end_events, end_type, des,\n    batter,\n    pitcher, fielder_2, fielder_3,\n    fielder_4, fielder_5, fielder_6,\n    fielder_7, fielder_8, fielder_9,\n    end_x, end_y, end_hit_location) |&gt;\n  dplyr::rename(x = end_x, y = end_y) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\"))",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-out-probs",
    "href": "lectures/lecture08.html#sec-out-probs",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Estimating Out Probabilities",
    "text": "Estimating Out Probabilities\nRecall that we need to estimate the probability that a batted ball results in an out given its coordinate. To do so, we first need to extract those at-bats that resulted in a ball being put in play. The variable end_type in def_atbat2024 records whether the last pitch of each at-bat resulted in a strike (end_type = S), a ball (end_type = B), or a ball being hit into play (end_type = X). As a sanity check, let’s take a look at the ending event for all at-bats with end_type = X. Reassuringly, none of them can occur without a ball being hit.\n\ntable(def_atbat2024$end_events[def_atbat2024$end_type == \"X\"], useNA = 'always')\n\n\n                   double               double_play               field_error \n                     7608                       336                      1093 \n                field_out           fielders_choice       fielders_choice_out \n                    72233                       373                       306 \n                force_out grounded_into_double_play                  home_run \n                     3408                      3152                      5326 \n                 sac_bunt                   sac_fly       sac_fly_double_play \n                      446                      1222                        13 \n                   single                    triple               triple_play \n                    25363                       685                         1 \n                     &lt;NA&gt; \n                        0 \n\n\nAs another sanity check, when we tabulate the end_events for all at-bats with end_type != X, we do not see any of the hitting events from above.\n\ntable(def_atbat2024$end_events[def_atbat2024$end_type != \"X\"], useNA = 'always')\n\n\n                             catcher_interf          hit_by_pitch \n                  308                    97                  1977 \n            strikeout strikeout_double_play          truncated_pa \n                40145                   107                   304 \n                 walk                  &lt;NA&gt; \n                14029                     0 \n\n\nSo, we can isolate those at-bats with a ball put in play by filtering on end_type.\n\nbip &lt;-\n  def_atbat2024 |&gt;\n  dplyr::filter(end_type == \"X\")\n\nThe columns end_events includes values fielders_choice and fielders_choice_out. Presumably, balls recorded as fielders_choice did not result in an out while those recorded as fielders_choice_out did. To check whether this is the case, we will search from the string “out” in the description of the at-bat (des) for all balls with event = fielders_choice\n\ntable(grepl(\"out\", bip$des[bip$end_events == \"fielders_choice\"]))\n\n\nFALSE  TRUE \n  371     2 \n\n\nCuriously, there are two instances where the play is recorded as a fielder’s choice but the string “out” appears. Investigating further, in one instance the string “out” appears as part of a player’s name while in the other, the play actually did result in an out.\n\nwhich(grepl(\"out\", bip$des) & bip$end_events == \"fielders_choice\")\n\n[1] 10235 65138\n\nbip$des[which(grepl(\"out\", bip$des) & bip$end_events == \"fielders_choice\")]\n\n[1] \"Mike Trout reaches on a fielder's choice, fielded by shortstop David Hamilton. Anthony Rendon to 3rd. Nolan Schanuel to 2nd. Throwing error by shortstop David Hamilton.\"        \n[2] \"Ryan Jeffers reaches on a fielder's choice, fielded by second baseman Colt Keith. Byron Buxton scores. Ryan Jeffers out at 2nd, catcher Jake Rogers to first baseman Mark Canha.\"\n\n\nWe will manually change the value of end_events in row 65138 to “fielders_choice_out”. For fitting our out probability model, we will focus only on those at-bats that did not end with a home run. We will also drop at-bats for which the batted ball locations are missing.\n\nbip$end_events[65138] &lt;- \"fielders_choice_out\"\nout_events &lt;- \n  c(\"double_play\", \"field_out\", \"fielders_choice_out\",\n    \"force_out\", \"grounded_into_double_play\", \n    \"sac_bunt\", \"sac_fly\", \"sac_fly_double_play\",\n    \"triple_play\")\nbip &lt;-\n  bip |&gt;\n  dplyr::filter(end_events != \"home_run\" & !is.na(x) & !is.na(y)) |&gt;\n  dplyr::mutate(Out = ifelse(end_events %in% out_events, 1, 0)) \n\n\nBinning and averaging\nOne potential approach works by dividing the field into small rectangular regions and computing the proportion of balls landing within each region that result in an out. To this end, we will first create a grid of 3ft x 3ft squares that covers the range of our batted ball locations.\n\nrange(bip$x)\n\n[1] -287.725  290.575\n\nrange(bip$y)\n\n[1] -91.85 459.35\n\n\n\n1grid_sep &lt;- 3\n2x_grid &lt;- seq(from = -300, to = 300, by = grid_sep)\ny_grid &lt;- seq(from = -100, to = 500, by = grid_sep)\n3raw_grid &lt;- expand.grid(x = x_grid, y = y_grid)\n\n\n1\n\nSeparation between grid points in each dimension.\n\n2\n\nCreates a sequence of evenly-spaced points\n\n3\n\nCreates a table with every combination of values in x_grid and y_grid\n\n\n\n\nThe table grid contains many combinations of x and y that do not represent plausible batted ball locations. In the figure below, we plot all batted ball locations and overlay all the grid points.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(bip$x, bip$y, \n     xlim = c(-300, 300), ylim = c(-100, 500),\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.2))\npoints(raw_grid$x, raw_grid$y, pch = 16, cex = 0.2, \n       col = adjustcolor(oi_colors[3], alpha.f = 0.3))\n\n\n\n\n\n\n\nFigure 4: The initial grid of points contains many locations at which we would not expect balls to be fielded.\n\n\n\n\n\nWe will remove those (x,y) pairs in grid for which either\n\nx + y &lt; -100: these are locations far below the third base line\ny - x &lt; -100: these are locations far below the first base line\nsqrt(x^2 +y^2 &lt; 580): these are locations very far from home plate\n\n\ngrid &lt;-\n  raw_grid |&gt;\n  dplyr::filter(y + x &gt; -100 & y - x &gt; -100 & sqrt(x^2 + y^2) &lt; 580)\nplot(bip$x, bip$y, \n     xlim = c(-300, 300), ylim = c(-100, 500),\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.2))\npoints(grid$x, grid$y, pch = 16, cex = 0.2, \n       col = adjustcolor(oi_colors[3], alpha.f = 0.3))\n\n\n\n\n\n\n\nFigure 5: A restricted grid of spatial locations.\n\n\n\n\n\nWe can use the cut() function to divide the x and y values in bip into small bins. Here is a quick example of how cut works: we first create a vector with a few numbers. The breaks argument of cut tells it the end points of each bin.\n\nx &lt;- c(0.11, 0.23, 0.45, 0.67, 0.99, 0.02) \ncut(x, breaks = seq(0, 1, by = 0.1))  \n\n[1] (0.1,0.2] (0.2,0.3] (0.4,0.5] (0.6,0.7] (0.9,1]   (0,0.1]  \n10 Levels: (0,0.1] (0.1,0.2] (0.2,0.3] (0.3,0.4] (0.4,0.5] ... (0.9,1]\n\n\nBelow, we first cut the values of x and y that appear in bip. Then, using a grouped summary, we can compute the number of balls hit to each bin and the proportion of those balls that result in an out.\n\nbin_probs &lt;-\n  bip |&gt;\n  dplyr::select(x, y, Out) |&gt;\n  dplyr::mutate(\n1    x_bin = cut(x, breaks = seq(-300-grid_sep/2, 300+grid_sep/2, by = grid_sep)),\n    y_bin = cut(y, breaks = seq(-100 - grid_sep/2, 500+grid_sep/2, by = grid_sep))) |&gt;\n  dplyr::group_by(x_bin, y_bin) |&gt;\n  dplyr::summarise(\n    out_prob = mean(Out), \n    n_balls = dplyr::n(),\n    .groups = \"drop\")\n\n\n1\n\nOffsetting by grid_sep/2 ensures that the x and y values in grid are the centers of the squares in our grid (and not one of the corners)\n\n\n\n\nWe can now bin the x and y values contained in grid and, using a left_join(), append a column that contains the estimated probability of an out hit to each 3ft x 3ft square in our grid. Note that if there is a combination of x_bin and y_bin in grid that is not contained in bin_prob, the estimated probability will be NA. These are grid squares in which no balls were hit.\n\ngrid_probs_bin &lt;-\n  grid |&gt;\n  dplyr::mutate(\n    x_bin = cut(x, breaks = seq(-300-grid_sep/2, 300+grid_sep/2, by = 3)),\n    y_bin = cut(y, breaks = seq(-100-grid_sep/2, 500+grid_sep/2, by = 3))) |&gt;\n  dplyr::left_join(y = bin_probs, by = c(\"x_bin\", \"y_bin\"))\n\nWe can now make a heatmap of our estimated probabilities by looping over the rows in grid_prob_bins, drawing a small rectangle with corners (x-1.5, y-1.5), (x + 1.5, y-1.5), (x+1.5, y+1.5), and (x-1.5, y+1.5), and shading the rectangle based on the corresponding out probability.\n\n\n1col_list &lt;- colorBlindness::Blue2DarkRed18Steps\n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\n2plot(1, type = \"n\",\n3     xlim = c(-300, 300), ylim = c(-100, 500),\n4     xaxt = \"n\", yaxt = \"n\", bty = \"n\",\n5     main = \"Naive out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n6  rect(xleft = grid_probs_bin$x[i] - grid_sep/2,\n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n       col = ifelse(\n         is.na(grid_probs_bin$out_prob[i]), \n7         adjustcolor(oi_colors[1], alpha.f = 0.2),\n8         adjustcolor(rgb(colorRamp(col_list,bias=1)(grid_probs_bin$out_prob[i])/255),\n                     alpha.f = 0.5)),\n9       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nThis is a popular diverging color palette, which ranges from blue to red, that is color-blind friendly. See here for more details.\n\n2\n\nSpecifying type = \"n\" tells R to make an empty plot\n\n3\n\nManually set the horizontal and vertical limits of the plotting area\n\n4\n\nxaxt = \"n\" and yaxt = \"n\" suppress the axis lines and bty = \"n\" suppresses the bounding box\n\n5\n\nmain controls the title of plot while setting xlab = \"\" and ylab = \"\" suppresses horizontal and vertical axis labels\n\n6\n\nTo use rect(), we must pass the coordinates of the bottom left and top right corners of the rectangle\n\n7\n\nIf there were no balls hit to a particular rectangle, we will color it gray. Otherwise, we will color it according to the fitted probability\n\n8\n\nThe expression rgb(colorRamp(col_list, bias=1)(grid_probs_bin$out_prob)/255) maps the fitted probability to a color in col_list using linear interpolation. Here 0% maps to dark blue and 100% maps to dark red.\n\n9\n\nSuppresses the border of the rectangle\n\n\n\n\n\n\n\n\n\n\nFigure 6: Empirical out probabilities based on binning balls in play.\n\n\n\n\n\n\n\nA Logistic Regression Model for Outs\nOur naively estimated out probabilities leave much to be desired. For one thing, there are lots of “gaps” in the fitted surface where our initial model can’t make a prediction. These are the cells in our grid into which no balls were hit. Of much greater concern are the sharp discontinuities evident in the figure. In fact, there are many regions where the fitted out probability jumps from 0% to 100% to 0% in the span of about 6 feet. Such discontinuities are artifacts of the small sample sizes within some of the bins. Indeed, we find that about 85% of the grid cells contain 10 or fewer observations\n\nmean(bin_probs$n_balls &lt;= 10)\n\n[1] 0.8541364\n\n\nLike we did to predict XG using shot distance, we can overcome this challenge by building a statistical model. In particular, we would like to model the log-odds of an out as a function of the batted balls location. That is, \\[\n\\log\\left(\\frac{\\mathbb{P}(\\textrm{out})}{1 - \\mathbb{P}(\\textrm{out})} \\right) = s(x,y),\n\\] where \\(s\\) is a smooth function of the batted balls location. What form should \\(s\\) take?\nOne possibility is to assume that there are parameters \\(\\beta_{0}, \\beta_{x},\\) and \\(\\beta_{y}\\) such that \\[\n\\log\\left(\\frac{\\mathbb{P}(\\textrm{out})}{1 - \\mathbb{P}(\\textrm{out})} \\right) = \\beta_{0} + \\beta_{x}x + \\beta_{y}y\n\\] We can estimate these parameters using glm() and then compute the fitted probability for every cell in our grid.\n\nlogit_fit &lt;-\n  glm(Out ~ x + y,\n      family = binomial(link = \"logit\"), data = bip)\ngrid_logit_preds &lt;-\n  predict(object = logit_fit, \n1          newdata = grid, type = \"response\")\ngrid_logit_cols &lt;- \n2  rgb(colorRamp(col_list,bias=1)(grid_logit_preds)/255)\n\n\n1\n\nTo get fitted values on the probability scale, we need to specify type = \"response\"\n\n2\n\nCreates a vector mapping the fitted probability at each grid location to a color between the two extremes\n\n\n\n\nThe fitted logistic regression model predicts that the probability of an out decreases as one moves vertically away from home plate. The model further predicts that balls hit into the outfield tend not to result in outs, which is at odds with what we see in the data. Indeed, our earlier plot revealed pockets of high out probability in the outfield, roughly corresponding to the usual positioning of the left, right, and center fielders.\n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", \n     xlim = c(-300, 300), ylim = c(-100, 500), \n     xaxt = \"n\", yaxt = \"n\", bty = \"n\", \n     main = \"Logistic regression out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n  rect(xleft = grid_probs_bin$x[i] - grid_sep/2, \n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n1       col = adjustcolor(grid_logit_cols[i], alpha.f = 0.5),\n       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nUnlike with binning-and-averaging, our parametric model is able to make predictions at grid cells not present in the training data. So, we don’t need to check whether the predicted probability is NA\n\n\n\n\n\n\n\n\n\n\nFigure 7: Logistic regression forecasts of out probabilities as a function of location.",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-gam",
    "href": "lectures/lecture08.html#sec-gam",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\nThe root cause of the conflict between our data and the logistic regression model forecasts in Figure 7 is the model’s strong — and totally unrealistic — assumption that the log-odds of an out is linear in both \\(x\\) and \\(y.\\) A more accurate model would allow the log-odds to vary non-linearly.\nGeneralized Additive Models (or GAMs) are an elegant way to introduce non-linearity and create spatially smooth maps. In our context, a GAM model expresses \\(s(x,y)\\) as a linear combination of a large number of two-dimensional splines functions. Each spline function is a piecewise polynomial that is localized to a small region of \\((x,y)\\) space, meaning that it is non-zero inside the region and zero outside the region. Such spline functions can be linearly combined to approximate really complicated functions arbitrarily well2\nWe can fit GAMs in R using the mgcv package, which can be installed using\n\ninstall.packages(\"mgcv\")\n\nThe package provides two fitting functions, gam() and bam(), which is recommended for very large data sets like bip. Both functions are similar lm() and glm() in that they require users to specify a formula and, in the case of non-continuous outcomes, a link function. The main difference is the term s(), which is used to signal to gam() or bam() that we want to smooth over whatever variables appear inside of the brackets of s(). In our case, because we wish to smooth over both x and y, we will include the terms s(x,y)\n\n\n\n\n\n\nWarning\n\n\n\nFitting our out probability model takes a few minutes\n\n\n\nlibrary(mgcv)\ngam_fit &lt;-\n  bam(formula = Out ~ s(x,y), \n      family = binomial(link=\"logit\"), data = bip)\n\nAfter fitting our GAM, we can visualize the estimated out probabilities:\n\ngrid_gam_preds &lt;- \n  predict(object = gam_fit,\n          newdata = grid, type = \"response\")\n\ngrid_gam_cols &lt;-   \n  rgb(colorRamp(col_list,bias=1)(grid_gam_preds)/255) \n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", \n     xlim = c(-300, 300), ylim = c(-100, 500), \n     xaxt = \"n\", yaxt = \"n\", bty = \"n\", \n     main = \"GAM out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n  rect(xleft = grid_probs_bin$x[i] - grid_sep/2, \n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n       col = adjustcolor(grid_gam_cols[i], alpha.f = 0.5),\n       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nThe out probabilities fitted by the GAM appear much more reasonable: balls hit within the in-field and close to the outfielders tend to be result in outs while balls hit in the gap between the infield and outfield tend not to result in outs.\n\nDividing Defensive Run Value\nRecall that if at-bat \\(i\\) results in a ball put in play, we will distribute \\(\\delta_{i}^{(f)} = -\\delta_{i} \\times \\hat{p}_{i}\\) to the fielders and assign \\(\\delta_{i}^{(p)} = -\\delta \\times (1 - \\hat{p}_{i})\\) to the pitcher. The following code adds a column to def_atbat2024 containing the predicted out probabilities from our fitted GAM. Note that this column contains NA values for those at-bats that did not result in a ball in play (e.g., they ended with a walk or a strikeout or a home run) For the vast majority of these at-bats, we will manually set \\(\\hat{p}_{i} = 0\\) so that the pitcher gets all the credit (i.e., \\(\\delta^{(p)}_{i} = -\\delta_{i}\\)).\n\nall_preds &lt;-\n  predict(object = gam_fit, \n1          newdata = def_atbat2024,\n          type = \"response\")\n2def_atbat2024$p_out &lt;- all_preds\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    p_out = dplyr::case_when(\n3      is.na(p_out) & end_events == \"home_run\" ~ 0,\n4      is.na(p_out) & end_type != \"X\" ~ 0,\n      .default = p_out))\n\n\n1\n\nWhen x and y are NA (i.e., whenever the at-bat doesn’t end with a ball in play), predict will return NA.\n\n2\n\nAdds fitted out probabilities to the data table\n\n3\n\nFor home runs, manually set p_out = 0 (so pitcher gets all credit/blame)\n\n4\n\nFor at-bats that end with a ball or strike, set p_out = 0 (so pitcher gets all credit/blame)\n\n\n\n\nEven after manually setting \\(\\hat{p}_{i} = 0\\) on the at-bats that didn’t end with the ball in play, there are still some NA values in the column p_out. On further inspection, it looks like these were at-bats in which the ball was put in play but are missing hit locations.\n\ntable(def_atbat2024$end_events[is.na(def_atbat2024$p_out)])\n\n\n                   double                 field_out grounded_into_double_play \n                        2                         5                         1 \n                   single                    triple \n                        4                         1 \n\nsum(is.na(def_atbat2024$x[is.na(def_atbat2024$p_out)]))\n\n[1] 13\n\n\nWe will remove these rows from our calculations\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::filter(!is.na(p_out))\n\nWe can now finally compute \\(\\delta^{(f)}_{i}\\) and \\(\\delta^{(p)}_{i}\\) for each at-bat.\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    delta_p = -1 * (1 - p_out) * RunValue,\n    delta_f = -1 * p_out * RunValue)",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-fielding",
    "href": "lectures/lecture08.html#sec-fielding",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Fielding Run Values",
    "text": "Fielding Run Values\nIf a ball is hit towards deep left field and results in the fielding team creating a large \\(-\\delta\\), how much blame should the third baseman, who is on the opposite side of the park receive? Following Baumer, Jensen, and Matthews (2015), we will apportion \\(\\delta_{i}^{(f)},\\) the portion of the run value \\(-\\delta_{i}\\) attributable to the fielding in at-bat \\(i\\), based on each fielder’s responsibility for making an out on the batted ball. Specifically, we will assign \\(w_{i,\\ell} \\times \\delta_{i}^{(f)}\\) to the fielder playing position \\(\\ell \\in \\{1, 2, \\ldots, 9\\}\\) during the at-bat where \\[\nw_{i,\\ell} = \\frac{\\hat{p}_{i,\\ell}}{\\hat{p}_{i,1} + \\cdots + \\hat{p}_{i,9}}\n\\] and \\(\\hat{p}_{i,\\ell}\\) is the probability that fielder \\(\\ell\\) makes the out given the location of the batted ball.\nTo estimate the \\(\\hat{p}_{i,\\ell}\\)’s, we will fit 9 separate GAMs, one for each fielding position. For fielding position \\(\\ell,\\) we will create a new variable that is equal to 1 if the ball in play resulted in an out was initially fielded by the player at position \\(\\ell.\\) Unlike our overall out probability model, which we fit using data from all batted balls, each position-specific GAM will be fitted using data from the subset of batted balls that are reasonably close to the typical position location. For instance, when fitting the model for the first basemen, we won’t include data from balls hit into deep left field. We compute the coordinates of the typical position by taking the average value of the x and y coordinates of all batted balls fielded by players at position \\(\\ell.\\) We fit each position-specific GAM using batted balls hit within 150 ft of this typical location3\n\n\n\n\n\n\nWarning\n\n\n\nThis code takes between several minutes to run\n\n\n\n1mid_x &lt;- rep(NA, times = 9)\nmid_y &lt;- rep(NA, times = 9)\n2fit_list &lt;- list()\nfit_time &lt;- rep(NA, times = 9)\nfor(l in 1:9){\n  print(paste0(\"Starting l = \", Sys.time()))\n  \n3  mid_x[l] &lt;- mean(bip$x[bip$end_hit_location == l], na.rm = TRUE)\n  mid_y[l] &lt;- mean(bip$y[bip$end_hit_location==l], na.rm = TRUE)\n  \n  train_df &lt;-\n    bip |&gt;\n4    dplyr::mutate(newOut = ifelse(Out == 1 & end_hit_location == l, 1, 0)) |&gt;\n    dplyr::filter(!is.na(x) & !is.na(y)) |&gt;\n5    dplyr::filter( sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150) |&gt;\n    dplyr::select(x,y,newOut) \n\n6  fit_time[l] &lt;-\n    system.time(\n7      fit_list[[l]] &lt;-\n        bam(formula = newOut ~ s(x, y),\n8            family = binomial(link=\"logit\"),data = train_df))[\"elapsed\"]\n  tmp_df &lt;-\n    def_atbat2024 |&gt; \n    dplyr::mutate(\n9      x = ifelse(sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150, x, NA),\n      y = ifelse(sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150, y, NA)\n    )\n    \n10  preds &lt;- predict(object = fit_list[[l]], newdata = tmp_df, type = \"response\")\n11  def_atbat2024[[paste0(\"p_out_\", l)]] &lt;- preds\n}\n12save(fit_list, file = \"position_gam_fits.RData\")\n\n\n1\n\nVectors containing coordinates for the typical fielder location\n\n2\n\nHolds all fitted objects, so that we can use them later to visualize position-specific out probabilities\n\n3\n\nCompute typical coordinates for position l\n\n4\n\nCreate position-specific outcome variable\n\n5\n\nSubsets to batted balls within 150 ft of the typical location\n\n6\n\nIt’s often helpful to track how long it takes to fit a model. system.time() can return, among other things, the elapsed time. We save the time for fitting each position’s model in the vector fit_time\n\n7\n\nInstead of creating a different object for each position’s fitted model, we will store all the fitted models in a single list, which can be saved.\n\n8\n\nsystem.time() returns three different time measures. The most pertinent one is the actual elapsed time, stored in the slot “elapsed”.\n\n9\n\nWe only want model predictions for balls hit within 150ft of the typical location. Passing NA x and y values to predict() is a hack to suppress model predictions for balls hit outside this radius. We will later over-write the resulting NA’s with 0’s.\n\n10\n\nfit_list[[l]] is the fitted object return by bam for position l.\n\n11\n\nCreates a column in def_atbat2024 holding the\n\n12\n\nSave’s the list containing all position-specific fits\n\n\n\n\n[1] \"Starting l = 2025-09-01 15:25:49.903227\"\n[1] \"Starting l = 2025-09-01 15:27:04.288984\"\n[1] \"Starting l = 2025-09-01 15:28:12.239582\"\n[1] \"Starting l = 2025-09-01 15:29:37.547715\"\n[1] \"Starting l = 2025-09-01 15:31:04.105531\"\n[1] \"Starting l = 2025-09-01 15:32:25.945627\"\n[1] \"Starting l = 2025-09-01 15:33:46.246784\"\n[1] \"Starting l = 2025-09-01 15:34:26.973188\"\n[1] \"Starting l = 2025-09-01 15:35:07.238709\"\n\n\nThe columns p_out_1, …, p_out_9 in def_atbat2024 contain several NA values. Some of these correspond to at-bats that did not result in ball being put in play (i.e., those with end_type = S or end_type = B) while others correspond to at-bats in which the ball was hit far away from the typical location of the fielder. For these latter at-bats, we will manually set the position-specific out probabilities as follows: 1. For home runs and balls hit outside the 150ft radius for each position, we set p_out_* = 0 2. For at-bats ending with a strike or ball, we keep p_out_* = NA. 3. For balls in play with missing x and y coordinates, we set p_out_* = NA\n\n\nCode\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    p_out_1 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[1])^2 + (y - mid_y[1])^2) &gt;= 150 ~ 0,\n      .default = p_out_1),\n    p_out_2 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[2])^2 + (y - mid_y[2])^2) &gt;= 150 ~ 0,\n      .default = p_out_2),\n    p_out_3 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[3])^2 + (y - mid_y[3])^2) &gt;= 150 ~ 0,\n      .default = p_out_3),\n    p_out_4 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[4])^2 + (y - mid_y[4])^2) &gt;= 150 ~ 0,\n      .default = p_out_4),\n    p_out_5 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[5])^2 + (y - mid_y[5])^2) &gt;= 150 ~ 0,\n      .default = p_out_5),\n    p_out_6 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[6])^2 + (y - mid_y[6])^2) &gt;= 150 ~ 0,\n      .default = p_out_6),\n    p_out_7 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[7])^2 + (y - mid_y[7])^2) &gt;= 150 ~ 0,\n      .default = p_out_7),\n    p_out_8 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[8])^2 + (y - mid_y[8])^2) &gt;= 150 ~ 0,\n      .default = p_out_8),\n    p_out_9 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[9])^2 + (y - mid_y[9])^2) &gt;= 150 ~ 0,\n      .default = p_out_9))\n\n\nWe now normalize the \\(\\hat{p}_{i,\\ell}\\) values to get the weights \\(w_{i,\\ell}\\) measuring the responsibility of each fielder for making an out in the at-bat.\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    total_weight = p_out_1 + p_out_2 + p_out_3 +\n      p_out_4 + p_out_5 + p_out_6 +\n      p_out_7 + p_out_8 + p_out_9,\n    w1 = p_out_1/total_weight,\n    w2 = p_out_2/total_weight,\n    w3 = p_out_3/total_weight,\n    w4 = p_out_4/total_weight,\n    w5 = p_out_5/total_weight,\n    w6 = p_out_6/total_weight,\n    w7 = p_out_7/total_weight,\n    w8 = p_out_8/total_weight,\n    w9 = p_out_9/total_weight)\n\nNow, for each fielding position, we can compute \\(w_{i,\\ell} \\times \\delta^{(f)}_{i}\\) and aggregate these values across players at that position to compute \\(\\textrm{RAA}_{\\ell}^{(f)},\\) which quantifies the total run value created by the player through their fielding at position \\(\\ell.\\) Negative values of \\(\\textrm{RAA}_{\\ell}^{(f)}\\) suggest that the player’s fielding at position \\(\\ell\\) netted positive run value for the batting team, thereby negatively impacthing his own team.\nHere is the calculation for first basemen (fielding position number \\(\\ell = 3\\)). Taking a looking at the largest \\(\\textrm{RAA}_{\\ell}^{f}\\) values, we see several recent Golden Glove winners who are known for their fielding prowess (Santana, Walker, Goldschmidt, Olson, Guerrero).\n\nraa_f3 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f3 = delta_f * w3) |&gt;\n  dplyr::group_by(fielder_3) |&gt;\n  dplyr::summarize(RAA_f3 = sum(RAA_f3, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_3)\n\nraa_f3 |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, RAA_f3) |&gt;\n  dplyr::arrange(dplyr::desc(RAA_f3)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   Name              RAA_f3\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 Carlos Santana      55.7\n 2 Christian Walker    54.6\n 3 Paul Goldschmidt    52.4\n 4 Bryce Harper        51.1\n 5 Matt Olson          51.0\n 6 Ryan Mountcastle    48.7\n 7 Vladimir Guerrero   47.1\n 8 Michael Toglia      46.5\n 9 Freddie Freeman     46.2\n10 Josh Naylor         46.2\n\n\nThe following code repeats this calculation for all fielding positions.\n\n\nShow the code computes the fielding RAA for all positions.\nraa_f1 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f1 = delta_f * w1) |&gt;\n  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarize(RAA_f1 = sum(RAA_f1, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = pitcher) |&gt;\n  dplyr::select(key_mlbam, RAA_f1)\n\nraa_f2 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f2 = delta_f * w2) |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarize(RAA_f2 = sum(RAA_f2, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_2) |&gt;\n  dplyr::select(key_mlbam, RAA_f2)\n\nraa_f3 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f3 = delta_f * w3) |&gt;\n  dplyr::group_by(fielder_3) |&gt;\n  dplyr::summarize(RAA_f3 = sum(RAA_f3, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_3) |&gt;\n  dplyr::select(key_mlbam, RAA_f3)\n\nraa_f4 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f4 = delta_f * w4) |&gt;\n  dplyr::group_by(fielder_4) |&gt;\n  dplyr::summarize(RAA_f4 = sum(RAA_f4, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_4) |&gt;\n  dplyr::select(key_mlbam, RAA_f4)\n\nraa_f5 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f5 = delta_f * w5) |&gt;\n  dplyr::group_by(fielder_5) |&gt;\n  dplyr::summarize(RAA_f5 = sum(RAA_f5, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_5) |&gt;\n  dplyr::select(key_mlbam, RAA_f5)\n\nraa_f6 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f6 = delta_f * w6) |&gt;\n  dplyr::group_by(fielder_6) |&gt;\n  dplyr::summarize(RAA_f6 = sum(RAA_f6, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_6) |&gt;\n  dplyr::select(key_mlbam, RAA_f6)\n\nraa_f7 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f7 = delta_f * w7) |&gt;\n  dplyr::group_by(fielder_7) |&gt;\n  dplyr::summarize(RAA_f7 = sum(RAA_f7, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_7) |&gt;\n  dplyr::select(key_mlbam, RAA_f7)\n\nraa_f8 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f8 = delta_f * w8) |&gt;\n  dplyr::group_by(fielder_8) |&gt;\n  dplyr::summarize(RAA_f8 = sum(RAA_f8, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_8) |&gt;\n  dplyr::select(key_mlbam, RAA_f8)\n\nraa_f9 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f9 = delta_f * w9) |&gt;\n  dplyr::group_by(fielder_9) |&gt;\n  dplyr::summarize(RAA_f9 = sum(RAA_f9, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_9) |&gt;\n  dplyr::select(key_mlbam, RAA_f9)\n\n\nLike we did when computing \\(\\textrm{RAA}^{(br)}\\) in Lecture 7, we will concatenate raa_f1, …, raa_f9 and sum every players total contributions across all fielding positions.\n\nraa_f &lt;-\n  raa_f1 |&gt;\n  dplyr::full_join(y = raa_f2, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f3, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f4, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f5, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f6, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f7, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f8, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f9, by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(\n    list(RAA_f1=0, RAA_f2=0, RAA_f3=0, RAA_f4=0, \n         RAA_f5=0, RAA_f6 = 0, RAA_f7=0, RAA_f8 = 0, RAA_f9=0)) |&gt;\n  dplyr::mutate(\n    RAA_f = RAA_f1 + RAA_f2 + RAA_f3 + RAA_f4 + \n      RAA_f5 + RAA_f6 + RAA_f7 + RAA_f8 + RAA_f9) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_f, RAA_f1, RAA_f2, RAA_f3, RAA_f4, RAA_f5,\n                RAA_f6, RAA_f7, RAA_f8, RAA_f9)",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-pitching",
    "href": "lectures/lecture08.html#sec-pitching",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Pitching Run Values",
    "text": "Pitching Run Values\nRecall that \\(\\delta_{i}^{(p)}\\) is the amount of run value created by the pitcher in at-bat \\(i.\\) Summing this value across all at-bats involving each pitcher, we obtain each pitchers \\(\\textrm{RAA}^{(p)}.\\)\n\nraa_p &lt;-\n  def_atbat2024 |&gt;\n  dplyr::select(pitcher, delta_p) |&gt;\n  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarise(RAA_p = sum(delta_p, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = pitcher) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_p)\n\nWe see that the two 2024 Cy Young Award winners4, Tarik Skubal and Chris Sale, have the highest \\(\\textrm{RAA}^{(p)}\\) values.\n\nraa_p |&gt;\n  dplyr::arrange(dplyr::desc(RAA_p)) |&gt;\n  dplyr::slice_head(n=10)\n\n# A tibble: 10 × 3\n   Name            key_mlbam RAA_p\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt;\n 1 Tarik Skubal       669373 14.7 \n 2 Chris Sale         519242 14.6 \n 3 Ryan Walker        676254 13.5 \n 4 Cade Smith         671922 13.3 \n 5 Paul Skenes        694973 12.3 \n 6 Emmanuel Clase     661403 11.2 \n 7 Kirby Yates        489446  9.99\n 8 Griffin Jax        643377  9.53\n 9 Edwin Uceta        670955  9.23\n10 Garrett Crochet    676979  9.13\n\n\nWe will save both raa_f and raa_p for later use\n\nsave(raa_f, raa_p, file = \"raa_defensive2024.RData\")",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-war",
    "href": "lectures/lecture08.html#sec-war",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Replacement Level",
    "text": "Replacement Level\nWe now combine the fielding and pitching RAA values with the baserunning and batting RAA values we computed in Lecture 7 into a single table.\n\nload(\"raa_offensive2024.RData\")\n\nraa &lt;-\n  raa_b |&gt;\n  dplyr::select(-Name) |&gt;\n  dplyr::full_join(y = raa_br |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_p |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(list(RAA_b = 0, RAA_br = 0, RAA_f = 0, RAA_p = 0)) |&gt;\n  dplyr::mutate(RAA = RAA_b + RAA_br + RAA_f + RAA_p) |&gt;\n  dplyr::left_join(y = player2024_lookup |&gt; dplyr::select(key_mlbam, Name), by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, RAA_b, RAA_br, RAA_f, RAA_p)\n\n\\(RAA\\) is a comprehensive measure of a player’s performance that accounts not only for the actual runs scored (or given up) due to his contributions but also the changes in the expected runs that can be attributed to his play. We constructed \\(RAA\\) so that larger numbers indicate better performance. While the absolute \\(RAA\\) values are useful on their own, they become even more useful — and easier to interpret — when calibrated to measure performance relative to a baseline player. As argued by Baumer, Jensen, and Matthews (2015), although comparing an individual players \\(RAA\\) to the league-average value is intuitive, average players are themselves quite valuable. Further, it is not reasonable to expect a team to be able to replace any player with a league average one. Thus, it is more useful to compare each player’s performance relative to a replacement-level player.\nAs discussed in class (and also in (Baumer, Jensen, and Matthews 2015, sec. 3.8)), existing definitions of replacement level are fairly arbitrary. For instance, back in 2010, FanGraphs asserted that a team of players making the minimum MLB salary would win 29.7% of its games (so between 48 and 49 games). Similar definitions have been adopted by other sites like Baseball Propsectus and Baseball Reference. Unfortunately, there is little empirical justification for this number.\nWe will instead use Baumer, Jensen, and Matthews (2015)’s roster-based definition of replacement level, which is motivated and computed as follows:\n\nEach of the 30 major league teams typically carries 25 players, 13 of whom are position players and 12 of whom are pitchers\nOn any given day, there are generally \\(12 \\times 30 = 360\\) pitchers and \\(13 \\times 30 = 390\\) active players.\nSo, we will treat the 390 position players with the most plate appearances and the 360 pitchers who faced the most batters as non-replacement level and everyone else as replacement-level.\n\nRemember that our table def_atbat2024 contains information from all available at-bats in the 2024 regular season. We can use the data in this table to count the number of plate appearances for each non-pitcher and number of batters faced by each pitcher. To this end, we first create lists of the MLB Advanced Media IDs of all pitchers and all non-pitchers who appear in our dataset.\n\n1all_players &lt;- unique(\n  c(def_atbat2024$batter, def_atbat2024$pitcher, def_atbat2024$fielder_2,\n    def_atbat2024$fielder_3, def_atbat2024$fielder_4, def_atbat2024$fielder_5,\n    def_atbat2024$fielder_6, def_atbat2024$fielder_7, def_atbat2024$fielder_8, def_atbat2024$fielder_9))\n2pitchers &lt;- unique(def_atbat2024$pitcher)\n\n3position_players &lt;- all_players[!all_players %in% pitchers]\n\n\n1\n\nGet the ID for all players\n\n2\n\nGet the ID for all pitchers\n\n3\n\nPull out the ID for all non-pitchers\n\n\n\n\nUsing grouped summaries, we can count the number of at-bats faced by each position player as a batter and by each pitcher. Since the overwhelming majority of at-bats involved only one batter, these counts effectively tell us the number of batters faced by each pitcher.\n\nposition_pa &lt;-\n  def_atbat2024 |&gt;\n1  dplyr::filter(batter %in% position_players) |&gt;\n  dplyr::group_by(batter) |&gt;\n2  dplyr::summarise(n = dplyr::n()) |&gt;\n3  dplyr::arrange(dplyr::desc(n)) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\npitcher_pa &lt;-\n  def_atbat2024 |&gt;\n4  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarise(n = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(n)) |&gt;\n5  dplyr::rename(key_mlbam = pitcher)\n\n\nrepl_position_players &lt;- position_pa$key_mlbam[-(1:390)]\nrepl_pitchers &lt;- pitcher_pa$key_mlbam[-(1:360)]\n\ncat(\"Cut-off for position players:\", position_pa$n[390], \"\\n\")\ncat(\"Cut-off for pitchers:\", pitcher_pa$n[360], \"\\n\")\n\n\n1\n\nThis removes at-bats in which a pitcher is hitting\n\n2\n\nCounts the number of at-bats in which each position player is batting\n\n3\n\nArranges the counts in decreasing order so that we can determine replacement-level cut-offs\n\n4\n\nSince the vector pitchers is just the unique values of def_atbat2024$pitcher, there is no need to filter\n\n5\n\nGet the IDs of the position players and pitchers outside, respectively, the top 390 and 360 numbers of plate appearances\n\n\n\n\nCut-off for position players: 131 \nCut-off for pitchers: 204 \n\n\nUltimately, we wish to compare each player’s \\(RAA\\) to the \\(RAA\\) that would have been created had the player been replaced by a replacement-level player. To estimate this counter-factual \\(RAA\\), we will divide the total \\(RAA\\) produced by all replacement-level players by the total number of plate appearances..\n\nrepl_position_raa &lt;-\n  raa |&gt;\n  dplyr::filter(key_mlbam %in% repl_position_players) |&gt;\n1  dplyr::inner_join(y = position_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n)\n\nrepl_pitch_raa &lt;-\n  raa |&gt;\n  dplyr::filter(key_mlbam %in% repl_pitchers) |&gt;\n  dplyr::inner_join(y = pitcher_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n) \n\nrepl_avg_pos &lt;- sum(repl_position_raa$RAA)/sum(repl_position_raa$n)\nrepl_avg_pitch &lt;- sum(repl_pitch_raa$RAA)/sum(repl_pitch_raa$n)\n\n\n1\n\nUsing an inner join ensures that we only append the n values for replacement-level players\n\n\n\n\nWe see that the replacement-level RAA per-at-bat is 0 for position players and -0.02 for pitchers. Multiplying the the replacement-level per-at-bat RAA values by the number of plate appearances faced by each non-replacement-level player gives us an estimate of how each player’s replacement-level “shadow” would perform. Finally, using the heuristic of 10 runs per win, dividing the difference between the actual RAA and the RAA for the replacement-level shadow yields our wins above replacement for position players.\n\nposition_war &lt;-\n  raa |&gt;\n  dplyr::filter(!key_mlbam %in% repl_position_players) |&gt;\n  dplyr::inner_join(y = position_pa, by = \"key_mlbam\") |&gt; \n  dplyr::select(Name, key_mlbam, RAA, n) |&gt;\n  dplyr::mutate(shadowRAA = n * repl_avg_pos) |&gt;\n  dplyr::mutate(WAR = (RAA - shadowRAA)/10)\n\nposition_war |&gt;\n  dplyr::arrange(dplyr::desc(WAR)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 6\n   Name              key_mlbam   RAA     n shadowRAA   WAR\n   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Bobby Witt           677951 165.    694     -1.36 16.6 \n 2 Gunnar Henderson     683002 117.    702     -1.37 11.8 \n 3 Elly De La Cruz      682829 114.    679     -1.33 11.5 \n 4 Jose Ramirez         608070 113.    657     -1.28 11.5 \n 5 Zach Neto            687263 110.    590     -1.15 11.1 \n 6 Marcus Semien        543760 108.    701     -1.37 10.9 \n 7 Ketel Marte          606466 101.    562     -1.10 10.2 \n 8 Vladimir Guerrero    665489  99.0   671     -1.31 10.0 \n 9 Francisco Lindor     596019  98.5   689     -1.35  9.98\n10 Jose Altuve          514888  98.2   661     -1.29  9.95\n\n\nWe can run a similar calculation for pitchers.\n\npitch_war &lt;-\n  raa |&gt;\n  dplyr::filter(!key_mlbam %in% repl_pitchers) |&gt;\n  dplyr::inner_join(y = pitcher_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n) |&gt;\n  dplyr::mutate(shadowRAA = n * repl_avg_pitch) |&gt;\n  dplyr::mutate(WAR = (RAA - shadowRAA)/10)\n\npitch_war |&gt;\n  dplyr::arrange(dplyr::desc(WAR)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 6\n   Name            key_mlbam   RAA     n shadowRAA   WAR\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Tarik Skubal       669373 17.2    730    -17.7   3.49\n 2 Chris Sale         519242 17.2    702    -17.0   3.42\n 3 Zack Wheeler       554430 10.1    759    -18.4   2.86\n 4 Paul Skenes        694973 14.6    495    -12.0   2.66\n 5 Garrett Crochet    676979  9.91   596    -14.5   2.44\n 6 Bryce Miller       682243  7.66   681    -16.5   2.42\n 7 Seth Lugo          607625  2.47   813    -19.7   2.22\n 8 Ryan Walker        676254 14.7    302     -7.33  2.21\n 9 Reynaldo Lopez     625643  8.82   542    -13.2   2.20\n10 Cade Smith         671922 14.2    289     -7.02  2.12",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#footnotes",
    "href": "lectures/lecture08.html#footnotes",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe official position numberings are: 1 (pitcher), 2 (catcher), 3 (first baseman), 4 (second baseman), 5 (third baseman), 6 (shortstop), 7 (left fielder), 8 (center fielder), and 9 (right fielder).↩︎\nIf you’re keen to learn more about splines, consider taking STAT 351 (Introduction to Nonparametric Statistics). A lot of the mathematical theory underpinning smoothing splines was developed by Prof. Grace Wahba, who was a long-serving member of the faculty here.↩︎\nThis choice is decidedly ad hoc. You should experiment with different cut-offs to see how the downstream results change!↩︎\nThe Cy Young Award is given to the best pitchers in the National League and American League.↩︎",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture10.html",
    "href": "lectures/lecture10.html",
    "title": "Lecture 10: NFl WAR",
    "section": "",
    "text": "In Lecture 9, we looked at two touchdowns from the 2024-25 NFL season:\n\nA 86-yard touchdown by Ladd McConckey on a pass from Justin Herbert (video link)\nA 64-yard touchdown by Kavontae Turpin on a pass from Cooper Rush (video link)\n\nThe McConckey touchdown was thrown on a 3rd and 26 from the Charger’s 14 yard line. From that position, teams are expected to score about -1.54 points; that is, starting from that positions, teams, on average, give up points rather than score them. Over the course of the play, the Chargers offense generated around 8.54 points of EPA. The Turpin touchdown, on the other hand, was thrown on a 3rd and 10 from the Cowboys’ 36 yard line, a position from which teams score about 0.77 on average. The Cowboys offense generated about 6.23 points of EPA during that play. How much of the created EPA on these plays should be credited to the quarterbacks and how much should be credited to the receivers? Which players\nIn this lecture, we will use multilevel modeling to answer these questions. We separately model EPA on passing and running plays. For passing plays, we will decompose the EPA into two parts, the expected points gained while the pass is in the air and the expected points gained on the ground after the catch (if the pass was caught). Each of our multilevel models will include random intercepts for players nested by their position (e.g., passer and receiver in the passing EPA models) and will also adjust for several important fixed effects. From these models, we will determine how many more expected points per play each player adds relative to the league average. Finally, like we did in Lecture 8, we will translate these expected points added to the win scale and conclude with a version of wins above replacement for offensive players in football. Our development closely follows the nflWAR model introduced by Yurko, Ventura, and Horowitz (2019) but makes some simplifications.",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#overview",
    "href": "lectures/lecture10.html#overview",
    "title": "Lecture 10: NFl WAR",
    "section": "",
    "text": "In Lecture 9, we looked at two touchdowns from the 2024-25 NFL season:\n\nA 86-yard touchdown by Ladd McConckey on a pass from Justin Herbert (video link)\nA 64-yard touchdown by Kavontae Turpin on a pass from Cooper Rush (video link)\n\nThe McConckey touchdown was thrown on a 3rd and 26 from the Charger’s 14 yard line. From that position, teams are expected to score about -1.54 points; that is, starting from that positions, teams, on average, give up points rather than score them. Over the course of the play, the Chargers offense generated around 8.54 points of EPA. The Turpin touchdown, on the other hand, was thrown on a 3rd and 10 from the Cowboys’ 36 yard line, a position from which teams score about 0.77 on average. The Cowboys offense generated about 6.23 points of EPA during that play. How much of the created EPA on these plays should be credited to the quarterbacks and how much should be credited to the receivers? Which players\nIn this lecture, we will use multilevel modeling to answer these questions. We separately model EPA on passing and running plays. For passing plays, we will decompose the EPA into two parts, the expected points gained while the pass is in the air and the expected points gained on the ground after the catch (if the pass was caught). Each of our multilevel models will include random intercepts for players nested by their position (e.g., passer and receiver in the passing EPA models) and will also adjust for several important fixed effects. From these models, we will determine how many more expected points per play each player adds relative to the league average. Finally, like we did in Lecture 8, we will translate these expected points added to the win scale and conclude with a version of wins above replacement for offensive players in football. Our development closely follows the nflWAR model introduced by Yurko, Ventura, and Horowitz (2019) but makes some simplifications.",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-data-prep",
    "href": "lectures/lecture10.html#sec-data-prep",
    "title": "Lecture 10: NFl WAR",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe will restrict our analysis to plays from the 2024-25 regular season and exclude all post-season/play-off games. Later, in Section 5.1, we will define replacement-level players on a position-by-position basis. So that we can look up player positions, we will re-load the roster information using nflfastR::fast_scraper_roster().\n\npbp2024 &lt;-\n  nflfastR::load_pbp(season = 2024) |&gt;\n  dplyr::filter(season_type == \"REG\") \nroster2024 &lt;-\n  nflfastR::fast_scraper_roster(seasons = 2024) |&gt;\n  dplyr::filter(!is.na(gsis_id))\n\n\nPassing Plays Data\nWe will extract information about all passing plays from pbp2024 into a data table called pass2024.\n\npass2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(\n    play_type == \"pass\" &\n    !is.na(posteam) & \n1    !grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc) &\n2    !grepl(\"sack\", desc)  ) |&gt;\n3  dplyr::filter(passer_player_id != receiver_player_id) |&gt;\n  dplyr::select(\n    passer_player_id,\n    receiver_player_id,\n    posteam, defteam,\n    air_yards, shotgun, \n    qb_hit, no_huddle,\n    posteam_type, \n    pass_location,\n    yards_after_catch,\n    epa, air_epa, yac_epa, complete_pass, desc) |&gt;\n4  dplyr::mutate(gsis_id = receiver_player_id) |&gt;\n5  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, position), by = \"gsis_id\") |&gt;\n6  dplyr::rename(receiver_position = position) |&gt;\n7  dplyr::select(-gsis_id)\n\n\n1\n\nRemove pass attempts on two-point conversions\n\n2\n\nnflfastR classifies sacks as passing plays. For the purposes of building a version of WAR for the NFL, we will treat sacks as running plays, similar to Yurko, Ventura, and Horowitz (2019).\n\n3\n\nRemove plays like this where the passer catches their own pass (usually off a deflection).\n\n4\n\nCopy receiver ID into a temporary column, which will be used append receiver position with a join.\n\n5\n\nAppend receiver position.\n\n6\n\nRename the column containing receiver position.\n\n7\n\nRemove the temporary column holding the receiver ID\n\n\n\n\nIn addition to the identity of the passer (passer_player_id) and intended receiver (receiver_player_id), the data table contains the following columns: * posteam: the identity of the team in possession of the ball * defteam: the identity of the team on defense * air_yards: the distance traveled by the pass through the air[^airyard] * shotgun: whether the pass was thrown out of the shotgun formation * qb_hit: whether the quarterback was hit while throwing * no_huddle: whether the play was run without a huddle * posteam_type: whether the home team had possession (i.e., was on offense) * pass_location: whether the pass was thrown to the left, right, or center of the field * yards_after_catch: for completions, how many yards did the receiver gain after catching the pass * epa: the expected points added on the play * air_epa and yac_epa: the expected points added through the air and running after the catch (for completed passes) * complete_pass: whether the pass was completed * receiver_position: the position of the actual receiver on completed passes and the intended receiver on incomplete passes\nFor every passing play, nflfastR computes two different EPA values. The first, air_epa is the expected points added through the air and yac_epa is the expected points added after the catch. For completed passes, computing air_epa and yac_epa is relatively straightforward: as soon as the pass is caught, imagine stopping the play and computing the expected points based on the resulting game state. Then, the difference between this intermediate EP and the starting EP of the original play is air_epa and the difference between the final EP and this intermediate EP is yac_epa. Computing air_epa and yac_epa for incomplete passes is trickier and involves computing the difference in expected points between the final state, initial state, and an intermediate state that would result had the pass been caught.\n\n\nRushing Plays Data\nSuccess in the run game depends not only on the skill of the individual runner to run through the space created by his teammates and evade defenders but also on the ability of his teammates (usually linemen) to create space near the line and to make blocks down the field. Unfortunately, publicly available NFL play-by-play does not record the identities of all 22 players on the field nor does it include information about play (i.e., whether the play involved a pulling guard). It does, however, contain two variables, run_location, and run_gap that provide some context about the play.\nWe start by building a data table containing all rushing plays. As noted above, we will treat sacks as rushing plays.\n\nrush2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(\n    !grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc)) |&gt;\n  dplyr::filter(\n1    !grepl(\"No play\", desc, ignore.case = TRUE)) |&gt;\n  dplyr::filter(\n    play_type == \"run\" |\n    grepl(\"sack\", desc, ignore.case = TRUE)) |&gt;\n  dplyr::select(\n    game_id, play_id,\n    posteam, defteam, \n2    passer_player_id,\n    rusher_player_id,\n    posteam_type,\n    shotgun, no_huddle,\n    run_location, run_gap, \n    epa, desc) \n\n\n1\n\nRemove the plays that are whistled dead or involve penalties like roughing the passer or are otherwise classified as “No Play”\n\n2\n\nOn sacks, the quarterback is sometimes not classified as a runner but as passer\n\n\n\n\nThe data table rush2024 contains several of the same variables pass2024 including the identities of the offensive and defensive teams, the EPA, and the indicators shotgun, no_huddle, and posteam_type. It also includes the ID for the runner and their listed position and two variables, run_location and run_gap, that contain some contextual information about the run. Unfortunately, there are a lot of missing values of rusher_player_id, all of which correspond to sacks.\n\nsum(is.na(rush2024$rusher_player_id))\n\n[1] 1327\n\nsum(grepl(\"sack\", rush2024$desc, ignore.case = TRUE) & is.na(rush2024$rusher_player_id))\n\n[1] 1327\n\n\n\nHandling Sacks\nnflfastR treats sacks as passing plays and not running plays. However, because the ball is not thrown during a sack, there is no well-defined air_epa or yac_epa. We instead follow the example of Yurko, Ventura, and Horowitz (2019) and treat sacks as running plays. Unfortunately, for the vast majority of sacks, the rusher_player_id variable is not available.\n\nsacks &lt;-\n  rush2024 |&gt;\n  dplyr::filter(grepl(\"sack\", desc, ignore.case = TRUE))\nmean(is.na(sacks$rusher_player_id))\n\n[1] 0.999247\n\n\nInstead, the id of the player who was sacked — whom we wish to treat as the rusher in our models — is saved as passer_player_id. We first verify that all sacks with a missing rusher_player_id do not also have a missing passer_player_id\n\n1sum(is.na(sacks$rusher_player_id))\n2sum(!is.na(sacks$passer_player_id) & is.na(sacks$rusher_player_id))\n\n\n1\n\nNumber of sacks for which we are missing the rusher_player_id\n\n2\n\nNumber of sacks for which are missing the rusher_player_id but are not missing the passer_player_id\n\n\n\n\n[1] 1327\n[1] 1327\n\n\nThe following code overwrites the missing rusher_player_id values on sacks and then appends the rusher’s position.\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::mutate(\n    rusher_player_id = \n      dplyr::case_when(\n        is.na(rusher_player_id) & grepl(\"sack\", desc, ignore.case = TRUE) ~ passer_player_id,\n        .default = rusher_player_id)) |&gt;\n  dplyr::mutate(gsis_id = rusher_player_id) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, position), by = \"gsis_id\") |&gt;\n  dplyr::rename(rusher_position = position) |&gt;\n  dplyr::select(-gsis_id)\n\n\n\nRun Context\nThe data table rush2024 contains two variables that describe the run play. The first, run_location, records whether the run was towards the left, right or middle of the offensive line. To describe run_gap, we need to introduce a little bit of additional notation. Every offensive play in American football begins with 5 offensive players (the “linemen”) on the line of scrimmage. The center is responsible for snapping the ball to the quarterback — thereby starting the play — and is positioned in the middle of the line. Next to the center are the two guards and next to the guards are the two tackles. The spaces between the linemen are known as “gaps” and are conventionally labelled with letters: “A” gap between the center and guard; “B” gap between guard and tackle, and “C” gap outside the tackle. Sometimes, but not always, a tight end lines up next to one or both of the tackles. When that happens, the space between the tackle and end is labelled “C” gap and the space outside the tight end is known as “D” gap.\nAlthough the naming convention is very prevalent within football (i.e, amongs coaches and players), the NFL play-by-play data does not use this convention. Instead, the variable run_gap takes the value guard for runs through the “B” gap, tackle for runs through the “C” gap, and end for runs through the “D” gap. For runs through the “B”, “C”, or “D” gap, the variable run_location takes on the value “left” or “right”, to indicate which side of the line But for runs through the “A” gap, run_gap is NA and run_location is equal to “middle”.\n\ntable(rush2024[,c(\"run_gap\", \"run_location\")], useNA = 'always')\n\n        run_location\nrun_gap  left middle right &lt;NA&gt;\n  end    1958      0  1805    0\n  guard  1822      0  1739    0\n  tackle 1645      0  1628    0\n  &lt;NA&gt;      0   3565     0 1447\n\n\nWe will combine these two variables into a composite variable run_context. For runs where both run_gap and run_location are not NA, we will concatenate the three variables (e.g., \"left end\" or \"right tackle\"). For runs through the “A” gap (when run_location == \"middle\" and run_gap is NA), we will set the contextual variable to be \"middle\". It turns out that the remaining 1447 running plays with missing run_location and run_gap involve a sack or fumble (and sometimes both!)\n\n1tmp &lt;-\n  rush2024 |&gt;\n  dplyr::filter(is.na(run_gap) & is.na(run_location))\n\n2all(\n  grepl(\"sack\", tmp$desc, ignore.case = TRUE) | #&lt;3\n3    grepl(\"fumble\", tmp$desc, ignore.case = TRUE) )\n\n\n1\n\nPull out all the plays missing both run_gap and run_location.\n\n2\n\nall returns TRUE if all elements in a logical vector are true\n\n3\n\nCheck whether the strings “sack” or “fumble” appears in the play description.\n\n\n\n\n[1] TRUE\n\n\nFor these plays, we will set run_context == \"other\". We further combine run_context with the identity of the offensive team (i.e., posteam) to create what Yurko, Ventura, and Horowitz (2019) describe as a “proxy for the offensive linemen or blockers involved in a rushing attempt.”\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::mutate(\n    run_context = dplyr::case_when(\n      run_location == \"middle\" ~ paste(posteam, \"middle\", sep = \"_\"),\n      is.na(run_location) & is.na(run_gap) ~ paste(posteam, \"other\", sep = \"_\"),\n      .default = paste(posteam, run_gap, run_location, sep = \"_\")))\n\n\n\n\nAccounting for Team Strengths\nRecall one of the main drawbacks of plus/minus was its inability to account for the quality of the player’s teammates. When trying to construct a version of WAR for offensive players in the NFL, we need to make sure that we do not overly reward players who happen to play for good teams or overly penalize players who play on bad offenses. In our multilevel models, we will quantify each offense’ running and passing strength using the average EPA on rushing and passing plays.\n\npass_strength &lt;-\n  pass2024 |&gt;\n  dplyr::group_by(posteam) |&gt;\n  dplyr::summarise(pass_strength = mean(epa,na.rm = TRUE))\nrush_strength &lt;-\n  rush2024 |&gt;\n  dplyr::group_by(posteam) |&gt;\n  dplyr::summarise(rush_strength = mean(epa, na.rm = TRUE))\n\n\npass2024 &lt;-\n  pass2024 |&gt;\n  dplyr::inner_join(pass_strength, by = \"posteam\") |&gt;\n  dplyr::inner_join(rush_strength, by = \"posteam\")\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::inner_join(pass_strength, by = \"posteam\") |&gt;\n  dplyr::inner_join(rush_strength, by = \"posteam\")",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-passing-models",
    "href": "lectures/lecture10.html#sec-passing-models",
    "title": "Lecture 10: NFl WAR",
    "section": "Passing Models",
    "text": "Passing Models\n\nConstructing Composite Outcomes\nWe’re now ready to determine how much individual players contribute to their team’s overall EPA on passing plays. While it is tempting to fit separate multilevel models to predict the values in the air_epa and yac_epa, we need to be a bit more careful in how we deal with incomplete passes. Specifically, on an incomplete pass, there is necessarily no run after the catch and any change in EP is driven entirely by what happens while the ball is in the air. To account for this, we will define two composite outcomes that are based on air_epa, yac_epa, and epa.\nFormally, say that there are \\(n\\) passing plays and for passing play \\(i,\\) let \\(\\Delta_{i}\\) be the observed EPA on the play. Let \\(\\delta_{i,\\textrm{air}}\\) and \\(\\delta_{i,\\textrm{yac}}\\) be the values of air_epa and yac_epa provided by nflfastR. We will decompose \\(\\Delta_{i} = \\Delta_{i,\\textrm{air}} + \\Delta_{i,\\textrm{yac}}\\) where \\[\n\\Delta_{i, \\textrm{air}} =\n\\begin{cases}\n\\delta_{i, \\textrm{air}} & \\textrm{if the pass is caught} \\\\\n\\Delta_{i} & \\textrm{if the pass is incomplete,}\n\\end{cases}\n\\] and \\[\n\\Delta_{i, \\textrm{yac}} =\n\\begin{cases}\n\\delta_{i, \\textrm{yac}} & \\textrm{if the pass is caught} \\\\\n0 & \\textrm{if the pass is incomplete.}\n\\end{cases}\n\\] The following code creates columns for these composite outcomes and also converts categorical predictors like defensive team and receiver position into factor variables.\n\npass2024 &lt;-\n  pass2024 |&gt;\n  dplyr::mutate(\n    Delta_air = ifelse(complete_pass == 1, air_epa, epa),\n    Delta_yac = ifelse(complete_pass == 1, yac_epa, 0),\n    passer_player_id = factor(passer_player_id),\n    receiver_player_id = factor(receiver_player_id),\n    posteam = factor(posteam),\n    defteam = factor(defteam),\n    pass_location = factor(pass_location),\n    receiver_position = factor(receiver_position))\n\n\n\nModeling \\(\\Delta_{\\textrm{air}}\\)\nOur first multilevel model, fitted to data from all passing plays, includes separate random intercepts for the passers, receivers, and defenses. It also accounts for the fixed effects of * air_yards: the distance traveled by the pass through the air * shotgun: whether the pass was thrown out of the shotgun formation * qb_hit: whether the quarterback was hit while throwing * no_huddle: whether the play was run without a huddle * posteam_type: whether the home team had possession (i.e., was on offense) * pass_location: whether the pass was thrown to the left, right, or center of the field * receiver_position: the position of the intended receiver * rush_strength: the average EPA per rush attempt for the offense\nBefore writing down our model, we introduce some more notation. Formally, suppose that there are \\(n_{q}\\) passers1, \\(n_{c}\\) receivers2, and \\(n_{D}\\) defenses. For passing play \\(i,\\) let \\(q[i], c[i],\\) and \\(d[i]\\) denote the identity of the passer, receiver, and defense involved in the play and let \\(\\boldsymbol{\\mathbf{x}}_{i}\\) be the vector of fixed effect covariates (i.e., the ones listed above).\nWe model \\[\n\\begin{align}\n\\Delta_{i, \\textrm{air}} &= Q_{q[i]} + C_{c[i]} + D_{d[i]} + \\boldsymbol{\\mathbf{x}}_{i}^{\\top}\\boldsymbol{\\beta} + \\epsilon_{i} \\quad \\text{for all passes}\\ i = 1, \\ldots, n \\\\\nQ_{q} &\\sim N(\\mu_{Q}, \\sigma^{2}_{Q}) \\quad \\textrm{for all passers}\\ q = 1, \\ldots, n_{Q} \\\\\nC_{c} &\\sim N(\\mu_{C}, \\sigma^{2}_{C}) \\quad \\textrm{for all pass catchers}\\ c = 1, \\ldots, n_{C},  \\\\\nD_{d} &\\sim N(\\mu_{D}, \\sigma^{2}_{D}) \\quad \\textrm{for all defenseive teams}\\ d = 1, \\ldots n_{D}.\n\\end{align}\n\\]\nWe can fit this model with lme4::lmer() using similar notation as was used in Lecture 9. Just like in that lecture, we can read off lots of important information and parameter estimates from the model summary. For instance, there is much more variability in the receiver effects than the passer or defense effects.\n\nlibrary(lme4)\n\n\n1\n\nWe want random intercepts for the passer, receiver, and defensive team\n\n2\n\nSpecify the fixed effects\n\n\n\n\nLoading required package: Matrix\n\nair_model &lt;-\n1  lmer(Delta_air ~ 1 + (1|passer_player_id) + (1 | receiver_player_id) + (1 | defteam )\n2       + air_yards + shotgun + qb_hit + no_huddle + posteam_type + pass_location + receiver_position + rush_strength,\n       data = pass2024)\nsummary(air_model)\n\n\nCorrelation matrix not shown by default, as p = 16 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Delta_air ~ 1 + (1 | passer_player_id) + (1 | receiver_player_id) +  \n    (1 | defteam) + air_yards + shotgun + qb_hit + no_huddle +  \n    posteam_type + pass_location + receiver_position + rush_strength\n   Data: pass2024\n\nREML criterion at convergence: 56640.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-9.6663 -0.4475 -0.0140  0.4521  4.2381 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n receiver_player_id (Intercept) 0.011544 0.10745 \n passer_player_id   (Intercept) 0.002242 0.04735 \n defteam            (Intercept) 0.001354 0.03680 \n Residual                       1.622752 1.27387 \nNumber of obs: 17000, groups:  \nreceiver_player_id, 492; passer_player_id, 99; defteam, 32\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)         -2.642055   0.573232  -4.609\nair_yards            0.035027   0.001077  32.537\nshotgun             -0.081497   0.028132  -2.897\nqb_hit              -0.276667   0.036182  -7.647\nno_huddle            0.134666   0.029805   4.518\nposteam_typehome    -0.008663   0.019728  -0.439\npass_locationmiddle  0.172170   0.026424   6.516\npass_locationright  -0.034797   0.022336  -1.558\nreceiver_positionDL  4.627420   1.401649   3.301\nreceiver_positionLB  3.750986   1.070748   3.503\nreceiver_positionOL  2.527147   0.729736   3.463\nreceiver_positionQB  3.661843   0.934966   3.917\nreceiver_positionRB  1.922465   0.573394   3.353\nreceiver_positionTE  2.279044   0.573261   3.976\nreceiver_positionWR  2.254500   0.573040   3.934\nrush_strength        0.307995   0.118268   2.604\n\n\nFor each passer \\(q,\\) \\(Q_{q}\\) is a latent3 quantity measuring how many expected points passer \\(q\\) adds through the air, after you account for the fixed effects and also the quality of the intended receiver and the defense. The quantity \\(\\mu_{Q}\\) represents the global average of these latent quantities. Similarly, \\(C_{c}\\) is the latent measurement of how many expected points receiver \\(c\\) adds while the pass is in the air4 after accounting for the fixed effects and the quality of the passer and of the opposing defense. And \\(D_{d}\\) is the latent measurement of how many expected points are added for the offensive team by defensive team \\(d\\) while the pass is in the air. Since EPA is signed so that positive values are considered good for the offense, large negative values of \\(D_{d}\\) are treated as good for the defense. In our model, the random intercepts \\(Q_{q}, C_{c},\\) and \\(D_{d}\\) are all modeled as noisy deviations away from league-wise averages \\(\\mu_{Q}, \\mu_{C},\\) and \\(\\mu_{D}.\\)\nUnfortunately, we cannot estimate \\(\\mu_{Q}, \\mu_{C},\\) and \\(\\mu_{D}\\) because these parameters are not identified. Without going into too many technical details5, there are infinitely many settings of the values of these three parameters that yield exactly the same fit to the data. Because we cannot estimate \\(\\mu_{Q}, \\mu_{C},\\) and \\(\\mu_{D},\\) we are unable to obtain estimates for the random intercepts \\(Q_{q}, C_{c},\\) and \\(D_{d}\\) like we did in Lecture 9. We are, however, able to estimate the deviations \\(Q_{q} - \\mu_{Q},\\) \\(C_{c} - \\mu_{C},\\) and \\(D_{d} - \\mu_{D}.\\) For each passer \\(q\\), the quantity \\(Q_{q} - \\mu_{Q}\\) measures many more expected points per play passer \\(q\\) adds through the air than the league average. Following Yurko, Ventura, and Horowitz (2019), we will refer to this number as passer \\(q\\)’s individual points added over average or \\(\\textrm{IPA}^{(Q)}_{\\textrm{air},q}\\)6. We can similarly define \\(\\textrm{IPA}^{(C)}_{\\textrm{air}, c}\\) for receivers. For each defense \\(d,\\) let \\(D_{d} - \\mu_{D}\\) is the number of points per passing play over league average given up by the defensive team while the bar is in the air. We will denote this quantity by \\(\\textrm{TPA}^{(D)}_{\\textrm{air},d},\\) with negative \\(\\textrm{TPA}\\) values corresponding to better defenses.\nWe can extract the IPA and TPA values using the function ranef().\n\n1tmp_air &lt;- ranef(air_model)\n\nair_passer_effects &lt;-\n  data.frame(\n2    gsis_id = rownames(tmp_air[[\"passer_player_id\"]]),\n    ipa_air_pass = tmp_air[[\"passer_player_id\"]][,1])\nair_receiver_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_air[[\"receiver_player_id\"]]),\n    ipa_air_rec = tmp_air[[\"receiver_player_id\"]][,1])\nair_defense_effects &lt;-\n  data.frame(\n    Team = rownames(tmp_air[[\"defteam\"]]),\n    tpa_air_def = tmp_air[[\"defteam\"]][,1])\n\n\n1\n\nExtract the passer, receiver, and defensive random intercepts in a large list\n\n2\n\nBecause we will eventually append player names by inner_join()’ing with roster2024, we’ll record the passer IDs in a column called gsis_id.\n\n\n\n\nTaking a quick look at the top-10 (resp. bottom-10) IPA passer values, we see a number of highly regarded (resp. heavily criticized) quarterbacks.\n\nair_passer_effects |&gt;\n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::select(full_name, ipa_air_pass) |&gt;\n  dplyr::arrange(dplyr::desc(ipa_air_pass)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n                  full_name ipa_air_pass\n1                Joe Burrow   0.06736333\n2               Sam Darnold   0.04033663\n3             Lamar Jackson   0.03292716\n4            Tua Tagovailoa   0.03281935\n5               Brock Purdy   0.03000549\n6                 Drew Lock  -0.02737783\n7                    Bo Nix  -0.02808707\n8  Dorian Thompson-Robinson  -0.03832239\n9           Spencer Rattler  -0.05005097\n10       Anthony Richardson  -0.07074920\n\n\n\n\nModeling \\(\\Delta_{\\textrm{yac}}\\)\nBy fitting a similar multilevel model to predict the \\(\\Delta_{i, \\textrm{yac}}\\)’s, we can estimate how much each passer, receiver, and defense contributes to EPA during the part of a passing play following a catch. Because these contributions can, by definition, only be made following a completed pass, we fit this model using only the data from completed pass7.\n\ncompletions &lt;-\n  pass2024 |&gt;\n  dplyr::filter(complete_pass==1)\n\nyac_model &lt;-\n  lmer(Delta_yac ~ 1 + (1|passer_player_id) + (1 | receiver_player_id) + (1 | defteam ) \n       + air_yards + shotgun + qb_hit + no_huddle + posteam_type + pass_location + receiver_position + rush_strength, \n       data = completions)\n\nboundary (singular) fit: see help('isSingular')\n\n\nWhile fitting the model, lmer printed a message saying that the fit was singular. The manual page for isSingular, which the message instructs us to visit, contains lots of information about singular models. Briefly, when fitting complicated multilevel models with lots of random effects, it can sometimes be the case that some of the “between” group variances are estimated to be exactly zero. Quoting from the manual &gt; While singular models are statistically well-defined … there are real concerns that (1) singular fits correspond to overfitted models that may have poor power; (2) chances of numerical problems or mis-convergence are higher for singular models … ; (3) standard inferential procedures such as Wald statistics and likelihood ratio tests may be inappropriate.\nTaking a look at the model summary, we see that the passer-to-passer variation has been set to zero.\n\nsummary(yac_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Delta_yac ~ 1 + (1 | passer_player_id) + (1 | receiver_player_id) +  \n    (1 | defteam) + air_yards + shotgun + qb_hit + no_huddle +  \n    posteam_type + pass_location + receiver_position + rush_strength\n   Data: completions\n\nREML criterion at convergence: 32367.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.9533 -0.5437 -0.2533  0.2882  7.4637 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev.\n receiver_player_id (Intercept) 0.0190108 0.1379  \n passer_player_id   (Intercept) 0.0000000 0.0000  \n defteam            (Intercept) 0.0006814 0.0261  \n Residual                       0.9296857 0.9642  \nNumber of obs: 11627, groups:  \nreceiver_player_id, 468; passer_player_id, 90; defteam, 32\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)          1.710173   0.689922   2.479\nair_yards           -0.022666   0.001214 -18.675\nshotgun             -0.046899   0.025546  -1.836\nqb_hit               0.037119   0.039172   0.948\nno_huddle           -0.174386   0.027107  -6.433\nposteam_typehome    -0.007157   0.018083  -0.396\npass_locationmiddle -0.057884   0.024339  -2.378\npass_locationright   0.011441   0.020532   0.557\nreceiver_positionDL -1.567487   1.193518  -1.313\nreceiver_positionLB -0.728621   0.974493  -0.748\nreceiver_positionOL -1.046652   0.781368  -1.340\nreceiver_positionQB -0.728794   0.974704  -0.748\nreceiver_positionRB -0.515251   0.689574  -0.747\nreceiver_positionTE -0.837277   0.689576  -1.214\nreceiver_positionWR -0.847828   0.689446  -1.230\nrush_strength        0.382583   0.105680   3.620\n\n\n\nCorrelation matrix not shown by default, as p = 16 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nWe can further confirm that this is the case by checking whether all the estimated deviation between each passer’s random intercept and the global mean across all passers (i.e., all \\(\\textrm{IPA}^{(Q)}_{\\textrm{yac},q})\\)’s) are equal to zero.\n\n1all(ranef(yac_model)[[\"passer_player_id\"]][,1] == 0)\n\n\n1\n\nChecks whether all of the passer IPA values are equal to 0.\n\n\n\n\n[1] TRUE\n\n\nThis phenomenon makes some intuitive sense: once the catch is made, there is very little for the passer, who is typically several yards away, to contribute. The manual entry for isSingular() goes on to say that “There is not yet consensus about how to deal with singularity, or more generally to choose which random-effects specifications (from a range of choices of varying complexity to use)”8 and suggests that one option is to “avoid fitting overly complex models in the first place.” So, at this point we have a choice: either we move forward with our model or we re-fit it without the passer random intercepts.\nRecall that we ultimately plan to aggregate the IPA from air yards, YAC, and rushing for all offensive players. If we move forward with our existing model, the YAC IPA values for passers are already set to zero. But if we re-fit the model, we will have to manually set these values to zero. To avoid this extra step, we’ll continue to use our current9.\n\ntmp_yac &lt;- ranef(yac_model)\n\nyac_passer_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_yac[[\"passer_player_id\"]]), \n    ipa_yac_pass = tmp_yac[[\"passer_player_id\"]][,1])\nyac_receiver_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_yac[[\"receiver_player_id\"]]),\n    ipa_yac_rec = tmp_yac[[\"receiver_player_id\"]][,1])\nyac_defense_effects &lt;-\n  data.frame(\n    Team = rownames(tmp_yac[[\"defteam\"]]),\n    tpa_yac_def = tmp_yac[[\"defteam\"]][,1])\n\nAmong the receivers with the largest \\(\\textrm{IPA}_{\\textrm{YAC}, c}^{(C)}\\) values, we see players like Ja’Marr Chase, Khalil Shakir, and Brock Bowers who led the league in yards after the catch during the 2024 regular season.\n\nyac_receiver_effects |&gt;\n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::arrange(dplyr::desc(ipa_yac_rec)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::select(full_name, ipa_yac_rec)\n\n         full_name ipa_yac_rec\n1  Marvin Mims Jr.   0.4038113\n2    Khalil Shakir   0.2763021\n3  KaVontae Turpin   0.2225070\n4    Xavier Worthy   0.2179003\n5     Tucker Kraft   0.2141390\n6     Brock Bowers   0.1885809\n7    Austin Ekeler   0.1830305\n8   Antonio Gibson   0.1818104\n9    Ja'Marr Chase   0.1778265\n10  Raheem Mostert   0.1729994",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-rush-model",
    "href": "lectures/lecture10.html#sec-rush-model",
    "title": "Lecture 10: NFl WAR",
    "section": "Rushing Model",
    "text": "Rushing Model\nAs Yurko, Ventura, and Horowitz (2019) notes, we cannot distinguish between designed quarterback runs from scrambles on broken plays using publicly available play-by-play data. So, we will fit two separate models of EPA for running plays, one for all quarterback runs (i.e., rusher_position==\"QB\") and one for all non-quarterback runs (i.e., rusher_position != \"QB\"). Before creating separate data tables for the two types of running plays, we convert several categorical covariates to factors.\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::mutate(\n    rusher_player_id = factor(rusher_player_id),\n    posteam = factor(posteam),\n    defteam = factor(defteam),\n    posteam_type = factor(posteam_type),\n    rusher_position == factor(rusher_position),\n    run_context = factor(run_context))\n\nqb_runs &lt;-\n  rush2024 |&gt;\n  dplyr::filter(rusher_position == \"QB\")\n\nnonqb_runs &lt;-\n  rush2024 |&gt;\n  dplyr::filter(rusher_position != \"QB\")\n\nWe now fit multilevel models to predict the EPA on running plays for both QB-runs and non-QB-runs. Like with our other models, we extract the number of expected points per play each player adds over the league average via their running.\n\nqbrun_fit &lt;-\n  lmer(epa ~ 1 + (1 | rusher_player_id) + (1 | defteam) +\n1         shotgun + no_huddle + posteam_type + pass_strength,\n       data = qb_runs)\nnonqb_run_fit &lt;-\n  lmer(epa ~ 1 + (1 | rusher_player_id) + (1 | defteam) +  \n2         shotgun + no_huddle + posteam_type + rusher_position + run_context + pass_strength,\n       data = nonqb_runs)\n\n\n1\n\nFor the quarterback run model, there is no need to include rusher_position since every rusher in the dataset plays the same position. We also exclude run_context since many of the observations in this data table are not designed runs.\n\n2\n\nFor the non-quarterback run model, we adjust for the runner’s position and the run context\n\n\n\n\nfixed-effect model matrix is rank deficient so dropping 2 columns / coefficients\n\ntmp_qbrun &lt;- ranef(qbrun_fit)\nqbrun_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_qbrun[[\"rusher_player_id\"]]), \n    ipa_qbrun = tmp_qbrun[[\"rusher_player_id\"]][,1])\n\ntmp_run &lt;- ranef(nonqb_run_fit) \nrun_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_run[[\"rusher_player_id\"]]),\n    ipa_run = tmp_run[[\"rusher_player_id\"]][,1])\n\nTaking a look at the top non-QB IPA values, we see several highly-regarded running backs\n\nrun_effects |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::select(full_name, ipa_run) |&gt;\n  dplyr::arrange(dplyr::desc(ipa_run)) |&gt;\n  dplyr::slice_head(n = 10)\n\n         full_name    ipa_run\n1    De'Von Achane 0.13626662\n2   Saquon Barkley 0.10131412\n3     Jahmyr Gibbs 0.09383069\n4      Jerome Ford 0.08473154\n5    Chuba Hubbard 0.08423388\n6     J.K. Dobbins 0.07766485\n7  Emari Demercado 0.07656891\n8    Dameon Pierce 0.06755530\n9      Rico Dowdle 0.06517368\n10     Taysom Hill 0.06219509",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-ipaa",
    "href": "lectures/lecture10.html#sec-ipaa",
    "title": "Lecture 10: NFl WAR",
    "section": "Total Individual Points Added Above Average",
    "text": "Total Individual Points Added Above Average\nFor each offensive player, we have estimates for the number of expected points they add per play through the air, after the catch, and through their running. By multiplying each player’s IPA by the number of times they attempted the corresponding play, we can compute each player’s individual points added above average or IPAA for every type of play.\nFor instance, let’s start with quarterbacks and how they generate EPA through the air on passing plays. Our modeling suggests that Joe Burrow created 42 more points of EP for his team through the air than a league-average quarterback would have. Burrow’s IPAA of 42 points through the air is substantially larger than the next best quarterbacks (Darnold and Jackson). Recall from Section 3.3 that every passer has an IPA of zero based on their contributions after the catch is made.\n\npasser_ipaa &lt;-\n  pass2024 |&gt;\n1  dplyr::group_by(passer_player_id) |&gt;\n  dplyr::summarise(n_pass = dplyr::n()) |&gt;\n2  dplyr::rename(gsis_id = passer_player_id) |&gt;\n3  dplyr::left_join(air_passer_effects, by = \"gsis_id\") |&gt;\n4  dplyr::left_join(yac_passer_effects, by = \"gsis_id\") |&gt;\n  dplyr::mutate(\n    ipaa_air_pass = ipa_air_pass * n_pass,\n    ipaa_yac_pass = ipa_yac_pass * n_pass) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\npasser_ipaa |&gt;\n  dplyr::select(full_name, n_pass, ipaa_air_pass, ipaa_yac_pass) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_air_pass)) |&gt;\n  dplyr::slice_head(n = 10)\n\n\n1\n\nCompute the number of passing attempts for each passer\n\n2\n\nRename the column with passer ID so that we can append their IPA values\n\n3\n\nAppend the IPA values for all passers\n\n4\n\nCompute the IPAA by multiplying number of attempts by IPA\n\n\n\n\n# A tibble: 10 × 4\n   full_name      n_pass ipaa_air_pass ipaa_yac_pass\n   &lt;chr&gt;           &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Joe Burrow        628         42.3              0\n 2 Sam Darnold       512         20.7              0\n 3 Lamar Jackson     454         14.9              0\n 4 Brock Purdy       440         13.2              0\n 5 Tua Tagovailoa    388         12.7              0\n 6 Justin Herbert    484         12.7              0\n 7 Baker Mayfield    562         11.1              0\n 8 C.J. Stroud       509          8.45             0\n 9 Russell Wilson    306          7.07             0\n10 Geno Smith        554          6.07             0\n\n\nWe can repeat these calculations for receivers on passing plays\n\nreceiver_ipaa &lt;-\n  pass2024 |&gt;\n  dplyr::group_by(receiver_player_id) |&gt;\n  dplyr::summarise(n_rec = dplyr::n()) |&gt; \n  dplyr::rename(gsis_id = receiver_player_id) |&gt; \n  dplyr::left_join(air_receiver_effects, by = \"gsis_id\") |&gt; \n  dplyr::left_join(yac_receiver_effects, by = \"gsis_id\") |&gt;\n  dplyr::mutate(\n   ipaa_air_rec = ipa_air_rec * n_rec,\n   ipaa_yac_rec = ipa_yac_rec * n_rec) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\nHere are the top 10 IPAA values for receivers based on their contributions while the ball is in the air:\n\nreceiver_ipaa |&gt;\n  dplyr::select(full_name, n_rec, ipaa_air_rec, ipaa_yac_rec) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_air_rec)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 4\n   full_name         n_rec ipaa_air_rec ipaa_yac_rec\n   &lt;chr&gt;             &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Tee Higgins         109         19.9       -8.07 \n 2 Amon-Ra St. Brown   141         19.6        7.93 \n 3 Ladd McConkey       112         16.5       -1.38 \n 4 Terry McLaurin      117         13.7       -7.16 \n 5 DeVonta Smith        89         12.7       -3.41 \n 6 Jauan Jennings      113         12.6      -13.3  \n 7 Jakobi Meyers       129         12.0        0.373\n 8 Justin Jefferson    153         11.4        8.66 \n 9 Courtland Sutton    135         11.0      -21.1  \n10 Jonnu Smith         111         10.1       18.4  \n\n\nAnd here are the top 10 IPAA values for receivers based on their contributions following a catch:\n\nreceiver_ipaa |&gt;\n  dplyr::select(full_name, n_rec, ipaa_air_rec, ipaa_yac_rec) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_yac_rec)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 4\n   full_name        n_rec ipaa_air_rec ipaa_yac_rec\n   &lt;chr&gt;            &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Ja'Marr Chase      175         4.84         31.1\n 2 Brock Bowers       153        -4.53         28.9\n 3 Khalil Shakir      100       -10.3          27.6\n 4 DJ Moore           140       -14.4          22.5\n 5 Brian Thomas Jr.   133        -2.92         22.4\n 6 Xavier Worthy       98       -10.7          21.4\n 7 Marvin Mims Jr.     52        -7.46         21.0\n 8 Jonnu Smith        111        10.1          18.4\n 9 Puka Nacua         106         4.33         16.4\n10 Josh Downs         107        -1.95         15.2\n\n\nIt is particularly interesting to note that there are several receivers whose contributions while the ball in the air are worse than league average but who are better than league average once they catch the ball.\nWe repeat this calculation for the IPA values arising from yards after the catch and for both types of running play.\n\nqbrun_ipaa &lt;-\n  qb_runs |&gt;\n  dplyr::group_by(rusher_player_id) |&gt;\n  dplyr::summarise(n_qbrun = dplyr::n()) |&gt;\n  dplyr::rename(gsis_id = rusher_player_id) |&gt;\n  dplyr::left_join(y = qbrun_effects, by = \"gsis_id\") |&gt;\n  dplyr::mutate(ipaa_qbrun = n_qbrun * ipa_qbrun) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\nrun_ipaa &lt;-\n  nonqb_runs |&gt;\n  dplyr::group_by(rusher_player_id) |&gt;\n  dplyr::summarise(n_run = dplyr::n()) |&gt;\n  dplyr::rename(gsis_id = rusher_player_id) |&gt;\n  dplyr::left_join(y = run_effects, by = \"gsis_id\") |&gt;\n  dplyr::mutate(ipaa_run = n_run * ipa_run) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\n\nrun_ipaa |&gt;\n  dplyr::select(full_name, ipaa_run) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_run)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   full_name      ipaa_run\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Saquon Barkley     35.1\n 2 De'Von Achane      27.7\n 3 Jahmyr Gibbs       23.5\n 4 Chuba Hubbard      21.1\n 5 Derrick Henry      16.9\n 6 Rico Dowdle        15.3\n 7 J.K. Dobbins       15.1\n 8 Bijan Robinson     14.5\n 9 Najee Harris       12.7\n10 Chase Brown        11.6\n\n\n\nReplacement Level for Skill Positions\nAlthough the IPAA values are interesting and informative, they represent a comparison between players and a nebulously defined average. Recall that IPAA is just a scaled version of IPA, which we defined to be the difference between the player-specific random intercept and a global mean parameter in our multilevel model. That mean does not represent the average of all observed players in the league. Instead, it captures the average intercept across a theoretical, infinite super-population of players from which we assume the set of observed players is sampled. So, it is not strictly correct to say that IPA measures a player’s expected contribution over and above a league-average player’s contribution on a per-play basis.\nTo facilitate comparisons that are easier to interpret, we now translate the IPAA values into measures of a player’s contribution relative to a replacement-level player. As discussed in the context of baseball, there are myriad ways to define what replacement level. Like we did in Lecture 8, we take a roster-based approach and define different replacement levels for different combinations of position and our EPA models. Noting that most NFL teams carry 3 running backs (RBs), 4 wide receivers (WRs), and 2 tight ends (TE), we define replacement levels based o on the following thresholds:\n\nWRs on passing plays: The top \\(32 \\times 4 = 128\\) WRs sorted by receiving attempts (i.e., n_rec in pass_ipaa) are non-replacement-level; everyone else is replacement-level\nTEs on passing plays: The top \\(32 \\times 2 = 64\\) TEs sorted by receiving attempts are non-replacement-level; everyone else is replacement-level\nRBs on passing plays: The top \\(32 \\times 3 = 96\\) RBs sorted by receiving attempts are non-replacement-level; everyone else is replacement-level\nWR/TEs on running plays: The top \\(32 \\times 1 = 32\\) WR and TEs sorted by rushing attempts are non-replacement-level; everyone else is replacement-level\nRBs on running plays: The top \\(32 \\times 3 = 96\\) RBs sorted by rusing attempts are non-replacement-level; everyone else is replacement-level.\n\nThese definitions allow for the possibility that a RB may be non-replacement-level based on his running but is at or below replacement-level when it comes to receiving. Like we did in Lecture 8, for every player we can compare his IPAA to the IPAA that a replacement level player would achieve if given the same number of attempts. To compute this “shadow” IPAA, we will multiply the number of attempts by the average IPA value across all corresponding replacement-level players.\nThe following code computes the receiving attempt thresholds for each position on passing plays. It then computes the replacement-level IPA\n\nwr_pass_threshold &lt;- \n  receiver_ipaa |&gt;\n  dplyr::filter(position == \"WR\") |&gt;\n  dplyr::arrange(desc(n_rec)) |&gt;\n1  dplyr::slice(128) |&gt;\n  dplyr::pull(n_rec)\n\nte_pass_threshold &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(position == \"TE\") |&gt;\n  dplyr::arrange(desc(n_rec)) |&gt;\n  dplyr::slice(64) |&gt;\n  dplyr::pull(n_rec)\n\nrb_pass_threshold &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(position == \"RB\") |&gt;\n  dplyr::arrange(desc(n_rec)) |&gt;\n  dplyr::slice(32) |&gt;\n  dplyr::pull(n_rec)\n\n\n1\n\nPlayers with fewer passing plays these value will be considered replacement-level at their position\n\n\n\n\nWe add columns to receiver_ipaa indicating whether each player is considered to be replacement-level at his position or not.\n\nreceiver_ipaa &lt;-\n  receiver_ipaa |&gt;\n  dplyr::mutate(\n    repl_wr = dplyr::case_when(\n      position == \"WR\" & n_rec &lt; wr_pass_threshold ~ 1,\n      position == \"WR\" & n_rec &gt;= wr_pass_threshold ~ 0,\n      position != \"WR\" ~ NA),\n    repl_te = dplyr::case_when(\n      position == \"TE\" & n_rec &lt; te_pass_threshold ~ 1,\n      position == \"TE\" & n_rec &gt;= te_pass_threshold ~ 0,\n      position != \"TE\" ~ NA),\n    repl_rb = dplyr::case_when(\n      position == \"RB\" & n_rec &lt; rb_pass_threshold ~ 1,\n      position == \"RB\" & n_rec &gt;= rb_pass_threshold ~ 0,\n      position != \"RB\" ~ NA))\n\nWe now average the IPA across all replacement-level players at each position in both phases of a passing play\n\nrepl_wr_ipa_air &lt;- \n  receiver_ipaa |&gt;\n  dplyr::filter(repl_wr == 1) |&gt;\n  dplyr::pull(ipa_air_rec) |&gt;\n  mean(na.rm = TRUE)\nrepl_wr_ipa_yac &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(repl_wr == 1) |&gt;\n  dplyr::pull(ipa_yac_rec) |&gt;\n  mean(na.rm = TRUE)\n\nrepl_te_ipa_air &lt;- \n  receiver_ipaa |&gt;\n  dplyr::filter(repl_te == 1) |&gt;\n  dplyr::pull(ipa_air_rec) |&gt;\n  mean(na.rm=TRUE)\nrepl_te_ipa_yac &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(repl_te == 1) |&gt;\n  dplyr::pull(ipa_yac_rec) |&gt;\n  mean(na.rm = TRUE)\n\nrepl_rb_ipa_air &lt;- \n  receiver_ipaa |&gt;\n  dplyr::filter(repl_rb == 1) |&gt;\n  dplyr::pull(ipa_air_rec) |&gt;\n  mean(na.rm = TRUE)\nrepl_rb_ipa_yac &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(repl_rb == 1) |&gt;\n  dplyr::pull(ipa_yac_rec) |&gt;\n  mean(na.rm = TRUE)\n\nFinally, we compute the difference between every player’s actual IPAA and their replacement-level shadow IPAA value, to obtain their individual points added over replacement (or IPAR):\n\nreceiver_ipar &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(position %in% c(\"WR\",\"RB\", \"TE\")) |&gt;\n  dplyr::mutate(\n    shadow_air_rec = dplyr::case_when(\n      position == \"WR\" ~ n_rec * repl_wr_ipa_air,\n      position == \"TE\" ~ n_rec * repl_te_ipa_air,\n      position == \"RB\" ~ n_rec * repl_rb_ipa_air),\n    shadow_yac_rec = dplyr::case_when(\n      position == \"WR\" ~ n_rec * repl_wr_ipa_yac,\n      position == \"TE\" ~ n_rec * repl_te_ipa_yac,\n      position == \"RB\" ~ n_rec * repl_rb_ipa_yac),\n    ipar_air_rec = ipaa_air_rec - shadow_air_rec,\n    ipar_yac_rec = ipaa_yac_rec - shadow_yac_rec)\n\nWe perform similar calculations to compute the IPAR value from running plays for each non-QB player:\n\n\nCode\nwrte_run_threshold &lt;-\n  run_ipaa |&gt;\n  dplyr::filter(position %in% c(\"WR\", \"TE\")) |&gt;\n  dplyr::arrange(desc(n_run)) |&gt;\n  dplyr::slice(32) |&gt;\n  dplyr::pull(n_run)\n\nrb_run_threshold &lt;-\n  run_ipaa |&gt;\n  dplyr::filter(position == \"RB\") |&gt;\n  dplyr::arrange(desc(n_run)) |&gt;\n  dplyr::slice(96) |&gt;\n  dplyr::pull(n_run)\n\n\nrun_ipaa &lt;-\n  run_ipaa |&gt;\n  dplyr::mutate(\n    repl_wrte = dplyr::case_when(\n      position %in% c(\"WR\", \"TE\") & n_run &lt; wrte_run_threshold ~ 1,\n      position %in% c(\"WR\", \"TE\") & n_run &gt;= wrte_run_threshold ~ 0,\n      !position %in% c(\"WR\", \"TE\") ~ NA),\n    repl_rb = dplyr::case_when(\n      position == \"RB\" & n_run &lt; rb_run_threshold ~ 1,\n      position == \"RB\" & n_run &gt;= rb_run_threshold ~ 0,\n      position != \"RB\" ~ NA))\n\n\n\nrepl_wrte_ipa_run &lt;- \n  run_ipaa |&gt;\n  dplyr::filter(repl_wrte == 1) |&gt;\n  dplyr::pull(ipa_run) |&gt;\n  mean(na.rm = TRUE)\nrepl_rb_ipa_run &lt;-\n  run_ipaa |&gt;\n  dplyr::filter(repl_rb == 1) |&gt;\n  dplyr::pull(ipa_run) |&gt;\n  mean(na.rm = TRUE)\n\n\nrun_ipar &lt;-\n  run_ipaa |&gt;\n  dplyr::filter(position %in% c(\"WR\", \"RB\", \"TE\")) |&gt;\n   dplyr::mutate(\n    shadow_run = dplyr::case_when(\n      position %in% c(\"WR\", \"TE\") ~ n_run * repl_wrte_ipa_run,\n      position == \"RB\" ~ n_run * repl_rb_ipa_run),\n    ipar_run = ipaa_run - shadow_run)\n\n\n\n\nComputing IPAR for QB’s\nWe defined replacement-levels for running backs, wide receivers, and tight ends based on typical patterns of roster construction. Due to the unique nature of the quarterback position — virtually every offensive play involves runs through the quarterback — we need to define “replacement” level somewhat differently. One option would be to create a group of 32 non-replacement quarterbacks by identifying the quarterback one each team who were involved in the most passing and rushing attempts. While simple to implement, this implicitly assumes that every NFL team has at least one non-replacement quarterback. Instead, we sort quarterbacks by the total number of passing and rushing plays in which they were involved and designate the top-32 to be non-replacement level.\n\nqb_ipaa &lt;-\n  passer_ipaa |&gt;\n  dplyr::filter(position == \"QB\") |&gt;\n  dplyr::select(gsis_id, full_name, n_pass, ipa_air_pass, ipaa_air_pass) |&gt;\n  dplyr::left_join(y = qbrun_ipaa |&gt; dplyr::select(gsis_id, n_qbrun, ipa_qbrun, ipaa_qbrun), by = \"gsis_id\") |&gt;\n  dplyr::mutate(n_plays = n_pass + n_qbrun)\n\nqb_threshold &lt;-\n  qb_ipaa |&gt;\n  dplyr::arrange(dplyr::desc(n_plays)) |&gt;\n  dplyr::slice(32) |&gt;\n  dplyr::pull(n_plays)\n\nqb_ipaa &lt;-\n  qb_ipaa |&gt;\n  dplyr::mutate(repl_qb = ifelse(n_plays &lt; qb_threshold, 1, 0))\n\nrepl_qb_ipa_air &lt;-\n  qb_ipaa |&gt;\n  dplyr::filter(repl_qb == 1) |&gt;\n  dplyr::pull(ipa_air_pass) |&gt;\n  mean(na.rm = TRUE)\n\nrepl_qb_ipa_qbrun &lt;-\n  qb_ipaa |&gt;\n  dplyr::filter(repl_qb == 1) |&gt;\n  dplyr::pull(ipa_qbrun) |&gt;\n  mean(na.rm = TRUE)\n\nqb_ipar &lt;-\n  qb_ipaa |&gt;\n  dplyr::mutate(\n    shadow_air = n_pass * repl_qb_ipa_air,\n    shadow_qbrun = n_qbrun * repl_qb_ipa_qbrun,\n    ipar_air_pass = ipaa_air_pass - shadow_air,\n    ipar_qbrun = ipaa_qbrun - shadow_qbrun)",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-pts-win",
    "href": "lectures/lecture10.html#sec-pts-win",
    "title": "Lecture 10: NFl WAR",
    "section": "From Points to Wins",
    "text": "From Points to Wins\nThe IPAR values are on the scale of point differential. Figure shows point differential plotted against wins. If we fit a line to these data — that is, if we find some \\(\\alpha\\) and \\(\\beta\\) such that \\(\\textrm{Wins} \\approx \\alpha + \\beta \\times \\textrm{PtsDiff}\\) — then we can multiply the IPAR values by the estimated slope to convert from the point differential scale to the win scale.\nTo get this, we load the schedules for the last 10 seasons using the function nflreader::load_schedules() from the nflreadr package, which can be installed using the code\n\ninstall.packages(\"nflreader\")\n\nThe following code loads schedules and computes point differentials and records.\n\nschedules &lt;-\n  nflreadr::load_schedules(seasons = 2015:2024) |&gt;\n  dplyr::filter(game_type == \"REG\") |&gt;\n  dplyr::select(season, away_team, home_team, home_score, away_score, result) |&gt;\n  dplyr::mutate(winning_team = ifelse(result &gt; 0, home_team, away_team),\n                losing_team = ifelse(result &lt; 0, home_team, away_team),\n                winning_ptsdiff = ifelse(winning_team == home_team, result, -1*result),\n                losing_ptsdiff = ifelse(losing_team == home_team, result, -1*result))\n\nwin_diff &lt;-\n  schedules |&gt;\n  dplyr::group_by(season, winning_team) |&gt;\n  dplyr::summarise(wins = dplyr::n(), win_diff = sum(winning_ptsdiff), .groups = 'drop') |&gt;\n  dplyr::rename(team = winning_team)\n\nloss_diff &lt;-\n  schedules |&gt;\n  dplyr::group_by(season, losing_team) |&gt;\n  dplyr::summarise(loss = dplyr::n(), loss_diff = sum(losing_ptsdiff), .groups = 'drop') |&gt;\n  dplyr::rename(team = losing_team)\n\nrecords &lt;-\n  win_diff |&gt;\n  dplyr::full_join(y = loss_diff, by = c(\"season\", \"team\")) |&gt;\n  dplyr::mutate(scoring_differential = win_diff + loss_diff)\n\nPlotting wins against total scoring differential, we see an obvious increasing trend\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(records$scoring_differential,\n     records$wins,\n     xlab = \"Total scoring margin\",\n     ylab = \"Wins\",\n     main = \"Wins vs Score Differential\",\n     pch = 16, cex = 0.5)\n\n\n\n\n\n\n\nFigure 1: Scoring differential is positively associated with wins\n\n\n\n\n\nWe fit a linear to these data\n\nwin_score_fit &lt;-\n  lm(wins~scoring_differential, data = records)\nbeta &lt;- coefficients(win_score_fit)[2]\nbeta\n\nscoring_differential \n          0.02807309 \n\n\nWe see that every additional point scored is associated with about an increase of about 0.02 wins, on average.\nWe can now multiply each IPAR value by this factor to determine how wins each player contributes above replacement in different phases of the game.\n\nall_skill_players &lt;-\n  unique(c(run_ipar$gsis_id, receiver_ipar$gsis_id))\n\nskill_war &lt;-\n  data.frame(gsis_id = all_skill_players) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position, team), by = \"gsis_id\") |&gt;\n  dplyr::left_join(y = run_ipar |&gt; dplyr::select(gsis_id, n_run, ipar_run), by = \"gsis_id\") |&gt;\n  dplyr::left_join(y = receiver_ipar |&gt; dplyr::select(gsis_id, n_rec, ipar_air_rec, ipar_yac_rec), by = \"gsis_id\") |&gt;\n  tidyr::replace_na(list(ipar_run=0, ipar_air_rec=0, ipar_yac_rec=0)) |&gt;\n  dplyr::mutate(\n    war_air_rec = ipar_air_rec * beta,\n    war_yac_rec = ipar_yac_rec * beta,\n    war_run = ipar_run * beta,\n    war = war_air_rec + war_yac_rec + war_run)\n\nqb_war &lt;-\n  qb_ipar |&gt;\n  dplyr::select(gsis_id, full_name, n_pass, n_qbrun, ipar_air_pass, ipar_qbrun) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, team, position), by = \"gsis_id\") |&gt;\n  tidyr::replace_na(list(ipar_air_pass=0, ipar_qbrun=0)) |&gt;\n  dplyr::mutate(\n    war_air_pass = ipar_air_pass * beta,\n    war_qbrun = ipar_qbrun * beta,\n    war = war_air_pass + war_qbrun)\n\nHere are the top-10 skill-position players based on our WAR calculations are\n\nskill_war |&gt;\n  dplyr::arrange(dplyr::desc(war)) |&gt;\n  dplyr::slice_head(n=10)\n\n      gsis_id         full_name position team n_run     ipar_run n_rec\n1  00-0039040     De'Von Achane       RB  MIA   203 29.664608808    87\n2  00-0034844    Saquon Barkley       RB  PHI   346 38.467788479    43\n3  00-0036900     Ja'Marr Chase       WR  CIN     3  0.064225554   175\n4  00-0039139      Jahmyr Gibbs       RB  DET   250 25.923785492    63\n5  00-0033858       Jonnu Smith       TE  MIA     2 -0.032029431   111\n6  00-0036963 Amon-Ra St. Brown       WR  DET     2 -0.003518778   141\n7  00-0039338      Brock Bowers       TE   LV     5 -0.069345894   153\n8  00-0032764     Derrick Henry       RB  BAL   325 20.091003420    22\n9  00-0039075        Puka Nacua       WR   LA    11  0.053233996   106\n10 00-0036322  Justin Jefferson       WR  MIN     1  0.002778183   153\n   ipar_air_rec ipar_yac_rec  war_air_rec war_yac_rec       war_run       war\n1     0.0690579    14.466115  0.001938669  0.40610855  8.327773e-01 1.2408245\n2     0.4847627     2.197136  0.013608788  0.06168040  1.079910e+00 1.1551989\n3     6.2418395    31.257835  0.175227726  0.87750403  1.803010e-03 1.0545348\n4    -1.2572827     6.203777 -0.035295810  0.17415921  7.277608e-01 0.8666242\n5    10.7653664    19.354982  0.302217105  0.54335416 -8.991651e-04 0.8446721\n6    20.7627215     8.044577  0.582873763  0.22583615 -9.878297e-05 0.8086111\n7    -3.6140405    30.181512 -0.101457287  0.84728831 -1.946754e-03 0.7438843\n8    -0.7543523     3.393370 -0.021176999  0.09526238  5.640166e-01 0.6381019\n9     5.1775439    16.442442  0.145349660  0.46159017  1.494443e-03 0.6084343\n10   12.6382021     8.777840  0.354793394  0.24642111  7.799217e-05 0.6012925\n\n\nAnd here are the top-10 quarterbacks based on our WAR calculations\n\nqb_war |&gt;\n  dplyr::arrange(dplyr::desc(war)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 11\n   gsis_id    full_name   n_pass n_qbrun ipar_air_pass ipar_qbrun team  position\n   &lt;chr&gt;      &lt;chr&gt;        &lt;int&gt;   &lt;int&gt;         &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   \n 1 00-0039910 Jayden Dan…    461     184        -8.14        90.9 WAS   QB      \n 2 00-0034857 Josh Allen     459     108         3.55        78.7 BUF   QB      \n 3 00-0034796 Lamar Jack…    454     150        16.6         52.0 BAL   QB      \n 4 00-0036389 Jalen Hurts    334     167         5.94        55.5 PHI   QB      \n 5 00-0039164 Anthony Ri…    247      94       -16.6         65.1 IND   QB      \n 6 00-0037834 Brock Purdy    440      92        14.8         31.7 SF    QB      \n 7 00-0039732 Bo Nix         546     102       -13.3         51.6 DEN   QB      \n 8 00-0035228 Kyler Murr…    520     102         0.547       36.8 ARI   QB      \n 9 00-0035710 Daniel Jon…    317      88         5.41        27.1 MIN   QB      \n10 00-0033873 Patrick Ma…    552      74         4.87        27.3 KC    QB      \n# ℹ 3 more variables: war_air_pass &lt;dbl&gt;, war_qbrun &lt;dbl&gt;, war &lt;dbl&gt;",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#footnotes",
    "href": "lectures/lecture10.html#footnotes",
    "title": "Lecture 10: NFl WAR",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBecause the vast majority of passes are thrown by quarterbacks, we’ll index passers with “q”.↩︎\nBecause “r” could refer to either receiver or runner, we’ll index receivers with “c” for pass catchers.↩︎\nThat is, unobservable.↩︎\nE.g., via his route running.↩︎\nIf you have taken STAT 310 or 312, the issue is that the likelihood function depends on these parameters only through their sum \\(\\mu_{Q} + \\mu_{C} + \\mu_{D}.\\) So, the data alone cannot distinguish configurations of these parameters that yield the same sum. For instance, setting \\(\\mu_{Q} = \\mu_{C} = \\mu_{D} = 0\\) yields exactly the same fit to the data \\(\\mu_{Q} = 5, \\mu_{C} = -5, \\mu_{D} = 0.\\) One way to get around this inject prior information and fit a hierarchical Bayesian model, but that is beyond the scope of this course.↩︎\nThe superscript \\(^{(Q)}\\) is there to remind us that this is for passers (i.e., quarterbacks) and the subscript \\(\\textrm{air}\\) reminds us that the IPA values are derived from our model for EPA through the air.↩︎\nIn the original nflWAR paper (Yurko, Ventura, and Horowitz 2019), they set \\(\\Delta_{\\textrm{air},i} =\\Delta_{\\textrm{yac},i} = \\Delta_{i}\\) on incomplete passes and fit the YAC EPA model using data from both complete and incomplete passes. This effectively “double counts” contributions of offensive players.↩︎\nPersonally, I disagree with the statement. A very natural way around this issue is to use a Bayesian hierarchical model. In fact, I’d go further and say that multilevel models of the type fitted by lmer() are just impoverished versions of Bayesian hierarchical models. But Bayesian models are regrettably outside the scope of the course and non-Bayesian multilevel models are very widely used so you have to learn a little about them.↩︎\nBut you should check whether our downstream results change if we estimate the YAC IPA values for receivers using a reduced model that excludes the passer random intercept!↩︎",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "slides/lecture01.html",
    "href": "slides/lecture01.html",
    "title": "Lecture 1",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/lecture03.html",
    "href": "slides/lecture03.html",
    "title": "Lecture 3: Estimating XG",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "slides/lecture05.html",
    "href": "slides/lecture05.html",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#motivation-beth-mead-in-euro-2022",
    "href": "slides/raw_slides/slides_02.html#motivation-beth-mead-in-euro-2022",
    "title": "STAT 479 Lecture 2",
    "section": "Motivation: Beth Mead in EURO 2022",
    "text": "Motivation: Beth Mead in EURO 2022\n\nBeth Mead scored 6 goals in EURO 2022. Which was more impressive?\n\nGoal in 15th minute against Austria link\nGoal in 37th minute against Norway link\nGoal in 33rd minute against Sweden link\n\n\n\n\nWe can argue endlessly about qualitative differences\n\nOne-on-one vs in-traffic; left or right foot;\nType of shot; time; score; …\n\n\n\n\n\nGoal: quantitative comparison"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#a-thought-experiment",
    "href": "slides/raw_slides/slides_02.html#a-thought-experiment",
    "title": "STAT 479 Lecture 2",
    "section": "A Thought Experiment",
    "text": "A Thought Experiment\n\nWhat if we could replay each shot over and over again?\nHow often would she score?\n\n\n\n\n\n\n\n\nThis long-run frequency is Expected Goals (XG)"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#statsbomb-hudl",
    "href": "slides/raw_slides/slides_02.html#statsbomb-hudl",
    "title": "STAT 479 Lecture 2",
    "section": "StatsBomb & Hudl",
    "text": "StatsBomb & Hudl\n\nPlayer location data for all events\n\nComputer vision + input from 5 human annotators\nHumans log events (shots, tackles, passes, etc.)\nComputer vision to extra location information\n\nLocations mapped to fixed coordinate system\nStatsBombR package: available via GitHub\n\n\ndevtools::install_github(\"statsbomb/StatsBombR\")"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#statsbomb-coordinates",
    "href": "slides/raw_slides/slides_02.html#statsbomb-coordinates",
    "title": "STAT 479 Lecture 2",
    "section": "StatsBomb Coordinates",
    "text": "StatsBomb Coordinates\n\nOffensive action moves left-to-right\nVertical coordinate increases as you move top-to-bottom\n\n\n\n\nStatsBomb Coordinates"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#free-competitions",
    "href": "slides/raw_slides/slides_02.html#free-competitions",
    "title": "STAT 479 Lecture 2",
    "section": "Free Competitions",
    "text": "Free Competitions\n\nFreeCompetitions() returns table of available competitions\n\n\n\n[1] \"Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please credit StatsBomb as your data source when using the data and visit https://statsbomb.com/media-pack/ to obtain our logos for public use.\"\n\n\n   competition_id season_id      competition_name season_name\n1              16       276      Champions League   1970/1971\n2              16        76      Champions League   1999/2000\n3               2        44        Premier League   2003/2004\n4             116        68 North American League        1977\n5              11         1               La Liga   2017/2018\n6              43        51        FIFA World Cup        1974\n7              53       106     UEFA Women's Euro        2022\n8               7       235               Ligue 1   2022/2023\n9              11        37               La Liga   2004/2005\n10             35        75    UEFA Europa League   1988/1989"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#free-matches",
    "href": "slides/raw_slides/slides_02.html#free-matches",
    "title": "STAT 479 Lecture 2",
    "section": "Free Matches",
    "text": "Free Matches\n\nStatsBombR::FreeCompetitions() |&gt;\n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt;\n  dplyr::select(match_id, home_team.home_team_name, away_team.away_team_name, home_score, away_score) |&gt;\n  dplyr::slice_sample(n=5)\n\n\n\n\n# A tibble: 5 × 5\n  match_id home_team.home_team_name away_team.away_team_…¹ home_score away_score\n     &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;                       &lt;int&gt;      &lt;int&gt;\n1  3845506 England Women's          Sweden Women's                  4          0\n2  3835326 Belgium Women's          Iceland Women's                 1          1\n3  3835330 Germany Women's          Spain Women's                   2          0\n4  3845507 Germany Women's          France Women's                  2          1\n5  3835322 Germany Women's          Denmark Women's                 4          0\n# ℹ abbreviated name: ¹​away_team.away_team_name"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#event-data",
    "href": "slides/raw_slides/slides_02.html#event-data",
    "title": "STAT 479 Lecture 2",
    "section": "Event Data",
    "text": "Event Data\n\nGet EURO2022 EventsExtract All Shots\n\n\n\neuro2022_events &lt;-\n  StatsBombR::FreeCompetitions() |&gt; \n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt; \n  StatsBombR::free_allevents() |&gt; \n  StatsBombR::allclean() |&gt; \n  StatsBombR::get.opposingteam()\n\n\n\n\neuro2022_shots &lt;-\n  euro2022_events |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#meads-shots",
    "href": "slides/raw_slides/slides_02.html#meads-shots",
    "title": "STAT 479 Lecture 2",
    "section": "Mead’s Shots",
    "text": "Mead’s Shots\n\nmead_shots &lt;-\n  euro2022_shots |&gt;\n  dplyr::filter(player.name == \"Bethany Mead\")\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y)\n\n# A tibble: 15 × 5\n   OpposingTeam     minute shot.body_part.name shot.technique.name     Y\n   &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;\n 1 Austria Women's      15 Right Foot          Lob                     1\n 2 Norway Women's       29 Right Foot          Normal                  0\n 3 Norway Women's       33 Head                Normal                  1\n 4 Norway Women's       37 Left Foot           Normal                  1\n 5 Norway Women's       52 Right Foot          Volley                  0\n 6 Norway Women's       80 Left Foot           Volley                  1\n 7 Northern Ireland      5 Head                Normal                  0\n 8 Northern Ireland     15 Right Foot          Half Volley             0\n 9 Northern Ireland     43 Left Foot           Normal                  1\n10 Northern Ireland     56 Right Foot          Normal                  0\n11 Northern Ireland     83 Right Foot          Normal                  0\n12 Sweden Women's        4 Head                Normal                  0\n13 Sweden Women's       19 Left Foot           Normal                  0\n14 Sweden Women's       33 Right Foot          Half Volley             1\n15 Sweden Women's       46 Left Foot           Normal                  0"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#thought-experiment-repeating-shots",
    "href": "slides/raw_slides/slides_02.html#thought-experiment-repeating-shots",
    "title": "STAT 479 Lecture 2",
    "section": "Thought Experiment: Repeating Shots",
    "text": "Thought Experiment: Repeating Shots\n\nWhat proportion of repetitions result in goals?\nAcross all repetitions, conditions kept the same"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#setup-notation",
    "href": "slides/raw_slides/slides_02.html#setup-notation",
    "title": "STAT 479 Lecture 2",
    "section": "Setup & Notation",
    "text": "Setup & Notation\n\nSuppose our data set contains \\(n\\) shots\nFor shot \\(i = 1, \\ldots, n,\\) we observe\n\n\\(Y_{i}\\): indicator shot resutled in goal (\\(Y = 1\\)) or not (\\(Y = 0\\))\n\\(\\boldsymbol{\\mathbf{X}}_{i}\\): vector of \\(p\\) features about the shot\n\nFeatures could include things like\n\nBody part used & shot technique\nDist. to nearest defenders"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#conditional-probability",
    "href": "slides/raw_slides/slides_02.html#conditional-probability",
    "title": "STAT 479 Lecture 2",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nAssumption: data are a representative sample from an infinite super-population of shots\nEach shot in super-population characterized by pair \\((\\boldsymbol{\\mathbf{X}}, Y)\\)\nRepeating shot with features \\(\\boldsymbol{\\mathbf{x}}\\) equivalent to sampling from slice of super-population with \\(\\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}.\\)\n\n\n\\[\n\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}] = \\mathbb{P}(Y = 1 \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}})\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#defining-the-super-population",
    "href": "slides/raw_slides/slides_02.html#defining-the-super-population",
    "title": "STAT 479 Lecture 2",
    "section": "Defining The Super-population",
    "text": "Defining The Super-population\n\nUltimate goal is to assess Mead’s performance in EURO 2022\n\nFocus on women’s internationals\nFocus on shots attempted w/ foot or head\n\n\n\nGet EventsExtract Shots\n\n\n\nwi_events &lt;-\n  StatsBombR::FreeCompetitions() |&gt; \n  dplyr::filter(competition_gender == \"female\" & competition_international) |&gt;\n  StatsBombR::FreeMatches() |&gt; \n  StatsBombR::free_allevents() |&gt; \n  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam()\n\n\n\n\nwi_shots &lt;-\n  wi_events |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;  \n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#idealized-calculation",
    "href": "slides/raw_slides/slides_02.html#idealized-calculation",
    "title": "STAT 479 Lecture 2",
    "section": "Idealized Calculation",
    "text": "Idealized Calculation\n\nSuppose our only feature is shot.body_part.name\n\n\n\n\n      Head  Left Foot Right Foot \n       920       1280       2560 \n\n\n\nIf we could access infinite super-population, compute \\(\\textrm{XG}(\\text{right-footed shot})\\) by\n\nForming subgroup containing only right-footed shots\nCalculating proportion of goals score"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#practical-calculation",
    "href": "slides/raw_slides/slides_02.html#practical-calculation",
    "title": "STAT 479 Lecture 2",
    "section": "Practical Calculation",
    "text": "Practical Calculation\n\nSince we cannot access infinite super-population, we must estimate XG from data\nWe do so by mimicking idealized calculation\n\nDivide data into subgroups based on shot.body_part.name\nCompute proportion of goals within subgroups\n\nRely on dplyr’s group_by() functionality to do this"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#estimating-xgbody-part",
    "href": "slides/raw_slides/slides_02.html#estimating-xgbody-part",
    "title": "STAT 479 Lecture 2",
    "section": "Estimating XG(body part)",
    "text": "Estimating XG(body part)\n\nxg_model1 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y), n = dplyr::n())\nxg_model1\n\n\n\n\n# A tibble: 3 × 3\n  shot.body_part.name   XG1     n\n  &lt;chr&gt;               &lt;dbl&gt; &lt;int&gt;\n1 Head                0.112   920\n2 Left Foot           0.114  1280\n3 Right Foot          0.111  2560"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#assessing-beth-meads-performance",
    "href": "slides/raw_slides/slides_02.html#assessing-beth-meads-performance",
    "title": "STAT 479 Lecture 2",
    "section": "Assessing Beth Mead’s Performance",
    "text": "Assessing Beth Mead’s Performance\n\nAppend XG estimates to mead_shots w/ left_join\nBased on bodypart, each goal is about equally impressive:\nModel suggests 11% of repeated shots would result in goals\n\n\nmead_shots &lt;- mead_shots |&gt;\n  dplyr::left_join(y = xg_model1 |&gt; dplyr::select(shot.body_part.name, XG1),\n                   by = \"shot.body_part.name\")\n\n\n\n\n# A tibble: 3 × 5\n  OpposingTeam    minute shot.body_part.name     Y   XG1\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Austria Women's     15 Right Foot              1 0.111\n2 Norway Women's      37 Left Foot               1 0.114\n3 Sweden Women's      33 Right Foot              1 0.111"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#accounting-for-shot-technique",
    "href": "slides/raw_slides/slides_02.html#accounting-for-shot-technique",
    "title": "STAT 479 Lecture 2",
    "section": "Accounting For Shot Technique",
    "text": "Accounting For Shot Technique\n\nEach goal was scored off a different type of shot\nWhat if we condition body part & shot technique?\n\n\ntable(wi_shots |&gt; dplyr::pull(shot.technique.name))\n\n\n     Backheel Diving Header   Half Volley           Lob        Normal \n           35            10           648            28          3720 \nOverhead Kick        Volley \n           17           302"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#new-xg-estimates",
    "href": "slides/raw_slides/slides_02.html#new-xg-estimates",
    "title": "STAT 479 Lecture 2",
    "section": "New XG Estimates",
    "text": "New XG Estimates\n\nXG EstimatesMead’s Performance\n\n\n\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarize(XG2 = mean(Y), n = dplyr::n(), .groups = \"drop\")\nxg_model2 |&gt; dplyr::arrange(dplyr::desc(XG2))\n\n# A tibble: 14 × 4\n   shot.body_part.name shot.technique.name    XG2     n\n   &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt; &lt;int&gt;\n 1 Right Foot          Lob                 0.208     24\n 2 Left Foot           Volley              0.163     98\n 3 Left Foot           Normal              0.121    947\n 4 Right Foot          Normal              0.121   1863\n 5 Head                Normal              0.113    910\n 6 Right Foot          Backheel            0.103     29\n 7 Right Foot          Half Volley         0.0892   426\n 8 Right Foot          Overhead Kick       0.0714    14\n 9 Left Foot           Half Volley         0.0676   222\n10 Right Foot          Volley              0.0637   204\n11 Head                Diving Header       0         10\n12 Left Foot           Backheel            0          6\n13 Left Foot           Lob                 0          4\n14 Left Foot           Overhead Kick       0          3\n\n\n\n\n\nmead_shots &lt;-\n  mead_shots |&gt;\n  dplyr::inner_join(\n    y = xg_model2 |&gt; dplyr::select(-n), \n    by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y, XG2) |&gt;\n  dplyr::slice(c(1, 4, 14))\n\n# A tibble: 3 × 6\n  OpposingTeam    minute shot.body_part.name shot.technique.name     Y    XG2\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n1 Austria Women's     15 Right Foot          Lob                     1 0.208 \n2 Norway Women's      37 Left Foot           Normal                  1 0.121 \n3 Sweden Women's      33 Right Foot          Half Volley             1 0.0892"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#accounting-for-more-features",
    "href": "slides/raw_slides/slides_02.html#accounting-for-more-features",
    "title": "STAT 479 Lecture 2",
    "section": "Accounting for more features",
    "text": "Accounting for more features\n\nOur 2-feature model is still too coarse\n\nDoesn’t account for distance of shot\nDoesn’t account for defenders and keeper\n\nStatsBomb records many more features about shots\n\n\n\n [1] \"shot.type.name\"      \"shot.technique.name\" \"shot.body_part.name\"\n [4] \"DistToGoal\"          \"DistToKeeper\"        \"AngleToGoal\"        \n [7] \"AngleToKeeper\"       \"AngleDeviation\"      \"avevelocity\"        \n[10] \"density\"             \"density.incone\"      \"distance.ToD1\"      \n[13] \"distance.ToD2\"       \"AttackersBehindBall\" \"DefendersBehindBall\"\n[16] \"DefendersInCone\"     \"InCone.GK\"           \"DefArea\"            \n\n\n\nHow can we adjust for these in our XG model?"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#digression-what-is-the-cone",
    "href": "slides/raw_slides/slides_02.html#digression-what-is-the-cone",
    "title": "STAT 479 Lecture 2",
    "section": "Digression: What is the cone?",
    "text": "Digression: What is the cone?\n\nCone: area between shot location & goalposts \n\n\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#idealized-computation",
    "href": "slides/raw_slides/slides_02.html#idealized-computation",
    "title": "STAT 479 Lecture 2",
    "section": "Idealized Computation",
    "text": "Idealized Computation\n\nIf we could access super-population, easy to condition on more features\n\nSlice population along \\(\\mathbf{\\boldsymbol{x}}\\)\nCompute proportion of goals among the slice with \\(\\mathbf{\\boldsymbol{X}} = \\mathbf{\\boldsymbol{x}}\\)\n\n\n\n\nCan we mimic this with our observed data?"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#practical-challenges",
    "href": "slides/raw_slides/slides_02.html#practical-challenges",
    "title": "STAT 479 Lecture 2",
    "section": "Practical Challenges",
    "text": "Practical Challenges\n\nAdjust for body part, technique, and # defenders in cone\n\n\n\n# A tibble: 10 × 5\n   shot.body_part.name shot.technique.name DefendersInCone    XG     n\n   &lt;chr&gt;               &lt;chr&gt;                         &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 Left Foot           Normal                            7 1         1\n 2 Right Foot          Lob                               2 1         1\n 3 Right Foot          Normal                           10 1         1\n 4 Left Foot           Volley                            0 0.343    35\n 5 Right Foot          Overhead Kick                     0 0.333     3\n 6 Right Foot          Overhead Kick                     4 0         2\n 7 Right Foot          Overhead Kick                     6 0         1\n 8 Right Foot          Volley                            5 0        12\n 9 Right Foot          Volley                            6 0         9\n10 Right Foot          Volley                            7 0         1"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#model-based-xg",
    "href": "slides/raw_slides/slides_02.html#model-based-xg",
    "title": "STAT 479 Lecture 2",
    "section": "Model-based XG",
    "text": "Model-based XG\n\n“Binning-and-averaging” not viable w/ many features due to small sample sizes\nStatistical models overcome these challenges by “borrowing strength”\n\\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) informed by shots with \\(\\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}\\) and \\(\\boldsymbol{\\mathbf{X}} \\approx \\boldsymbol{\\mathbf{x}}\\)\nHow a model “borrows strength” depends on its underlying assumptions\n\n\n\nSeveral challenges:\n\n\\(\\mathbf{\\boldsymbol{X}}\\) may be high-dimensional\n\\(\\textrm{XG}\\) likely depends on many interactions\n\\(\\textrm{XG}\\) likely highly non-linear\n\nStatsBomb has a proprietary model"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#statsbombs-xg-estimates",
    "href": "slides/raw_slides/slides_02.html#statsbombs-xg-estimates",
    "title": "STAT 479 Lecture 2",
    "section": "StatsBomb’s XG Estimates",
    "text": "StatsBomb’s XG Estimates\n\nshot.statsbomb_xg records proprietary XG estimate\nGoal against Sweden had smallest XG and is, therefore, the most impressive\n\n\n\n# A tibble: 3 × 5\n  OpposingTeam    minute   XG1   XG2 shot.statsbomb_xg\n  &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;\n1 Austria Women's     15 0.111 0.208             0.361\n2 Norway Women's      37 0.114 0.121             0.444\n3 Sweden Women's      33 0.111 0.089             0.091"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#did-mead-outperform-expectations",
    "href": "slides/raw_slides/slides_02.html#did-mead-outperform-expectations",
    "title": "STAT 479 Lecture 2",
    "section": "Did Mead Outperform Expectations?",
    "text": "Did Mead Outperform Expectations?\n\nHow to interpret the difference \\(Y_{i} - \\textrm{XG}_{i}\\) when it is\n\nLarge & positive\nLarge & negative\nClose to 0\n\n\n\n\nBeth Mead scored 2.89 more goals than expected\n\n\nsum(mead_shots$Y - mead_shots$shot.statsbomb_xg)\n\n[1] 2.896323"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#assessing-all-euro2022-players",
    "href": "slides/raw_slides/slides_02.html#assessing-all-euro2022-players",
    "title": "STAT 479 Lecture 2",
    "section": "Assessing All EURO2022 Players",
    "text": "Assessing All EURO2022 Players\n\ngoe &lt;- \n  euro2022_shots |&gt;\n  dplyr::mutate(diff = Y - shot.statsbomb_xg) |&gt;\n  dplyr::group_by(player.name) |&gt;\n  dplyr::summarise(GOE = sum(diff), Goals = sum(Y), n_shots = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(GOE)) \ngoe |&gt; dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n# A tibble: 10 × 4\n   player.name                GOE Goals n_shots\n   &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Alexandra Popp           3.34      6      16\n 2 Bethany Mead             2.90      6      15\n 3 Alessia Russo            1.79      4      12\n 4 Francesca Kirby          1.79      2       5\n 5 Lina Magull              1.70      3      14\n 6 Ada Stolsmo Hegerberg   -0.829     0       8\n 7 Nadia Nadim             -0.886     0       6\n 8 Lauren Hemp             -1.26      1      11\n 9 Emma Stina Blackstenius -2.15      1      17\n10 Wendie Renard           -2.39      0      17"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#looking-ahead",
    "href": "slides/raw_slides/slides_02.html#looking-ahead",
    "title": "STAT 479 Lecture 2",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nI Strongly recommend you work through code yourself\n\nFull code available here\nTry more binning-and-averaging XG estimates w/ other features\nTry to replicate goals over expected with EURO 2025 data"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#motivation",
    "href": "slides/raw_slides/slides_04.html#motivation",
    "title": "STAT 479: Lecture 4",
    "section": "Motivation",
    "text": "Motivation\n\nHow do NBA players help their teams win?\nHow do we quantify contributions?\n\n\n\nIdea: good players do things that show up in the box score\n\nPoints, rebounds, assists, steals, blocks, turnovers\nEasy to collect, sort, explain\n\n\n\n\n\nFails to account for roles\n\n10 assists for a guard vs 10 rebounds by a center\n\nProblem: should some stats weigh more heavily than others?\n\n\n\n\nBigger problem: Not everything appears in the box score\n\nSetting screens, rotating on defense, communicating\nGood shot selection, diving for loose balls\nThe “little things”"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#plusminus",
    "href": "slides/raw_slides/slides_04.html#plusminus",
    "title": "STAT 479: Lecture 4",
    "section": "Plus/Minus",
    "text": "Plus/Minus\n\n\n\nDefinition: Plus/Minus\n\n\nA player’s plus/minus is the point differential that a player’s team accrues while they are on the court.\n\n\n\n\nIntuition: if your team outscores opponent while you’re on the court, you must be doing something right\nTo compute, must know who is on the court at all times"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#play-by-play-nba-data",
    "href": "slides/raw_slides/slides_04.html#play-by-play-nba-data",
    "title": "STAT 479: Lecture 4",
    "section": "Play-by-Play NBA Data",
    "text": "Play-by-Play NBA Data\n\n\n\n\n\n\nEntry created when player does something tracked by scorekeeper\nCan use the hoopR package to scrape play-by-play data into R"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#stint-level-data",
    "href": "slides/raw_slides/slides_04.html#stint-level-data",
    "title": "STAT 479: Lecture 4",
    "section": "Stint-Level Data",
    "text": "Stint-Level Data\n\nStint: period of play b/w substitutions where the same 10 players remain on the court.\nCan form a data table from play-by-play log where\n\nRows correspond to stints\nColumns for game context: start & end scores, length in minutes, etc.\nColumn for every player’s signed on-court indicator\n\nSigned on-court indicators:\n\n+1 if on-court and playing at home\n-1 if on-court and playing on the road\n0 if not on court"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#data-snapshot",
    "href": "slides/raw_slides/slides_04.html#data-snapshot",
    "title": "STAT 479: Lecture 4",
    "section": "Data Snapshot",
    "text": "Data Snapshot\n\nContext columns:\n\nGame & Stint ID; num. possessions; duration;\nStart & end scores & times; point differential\n\n569 columns of signed on-court indicators\n\n\nContextOn-Court IndicatorsPlayer IDs\n\n\n\n\n# A tibble: 5 × 7\n  stint_id n_pos start_minutes minutes home_points away_points pts_diff\n     &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1        1    14          0      5.42           18          12        6\n2        2     5          5.42   1.51            2           2        0\n3        3     1          6.93   0.220           0           2       -2\n4        4     4          7.15   2.17            9           1        8\n5        5    13          9.32   4.13           13          11        2\n\n\n\n\n\n\n# A tibble: 5 × 7\n  stint_id `201143` `201950` `1627759` `1628369` `1628436` `1630202`\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1        1        1        1         1         1         0         0\n2        2        1        0         0         1         1         1\n3        3        1        0         0         1         1         1\n4        4        0        0         0         1         1         1\n5        5        0        1         0         1         1         1\n\n\n\n\n\nplayer_table &lt;-\n  hoopR::nba_commonallplayers()[[\"CommonAllPlayers\"]] |&gt;\n  dplyr::select(PERSON_ID, DISPLAY_FIRST_LAST) |&gt;\n  dplyr::rename(id = PERSON_ID, FullName = DISPLAY_FIRST_LAST) |&gt;\n  dplyr::mutate(\n    Name = stringi::stri_trans_general(FullName, \"Latin-ASCII\")) \n\nplayer_table |&gt;\n  dplyr::filter(id %in% c(\"201143\", \"201950\", \"1627759\")) |&gt;\n  dplyr::pull(FullName)\n\n[1] \"Jaylen Brown\" \"Jrue Holiday\" \"Al Horford\""
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-individual---concept",
    "href": "slides/raw_slides/slides_04.html#computing-individual---concept",
    "title": "STAT 479: Lecture 4",
    "section": "Computing Individual +/- (Concept)",
    "text": "Computing Individual +/- (Concept)\n\nConsider Shai Gilgeous-Alexander (2024-25 MVP)\nTo compute SGA’s +/-:\n\nSum the home team point differentials for all stints where SGA was on the court and playing at home.\nSum the negative of the home team point differentials for all shifts where SGA was on the court and playing on the road.\nAdd the two totals from Steps 1 and 2."
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-individual---formula",
    "href": "slides/raw_slides/slides_04.html#computing-individual---formula",
    "title": "STAT 479: Lecture 4",
    "section": "Computing Individual +/- (Formula)",
    "text": "Computing Individual +/- (Formula)\n\n\\(\\Delta_{i}\\): home team point differential in shift \\(i\\)\n\\(x_{i, \\textrm{SGA}}\\): SGA’s signed on-court indicator:\n\n\\(x_{i, \\textrm{SGA}} = 0\\) if SGA off-court in stint \\(i\\)\n\\(x_{i, \\textrm{SGA}} = 1(-1)\\) if SGA on-court at home (away) in stint \\(i\\)\n\nSGA’s +/- is just \\[\n\\sum_{i = 1}^{n}{x_{i,\\textrm{SGA}} \\times \\Delta_{i}}.\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-individual---code",
    "href": "slides/raw_slides/slides_04.html#computing-individual---code",
    "title": "STAT 479: Lecture 4",
    "section": "Computing Individual +/- (Code)",
    "text": "Computing Individual +/- (Code)\n\nWhen SGA was on the floor, Thunder outscored opponents by 888 pts\nWhen Jokic was on the floor, Nuggets outscored opponents by 452\n\n\nSGAJokic\n\n\n\nshai_id &lt;- player_table |&gt;\n  dplyr::filter(Name == \"Shai Gilgeous-Alexander\") |&gt; dplyr::pull(id) \nshai_x &lt;- rapm_data |&gt; dplyr::pull(shai_id) \ndelta &lt;- rapm_data |&gt; dplyr::pull(pts_diff) \nsum(shai_x * delta)\n\n\n\n\n[1] 888\n\n\n\n\n\n\njokic_id &lt;-\n  player_table |&gt;\n  dplyr::filter(Name == \"Nikola Jokic\") |&gt;\n  dplyr::pull(id)\n\njokic_x &lt;- rapm_data |&gt; dplyr::pull(jokic_id) \nsum(jokic_x * delta) \n\n[1] 452"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#notation",
    "href": "slides/raw_slides/slides_04.html#notation",
    "title": "STAT 479: Lecture 4",
    "section": "Notation",
    "text": "Notation\n\n\\(n\\): total number of stints in the season\n\\(p\\): total number of players\nFor each stint \\(i = 1, \\ldots, n\\) and player \\(j = 1, \\ldots, p\\):\n\n\\(x_{ij} = 1\\) if player \\(j\\) on-court at home in stint \\(i\\)\n\\(x_{ij} = -1\\) if player \\(j\\) on-court on road in stint \\(i\\)\n\\(x_{ij} = 0\\)\n\n\\(\\Delta_{i}\\): home-team differential in stint \\(i\\) . . .\nPlayer \\(j\\)’s +/-: \\(\\sum_{i}{x_{ij}\\Delta_{i}}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#stint-design-matrix",
    "href": "slides/raw_slides/slides_04.html#stint-design-matrix",
    "title": "STAT 479: Lecture 4",
    "section": "Stint Design Matrix",
    "text": "Stint Design Matrix\n\nArrange \\(x_{ij}\\)’s into an \\(n \\times p\\) matrix\n\n\\[\n\\boldsymbol{\\mathbf{X}} =\n\\begin{pmatrix}\nx_{1,1} & \\cdots & x_{1,p} \\\\\n\\vdots & & \\vdots \\\\\nx_{n,1} & \\cdots & x_{n,p}\n\\end{pmatrix}\n\\]\n\n\nCollect all \\(n\\) \\(\\Delta_{i}\\)’s into a vector of length \\(n\\) \\[\n\\boldsymbol{\\Delta} = \\begin{pmatrix} \\Delta_{1} \\\\ \\vdots \\\\ \\Delta_{n} \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-all--s",
    "href": "slides/raw_slides/slides_04.html#computing-all--s",
    "title": "STAT 479: Lecture 4",
    "section": "Computing all +/-’s",
    "text": "Computing all +/-’s\n\nCan compute all player’s w/ matrix-vector multiplication \\(\\boldsymbol{\\mathbf{X}}^{\\top}\\boldsymbol{\\Delta}\\) \\[\n\\begin{pmatrix}\nx_{1,1} & \\cdots & x_{n,1} \\\\\n\\vdots & & \\vdots \\\\\nx_{1,p} & \\cdots & x_{n,p}\n\\end{pmatrix}\n\\begin{pmatrix} \\Delta_{1} \\\\ \\vdots \\\\ \\Delta_{n} \\end{pmatrix}\n=\n\\begin{pmatrix}\nx_{1,1}\\Delta_{1} + x_{2,1}\\Delta_{2} + \\cdots + x_{n,1}\\Delta_{n}\\\\\n\\vdots \\\\\nx_{1,p}\\Delta_{1} + x_{2,p}\\Delta_{2} + \\cdots + x_{n,p}\\Delta_{n}\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-plusminus",
    "href": "slides/raw_slides/slides_04.html#computing-plusminus",
    "title": "STAT 479: Lecture 4",
    "section": "Computing Plus/Minus",
    "text": "Computing Plus/Minus\n\nBuilding \\(\\boldsymbol{\\mathbf{X}}\\)Computing Plus/MinusSelected Players\n\n\n\ncontext_vars &lt;-\n  c(\"game_id\", \"stint_id\", \"n_pos\", \n    \"start_home_score\", \"start_away_score\", \"start_minutes\",\n    \"end_home_score\", \"end_away_score\", \"end_minutes\",\n    \"home_points\", \"away_points\", \"minutes\",\n    \"pts_diff\", \"margin\")\n\nX_full &lt;- as.matrix( \n    rapm_data |&gt; dplyr::select(- tidyr::all_of(context_vars))) \n\n\n\n\npm &lt;-\n  data.frame( \n    id = colnames(X_full), \n    pm = crossprod(x = X_full, y = rapm_data |&gt; dplyr::pull(pts_diff)), \n    n_pos = crossprod(abs(X_full), y = rapm_data |&gt; dplyr::pull(n_pos)), \n    minutes = crossprod(abs(X_full), y = rapm_data |&gt; dplyr::pull(minutes))) |&gt; \n  dplyr::inner_join(y = player_table |&gt; dplyr::select(id, Name), by = \"id\") |&gt; \n  dplyr::select(id, Name, pm, n_pos, minutes) |&gt; \n  dplyr::arrange(dplyr::desc(pm))\n\n\n\n\n\n                     Name  pm\n1 Shai Gilgeous-Alexander 888\n2            Jayson Tatum 474\n3            Nikola Jokic 452\n4   Giannis Antetokounmpo 331\n5             Luka Doncic 276\n6           Anthony Davis -78\n7            LeBron James -88"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#visualizing-plusminus",
    "href": "slides/raw_slides/slides_04.html#visualizing-plusminus",
    "title": "STAT 479: Lecture 4",
    "section": "Visualizing Plus/Minus",
    "text": "Visualizing Plus/Minus\n\n\n\n\n\n\n\n\nFigure 1: Large gap b/w SGA and rest of the league"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#plusminus-and-possessions",
    "href": "slides/raw_slides/slides_04.html#plusminus-and-possessions",
    "title": "STAT 479: Lecture 4",
    "section": "Plus/Minus and Possessions",
    "text": "Plus/Minus and Possessions\n\n\n\n\n\n\n\n\nFigure 2: Variability in +/- increases with number of possessions!"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#issues-with-plusminus",
    "href": "slides/raw_slides/slides_04.html#issues-with-plusminus",
    "title": "STAT 479: Lecture 4",
    "section": "Issues with Plus/Minus",
    "text": "Issues with Plus/Minus\n\npm |&gt; dplyr::slice_head(n=10)\n\n        id                    Name  pm n_pos minutes\n1  1628983 Shai Gilgeous-Alexander 888  8159 2837.31\n2  1629652           Luguentz Dort 561  5584 1955.17\n3  1630198              Isaiah Joe 552  4896 1715.11\n4  1628401           Derrick White 509  6129 2327.72\n5  1630596             Evan Mobley 508  5944 2049.43\n6  1630598           Aaron Wiggins 507  4868 1683.08\n7  1628378        Donovan Mitchell 491  6101 2106.53\n8  1628369            Jayson Tatum 474  7498 2840.95\n9  1628386           Jarrett Allen 459  6163 2142.82\n10  203999            Nikola Jokic 452  7907 2707.97\n\n\n\nIs Lou Dort really better than Jayson Tatum???\n\n\n\nDifferences in +/- could be a result of\n\nDifferences in skill\nDifferences in playing time\nDifferences in teammate & opponent quality"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#from-totals-to-rates",
    "href": "slides/raw_slides/slides_04.html#from-totals-to-rates",
    "title": "STAT 479: Lecture 4",
    "section": "From Totals to Rates",
    "text": "From Totals to Rates\n\nComparing totals favors players with more playing time\nAPM works with rates: point differential per 100 possessions\n\n\n\n# A tibble: 10 × 4\n   stint_id pts_diff n_pos margin\n      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1        1        6    14   42.9\n 2        2        0     5    0  \n 3        3       -2     1 -200  \n 4        4        8     4  200  \n 5        5        2    13   15.4\n 6        6        4     8   50  \n 7        8        6     9   66.7\n 8        9        5    29   17.2\n 9       10        0     6    0  \n10       11       -2     5  -40"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#an-initial-apm-model",
    "href": "slides/raw_slides/slides_04.html#an-initial-apm-model",
    "title": "STAT 479: Lecture 4",
    "section": "An Initial APM Model",
    "text": "An Initial APM Model\n\nAssociate each player \\(j\\) with a latent strength \\(\\alpha_{j}\\)\n\\(\\alpha_{j}\\)’s are unknown: they must be estimated from data\n\\(Y_{i}\\): point differential per 100 possessions in stint \\(i\\)\n\\(h_{1}(i), \\ldots, h_{5}(i)\\) & \\(a_{1}(i), \\ldots, a_{5}(i)\\): identities of players on court in stint \\(i\\)\n\n\n\\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#interpretation",
    "href": "slides/raw_slides/slides_04.html#interpretation",
    "title": "STAT 479: Lecture 4",
    "section": "Interpretation",
    "text": "Interpretation\n\nIndividual \\(\\alpha_{j}\\)’s are meaningless!\n\\(\\alpha_{j}\\): change in point differential per 100 possessions b/w\n\nPlaying 5-on-5 w/ player \\(j\\) on the court\nPlaying 5-on-4 w/ player \\(j\\) off the court\n\nDifferences (or contrasts) like \\(\\alpha_{j}-\\alpha_{j'}\\) are much more useful"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#example",
    "href": "slides/raw_slides/slides_04.html#example",
    "title": "STAT 479: Lecture 4",
    "section": "Example",
    "text": "Example\n\nDec 23, 2024 game Dallas Mavericks (away) at Golden State Warriors (home):\n\nDAL: Luka Doncic, Dereck Lively II, Kyrie Irving, P.J. Washington, and Klay Thompson\nGSW: Stephen Curry, Buddy Hield, Andrew Wiggins, Jonathan Kuminga, and Kevon Looney.\n\nPer 100 possessions, with these lineups DAL expects to outscore GSW by \\[\n-1 \\times (\\alpha_{0} + \\alpha_{SC} + \\alpha_{BH} + \\alpha_{AW} + \\alpha_{JK} + \\alpha_{KL}) +(\\alpha_{LD} + \\alpha_{KI} + \\alpha_{DL} + \\alpha_{PW} + \\alpha_{KT}).\n\\]\n\n\n\nNow imagine that you replaced Doncic w/ Anthony Davis.\nPer 100 possessions, DAL expects to outscore GSW by \\[\n-1 \\times (\\alpha_{0} + \\alpha_{SC} + \\alpha_{BH} + \\alpha_{AW} + \\alpha_{JK} + \\alpha_{KL}) +(\\alpha_{AD} + \\alpha_{KI} + \\alpha_{DL} + \\alpha_{PW} + \\alpha_{KT}).\n\\]\n\n\n\n\nDAL expects to score \\(\\alpha_{\\textrm{AD}} - \\alpha_{\\textrm{LD}}\\) more points per 100 possessions with Davis than Doncic, all else being equal"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#apm-as-a-linear-model",
    "href": "slides/raw_slides/slides_04.html#apm-as-a-linear-model",
    "title": "STAT 479: Lecture 4",
    "section": "APM As A Linear Model",
    "text": "APM As A Linear Model\n\nAppend a column of 1’s to \\(\\boldsymbol{\\mathbf{X}}\\) to form \\(n \\times (p+1)\\) matrix \\(\\boldsymbol{\\mathbf{Z}}\\)\nLet \\(\\boldsymbol{\\mathbf{z}}_{i}\\) be the \\(i\\)-th row of \\(\\boldsymbol{\\mathbf{Z}}\\)\nAPM asserts: \\(Y_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}.\\)\n\n\n\nTempting to estimate \\(\\boldsymbol{\\alpha}\\) with least squares \\[\n\\textrm{argmin} \\sum_{i = 1}^{n}{\\left( Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} \\right)^{2}} .\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#singularity",
    "href": "slides/raw_slides/slides_04.html#singularity",
    "title": "STAT 479: Lecture 4",
    "section": "Singularity",
    "text": "Singularity\n\nLeast squares problem does not have a unique solution!\nColumns of \\(\\boldsymbol{\\mathbf{Z}}\\) are linearly dependent\n\nThe first element in each row is equal to 1 (for intercept)\n5 entries equal to 1 (for home players)\n5 entries equal to -1 (for away players)\n\nIf you know all but one column, you can perfectly determine that column\n\\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) not invertible"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#non-identifiability",
    "href": "slides/raw_slides/slides_04.html#non-identifiability",
    "title": "STAT 479: Lecture 4",
    "section": "Non-identifiability",
    "text": "Non-identifiability\n\nRecall that the model asserts \\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\]\nImagine we add 5 to every \\(\\alpha_{j}\\): right-hand side remains unchanged\nSo, we can’t hope to learn \\(\\alpha_{j}\\)’s exactly!"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#baseline-contrasts",
    "href": "slides/raw_slides/slides_04.html#baseline-contrasts",
    "title": "STAT 479: Lecture 4",
    "section": "Baseline Contrasts",
    "text": "Baseline Contrasts\n\nClassify certain players as “baseline”-level (e.g., \\(&lt; 250\\) minutes)\nRe-number players so first \\(p'\\) are non-baseline\nAssumption: \\(\\alpha_{j} = \\mu\\) for all baseline players \\(j &gt; p'\\)\n\nAll baseline players assumed to have the same underlying skill\n\n\n\n\nFor non-baseline \\(j = 1, \\ldots, p,\\) let \\(\\beta_{j} = \\alpha_{j} - \\mu\\)\n\\(\\beta_{j}\\): effect of replacing player \\(j\\) with a baseline player"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#a-re-parametrized-model",
    "href": "slides/raw_slides/slides_04.html#a-re-parametrized-model",
    "title": "STAT 479: Lecture 4",
    "section": "A Re-parametrized Model",
    "text": "A Re-parametrized Model\n\n\\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\) be the \\(n \\times (p'+1)\\) submatrix of \\(\\boldsymbol{\\mathbf{Z}}\\) s.t.\n\nFirst column is all 1’s\nRemaining columns: signed on-court indicators for non-baseline players\n\nTurns out:\n\n\\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\boldsymbol{\\beta} = \\boldsymbol{\\mathbf{Z}}\\boldsymbol{\\alpha}\\)\n\\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\)\nCan minimize \\(\\sum_{i = 1}^{n}{\\left(Y_{i} - \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta}\\right)^{2}}\\)\n\n\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\tilde{\\boldsymbol{\\mathbf{Z}}}\\right)^{-1}\\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\boldsymbol{\\mathbf{Y}}.\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#estimating-boldsymbolbeta",
    "href": "slides/raw_slides/slides_04.html#estimating-boldsymbolbeta",
    "title": "STAT 479: Lecture 4",
    "section": "Estimating \\(\\boldsymbol{\\beta}\\)",
    "text": "Estimating \\(\\boldsymbol{\\beta}\\)\n\nDefining BaselinesEstimating \\(\\boldsymbol{\\beta}\\)Top-10 APM’s\n\n\n\nnonbaseline_id &lt;-\n  pm |&gt;\n  dplyr::filter(minutes &gt;= 250) |&gt;\n  dplyr::pull(id)\n\n\n\n\napm_df &lt;-\n  rapm_data |&gt;\n  dplyr::select(tidyr::all_of(c(\"margin\", nonbaseline_id)))\napm_fit &lt;- lm(margin ~ ., data = apm_df)\nbeta0 &lt;- coefficients(apm_fit)[1]\nbeta &lt;- coefficients(apm_fit)[-1]\n\n\n\n\n\n                    Name      apm\n1          Tobias Harris 17.01331\n2         Mouhamed Gueye 16.79320\n3           Devin Carter 16.60148\n4             Trae Young 15.13287\n5  Giannis Antetokounmpo 14.61082\n6            Isaiah Wong 14.50091\n7           Nikola Jokic 14.40837\n8         Alperen Sengun 13.55544\n9        Quenton Jackson 13.13962\n10    Karl-Anthony Towns 13.13714"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#missing-context",
    "href": "slides/raw_slides/slides_04.html#missing-context",
    "title": "STAT 479: Lecture 4",
    "section": "Missing Context",
    "text": "Missing Context\n\nAPM does not account for context &gt; “can artificially can artificially inflate the importance of performance in low-leverage situations, when the outcome of the game is essentially decided, while simultaneously deflating the importance of high-leverage performance, when the final outcome is still in question. For instance, point diﬀerential-based metrics model the home team’s lead dropping from 5 points to 0 points in the last minute of the first half in exactly the same way that they model the home team’s lead dropping from 30 points to 25 points in the last minute of the second half”."
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#a-weighted-version-of-apm",
    "href": "slides/raw_slides/slides_04.html#a-weighted-version-of-apm",
    "title": "STAT 479: Lecture 4",
    "section": "A Weighted Version of APM",
    "text": "A Weighted Version of APM\n\nIntroduce weight \\(w_{i}\\) for stint \\(i\\)\nFind \\(\\tilde{\\boldsymbol{\\beta}}\\) minimizing \\(w_{i}\\left(Y_{i} - \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta}\\right)^{2}\\)\nSolution: \\(\\boldsymbol{\\mathbf{W}}\\) diagonal matrix w/ entries \\(w_{i}\\) \\[\n\\hat{\\boldsymbol{\\beta}}_{w} = \\left( \\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\tilde{\\boldsymbol{\\mathbf{Z}}}\\right)^{-1}\\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{Y}}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#estimating-wapm",
    "href": "slides/raw_slides/slides_04.html#estimating-wapm",
    "title": "STAT 479: Lecture 4",
    "section": "Estimating wAPM",
    "text": "Estimating wAPM\n\nExample weights:\n\n\\(w_{i} = 1\\) if lead \\(&lt; 10\\)\n\\(w_{i} = 0\\) if one team leads by \\(&gt; 30\\) at start of \\(i\\)\n\\(w_{i} = 1 - (\\textrm{StartDiff} - 10)/20\\): if \\(10 \\leq \\textrm{lead} \\leq 30\\)\n\n\n\nDefining WeightsFit wAPMTop-10 wAPM\n\n\n\nwapm_df &lt;-\n  rapm_data |&gt;\n  dplyr::mutate(\n    start_diff = abs(start_home_score - start_away_score), \n    w = dplyr::case_when(\n      start_diff &lt; 10 ~ 1, \n      start_diff &gt; 30 ~ 0, \n      .default = 1 - (start_diff-10)/20)) |&gt; \n  dplyr::select(tidyr::all_of(c(\"margin\", \"w\", nonbaseline_id)))\n\n\n\n\nwapm_fit &lt;- \n  lm(formula = margin ~ . - w, \n     weights = w, \n     data = wapm_df) \n\n\n\n\n\n                Name     wapm\n1       Devin Carter 19.48318\n2      Tobias Harris 17.11044\n3    Lauri Markkanen 15.20344\n4     Mouhamed Gueye 14.76474\n5         Trae Young 14.52393\n6       Nikola Jokic 14.14573\n7     Alperen Sengun 14.03023\n8         OG Anunoby 13.96219\n9  Jordan McLaughlin 13.73760\n10      Jericho Sims 13.27850"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#looking-ahead",
    "href": "slides/raw_slides/slides_04.html#looking-ahead",
    "title": "STAT 479: Lecture 4",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n(w)APM is quite sensitive to certain choices:\n\nDefinition of baseline players\nChoice of weights\n\nConstant baseline skill assumption is highly unsatisfactory\nIssues due to inability to use least squares\nNext time: alternative estimation strategy\n\nAvoids having to specify baseline players\nEstimates a latent strength for all players"
  }
]