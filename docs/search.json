[
  {
    "objectID": "slides/raw_slides/slides_13.html#recap-of-lecture-12",
    "href": "slides/raw_slides/slides_13.html#recap-of-lecture-12",
    "title": "STAT 479 Lecture 13",
    "section": "Recap of Lecture 12",
    "text": "Recap of Lecture 12\n\nBradley-Terry model: team \\(j\\) has latent strength \\(\\lambda_{j}\\)\n\\(\\textrm{Log-odds}(i \\textrm{beats} j) = \\lambda_{i} - \\lambda_{j}\\) \\[\n\\mathbb{P}(i \\textrm{beats} j ) = \\frac{1}{1 + e^{-(\\lambda_{i} - \\lambda_{j})}} = \\frac{e^{\\lambda_{i}}}{e^{\\lambda_{i}} + e^{\\lambda_{j}}}.\n\\]\n\n\n\n2024 NCAA D1 Women’s Hockey results: \\(\\hat{\\lambda}_{\\textrm{WIS}} \\approx 4.68\\) & \\(\\hat{\\lambda}_{\\textrm{OSU}} \\approx 3.18\\)\n\n\n1/(1 + exp(-1 * (4.68 - 3.18)))\n\n[1] 0.8175745"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#bt-with-home-advantage-i",
    "href": "slides/raw_slides/slides_13.html#bt-with-home-advantage-i",
    "title": "STAT 479 Lecture 13",
    "section": "BT with Home Advantage I",
    "text": "BT with Home Advantage I\n\nBasic BT model: \\(\\mathbb{P}(i \\textrm{beats} j)\\) does not depend on location\nElaborated model: introduce a latent intercept \\(\\lambda_{0}\\) so that \\[\n\\begin{align*}\n\\mathbb{P}(\\textrm{team i beats team j at i's home}) &= \\frac{e^{\\lambda_{i} + \\lambda_{0}}}{e^{\\lambda_{i} + \\lambda_{0}} + e^{\\lambda_{j}}} \\\\\n& \\\\\n&= \\frac{1}{1 + e^{-1 \\times (\\lambda_{i} + \\lambda_{0} - \\lambda_{j})}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#bt-w-home-advantage-ii",
    "href": "slides/raw_slides/slides_13.html#bt-w-home-advantage-ii",
    "title": "STAT 479 Lecture 13",
    "section": "BT w/ Home Advantage II",
    "text": "BT w/ Home Advantage II\n\nProbability that \\(i\\) beats \\(j\\) now depends on location\nAt \\(i\\)’s home: \\(\\frac{e^{\\lambda_{0} + \\lambda_{i}}}{e^{\\lambda_{0} + \\lambda_{i}} + e^{\\lambda_{j}}}\\)\n\n\n\nAt \\(j\\)’s home: \\(\\frac{e^{\\lambda_{i}}}{e^{\\lambda_{i}} + e^{\\lambda_{j} + \\lambda_{0}}}\\)\n\n\n\n\nAt a neutral site: \\(\\frac{e^{\\lambda_{i}}}{e^{\\lambda_{i}} + e^{\\lambda_{j}}}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#data-preparation-i",
    "href": "slides/raw_slides/slides_13.html#data-preparation-i",
    "title": "STAT 479 Lecture 13",
    "section": "Data Preparation I",
    "text": "Data Preparation I\n\nData scraped from USCHO does not record game location\nStep 1: is game at neutral site\nManually impute using some heuristics\n\nMost regular season games at listed home team’s home\nTournaments on case-by-case basis\n\nSee course notes for details"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#data-preparation-ii",
    "href": "slides/raw_slides/slides_13.html#data-preparation-ii",
    "title": "STAT 479 Lecture 13",
    "section": "Data Preparation II",
    "text": "Data Preparation II\n\nAdd neutral site indicator\n\n\nload(\"wd1hockey_regseason_2024_2025.RData\")\n\nno_ties &lt;-\n  no_ties |&gt;\n  dplyr::mutate(\n    neutral = dplyr::case_when(\n      grepl(\"IceBreaker\", Notes) ~ 1,\n      grepl(\"East/West\", Notes) & Home != \"Minnesota\" ~ 1,\n      grepl(\"Nutmeg\", Notes) & Home != \"Sacred Heart\" ~ 1,\n      grepl(\"Mayor\", Notes) & Home == \"Rensselaer\" ~ 1,\n      grepl(\"Smashville\", Notes) ~ 1,\n      grepl(\"Beanpot\", Notes) & Home != \"Northeastern\" ~ 1, \n      Date == \"3/7/2025\" & Home == \"St. Lawrence\" & Opponent == \"Colgate\" ~ 1,\n      Date == \"3/7/2025\" & Home == \"Minnesota\" & Opponent == \"Ohio State\" ~ 1,\n      Date == \"3/8/2025\" & Home == \"Minnesota\" & Opponent == \"Wisconsin\" ~ 1,\n      .default = 0))"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#data-preparation-iii",
    "href": "slides/raw_slides/slides_13.html#data-preparation-iii",
    "title": "STAT 479 Lecture 13",
    "section": "Data Preparation III",
    "text": "Data Preparation III\n\nBradleyTerry2 expects data in following format\n\nOne row per combination of home team, away team, and location\nColumns counting home team and away team wins\n\nNew columns home.team and away.team\n\nThese variables are themselves data frames\nColumns recording team identity & whether they are at home\n\n\n\nunik_teams &lt;- sort(unique(c(no_ties$Home, no_ties$Opponent)))\n\nresults &lt;-\n  no_ties |&gt;\n  dplyr::rename(home.team = Home, away.team = Opponent) |&gt;\n  dplyr::group_by(home.team, away.team, neutral) |&gt; \n  dplyr::summarise(\n    home.win = sum(Home_Winner), \n    away.win = sum(Opp_Winner), .groups = 'drop') |&gt; \n  dplyr::mutate(\n    home.team = factor(home.team, levels = unik_teams), \n    away.team = factor(away.team,levels = unik_teams),\n    home.athome = ifelse(neutral == 1, 0, 1),\n    away.athome = 0)"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#model-fitting",
    "href": "slides/raw_slides/slides_13.html#model-fitting",
    "title": "STAT 479 Lecture 13",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nTemporary Data TableModel FitSummary\n\n\n\ntmp_df &lt;- data.frame(home.win = results$home.win, away.win = results$away.win)\ntmp_df$home.team &lt;- data.frame(team = results$home.team, at.home = results$home.athome)\ntmp_df$away.team &lt;- data.frame(team = results$away.team, at.home = results$away.athome)\n\n\n\n\nfit &lt;-\n  BradleyTerry2::BTm(\n    outcome = cbind(home.win, away.win),\n    player1 = home.team, player2 = away.team,\n    formula = ~ team + at.home,\n    refcat = \"New Hampshire\",\n    id = \"team\",\n    data = tmp_df) \n\n\n\n\nsummary(fit)\n\n\nCall:\nBradleyTerry2::BTm(outcome = cbind(home.win, away.win), player1 = home.team, \n    player2 = away.team, formula = ~team + at.home, id = \"team\", \n    refcat = \"New Hampshire\", data = tmp_df)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \nteamAssumption        -4.74585    1.24679  -3.806 0.000141 ***\nteamBemidji State     -0.75271    0.80275  -0.938 0.348415    \nteamBoston College     0.88528    0.52440   1.688 0.091375 .  \nteamBoston University  1.13510    0.52380   2.167 0.030229 *  \nteamBrown              0.14970    0.65910   0.227 0.820326    \nteamClarkson           1.22543    0.62242   1.969 0.048975 *  \nteamColgate            2.07470    0.65875   3.149 0.001636 ** \nteamConnecticut        1.11779    0.53080   2.106 0.035217 *  \nteamCornell            2.63595    0.71812   3.671 0.000242 ***\nteamDartmouth         -0.79392    0.69582  -1.141 0.253875    \nteamFranklin Pierce   -3.94938    1.23673  -3.193 0.001406 ** \nteamHarvard           -2.17071    0.87974  -2.467 0.013608 *  \nteamHoly Cross        -0.50866    0.52815  -0.963 0.335499    \nteamLindenwood        -1.47362    0.71219  -2.069 0.038532 *  \nteamLIU               -3.07687    1.21642  -2.529 0.011425 *  \nteamMaine             -0.38395    0.53066  -0.724 0.469350    \nteamMercyhurst         0.51099    0.64323   0.794 0.426955    \nteamMerrimack         -0.78126    0.52339  -1.493 0.135519    \nteamMinnesota          2.76577    0.74846   3.695 0.000220 ***\nteamMinnesota Duluth   2.17957    0.76679   2.842 0.004477 ** \nteamMinnesota State    0.78237    0.78409   0.998 0.318375    \nteamNortheastern       1.02645    0.51130   2.008 0.044695 *  \nteamOhio State         3.27350    0.79365   4.125 3.71e-05 ***\nteamPenn State         2.02649    0.71347   2.840 0.004507 ** \nteamPost              -4.42233    1.23567  -3.579 0.000345 ***\nteamPrinceton          0.90657    0.66577   1.362 0.173298    \nteamProvidence         0.66011    0.52149   1.266 0.205585    \nteamQuinnipiac         1.07109    0.59894   1.788 0.073726 .  \nteamRensselaer        -0.29970    0.61486  -0.487 0.625954    \nteamRIT               -0.21963    0.66882  -0.328 0.742616    \nteamRobert Morris     -1.61446    0.71727  -2.251 0.024397 *  \nteamSacred Heart      -3.33864    1.22272  -2.731 0.006324 ** \nteamSt. Anselm        -4.02309    1.20138  -3.349 0.000812 ***\nteamSt. Cloud State    1.28838    0.75363   1.710 0.087349 .  \nteamSt. Lawrence       1.31564    0.62136   2.117 0.034229 *  \nteamSt. Michael's     -5.91267    1.29972  -4.549 5.39e-06 ***\nteamSt. Thomas         0.08436    0.80906   0.104 0.916954    \nteamStonehill         -4.17369    1.23819  -3.371 0.000749 ***\nteamSyracuse          -0.39362    0.65720  -0.599 0.549213    \nteamUnion             -0.05669    0.59147  -0.096 0.923646    \nteamVermont           -0.56482    0.52659  -1.073 0.283454    \nteamWisconsin          4.77395    0.99956   4.776 1.79e-06 ***\nteamYale               0.82238    0.63286   1.299 0.193779    \nat.home                0.31079    0.09542   3.257 0.001126 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 817.92  on 476  degrees of freedom\nResidual deviance: 455.43  on 432  degrees of freedom\nAIC: 661.38\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#fitted-probabilities",
    "href": "slides/raw_slides/slides_13.html#fitted-probabilities",
    "title": "STAT 479 Lecture 13",
    "section": "Fitted Probabilities",
    "text": "Fitted Probabilities\n\nEstimatesLog-OddsProbabilities\n\n\n\nlambda0_hat &lt;- coef(fit)[\"at.home\"] \nlambda_hat &lt;- BradleyTerry2::BTabilities(fit)\ncat(\"Intercept: \", lambda0_hat, \"\\n\")\n\nIntercept:  0.3107851 \n\ncat(\"WIS: \", lambda_hat[\"Wisconsin\", \"ability\"], \"\\n\")\n\nWIS:  4.773946 \n\ncat(\"OSU: \", lambda_hat[\"Ohio State\", \"ability\"], \"\\n\")\n\nOSU:  3.273501 \n\n\n\n\n\nLog-odds that WIS beats OSU:\n\nAt WIS: \\(\\hat{\\lambda}_{\\textrm{WIS}} + \\hat{\\lambda}_{0} - \\hat{\\lambda}_{\\textrm{OSU}}.\\)\nAt OSU: \\(\\hat{\\lambda}_{\\textrm{WIS}} - (\\hat{\\lambda_{\\textrm{OSU}}} + \\hat{\\lambda}_{0}).\\)\nNeutral site: \\(\\hat{\\lambda}_{\\textrm{WIS}} - \\hat{\\lambda}_{\\textrm{OSU}}.\\)\n\n\n\n\n\ndiff: quantifies different in team strengths (\\(\\textrm{WIS} - \\textrm{OSU}\\))\n\n\ndiff &lt;- lambda_hat[\"Wisconsin\",\"ability\"] - lambda_hat[\"Ohio State\", \"ability\"]\ncat(\"At WIS:\", round(100 * 1/(1 + exp(-1 * (diff + lambda0_hat))), digits = 2), \"%\\n\")\n\nAt WIS: 85.95 %\n\ncat(\"AT OSU:\", round(100 * 1/(1 + exp(-1 * (diff - lambda0_hat))), digits = 2), \"%\\n\")\n\nAT OSU: 76.67 %\n\ncat(\"AT neutral site:\", round(100 * 1/(1 + exp(-1 * (diff))), digits = 2), \"%\\n\")\n\nAT neutral site: 81.76 %"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#background",
    "href": "slides/raw_slides/slides_13.html#background",
    "title": "STAT 479 Lecture 13",
    "section": "Background",
    "text": "Background\n\nSingle-elimination tournament w/ last four teams\nWIS, OSU, COR, and MIN\nFirst semi-final (SF1): WIS vs MIN\nSecond semi-final (SF2): OSU vs COR\nFinals: winner SF1 vs winner SF2\nAll games played at Ridder Arena (MIN’s home ice)"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#computing-matchup-probabilities",
    "href": "slides/raw_slides/slides_13.html#computing-matchup-probabilities",
    "title": "STAT 479 Lecture 13",
    "section": "Computing Matchup Probabilities",
    "text": "Computing Matchup Probabilities\n\ndiff: difference in team strengths (log-odds scale)\nprob: \\(\\mathbb{P}(\\textrm{higher seed beats lower seed})\\)\n\n\nseeds &lt;- data.frame(\n  Team = c(\"Wisconsin\", \"Ohio State\", \"Cornell\", \"Minnesota\"),\n  Seed = 1:4)\n\npossible_matchups &lt;-\n  expand.grid(Hi = seeds$Team, Lo = seeds$Team) |&gt;\n  as.data.frame() |&gt;\n  dplyr::inner_join(y = seeds |&gt; dplyr::rename(Hi = Team, Hi.Seed=Seed), by = \"Hi\") |&gt;\n  dplyr::inner_join(y = seeds |&gt; dplyr::rename(Lo = Team, Lo.Seed=Seed), by = \"Lo\") |&gt;\n  dplyr::filter(Hi.Seed &lt; Lo.Seed) |&gt;\n  dplyr::mutate(neutral = ifelse(Hi == \"Minnesota\" | Lo == \"Minnesota\", 0, 1)) |&gt;\n  dplyr::mutate(\n    diff = lambda_hat[Hi, \"ability\"] - lambda_hat[Lo, \"ability\"],\n    prob = dplyr::case_when(\n      neutral == 1 ~ 1/(1 + exp(-1 * diff)),\n      neutral == 0 & Hi == \"Minnesota\" ~ 1/(1 + exp(-1 * (diff + lambda0_hat))),\n      neutral == 0 & Lo == \"Minnesota\" ~ 1/(1 + exp(-1 * (diff - lambda0_hat)))))"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#matchup-probabilities",
    "href": "slides/raw_slides/slides_13.html#matchup-probabilities",
    "title": "STAT 479 Lecture 13",
    "section": "Matchup Probabilities",
    "text": "Matchup Probabilities\n\n\n          Hi         Lo Hi.Seed Lo.Seed neutral       diff      prob\n1  Wisconsin Ohio State       1       2       1  1.5004449 0.8176408\n2  Wisconsin    Cornell       1       3       1  2.1379945 0.8945416\n3 Ohio State    Cornell       2       3       1  0.6375496 0.6541993\n4  Wisconsin  Minnesota       1       4       0  2.0081754 0.8451936\n5 Ohio State  Minnesota       2       4       0  0.5077305 0.5490778\n6    Cornell  Minnesota       3       4       0 -0.1298191 0.3915970"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#semi-final-simulation",
    "href": "slides/raw_slides/slides_13.html#semi-final-simulation",
    "title": "STAT 479 Lecture 13",
    "section": "Semi-Final Simulation",
    "text": "Semi-Final Simulation\n\nsemis &lt;- \n  data.frame(Hi = c(\"Wisconsin\", \"Ohio State\"), Lo = c(\"Minnesota\", \"Cornell\")) |&gt;\n  dplyr::left_join(possible_matchups |&gt; dplyr::select(Hi, Lo, prob), by = c(\"Hi\", \"Lo\"))\n\nset.seed(479)\nsf_winners &lt;- c(NA, NA)\n\nsf_outcomes &lt;- rbinom(n = nrow(semis), size = 1, prob = semis$prob)\nfor(i in 1:nrow(semis)){\n  if(sf_outcomes[i] == 1) sf_winners[i] &lt;- semis$Hi[i]\n  else sf_winners[i] &lt;- semis$Lo[i]\n}\ncat(\"Semi-final outcomes:\", sf_outcomes, \"\\n\")\ncat(\"Semi-final winners:\", sf_winners, \"\\n\")\n\nSemi-final outcomes: 0 1 \nSemi-final winners: Minnesota Ohio State"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#finals-simulation",
    "href": "slides/raw_slides/slides_13.html#finals-simulation",
    "title": "STAT 479 Lecture 13",
    "section": "Finals Simulation",
    "text": "Finals Simulation\n\nfinals &lt;- \n  data.frame(Team1 = sf_winners[1], Team2 = sf_winners[2]) |&gt;\n  dplyr::left_join(y = seeds |&gt; dplyr::rename(Team1 = Team, Team1.Seed = Seed), by = \"Team1\") |&gt;\n  dplyr::left_join(y = seeds |&gt; dplyr::rename(Team2 = Team, Team2.Seed = Seed), by = \"Team2\") |&gt;\n  dplyr::mutate(\n    Hi = ifelse(Team1.Seed &lt; Team2.Seed, Team1, Team2),\n    Lo = ifelse(Team1.Seed &lt; Team2.Seed, Team2, Team1)) |&gt;\n  dplyr::select(Hi, Lo) |&gt; \n  dplyr::left_join(\n    y = possible_matchups |&gt; dplyr::select(Hi, Lo, prob), by = c(\"Hi\", \"Lo\"))\n\nfinal_outcome &lt;- rbinom(n = 1, size = 1, prob = finals$prob)\nif(final_outcome == 1){\n  final_winner &lt;- finals$Hi[1]\n} else{\n  final_winner &lt;- finals$Lo[1]\n}\n\ncat(\"Finals outcome:\", final_outcome, \"\\n\")\n\nFinals outcome: 1 \n\ncat(\"Finals winner:\", final_winner, \"\\n\")\n\nFinals winner: Ohio State"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#repeated-simulations",
    "href": "slides/raw_slides/slides_13.html#repeated-simulations",
    "title": "STAT 479 Lecture 13",
    "section": "Repeated Simulations",
    "text": "Repeated Simulations\n\nEmbed earlier code into large for loop\nIn each iteration, we will save\n\nSemi-finals winners:SF1_winner and SF2_winner\nTeams in finals: Finals_Hi and Finals_Lo\nEventual Champion Champion\n\nUsing simulations, we will estimate\n\n\\(\\mathbb{P}(\\textrm{WIS makes it to and wins the finals})\\)\n\\(\\mathbb{P}(\\textrm{WIS wins the finals} \\vert \\textrm{Wisconsin makes it to the finals}).\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#simulation-snapshot",
    "href": "slides/raw_slides/slides_13.html#simulation-snapshot",
    "title": "STAT 479 Lecture 13",
    "section": "Simulation Snapshot",
    "text": "Simulation Snapshot\n\nResults of the first 10 simulations\n\n\n\n   SF1_Winner SF2_Winner  Finals_Hi  Finals_Lo   Champion\n1   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n2   Wisconsin Ohio State  Wisconsin Ohio State Ohio State\n3   Wisconsin    Cornell  Wisconsin    Cornell  Wisconsin\n4   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n5   Wisconsin    Cornell  Wisconsin    Cornell  Wisconsin\n6   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n7   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n8   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n9   Wisconsin    Cornell  Wisconsin    Cornell  Wisconsin\n10  Minnesota Ohio State Ohio State  Minnesota  Minnesota\n\n\n\n\ntable(results$Champion)\n\n\n   Cornell  Minnesota Ohio State  Wisconsin \n       502        765       1544       7189"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#estimating-complex-probabilities",
    "href": "slides/raw_slides/slides_13.html#estimating-complex-probabilities",
    "title": "STAT 479 Lecture 13",
    "section": "Estimating Complex Probabilities",
    "text": "Estimating Complex Probabilities\n\nAcross the 10,000 simulations:\n\nWIS made the finals 8496 times\nWIS made and won the finals 7189 times\n\n\\(\\mathbb{P}(\\textrm{WIS wins the finals} \\vert \\textrm{WIS makes it to the finals}) \\approx 84.6\\%.\\)\n\n\nsum(results$SF1_Winner == \"Wisconsin\" & \n      results$Champion == \"Wisconsin\")/sum(results$SF1_Winner == \"Wisconsin\")\n\n[1] 0.8461629"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#alternative-rules",
    "href": "slides/raw_slides/slides_13.html#alternative-rules",
    "title": "STAT 479 Lecture 13",
    "section": "Alternative Rules",
    "text": "Alternative Rules\n\nWhat if higher seed always gets to play at home?\nSee lecture notes for simulation\nVery similar code but uses different matchup probabilities\nUnder alternative, WIS wins championship about 77% of the time (as opposed to 71% at neutral site)"
  },
  {
    "objectID": "slides/raw_slides/slides_13.html#introducing-covariates",
    "href": "slides/raw_slides/slides_13.html#introducing-covariates",
    "title": "STAT 479 Lecture 13",
    "section": "Introducing Covariates",
    "text": "Introducing Covariates\n\nWhat if we had team-level covariates?\n\nOffensive/defensive rating, record to date, other measures of strength\n\nPossible to elaborate model: \\(\\lambda_{j} = U_{j} + \\boldsymbol{\\mathbf{x}}_{j}^{\\top}\\beta\\)\nUnder elaborated model \\[\n\\log\\left(\\frac{\\mathbb{P}(i \\textrm{ beats } j)}{\\mathbb{P}(j \\textrm{ beats } i)}\\right) = U_{i} - U_{j} + (\\boldsymbol{\\mathbf{x}}_{i} - \\boldsymbol{\\mathbf{x}}_{j})^{\\top}\\beta\n\\]\n\nDepends on difference in observed covariates & latent team-specific random effect"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#motivation",
    "href": "slides/raw_slides/slides_11.html#motivation",
    "title": "STAT 479 Lecture 11",
    "section": "Motivation",
    "text": "Motivation\n\n\n\n\n\n\n\n\n\n\n\n(a) Pitch called a ball\n\n\n\n\n\n\n\n\n\n\n\n(b) Pitch called a strike\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\nBoth pitches miss the strike zone and should, by rule, be called balls\nBut pitch on the right was called a strike\n\n\n\nHow much do catchers influence umpires’ calls?"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#framing",
    "href": "slides/raw_slides/slides_11.html#framing",
    "title": "STAT 479 Lecture 11",
    "section": "Framing",
    "text": "Framing\n\nAbility of catcher to receive pitch so as to increase \\(\\mathbb{P}(\\textrm{called strike})\\)\nAbility to “steal a strike” / “turn balls into strikes”\nStudied by the sabermetrics community since 2008\nLots of popular press attention b/w 2014 & 2016"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#jonathan-lucroy-needs-a-raise",
    "href": "slides/raw_slides/slides_11.html#jonathan-lucroy-needs-a-raise",
    "title": "STAT 479 Lecture 11",
    "section": "Jonathan Lucroy Needs a Raise",
    "text": "Jonathan Lucroy Needs a Raise\n\nAccording to Baseball Prospectus, Lucroy produced 121 stolen strikes last season and in the past five seasons clocks in at more than 1,000, the most in MLB. And if you believe the metrics, these stolen strikes have been worth about 18 wins during his five-year career – just shy of what Giancarlo Stanton’s entire output has added up to during the same time. Still, Lucroy’s discreetly prodigious output has been underestimated. By fans. By the media. By his own team. And certainly by the game’s salary structure. Even in today’s post-Moneyball world, pitch framing is viewed through a skeptical lens; a value-added talent, sure, but one for which teams are reluctant to pay. While Stanton cashed in with a 13-year, $325 million contract this offseason and Mike Trout begins the first year of his $144.5 million deal, Lucroy was actually more valuable last year. For that he earned $2 million; this year, he’ll make $3 million.\n\n\nPut another way: The most impactful player in baseball today is the game’s 17th highest-paid catcher."
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#overview",
    "href": "slides/raw_slides/slides_11.html#overview",
    "title": "STAT 479 Lecture 11",
    "section": "Overview",
    "text": "Overview\n\nGoal: estimate how many runs catcher saves his team\nMultilevel model to predict \\(\\mathbb{P}(\\textrm{called strike})\\)\n\nRandom intercepts for batter, pitcher, catcher\nFixed effect: baseline called strike prob. based on previous season\n\n“Runs Saved Above Replacement”"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#elaborated-run-expectancy",
    "href": "slides/raw_slides/slides_11.html#elaborated-run-expectancy",
    "title": "STAT 479 Lecture 11",
    "section": "Elaborated Run Expectancy",
    "text": "Elaborated Run Expectancy\n\nLectures 6–8: run expectancy at the at-bat level\nToday: run expectancy at the pitch level\n\\(\\rho(\\textrm{o}, \\textrm{counts})\\):\n\nAvg. runs scored following pitch in given count & game state\n3 outs \\(\\times\\) 12 counts"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#identifying-taken-pitches-i",
    "href": "slides/raw_slides/slides_11.html#identifying-taken-pitches-i",
    "title": "STAT 479 Lecture 11",
    "section": "Identifying Taken Pitches I",
    "text": "Identifying Taken Pitches I\n\ndescription: pitch-level descriptor\n\n\ntable(statcast2024$description)\n\n\n                   ball            blocked_ball           bunt_foul_tip \n                 231032                   14717                      15 \n          called_strike                    foul               foul_bunt \n                 113912                  126012                    1208 \n               foul_tip            hit_by_pitch           hit_into_play \n                   7218                    1979                  121751 \n            missed_bunt                pitchout         swinging_strike \n                    196                      52                   73209 \nswinging_strike_blocked \n                   3834"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#identifying-taken-pitches-ii",
    "href": "slides/raw_slides/slides_11.html#identifying-taken-pitches-ii",
    "title": "STAT 479 Lecture 11",
    "section": "Identifying Taken Pitches II",
    "text": "Identifying Taken Pitches II\n\nManually classify description values as called balls or strikes\n\n\nswing_descriptions &lt;- \n  c(\"bunt_foul_tip\", \"foul\", \"foul_bunt\", \"foul_tip\",\n    \"hit_into_play\", \"missed_bunt\", \"swinging_strike\",\n    \"swinging_strike_block\")\n\ntaken2024 &lt;- statcast2024 |&gt;\n  dplyr::filter(!description %in% swing_descriptions) |&gt; \n  dplyr::mutate(\n    Y = ifelse(description == \"called_strike\", 1, 0), \n    Count = paste(balls, strikes, sep = \"-\")) |&gt; \n  dplyr::select(\n    Y, plate_x, plate_z, Count, Outs,\n    batter, pitcher, fielder_2, \n    stand, p_throws, RunsRemaining, sz_top, sz_bot) |&gt; \n  dplyr::mutate(\n    Count = factor(Count),\n    batter = factor(batter),\n    pitcher = factor(pitcher),\n    fielder_2 = factor(fielder_2),\n    stand = factor(stand),\n    p_throws = factor(p_throws))"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#pitch-level-run-expectancy",
    "href": "slides/raw_slides/slides_11.html#pitch-level-run-expectancy",
    "title": "STAT 479 Lecture 11",
    "section": "Pitch-level Run Expectancy",
    "text": "Pitch-level Run Expectancy\n\nOnly use taken pitches in each out-base runner-count state\n\ner_balls: run expectancy following a called ball in given state\ner_strikes: run expectancy following called strike in given state\n\n\n\nCodeRun Expectancy Matrix\n\n\n\ner_balls &lt;-\n  taken2024 |&gt;\n  dplyr::filter(Y == 0) |&gt;\n  dplyr::group_by(Count, Outs) |&gt;\n  dplyr::summarise(er_ball = mean(RunsRemaining), .groups = 'drop')\n\ner_strikes &lt;-\n  taken2024 |&gt;\n  dplyr::filter(Y == 1) |&gt;\n  dplyr::group_by(Count, Outs) |&gt;\n  dplyr::summarise(er_strike = mean(RunsRemaining), .groups = 'drop')\n\ner_taken &lt;-\n  er_balls |&gt;\n  dplyr::left_join(y = er_strikes, by = c(\"Count\", \"Outs\")) |&gt;\n  dplyr::mutate(value = er_ball - er_strike)\n\n\n\n\n\n# A tibble: 10 × 5\n   Count  Outs er_ball er_strike  value\n   &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 0-0       0   0.731     0.605 0.126 \n 2 0-0       1   0.547     0.425 0.122 \n 3 0-0       2   0.283     0.193 0.0902\n 4 0-1       0   0.660     0.525 0.135 \n 5 0-1       1   0.464     0.365 0.0990\n 6 0-1       2   0.234     0.166 0.0684\n 7 0-2       0   0.601     0.354 0.247 \n 8 0-2       1   0.408     0.211 0.197 \n 9 0-2       2   0.163     0     0.163 \n10 1-0       0   0.800     0.643 0.157"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#most-least-valuable-called-strikes",
    "href": "slides/raw_slides/slides_11.html#most-least-valuable-called-strikes",
    "title": "STAT 479 Lecture 11",
    "section": "Most & Least Valuable Called Strikes",
    "text": "Most & Least Valuable Called Strikes\n\nFor fielding team, a called strike\n\nIn a 3-2 count w/ 0 outs saves 0.8 runs\nIn a 0-1 count w/ 2 outs saves 0.07 runs\n\n\n\n\n# A tibble: 4 × 5\n  Count  Outs er_ball er_strike  value\n  &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 3-2       0   1.12      0.321 0.800 \n2 3-2       1   0.815     0.203 0.612 \n3 0-0       2   0.283     0.193 0.0902\n4 0-1       2   0.234     0.166 0.0684"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#pitch-location",
    "href": "slides/raw_slides/slides_11.html#pitch-location",
    "title": "STAT 479 Lecture 11",
    "section": "Pitch Location",
    "text": "Pitch Location\n\nplate_x and plate_z coordinates of pitch as it cross front of home plate\nplate_x measured from catcher’s perspective (right-handed batters stand on the left)\n\n\n\n\n\n\n\n\n\nFigure 2: All called balls and strikes"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#high-level-model",
    "href": "slides/raw_slides/slides_11.html#high-level-model",
    "title": "STAT 479 Lecture 11",
    "section": "High-Level Model",
    "text": "High-Level Model\n\nLevel 1: Fixed effects of location (\\(x\\) and \\(z\\)) may be non-linear \\[\n\\log \\left( \\frac{\\mathbb{P}(Y_{i} = 1)}{\\mathbb{P}(Y_{i} = 0)} \\right) = B_{b[i]} + C_{c[i]} + P_{p[i]} + f(x_{i}, z_{i})\n\\]\nLevel 2: Random intercepts for Batter, Pither, Catcher \\[\n\\begin{align}\nB_{i} &= \\mu_{B} + u^{(B)}_{b[i]}; u^{(B)}_{b} \\sim \\mathcal{N}(0,\\sigma^{2(B)}) \\\\\nC_{i} &= \\mu_{C} + u^{(C)}_{c[i]}; u^{(C)}_{c} \\sim \\mathcal{N}(0,\\sigma^{2(C)}) \\\\\nP_{i} &= \\mu_{P} + u^{(P)}_{p[i]}; u^{(P)}_{p} \\sim \\mathcal{N}(0,\\sigma^{2(P)})\n\\end{align}\n\\]\n\n\n\nIdea #1: Assume \\(f(x,z) = \\beta_{X}x + \\beta_{Z}z\\)\n\nProblem: assumes \\(\\mathbb{P}(\\textrm{called strike})\\) is monotonic"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#adjusting-for-historical-tendencies",
    "href": "slides/raw_slides/slides_11.html#adjusting-for-historical-tendencies",
    "title": "STAT 479 Lecture 11",
    "section": "Adjusting for Historical Tendencies",
    "text": "Adjusting for Historical Tendencies\n\nInstead of trying to pre-specify functional form of \\(f(x,z)\\) \\[\n\\log \\left( \\frac{\\mathbb{P}(Y_{i} = 1)}{\\mathbb{P}(Y_{i} = 0)} \\right) = B_{b[i]} + C_{c[i]} + P_{p[i]} + \\beta_{p} \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})}\\right)\n\\]\n\\(\\hat{p}(x,z)\\): baseline called strike prob. estimated from 2023 season"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#historical-gam",
    "href": "slides/raw_slides/slides_11.html#historical-gam",
    "title": "STAT 479 Lecture 11",
    "section": "Historical GAM",
    "text": "Historical GAM\n\nFilter out pitches too far away from strike zone\n\nplate_x outside [-1.5, 1.5]\nplate_z outside [1,6]\n\n\n\nlibrary(mgcv)\nhgam_fit &lt;- \n  bam(formula = Y ~ stand + p_throws + s(plate_x, plate_z),  \n      family = binomial(link=\"logit\"), \n      data = taken2023)"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#historical-called-strike-probability",
    "href": "slides/raw_slides/slides_11.html#historical-called-strike-probability",
    "title": "STAT 479 Lecture 11",
    "section": "Historical Called Strike Probability",
    "text": "Historical Called Strike Probability\n\n\n\n\n\n\n\n\nFigure 3: Historical called strike probabilities for a right-handed batter facing a right-handed pitcher."
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#appending-baseline",
    "href": "slides/raw_slides/slides_11.html#appending-baseline",
    "title": "STAT 479 Lecture 11",
    "section": "Appending Baseline",
    "text": "Appending Baseline\n\nHistorical GAM trained on 2023 data\nUse it to make predictions on all 2024 taken pitches\n\n\nbaseline &lt;- \n  predict(object = hgam_fit,\n          newdata = taken2024, \n          type = \"link\")\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::mutate(baseline = baseline) |&gt;\n  dplyr::filter(abs(plate_x) &lt;= 1.5 & plate_z &gt;= 1 & plate_z &lt;= 6)"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#fitting-our-model",
    "href": "slides/raw_slides/slides_11.html#fitting-our-model",
    "title": "STAT 479 Lecture 11",
    "section": "Fitting our Model",
    "text": "Fitting our Model\n\nlibrary(lme4)\nmultilevel_fit &lt;-\n  glmer(formula = Y ~ 1 + (1 | fielder_2) + (1 | batter)  + (1 | pitcher) +  baseline, \n        family = binomial(link = \"logit\"),\n        data = taken2024)\nsummary(multilevel_fit)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Y ~ 1 + (1 | fielder_2) + (1 | batter) + (1 | pitcher) + baseline\n   Data: taken2024\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n 111223.6  111276.3  -55606.8  111213.6    274780 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-69.844  -0.135  -0.014   0.116 161.727 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n pitcher   (Intercept) 0.05122  0.2263  \n batter    (Intercept) 0.02629  0.1621  \n fielder_2 (Intercept) 0.04360  0.2088  \nNumber of obs: 274785, groups:  pitcher, 851; batter, 649; fielder_2, 100\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.084101   0.027122   3.101  0.00193 ** \nbaseline    1.027338   0.004563 225.126  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr)\nbaseline 0.023"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#extracting-uc_c",
    "href": "slides/raw_slides/slides_11.html#extracting-uc_c",
    "title": "STAT 479 Lecture 11",
    "section": "Extracting \\(u^{(C)}_{c}\\)",
    "text": "Extracting \\(u^{(C)}_{c}\\)\n\nModel: \\(C_{c} = \\mu_{C} + u^{(C)}_{c}\\) where \\(u^{(C)}_{c} \\sim \\mathcal{N}(0,\\sigma^{2}_{c})\\)\nReminder: cannot directly estimate catcher-specific intercepts \\(C_{c}\\) or global average \\(\\mu_{C}\\)\nWe can extract deviations \\(u^{(C)}_{c}\\) using ranef()\n\n\ntmp &lt;- ranef(multilevel_fit)\ncatcher_u &lt;- \n  data.frame(\n    fielder_2 = as.integer(rownames(tmp[[\"fielder_2\"]])),\n    catcher_u = tmp[[\"fielder_2\"]][,1])"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#defining-replacement-level",
    "href": "slides/raw_slides/slides_11.html#defining-replacement-level",
    "title": "STAT 479 Lecture 11",
    "section": "Defining Replacement Level",
    "text": "Defining Replacement Level\n\nNon-replacement: top \\(30 \\times 2\\) catchers sorted by number of pitches caught\n\\(\\overline{u}^{(C)}_{R}\\): average \\(u^{(C)}_{c}\\) among all replacement-level catchers\n\n\ncatcher_counts &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarise(count = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(count))\n\ncatcher_threshold &lt;- \n  catcher_counts |&gt;\n  dplyr::slice(60) |&gt;\n  dplyr::pull(count)\n\ncatcher_u &lt;-\n  catcher_u |&gt;\n  dplyr::left_join(y = catcher_counts, by = \"fielder_2\")\n\nrepl_u &lt;-\n  catcher_u |&gt;\n  dplyr::filter(count &lt; catcher_threshold) |&gt;\n  dplyr::pull(catcher_u) |&gt;\n  mean()"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#counterfactual-predictions",
    "href": "slides/raw_slides/slides_11.html#counterfactual-predictions",
    "title": "STAT 479 Lecture 11",
    "section": "Counterfactual Predictions",
    "text": "Counterfactual Predictions\n\nFor every taken pitch, use model to predict\n\nCalled strike prob. with original catcher\nCalled strike prob. with a replacement-level catcher\n\n\n\n\nOriginal log-odds:\n\n\\[\n\\mu_{C} + u_{c[i]}^{(C)} + B_{b[i]} + P_{p[i]} + \\hat{\\beta}_{p} \\times \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})} \\right)\n\\]\n\n\n\nCounter-factual log-odds:\n\n\\[\n\\mu_{C} + \\overline{u}_{R}^{(C)} + B_{b[i]} + P_{p[i]} + \\hat{\\beta}_{p} \\times \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})} \\right)\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#computing-counterfactual-predictions",
    "href": "slides/raw_slides/slides_11.html#computing-counterfactual-predictions",
    "title": "STAT 479 Lecture 11",
    "section": "Computing Counterfactual Predictions",
    "text": "Computing Counterfactual Predictions\n\nOriginalCounterfactual\n\n\n\nml_preds &lt;-\n  predict(object = multilevel_fit,\n          newdata = taken2024,\n          type = \"link\")\ntaken2024 &lt;- taken2024 |&gt;\n  dplyr::mutate(fitted_log_odds = ml_preds)\n\n\n\n\nSubtract original \\(u^{(C)}_{c}\\) and add \\(\\overline{u}^{(C)}_{R}\\) to log-odds\n\n\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::left_join(\n    y = catcher_u |&gt; dplyr::select(fielder_2, catcher_u) |&gt;\n      dplyr::mutate(fielder_2 = factor(fielder_2)),\n    by = \"fielder_2\") |&gt;\n  dplyr::mutate(repl_log_odds = fitted_log_odds - catcher_u + repl_u)"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#computing-rsar",
    "href": "slides/raw_slides/slides_11.html#computing-rsar",
    "title": "STAT 479 Lecture 11",
    "section": "Computing RSAR",
    "text": "Computing RSAR\n\nCrediting CatcherTotalsTop-10 RSAR\n\n\n\nWeight change in called strike prob. by value of called strike\n\n\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::mutate(\n    fitted_prob = 1/(1 + exp(-1 * fitted_log_odds)),\n    repl_prob = 1/(1 + exp(-1 * repl_log_odds)),\n    rsar = value * (fitted_prob - repl_prob))\n\n\n\n\nSum rsar value across whole season\n\n\nrsar &lt;-\n  taken2024 |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarise(rsar = sum(rsar), n = dplyr::n()) |&gt; \n  dplyr::rename(key_mlbam = fielder_2) |&gt;\n  dplyr::left_join(player2024_lookup |&gt;\n                     dplyr::select(key_mlbam, Name) |&gt;\n                     dplyr::mutate(key_mlbam = factor(key_mlbam)), by = \"key_mlbam\")\n\n\n\n\nrsar |&gt; \n  dplyr::arrange(dplyr::desc(rsar)) |&gt;\n  dplyr::select(Name, rsar) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   Name               rsar\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Patrick Bailey     29.9\n 2 Cal Raleigh        19.8\n 3 Austin Wells       17.9\n 4 Alejandro Kirk     17.2\n 5 Jake Rogers        16.8\n 6 Christian Vazquez  15.1\n 7 Jose Trevino       14.6\n 8 Francisco Alvarez  13.4\n 9 Bo Naylor          11.5\n10 Yasmani Grandal    11.0"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#uncertainty-quantification",
    "href": "slides/raw_slides/slides_11.html#uncertainty-quantification",
    "title": "STAT 479 Lecture 11",
    "section": "Uncertainty Quantification",
    "text": "Uncertainty Quantification\n\nTop framers appear to save nearly 30 runs over replacement\nTranslates to about 3 wins\nExercise: use the bootstrap to quantify uncertainty\n\nCreate several bootstrap re-samples of 2024 taken pitches\nMust re-fit the multilevel model and compute GSAR\nUse the original replacement-level\n\nQuantifying uncertainty: critical to determining monetary value of framing"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#my-own-research",
    "href": "slides/raw_slides/slides_11.html#my-own-research",
    "title": "STAT 479 Lecture 11",
    "section": "My Own Research",
    "text": "My Own Research\n\nI’ve studied framing since 2015\n\nInitial paper: Bayesian models to “borrow strength” across umpires\nConclusion: lots of uncertainty in framing effects\n\nMore recently: use Bayesian Additive Regression Trees (BART) to model \\(\\mathbb{P}(\\textrm{strike})\\)\n\nFaster & more flexible: (2 hours on a desktop vs 50 hours on a cluster)\nBetter predictions: mis-classification of 7% vs 10%\nSimilar conclusions: possibly large effect but lots of uncertainty"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#framing-effectsin-2009",
    "href": "slides/raw_slides/slides_11.html#framing-effectsin-2009",
    "title": "STAT 479 Lecture 11",
    "section": "Framing Effectsin 2009",
    "text": "Framing Effectsin 2009"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#framing-over-time",
    "href": "slides/raw_slides/slides_11.html#framing-over-time",
    "title": "STAT 479 Lecture 11",
    "section": "Framing Over Time",
    "text": "Framing Over Time"
  },
  {
    "objectID": "slides/raw_slides/slides_11.html#announcements",
    "href": "slides/raw_slides/slides_11.html#announcements",
    "title": "STAT 479 Lecture 11",
    "section": "Announcements",
    "text": "Announcements\n\nProjects are due tomorrow at noon\n\nIf you need an extension, email me\nPeer review assignments will be made on Canvas by Sunday night\nPeer review due on Sunday 10/18\n\nNext week: new unit on ranking & simulation\n\nLectures 12 & 13: hockey power rankings & the NCAA D1 National Championship\nLecture 14 & 15: Markov chain simulations\nLecture 16 & 17: Mock drafts & other simulations\n\nProject 2 Information will be posted by Sunday evening"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#comparing-football-plays",
    "href": "slides/raw_slides/slides_09.html#comparing-football-plays",
    "title": "STAT 479 Lecture 9",
    "section": "Comparing Football Plays",
    "text": "Comparing Football Plays\n\nWhich of these two touchdown plays is more impressive?\n\n86-yard touchdown pass from Justin Herbert to Ladd McConckey (video link)\n64-yard touchdown pass from Cooper Rush to KaVontae Turpin (video link)\n\n\n\n\nPlays share many similarities but important differences\n\nContext: down & distance, time left, score\nTime of pass: number of pass rushers, pocket integrity\nReceiver’s actions after the catch\n\n\n\n\n\nToday: expected points facilitated nuanced comparison\nMultilevel models to determine which passers generate most EP per attempt"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#play-by-play-football-data",
    "href": "slides/raw_slides/slides_09.html#play-by-play-football-data",
    "title": "STAT 479 Lecture 9",
    "section": "Play-by-Play Football Data",
    "text": "Play-by-Play Football Data\n\nLoading DataGame-LevelStarting ContextPlay Info\n\n\n\nUse the nflfastR package\n\n\npbp2024 &lt;- nflfastR::load_pbp(season = 2024)\n\n\n\n\n\n# A tibble: 10 × 5\n   game_id          week season_type home_team away_team\n   &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;    \n 1 2024_08_NYG_PIT     8 REG         PIT       NYG      \n 2 2024_08_MIN_LA      8 REG         LA        MIN      \n 3 2024_15_NE_ARI     15 REG         ARI       NE       \n 4 2024_10_CIN_BAL    10 REG         BAL       CIN      \n 5 2024_01_GB_PHI      1 REG         PHI       GB       \n 6 2024_08_KC_LV       8 REG         LV        KC       \n 7 2024_03_SF_LA       3 REG         LA        SF       \n 8 2024_18_NYG_PHI    18 REG         PHI       NYG      \n 9 2024_11_BAL_PIT    11 REG         PIT       BAL      \n10 2024_09_NO_CAR      9 REG         CAR       NO       \n\n\n\n\n\n\n# A tibble: 10 × 7\n   time  posteam_score defteam_score side_of_field yardline_100  down ydstogo\n   &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 10:23             0             0 NYG                     13     4      11\n 2 02:40            28            20 LA                      53     2      10\n 3 12:24             0             0 ARI                     33     3       5\n 4 12:58             0             0 CIN                     56     3       9\n 5 05:34            13            12 GB                      15    NA       0\n 6 02:00            NA            NA &lt;NA&gt;                    NA    NA       0\n 7 15:00             7            14 LA                      70     1      10\n 8 00:00             7             0 &lt;NA&gt;                    NA    NA       0\n 9 13:27             9             7 PIT                     58     2       7\n10 04:13            12             7 CAR                     15    NA       0\n\n\n\n\n\n\n# A tibble: 10 × 2\n   play_type   desc                                                             \n   &lt;chr&gt;       &lt;chr&gt;                                                            \n 1 field_goal  (10:23) 9-C.Boswell 31 yard field goal is GOOD, Center-46-C.Kunt…\n 2 run         (2:40) 17-P.Nacua right end to LA 45 for -2 yards (7-B.Murphy, 4…\n 3 no_play     (12:24) (Shotgun) 4-A.Gibson left tackle pushed ob at ARI 18 for…\n 4 pass        (12:58) (Shotgun) 9-J.Burrow pass deep middle to 1-J.Chase to BA…\n 5 extra_point 4-J.Elliott extra point is GOOD, Center-49-R.Lovato, Holder-10-B…\n 6 no_play     Timeout #2 by LV at 02:00.                                       \n 7 pass        (15:00) (Shotgun) 9-M.Stafford pass short middle to 88-J.Whittin…\n 8 &lt;NA&gt;        END QUARTER 1                                                    \n 9 pass        (13:27) (No Huddle) 3-R.Wilson pass short middle to 30-J.Warren …\n10 extra_point 19-B.Grupe extra point is GOOD, Center-49-Z.Wood, Holder-43-M.Ha…\n\n\n\nAlso several indicators: fumble, complete_pass, passing_yards\nCheck out the full listing"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#player-id",
    "href": "slides/raw_slides/slides_09.html#player-id",
    "title": "STAT 479 Lecture 9",
    "section": "Player ID",
    "text": "Player ID\n\nnflfastR uses unique 9-digit id for each player (gsis_id)\n\n\nroster2024 &lt;-\n  nflfastR::fast_scraper_roster(seasons = 2024)\n\nroster2024 |&gt;\n  dplyr::filter(full_name == \"Jordan Love\") |&gt;\n  dplyr::select(full_name, gsis_id)\n\n# A tibble: 1 × 2\n  full_name   gsis_id   \n  &lt;chr&gt;       &lt;chr&gt;     \n1 Jordan Love 00-0036264"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#overivew",
    "href": "slides/raw_slides/slides_09.html#overivew",
    "title": "STAT 479 Lecture 9",
    "section": "Overivew",
    "text": "Overivew\n\nGoal: estimate avg. number of points eventually scored by teams from similar situation\nEPA: diff. in post- and pre-play EP\n\n\\(\\textrm{EPA} &gt; 0\\): successful for offense\n\\(\\textrm{EPA} &lt; 0\\): unsuccessful for offense\n\nnflfastR’s EP model to predict next scoring event in half\n\nTouchdown (7), field goal (3), safety (2)\nOpposing touchdown (-7), field goal (-3), and safety (-2)\nNo score (0)"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#expected-points-1",
    "href": "slides/raw_slides/slides_09.html#expected-points-1",
    "title": "STAT 479 Lecture 9",
    "section": "Expected Points",
    "text": "Expected Points\n\nVector of next score probabilities given play features \\(\\boldsymbol{\\mathbf{z}}\\): \\(\\boldsymbol{\\pi}(\\boldsymbol{\\mathbf{z}}) = (\\pi_{\\textrm{TD}}(\\boldsymbol{\\mathbf{z}}), \\ldots, \\pi_{\\textrm{oppFG}}(\\boldsymbol{\\mathbf{z}}))\\)\nEstimated w/ regression tree ensemble using XGBoost\n\n\n\n\nDefinition: Expected Points\n\n\nGiven a game state feature vector \\(\\boldsymbol{\\mathbf{z}}\\) and vector of drive outcome probabilities \\(\\boldsymbol{\\pi}(\\boldsymbol{\\mathbf{z}}),\\) the expected points \\(\\textrm{EP}(\\boldsymbol{\\mathbf{z}})\\) is \\[\n\\begin{align}\n\\textrm{EP}(\\boldsymbol{\\mathbf{z}}) &=  7\\times\\pi_{\\textrm{TD}}(\\boldsymbol{\\mathbf{z}}) +\n3\\times\\pi_{\\textrm{FG}}(\\boldsymbol{\\mathbf{z}}) +\n2\\times\\pi_{\\textrm{SAF}}(\\boldsymbol{\\mathbf{z}}) \\\\\n~&~~-2\\times\\pi_{\\textrm{oSAF}}(\\boldsymbol{\\mathbf{z}}) -\n3\\times\\pi_{\\textrm{oFG}}(\\boldsymbol{\\mathbf{z}})\n- 7\\times\\pi_{\\textrm{oTD}}(\\boldsymbol{\\mathbf{z}})\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#basic-use-comparing-plays",
    "href": "slides/raw_slides/slides_09.html#basic-use-comparing-plays",
    "title": "STAT 479 Lecture 9",
    "section": "Basic Use: Comparing Plays",
    "text": "Basic Use: Comparing Plays\n\nep and epa: starting EP and EP added during play\nMcConkey TD had highest EPA: 7 - (-1.54) = 8.54\nTurpin TD had EPA 7 - 0.77 = 6.23\n\n\npbp2024 |&gt;\n  dplyr::slice_max(epa) |&gt;\n  dplyr::select(ep, epa, desc)\n\n# A tibble: 1 × 3\n     ep   epa desc                                                              \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                                             \n1 -1.54  8.54 (10:50) (Shotgun) 10-J.Herbert pass deep middle to 15-L.McConkey …"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#basic-uses-comparing-teams",
    "href": "slides/raw_slides/slides_09.html#basic-uses-comparing-teams",
    "title": "STAT 479 Lecture 9",
    "section": "Basic Uses: Comparing Teams",
    "text": "Basic Uses: Comparing Teams\n\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\npbp2024 |&gt;\n  dplyr::group_by(posteam) |&gt;\n  dplyr::summarize(epa = mean(epa, na.rm = TRUE)) |&gt; \n  dplyr::arrange(desc(epa)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):(dplyr::n()))) \n\n# A tibble: 10 × 2\n   posteam     epa\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 BAL      0.143 \n 2 BUF      0.141 \n 3 DET      0.135 \n 4 WAS      0.115 \n 5 TB       0.103 \n 6 NE      -0.0643\n 7 NYG     -0.0829\n 8 TEN     -0.0896\n 9 LV      -0.107 \n10 CLE     -0.151"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#league-average",
    "href": "slides/raw_slides/slides_09.html#league-average",
    "title": "STAT 479 Lecture 9",
    "section": "League-Average",
    "text": "League-Average\n\nEPA on a future pass if you don’t know anything else?\nLeague average mean(pbp2024$epa) seems reasonable\n\n\n\n\n\n\n\n\n\n\nFigure 1: EPA on all regular season passes"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#player-specific-model",
    "href": "slides/raw_slides/slides_09.html#player-specific-model",
    "title": "STAT 479 Lecture 9",
    "section": "Player-Specific Model",
    "text": "Player-Specific Model\n\nWould your prediction change if you knew passer identity?\n\nIf Patrick Mahomes was throwing the pass, expect higher EPA\nIf Daniel Jones was throwing the pass, expect lower EPA\n\n\n\n\n\\(I\\): number of unique passers in data set\n\\(Y_{ij}\\): EPA on pass \\(j\\) thrown by player \\(i\\)\n\\(\\alpha_{i}\\): avg. EPA/pass for player \\(i\\) (unknown)\nModel: \\(Y_{ij} = \\alpha_{i} + \\epsilon_{ij}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#estimating-alpha_i",
    "href": "slides/raw_slides/slides_09.html#estimating-alpha_i",
    "title": "STAT 479 Lecture 9",
    "section": "Estimating \\(\\alpha_{i}\\)",
    "text": "Estimating \\(\\alpha_{i}\\)\n\nIdea 1: group by passer_player_id & compute mean(epa) for each player\nEquivalent (and more extensible): fit a linear model w/ least squares\n\n\nSetupEstimationExampleTop-10\n\n\n\nRegress epa on categorical passer_player_id\nMust convert passer_player_id to a factor()\nUseful to manually set a reference player (e.g., Aaron Rodgers)\n\n\nrodgers_id &lt;- \n  roster2024 |&gt; \n  dplyr::filter(full_name == \"Aaron Rodgers\") |&gt; \n  dplyr::pull(gsis_id)\n\npass2024 &lt;-\n  pass2024 |&gt;\n  dplyr::mutate(\n    passer_player_id = factor(passer_player_id),\n    passer_player_id = relevel(passer_player_id, ref = rodgers_id))\n\n\n\n\nEstimates \\(\\beta_{0} = \\alpha_{\\textrm{Rodgers}}\\) and \\(\\beta_{i} = \\alpha_{i} - \\alpha_{\\textrm{Rodgers}}\\)\n\n\nols_fit &lt;-\n  lm(formula = epa ~ 1 + passer_player_id,\n     data = pass2024)\nols_betas &lt;- coefficients(ols_fit)\nols_betas[1:5]\n\n               (Intercept) passer_player_id00-0026158 \n                0.11303340                 0.04427535 \npasser_player_id00-0026300 passer_player_id00-0026498 \n               -0.32316791                 0.05623275 \npasser_player_id00-0027973 \n               -0.19838613 \n\n\n\n\n\ndak_id &lt;- \n  roster2024 |&gt; \n  dplyr::filter(full_name == \"Dak Prescott\") |&gt; \n  dplyr::pull(gsis_id)\nmean(pass2024$epa[pass2024$passer_player_id == dak_id]) \n\n[1] 0.05691811\n\nols_betas[paste0(\"passer_player_id\", dak_id)] + ols_betas[\"(Intercept)\"]\n\npasser_player_id00-0033077 \n                0.05691811 \n\n\n\n\n\n\n          full_name       ols\n1           AJ Cole  4.333918\n2  Courtland Sutton  3.745369\n3          Jack Fox  3.637429\n4  Justin Jefferson  3.050907\n5      Stefon Diggs  2.762919\n6   Miles Killebrew -2.677720\n7        J.K. Scott -2.706606\n8       Bryan Anger -2.843511\n9     Johnny Hekker -2.856017\n10     Keenan Allen -5.911163"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#small-sample-size",
    "href": "slides/raw_slides/slides_09.html#small-sample-size",
    "title": "STAT 479 Lecture 9",
    "section": "Small Sample Size",
    "text": "Small Sample Size\n\n\n\n\n\n\n\n\nFigure 2: The variability in per-pass decreases dramatically as the number of passes increases."
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#thought-experiment",
    "href": "slides/raw_slides/slides_09.html#thought-experiment",
    "title": "STAT 479 Lecture 9",
    "section": "Thought-Experiment",
    "text": "Thought-Experiment\n\nIf \\(i\\) threw many passes, \\(\\hat{\\alpha}_{i}\\) accurately estimates latent ability \\(\\alpha_{i}\\)\n\nPrefer to use \\(\\hat{\\alpha}_{i}\\) over global mean \\(\\overline{y}\\)\n\nIf \\(i\\) threw very few passes, \\(\\hat{\\alpha}_{i}\\) can be very noisy\n\nGlobal mean \\(\\overline{y}\\) arguably better than \\(\\hat{\\alpha}_{i}\\)\n\nWhat about for players b/w extremes?\n\n\n\nIdea: \\(w_{i} \\times \\hat{\\alpha}_{i} + (1 - w_{i}) \\times \\overline{y}\\)\n\nIf \\(n_{i}\\) very large, \\(w_{i}\\) should be closer to 1\nIf \\(n_{i}\\) very small, \\(w_{i}\\) should be closer to 0\n\nProblem: how to specify weights in data-driven way?"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#model-specification",
    "href": "slides/raw_slides/slides_09.html#model-specification",
    "title": "STAT 479 Lecture 9",
    "section": "Model Specification",
    "text": "Model Specification\n\nLevel 1: observed EPA for player \\(i\\) normally distributed around \\(\\alpha_{i}\\)\nLevel 2: Latent player abilities \\(\\alpha_{i}\\)’s are themselves normally distributed around \\(\\mu\\) \\[\n\\begin{align}\n\\textrm{Level 1}&: &\\quad  Y_{ij} &= \\alpha_{i} + \\epsilon_{ij}; \\epsilon_{ij} \\sim N(0, \\sigma^{2}) \\quad \\textrm{for all}\\ j = 1, \\ldots, n_{i},\\ i = 1, \\ldots, I \\\\\n\\textrm{Level 2}&: &\\quad \\alpha_{i} &= \\alpha_{0} + u_{i}; u_{i} \\sim N(0, \\sigma^{2}_{\\alpha}) \\quad \\textrm{for all}\\ i = 1, \\ldots, I\n\\end{align}\n\\]\n\\(\\alpha_{0}\\): average per-pass EPA over super-population of passers\n\\(\\sigma\\) captures “within-player” variability in EPA pass-to-pass\n\\(\\sigma_{\\alpha}\\) captures “between-player” variability in per-pass EPA\n\n\n\nLevel 2 responses \\(\\alpha_{i}\\) are not observable\n“Borrowing strength”: predition for player \\(i'\\) informed by their data and everyone else’s data\n\nEstimate of \\(\\alpha_{i}\\) informed by \\(i\\)’s data **and \\(\\alpha_{0}\\)\nEstimates of \\(\\alpha_{i}\\) determine estimate of \\(\\alpha_{0}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#hierarchical-structure",
    "href": "slides/raw_slides/slides_09.html#hierarchical-structure",
    "title": "STAT 479 Lecture 9",
    "section": "Hierarchical Structure",
    "text": "Hierarchical Structure\n\nOften data exhibits hierarchical grouping structure\n\nPasses group by QB, players in teams etc.\n\nGrouping variable may be relevant to outcome & induces correlation\nOutcomes from same group may be tightly clustered around group average"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#fitting-multilevel-models",
    "href": "slides/raw_slides/slides_09.html#fitting-multilevel-models",
    "title": "STAT 479 Lecture 9",
    "section": "Fitting Multilevel Models",
    "text": "Fitting Multilevel Models\n\nThe (1 | passer_player_id) tells lmer() to include a random intercept for each passer\n\n\nlibrary(lme4)\nmultilevel_fit &lt;-\n  lmer(formula = epa ~ 1 + (1|passer_player_id), \n       data = pass2024)"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#extracting-estimates",
    "href": "slides/raw_slides/slides_09.html#extracting-estimates",
    "title": "STAT 479 Lecture 9",
    "section": "Extracting Estimates",
    "text": "Extracting Estimates\n\nUse ranef() to extract estimates \\(\\hat{u}_{i}\\)’s\nUse coef to get \\(\\hat{\\alpha}_{i} = \\hat{\\alpha}_{0} + \\hat{u}_{i}\\)\n\nOnly valid when there is one grouping variable\nW/ more grouping variables, random intercepts not identified\n\ncoef() and ranef() return lists w/ one element per grouping variable\n\nEach list element is a data table\n\n\n\ntmp &lt;- coef(multilevel_fit)\ntmp[[\"passer_player_id\"]] |&gt; dplyr::slice_head(n = 5)\n\n           (Intercept)\n00-0023459  0.11643217\n00-0026158  0.14662915\n00-0026300  0.12284041\n00-0026498  0.16012352\n00-0027973  0.01971509"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#estimated-random-intercepts-hatalpha_i",
    "href": "slides/raw_slides/slides_09.html#estimated-random-intercepts-hatalpha_i",
    "title": "STAT 479 Lecture 9",
    "section": "Estimated Random Intercepts \\(\\hat{\\alpha}_{i}\\)",
    "text": "Estimated Random Intercepts \\(\\hat{\\alpha}_{i}\\)\n\nCodeTop & Bottom 5 Passers\n\n\n\nCreate temporary table lmer_alpha with player name, id, and estimate\nJoin lmer_alpha to alphas (so we can compare with OLS estimates)\n\n\nlmer_alpha &lt;- data.frame( \n    lmer = tmp[[\"passer_player_id\"]][,1], \n    gsis_id = rownames(tmp[[\"passer_player_id\"]])) \n\nalphas &lt;- alphas |&gt;\n  dplyr::inner_join(y = lmer_alpha, by = \"gsis_id\")\n\n\n\n\n\n                  full_name        lmer   n\n1             Lamar Jackson  0.36825428 469\n2                Jared Goff  0.33315635 536\n3                Josh Allen  0.27342658 482\n4                Joe Burrow  0.27172057 651\n5            Baker Mayfield  0.27062324 569\n6               Andy Dalton  0.01971509 160\n7                 Drew Lock  0.01595111 180\n8        Anthony Richardson  0.01197563 261\n9           Spencer Rattler -0.04081740 227\n10 Dorian Thompson-Robinson -0.13715503 118"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#visualizing-multilevel-estimates",
    "href": "slides/raw_slides/slides_09.html#visualizing-multilevel-estimates",
    "title": "STAT 479 Lecture 9",
    "section": "Visualizing Multilevel Estimates",
    "text": "Visualizing Multilevel Estimates\n\nMultilevel model pulls player-specific means to global mean\n\nAmount of data dictates degree to which original estimate is pulled\n\n\n\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#adjusting-for-additional-covariates",
    "href": "slides/raw_slides/slides_09.html#adjusting-for-additional-covariates",
    "title": "STAT 479 Lecture 9",
    "section": "Adjusting For Additional Covariates",
    "text": "Adjusting For Additional Covariates\n\nSimple model doesn’t account for formation, whether QB was hit while throwing, etc.\n\n\npass2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(play_type == \"pass\" & season_type == \"REG\") |&gt; \n  dplyr::filter(!grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc) &\n                  !grepl(\"sacked\", desc)) |&gt;\n  dplyr::select(epa, passer_player_id, \n                air_yards, \n                posteam_type, shotgun, no_huddle, qb_hit,\n                pass_location,\n                desc) |&gt;\n  dplyr::mutate(posteam_type = factor(posteam_type),\n                pass_location = factor(pass_location))"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#including-fixed-effects",
    "href": "slides/raw_slides/slides_09.html#including-fixed-effects",
    "title": "STAT 479 Lecture 9",
    "section": "Including Fixed Effects",
    "text": "Including Fixed Effects\n\n\\(\\boldsymbol{\\mathbf{x}}\\) contains: air_yards, shotgun, qb_hit, pass_location, posteam_type\nEffects of these factors are constant (i.e., fixed) across all passers \\[\n\\begin{align}\n\\textrm{Level 1}&: &\\quad  Y_{ij} &= \\alpha_{i} + \\boldsymbol{\\mathbf{x}}_{ij}^{\\top}\\boldsymbol{\\beta} +  \\epsilon_{ij}; \\quad \\epsilon_{ij} \\sim N(0, \\sigma^{2}) \\\\\n\\textrm{Level 2}&: &\\quad \\alpha_{i} &= \\alpha_{0} + u_{i}; \\quad u_{i} \\sim N(0, \\sigma^{2}_{\\alpha})\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#fitting-our-elaborated-model",
    "href": "slides/raw_slides/slides_09.html#fitting-our-elaborated-model",
    "title": "STAT 479 Lecture 9",
    "section": "Fitting Our Elaborated Model",
    "text": "Fitting Our Elaborated Model\n\nml_fit_full &lt;-\n  lme4::lmer(formula = epa ~ 1 + (1|passer_player_id) + \n               air_yards + posteam_type + shotgun +\n               no_huddle + qb_hit + pass_location, data = pass2024)\n\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: epa ~ 1 + (1 | passer_player_id) + air_yards + posteam_type +  \n    shotgun + no_huddle + qb_hit + pass_location\n   Data: pass2024\n\nREML criterion at convergence: 65016.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.5225 -0.5478 -0.0841  0.5538  5.4631 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n passer_player_id (Intercept) 0.01344  0.1159  \n Residual                     2.28043  1.5101  \nNumber of obs: 17727, groups:  passer_player_id, 103\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)          0.133559   0.039314   3.397\nair_yards            0.016728   0.001132  14.772\nposteam_typehome    -0.010065   0.022883  -0.440\nshotgun             -0.109698   0.032192  -3.408\nno_huddle            0.021179   0.034703   0.610\nqb_hit              -0.470851   0.039760 -11.842\npass_locationmiddle  0.135755   0.030724   4.418\npass_locationright  -0.053875   0.025769  -2.091\n\nCorrelation of Fixed Effects:\n            (Intr) ar_yrd pstm_t shotgn n_hddl qb_hit pss_lctnm\nair_yards   -0.207                                             \npostm_typhm -0.280  0.012                                      \nshotgun     -0.676 -0.005 -0.011                               \nno_huddle   -0.039 -0.009 -0.024 -0.105                        \nqb_hit      -0.078 -0.067  0.008  0.005  0.011                 \npss_lctnmdd -0.240 -0.060 -0.013 -0.041 -0.002 -0.019          \npss_lctnrgh -0.348  0.007 -0.010  0.010 -0.003 -0.019  0.441"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#top--and-bottom-5-passers",
    "href": "slides/raw_slides/slides_09.html#top--and-bottom-5-passers",
    "title": "STAT 479 Lecture 9",
    "section": "Top- and Bottom-5 Passers",
    "text": "Top- and Bottom-5 Passers\n\nAfter adjusting for important fixed effects\n\nLamar Jackson has highest EPA/pass\nDorian Thompson-Robinson had the lowest EPA/pass\n\n\n\n\n                  full_name        lmer\n1             Lamar Jackson  0.34106738\n2                Jared Goff  0.32096595\n3                Joe Burrow  0.29762396\n4            Tua Tagovailoa  0.26988653\n5                Josh Allen  0.26577678\n6              Bailey Zappe  0.03321274\n7               Andy Dalton  0.02969282\n8        Anthony Richardson -0.01605454\n9           Spencer Rattler -0.03511495\n10 Dorian Thompson-Robinson -0.08925603"
  },
  {
    "objectID": "slides/raw_slides/slides_09.html#looking-ahead",
    "href": "slides/raw_slides/slides_09.html#looking-ahead",
    "title": "STAT 479 Lecture 9",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nEPA is based on starting & ending game state\n2 phases of passing play: ball in air + after the catch\nCurrently analysis implicitly credits QBs for both phases\nLecture 10: divide total EPA among relevant offensive players\nDevelop a version of WAR\n\nMultilevel model to estimate per-play contribution\nScale model estimates by opportunities to create replacement-level shadow"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#recap",
    "href": "slides/raw_slides/slides_07.html#recap",
    "title": "STAT 479 Lecture 7",
    "section": "Recap",
    "text": "Recap\n\nExpected runs: avg. runs scored in remainder of half-inning from game state\n\nGame state characterized by Outs & Baserunner configuration\nBaserunner strings: \"101\" means runner on 1st & 3rd, nobody on 2nd\nEstimated \\(\\rho(\\textrm{o}, \\textrm{br})\\) using all at-bats in 2024\n\nDuring at-bat game state changes from \\((\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\) to \\((\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}})\\)\n\n\n\\[\n\\textrm{RunValue} = \\textrm{RunsScored} +\\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}) - \\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\n\\]\n\nSeveral top batters created the most run value\nDo batters deserve all the credit? How much credit to baserunners?"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#road-to-war",
    "href": "slides/raw_slides/slides_07.html#road-to-war",
    "title": "STAT 479 Lecture 7",
    "section": "Road to WAR",
    "text": "Road to WAR\n\nToday: divide \\(\\textrm{RunValue}\\) b/w batter & baserunner\nConservation of runs: if batting team creates \\(\\delta\\) run value then fielding team creates \\(-\\delta\\)\n\n\n\nUltimately, we’ll sum each player’s across across each phase\n\nOffensive: Batting & Baserunning (today)\nDefensive: Pitching & Fielding (Lecture 8)\n\nWe’ll then develop a version of wins above replacement (WAR)\n\nIntroduce a roster-based definition of “replacement level”\nEstimate the performance of a replacement-level “shadow” for each player\nConvert runs to wins"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#data-preparation-overview",
    "href": "slides/raw_slides/slides_07.html#data-preparation-overview",
    "title": "STAT 479 Lecture 7",
    "section": "Data Preparation (Overview)",
    "text": "Data Preparation (Overview)\n\nCreate a table with rows for each at-bat and columns for\n\nStarting & ending Outs and BaseRunner, RunValue\nbatter, on_1b, on_2b, on_3b\nend_event & des: narrative description of what happened in at-bat\n\n\n\n\nSome entries in end_events are missing\n\n\n\n\n                                     catcher_interf                    double \n                      308                        97                      7608 \n              double_play               field_error                 field_out \n                      336                      1093                     72233 \n          fielders_choice       fielders_choice_out                 force_out \n                      373                       306                      3408 \ngrounded_into_double_play              hit_by_pitch                  home_run \n                     3152                      1977                      5326 \n                 sac_bunt                   sac_fly       sac_fly_double_play \n                      446                      1222                        13 \n                   single                 strikeout     strikeout_double_play \n                    25363                     40145                       107 \n                   triple               triple_play              truncated_pa \n                      685                         1                       304 \n                     walk \n                    14029"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#additional-prep",
    "href": "slides/raw_slides/slides_07.html#additional-prep",
    "title": "STAT 479 Lecture 7",
    "section": "Additional Prep",
    "text": "Additional Prep\n\nSee lecture notes for full code\ngrepl(pattern, x) return TRUE if pattern found in the string x\n\n\nMissing EventsManual CorrectionGameState\n\n\n\n\n# A tibble: 4 × 3\n   Outs end_Outs des                                                            \n  &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                                                          \n1     0        0 Mookie Betts walks.                                            \n2     2        3 Xander Bogaerts strikes out on automatic strike.               \n3     2        2 Logan Webb intentionally walks Ha-Seong Kim.                   \n4     2        3 With Vinnie Pasquantino batting, Bobby Witt Jr. picked off and…\n\n\n[1] \"With Vinnie Pasquantino batting, Bobby Witt Jr. picked off and caught stealing 2nd base, first baseman Ryan Mountcastle.\"\n\n\n\n\n\natbat2024 &lt;-\n  raw_atbat2024 |&gt;\n  dplyr::mutate(\n    end_events = dplyr::case_when(\n      end_events == \"\" & grepl(\"walk\", des) ~ \"walk\",\n      end_events == \"\" & grepl(\"strikes out\", des) ~ \"strikeout\",\n      end_events == \"\" & end_Outs == 3 ~ \"truncated_pa\",\n      end_events == \"\" & grepl(\"flies out\", des) ~ \"field_out\",\n      .default = end_events))\n\n\n\n\nConcatenate Outs and Baserunner: e.g. `“0.101”: 0 outs, runners on 1st & 3rd\n\n\natbat2024 &lt;-\n  atbat2024 |&gt;\n  dplyr::mutate(GameState = paste(Outs, BaseRunner, sep = \".\"))"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#adjusted-run-value",
    "href": "slides/raw_slides/slides_07.html#adjusted-run-value",
    "title": "STAT 479 Lecture 7",
    "section": "Adjusted Run Value",
    "text": "Adjusted Run Value\n\nFor at-bat starting in state \\(g\\) and ending in \\(e\\), how much run value should we expect batting team to create?\n\nExpect more run value w/ fewer outs\nAt-bats ending w/ home runs have more run value than those ending w/ strike outs\n\nHow do players perform over and above expectations?\n\n\n\n\\(\\delta_{i} = \\mathbb{E}[\\delta \\vert \\textrm{g} = \\textrm{g}_{i}, \\textrm{e} = \\textrm{e}_{i}] + \\epsilon_{i}\\)\nLet \\(\\mu = \\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}]\\):"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#game-state-event-combinations",
    "href": "slides/raw_slides/slides_07.html#game-state-event-combinations",
    "title": "STAT 479 Lecture 7",
    "section": "Game State & Event Combinations",
    "text": "Game State & Event Combinations\n\nCan’t use simple grouped summary / “binning and averaging”\n\n\n\n# A tibble: 10 × 3\n   GameState end_events                n\n   &lt;chr&gt;     &lt;chr&gt;                 &lt;int&gt;\n 1 0.001     catcher_interf            1\n 2 0.001     fielders_choice_out       1\n 3 0.011     triple                    1\n 4 0.101     strikeout_double_play     1\n 5 0.101     triple_play               1\n 6 1.000     strikeout              7490\n 7 0.000     strikeout              9814\n 8 2.000     field_out             11563\n 9 1.000     field_out             14573\n10 0.000     field_out             20148"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#a-model-for-mu",
    "href": "slides/raw_slides/slides_07.html#a-model-for-mu",
    "title": "STAT 479 Lecture 7",
    "section": "A Model for \\(\\mu\\)",
    "text": "A Model for \\(\\mu\\)\n\n\\(\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}] = \\alpha_{\\textrm{g}} + \\alpha_{\\textrm{e}}.\\)\nIntroduce\n\nGame-state parameters: \\(\\alpha_{0.000}, \\ldots, \\alpha_{2.111}\\)\nEvent parameters: \\(\\alpha_{\\textrm{catcher\\_interf}}, \\ldots, \\alpha_{\\textrm{walk}}\\)\n\nAvg. run value for hitting single w/ 2 outs and no runners on: \\(\\alpha_{\\textrm{2.000}} + \\alpha_{\\textrm{single}}\\)\n\n\n\nWe must estimate \\(\\alpha_{\\textrm{g}}\\)’s and \\(\\alpha_{\\textrm{e}}\\)’s"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#estimating-alpha_g-and-alpha_e",
    "href": "slides/raw_slides/slides_07.html#estimating-alpha_g-and-alpha_e",
    "title": "STAT 479 Lecture 7",
    "section": "Estimating \\(\\alpha_{g}\\) and \\(\\alpha_{e}\\)",
    "text": "Estimating \\(\\alpha_{g}\\) and \\(\\alpha_{e}\\)\n\\[\n\\hat{\\boldsymbol{\\alpha}} = \\textrm{argmin} \\sum_{i = 1}^{n}{(\\delta_{i} - \\alpha_{g_{i}} - \\alpha_{e_{i}})^2},\n\\]\n\nLinear regression without an intercept\n\n\n\nFirst step: data frame w/ \\(\\delta, \\textrm{g}\\) and \\(\\textrm{e}\\)\nMust convert \\(\\textrm{g}\\) and \\(\\textrm{e}\\) to factor variables\n\n\ntmp_df &lt;-\n  atbat2024 |&gt;\n  dplyr::select(RunValue, GameState, end_events) |&gt;\n  dplyr::mutate(\n    GameState = factor(GameState),\n    end_events = factor(end_events))"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#model-fitting",
    "href": "slides/raw_slides/slides_07.html#model-fitting",
    "title": "STAT 479 Lecture 7",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nstate_event_fit &lt;- lm(RunValue ~ -1 + GameState + end_events, \n                      data = tmp_df)\n\n\nAccess estimates \\(\\hat{\\alpha}_{g}\\) and \\(\\hat{\\alpha}_{e}\\)’s w/ coef\n\n\nalpha_hat &lt;- coef(state_event_fit)\nc(alpha_hat[\"GameState2.000\"], alpha_hat[\"end_eventssingle\"])\n\n  GameState2.000 end_eventssingle \n       0.3562635        0.1150004 \n\n\n\nOn average, a single with 2 outs and no runners on creates 0.471 run value"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#decomposing-delta",
    "href": "slides/raw_slides/slides_07.html#decomposing-delta",
    "title": "STAT 479 Lecture 7",
    "section": "Decomposing \\(\\delta\\)",
    "text": "Decomposing \\(\\delta\\)\n\nLet \\(\\hat{\\mu}_{i} = \\hat{\\alpha}_{\\textrm{g}_{i}} + \\hat{\\alpha}_{\\textrm{e}_{i}}\\)\n\nEstimated exp. run value for at-bat \\(i\\) based on starting state and ending event\n\nLet \\(\\eta_{i} = \\delta_{i} - \\hat{\\mu}_{i}\\)\n\n\n\nFollowing Baumer et al. (2015):\n\nAttribute \\(\\hat{\\mu}_{i}\\) to batter\nDivide \\(\\eta_{i}\\) b/w base runners\n\n\n\natbat2024$eta &lt;- state_event_fit$residuals\natbat2024$mu &lt;- state_event_fit$fitted.values"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#baserunning-expectations",
    "href": "slides/raw_slides/slides_07.html#baserunning-expectations",
    "title": "STAT 479 Lecture 7",
    "section": "Baserunning Expectations",
    "text": "Baserunning Expectations\n\nOn a single, we would expect:\n\nBatter to reach 1st\nOther base runners to advance 1 base\n\nMarch 20, 2024: Ohtani singled in 0.110 against Padres:\n\nOhtani reaches 1st, runner on 1st advances to 2nd\nRunner on 2nd scored\n\nInstead of dividing \\(\\eta_{i}\\) equally, we should give more to runner originally on 2nd"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#weighted-allocation",
    "href": "slides/raw_slides/slides_07.html#weighted-allocation",
    "title": "STAT 479 Lecture 7",
    "section": "Weighted Allocation",
    "text": "Weighted Allocation\n\nConsider runner on base \\(j\\) in at-bat \\(i\\)\n\\(k_{ij}\\): number of bases advanced by runner \\(j\\) in at-bat \\(i\\)\n\\(\\textrm{e}_{i}\\): ending event of at-bat \\(i\\)\n\n\n\nLet \\(\\kappa_{ij} = \\mathbb{P}(K &lt; k_{ij} \\vert \\textrm{e}_{i})\\)\nAssign \\(\\eta_{i} \\times \\kappa_{ij}/\\sum_{j'}{\\kappa_{ij'}}\\) to baserunner \\(j\\)\nWhy condition on ending event??"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#computing-k_ij-overview",
    "href": "slides/raw_slides/slides_07.html#computing-k_ij-overview",
    "title": "STAT 479 Lecture 7",
    "section": "Computing \\(k_{ij} (Overview)\\)",
    "text": "Computing \\(k_{ij} (Overview)\\)\n\nSee lecture notes for full code\nWrote functions to determine how many bases each runner (including batter) advanced\nCompared starting and ending on_1b, on_2b, and on_3b\nAlso checked des to see if any player scored or was run out"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#computing-kappa_ij-code",
    "href": "slides/raw_slides/slides_07.html#computing-kappa_ij-code",
    "title": "STAT 479 Lecture 7",
    "section": "Computing \\(\\kappa_{ij}\\) (Code)",
    "text": "Computing \\(\\kappa_{ij}\\) (Code)\n\nbaserunning &lt;-\n  atbat2024 |&gt;\n  dplyr::rowwise() |&gt;\n  dplyr::mutate(\n    mvt_batter = \n      mvt_batter(batter, Outs, bat_score, \n                 end_on_1b, end_on_2b, end_on_3b, \n                 end_Outs, end_bat_score, des),\n    mvt_1b = \n      mvt_1b(on_1b, Outs, bat_score, \n             end_on_1b, end_on_2b, end_on_3b,\n             end_Outs, end_bat_score, des),\n    mvt_2b = \n      mvt_2b(on_2b, Outs, bat_score, \n             end_on_2b, end_on_3b, \n             end_Outs, end_bat_score, des),\n    mvt_3b = \n      mvt_3b(on_3b, Outs, bat_score, \n             end_on_3b, end_Outs, end_bat_score, des)) |&gt;\n  dplyr::ungroup()"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#cumulative-baserunning-probabilities",
    "href": "slides/raw_slides/slides_07.html#cumulative-baserunning-probabilities",
    "title": "STAT 479 Lecture 7",
    "section": "Cumulative Baserunning Probabilities",
    "text": "Cumulative Baserunning Probabilities\n\nWe have \\(k_{ij}\\) for all baserunners involved in at-bat \\(i\\)\nNeed to compute \\(\\kappa_{ij} = \\mathbb{P}(K &lt; k_{ij} \\vert \\textrm{e}_{i})\\)\n\n\nbr_1b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_1b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_1b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_1b &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_1b &lt;= 2, na.rm = TRUE),\n    kappa_3 = mean(mvt_1b &lt;= 3, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_1b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_1b\") |&gt;\n  dplyr::mutate(mvt_1b = ifelse(mvt_1b == \"NA\", NA, mvt_1b),\n                mvt_1b = as.numeric(mvt_1b))"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#example",
    "href": "slides/raw_slides/slides_07.html#example",
    "title": "STAT 479 Lecture 7",
    "section": "Example",
    "text": "Example\n\nSingleStrikeout\n\n\n\nAfter a single, runner on first advances\n\n0 bases w/ prob 2%; 1 base w/ prob 63% (0.647-0.0194)\n2 bases w/ prob 31%; 3 bases w/ prob 4%\n\n\n\nbr_1b_probs |&gt; dplyr::filter(end_events == \"single\")\n\n# A tibble: 5 × 3\n  end_events mvt_1b kappa_1b\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 single          0   0.0194\n2 single          1   0.647 \n3 single          2   0.957 \n4 single          3   1     \n5 single         NA   0     \n\n\n\n\n\nAfter a strikeout, runner on first advances\n\n0 bases w/ prob 93%; 1 base w/ prob 6.4%\n2 bases w/ prob 0.6%; 3 bases w/ prob 0%\n\n\n\nbr_1b_probs |&gt; dplyr::filter(end_events == \"strikeout\")\n\n# A tibble: 5 × 3\n  end_events mvt_1b kappa_1b\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 strikeout       0    0.930\n2 strikeout       1    0.994\n3 strikeout       2    1.000\n4 strikeout       3    1    \n5 strikeout      NA    0"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#baserunning-weights",
    "href": "slides/raw_slides/slides_07.html#baserunning-weights",
    "title": "STAT 479 Lecture 7",
    "section": "Baserunning Weights",
    "text": "Baserunning Weights\n\nRemember: assign \\(\\eta_{i} \\times \\frac{\\kappa_{ij}}{\\sum_{j'}{\\kappa_{ij'}}}\\) to runner \\(j\\) in at-bat \\(i\\)\nAdd columns for \\(\\kappa\\) to baserunning\n\n\nbaserunning &lt;-\n  baserunning |&gt;\n  dplyr::inner_join(y = br_batter_probs, by = c(\"end_events\", \"mvt_batter\")) |&gt;\n  dplyr::inner_join(y = br_1b_probs, by = c(\"end_events\", \"mvt_1b\")) |&gt;\n  dplyr::inner_join(y = br_2b_probs, by = c(\"end_events\", \"mvt_2b\")) |&gt;\n  dplyr::inner_join(y = br_3b_probs, by = c(\"end_events\", \"mvt_3b\")) |&gt;\n  dplyr::mutate(\n    total_kappa = kappa_batter + kappa_1b + kappa_2b + kappa_3b,\n    norm_batter = kappa_batter/total_kappa,\n    norm_1b = kappa_1b/total_kappa,\n    norm_2b = kappa_2b/total_kappa,\n    norm_3b = kappa_3b/total_kappa)"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#illustration",
    "href": "slides/raw_slides/slides_07.html#illustration",
    "title": "STAT 479 Lecture 7",
    "section": "Illustration",
    "text": "Illustration\n\nOhtani’s At-Bats\\(\\kappa\\)’sWeights\n\n\n\n1st, 2nd, 3rd, & 5th at-bat: Ohtani reaches 1st (mvt_batter = 1)\n5th at-bat: runner on second scored & should get more credit\n\n\n\n# A tibble: 5 × 3\n  mvt_batter end_events des                                                     \n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                                                   \n1          1 force_out  Shohei Ohtani grounds into a force out, shortstop Ha-Se…\n2          1 single     Shohei Ohtani singles on a sharp line drive to right fi…\n3          1 force_out  Shohei Ohtani grounds into a force out, third baseman T…\n4          0 field_out  Shohei Ohtani grounds out softly, pitcher Wandy Peralta…\n5          1 single     Shohei Ohtani singles on a line drive to left fielder J…\n\n\n[1] \"Shohei Ohtani grounds into a force out, shortstop Ha-Seong Kim to second baseman Xander Bogaerts. Mookie Betts out at 2nd. Shohei Ohtani to 1st.\"\n[2] \"Shohei Ohtani grounds into a force out, third baseman Tyler Wade to shortstop Ha-Seong Kim. Mookie Betts out at 2nd. Shohei Ohtani to 1st.\"      \n[3] \"Shohei Ohtani singles on a line drive to left fielder José Azocar. Gavin Lux scores. Mookie Betts to 2nd.\"                                       \n\n\n\n\n\n1st & 3rd at-bat: runner on 1st forced out at 2nd\n\nOn force outs, runner on 1st advances 0 bases about 95% of the time\n\n5th at-bat: runner scores from 2nd (i.e., advances 2 bases)\n\nkappa_2b = 1: runner on 2nd always advances \\(&lt;= 2\\) bases!\n\n\n\n\n# A tibble: 5 × 8\n  mvt_1b mvt_2b mvt_3b mvt_batter kappa_1b kappa_2b kappa_3b kappa_batter\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1      0     NA     NA          1    0.955        0        0        0.988\n2     NA     NA     NA          1    0            0        0        0.974\n3      0     NA     NA          1    0.955        0        0        0.988\n4     NA     NA     NA          0    0            0        0        1    \n5      1      2     NA          1    0.647        1        0        0.974\n\n\n\n\n\n1st & 3rd at-bat: runner on first out at 2nd\n\nmvt_1b = 0 and norm_1b is less than norm_batter\n\n5th at-bat: runner scores from 2nd & should get more credit\n\nnorm_2b is larger than norm_1b and norm_batter\n\n\n\n\n# A tibble: 5 × 4\n  norm_1b norm_2b norm_3b norm_batter\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1   0.491   0           0       0.509\n2   0       0           0       1    \n3   0.491   0           0       0.509\n4   0       0           0       1    \n5   0.247   0.382       0       0.372"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#baserunning-run-value",
    "href": "slides/raw_slides/slides_07.html#baserunning-run-value",
    "title": "STAT 479 Lecture 7",
    "section": "Baserunning Run Value",
    "text": "Baserunning Run Value\n\n\\(\\eta_{i} \\times \\frac{\\kappa_{ij}}{\\sum_{j'}{\\kappa_{ij'}}}\\): run value created by baserunner \\(j\\) in at-bat \\(i\\)\nFor each player and base, sum contributions\n\\(\\textrm{RAA}_{1b}\\): run value created by baserunning from 1st base\n\n\nraa_1b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_1b)) |&gt;\n  dplyr::mutate(RAA_1b = norm_1b * eta) |&gt;\n  dplyr::group_by(on_1b) |&gt;\n  dplyr::summarise(RAA_1b = sum(RAA_1b)) |&gt;\n  dplyr::rename(key_mlbam = on_1b)"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#computing-textrmraatextrmbr",
    "href": "slides/raw_slides/slides_07.html#computing-textrmraatextrmbr",
    "title": "STAT 479 Lecture 7",
    "section": "Computing \\(\\textrm{RAA}^{(\\textrm{br})}\\)",
    "text": "Computing \\(\\textrm{RAA}^{(\\textrm{br})}\\)\n\nSumming over the four baserunning positions gives us \\(\\textrm{RAA}^{(\\textrm{br})}\\)\nBaumer et al. (2015): run value created above average through baserunning\n\n\nCodeTop-10\n\n\n\nraa_br &lt;-\n  raa_batter |&gt;\n  dplyr::full_join(y = raa_1b, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_2b, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_3b, by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(list(RAA_batter = 0, RAA_1b = 0, RAA_2b = 0, RAA_3b = 0)) |&gt;\n  dplyr::mutate(RAA_br = RAA_batter + RAA_1b + RAA_2b + RAA_3b) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_br, RAA_batter, RAA_1b, RAA_2b, RAA_3b)\n\n\n\n\n\n# A tibble: 10 × 6\n   Name               RAA_br RAA_batter RAA_1b RAA_2b RAA_3b\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Jose Ramirez         32.7       7.32   8.49  8.18   8.75 \n 2 Maikel Garcia        29.8       1.63  13.5   6.79   7.82 \n 3 Brandon Nimmo        27.3      21.9   -2.50  6.96   0.917\n 4 Corbin Carroll       27.2      -1.10  14.4   8.72   5.16 \n 5 Kyle Schwarber       26.6       7.35   5.81  6.92   6.54 \n 6 Jurickson Profar     26.0       8.69   9.97 -0.921  8.31 \n 7 Bobby Witt           24.9       2.37  10.6  10.8    1.14 \n 8 Josh Naylor          24.2      12.5    5.78  8.07  -2.16 \n 9 Vinnie Pasquantino   23.8      14.5    5.42 -2.89   6.78 \n10 Kyle Isbel           23.0       5.53   8.21  3.52   5.79"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#textrmraab",
    "href": "slides/raw_slides/slides_07.html#textrmraab",
    "title": "STAT 479 Lecture 7",
    "section": "\\(\\textrm{RAA}^{(b)}\\)",
    "text": "\\(\\textrm{RAA}^{(b)}\\)\n\nRemember, we decomposed \\(\\delta_{i} = \\hat{\\mu}_{i} + \\eta_{i}\\)\nDistributed \\(\\eta_{i}\\) among all base runners involve in at-bat \\(i\\)\n\n\nCodeTop-10\n\n\n\nbatting &lt;- \n  atbat2024 |&gt;\n  dplyr::select(batter, mu) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\nraa_b &lt;-\n  batting |&gt;\n  dplyr::group_by(key_mlbam) |&gt;\n  dplyr::summarise(RAA_b = sum(mu)) |&gt;\n  dplyr::left_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_b)\n\n\n\n\n\n# A tibble: 10 × 3\n   Name              key_mlbam RAA_b\n   &lt;chr&gt;                 &lt;int&gt; &lt;dbl&gt;\n 1 Aaron Judge          592450  85.1\n 2 Shohei Ohtani        660271  70.8\n 3 Juan Soto            665742  67.7\n 4 Bobby Witt           677951  61.1\n 5 Vladimir Guerrero    665489  47.3\n 6 Gunnar Henderson     683002  44.8\n 7 Brent Rooker         667670  44.3\n 8 Yordan Alvarez       670541  42.1\n 9 Marcell Ozuna        542303  36.3\n10 Ketel Marte          606466  35.2"
  },
  {
    "objectID": "slides/raw_slides/slides_07.html#looking-ahead",
    "href": "slides/raw_slides/slides_07.html#looking-ahead",
    "title": "STAT 479 Lecture 7",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nDivided run value \\(\\delta_{i}\\) b/w batting & baserunning\nNext time: divide \\(-\\delta_{i}\\) b/w pitching & fielding"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#motivation",
    "href": "slides/raw_slides/slides_05.html#motivation",
    "title": "STAT 479: Lecture 5",
    "section": "Motivation",
    "text": "Motivation\n\nHow do NBA players help their teams win?\nHow do we quantify contributions?\n\n\n\nPlus/Minus: Easy to compute\n\nHard to separate skill from opportunities\nDoesn’t adjust for teammate quality\n\n\n\n\n\nAdjusted Plus/Minus:\n\nRegress point differential per 100 possession on signed on-court indicators\nIntroduces fairly arbitrary baseline\nAssumes all baseline players have same underlying skill"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#the-original-model",
    "href": "slides/raw_slides/slides_05.html#the-original-model",
    "title": "STAT 479: Lecture 5",
    "section": "The Original Model",
    "text": "The Original Model\n\n\\(n\\): total number of stints\n\\(p\\): total number of players\n\\(Y_{i}\\): point differential per 100 possessions in stint \\(i\\)\n\n\\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#matrix-notation",
    "href": "slides/raw_slides/slides_05.html#matrix-notation",
    "title": "STAT 479: Lecture 5",
    "section": "Matrix Notation",
    "text": "Matrix Notation\n\nFor each stint \\(i\\) and player \\(j\\), signed indicator \\(x_{ij}\\):\n\n\\(x_{ij} = 1\\) if player \\(j\\) on-court at home in stint \\(i\\)\n\\(x_{ij} = -1\\) if player \\(j\\) on-court and away in stint \\(i\\)\n\\(x_{ij} = 0\\) if player \\(j\\) off-court in stint \\(i\\)\n\n\n\n\n\\(\\boldsymbol{\\mathbf{X}}\\): \\(n \\times p\\) matrix of signed indicators\n\nRows correspond to stints:\nColumns correspond to players\n\n\\(\\boldsymbol{\\mathbf{Z}}\\): \\(n \\times (p+1)\\) matrix\n\nFirst column is all ones; remaining are \\(\\boldsymbol{\\mathbf{X}}\\)\n\\(i\\)-th row is \\(\\boldsymbol{\\mathbf{x}}_{i}\\)\n\n\\(Y_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#problems-w-original-model",
    "href": "slides/raw_slides/slides_05.html#problems-w-original-model",
    "title": "STAT 479: Lecture 5",
    "section": "Problems w/ Original Model",
    "text": "Problems w/ Original Model\n\nIndividual \\(\\alpha_{j}\\)’s are not statistically identifiable\n\nE.g. \\(\\alpha_{j} \\rightarrow \\alpha_{j}+5\\) yields same fit to data\n\n\\(\\boldsymbol{\\mathbf{Z}}\\) is not of full-rank\n\nCan’t invert \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\)\n\nCan’t estimate \\(\\boldsymbol{\\alpha}\\) with least squares!"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#a-re-parametrized-model",
    "href": "slides/raw_slides/slides_05.html#a-re-parametrized-model",
    "title": "STAT 479: Lecture 5",
    "section": "A Re-parametrized Model",
    "text": "A Re-parametrized Model\n\nAssume \\(\\alpha_{j} = \\mu\\) for all baseline players \\(j\\)\nFor all non-baseline players, \\(\\beta_{j} = \\alpha_{j} - \\mu\\)\n\\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\): drop baseline columns from \\(\\boldsymbol{\\mathbf{Z}}\\)\n\\(Y_{i} = \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta} + \\epsilon_{i}\\)\nCan estimate \\(\\boldsymbol{\\beta}\\) with least squares"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#problems-w-re-parametrized-model",
    "href": "slides/raw_slides/slides_05.html#problems-w-re-parametrized-model",
    "title": "STAT 479: Lecture 5",
    "section": "Problems w/ Re-Parametrized Model",
    "text": "Problems w/ Re-Parametrized Model\n\n250 minute cut-off for baseline is very arbitrary\nRestrictive to assume baseline players have same skill\n\n\n\nBaseline assumption needed to use least squares\n… but what if we don’t use least squares"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#ridge-regression",
    "href": "slides/raw_slides/slides_05.html#ridge-regression",
    "title": "STAT 479: Lecture 5",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nOriginal APM problem: minimize \\(\\sum_{i = 1}^{n}{\\left(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha}\\right)^{2}}\\)\n\n\n\nInstead for a fixed \\(\\lambda &gt; 0\\) let’s minimize \\(\\sum_{i = 1}^{n}{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}} + \\lambda \\times \\sum_{j = 0}^{p}{\\alpha_{j}^{2}},\\)\n\n\n\n\nFirst term: minimized when all \\(Y_{i} \\approx \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha}\\)\nSecond term: shrinkage penalty tries to keep all \\(\\alpha_{j}\\)’s near 0\n\\(\\lambda\\): trades-off these two terms"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#analytic-solution",
    "href": "slides/raw_slides/slides_05.html#analytic-solution",
    "title": "STAT 479: Lecture 5",
    "section": "Analytic Solution",
    "text": "Analytic Solution\n\nFor all \\(\\lambda\\), minimizer is \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda) = \\left(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}} + \\lambda I \\right)^{-1}\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Y}}.\\)\nThis is almost the OLS solution\n\nSlightly perturb \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) so it becomes invertible\nE.g., by adding \\(\\lambda\\) to its diagonal"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#cross-validation-for-lambda",
    "href": "slides/raw_slides/slides_05.html#cross-validation-for-lambda",
    "title": "STAT 479: Lecture 5",
    "section": "Cross-Validation for \\(\\lambda\\)",
    "text": "Cross-Validation for \\(\\lambda\\)\n\nIdea: select \\(\\lambda\\) yielding smallest out-of-sample prediction error\nProblem: don’t have a separate validation dataset to compute out-of-sample error\n\n\n\nSolution: cross-validation to estimate out-of-sample error\n\nCreate a grid of \\(\\lambda\\) values & several train/test splits\n\nFor each \\(\\lambda\\) and train/test split:\n\nCompute \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda)\\) w/ training data\nEvaluate prediction error using testing data\n\n\nFor each \\(\\lambda,\\) average testing prediction error across splits\nIdentify \\(\\hat{\\lambda}\\) w/ smallest average testing error\n\n\n\n\n\nCompute \\(\\hat{\\boldsymbol{\\alpha}}(\\hat{\\lambda})\\) w/ all data"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#ridge-regression-in-practice",
    "href": "slides/raw_slides/slides_05.html#ridge-regression-in-practice",
    "title": "STAT 479: Lecture 5",
    "section": "Ridge Regression in Practice",
    "text": "Ridge Regression in Practice\n\nImplemented in the glmnet package\ncv.glmnet(): performs cross-validation\n\nAutomatically creates grid of \\(\\lambda\\)\nDefault: 10 train/test splits\n\nImportant to set standardize = FALSE\n\n\nset.seed(479) \nlibrary(glmnet) \ncv_fit &lt;-cv.glmnet(x = X_full, y = Y, \n            alpha = 0,\n            standardize = FALSE)"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#finding-hatlambda",
    "href": "slides/raw_slides/slides_05.html#finding-hatlambda",
    "title": "STAT 479: Lecture 5",
    "section": "Finding \\(\\hat{\\lambda}\\)",
    "text": "Finding \\(\\hat{\\lambda}\\)\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nlambda_min &lt;- cv_fit$lambda.min"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#regularized-adjusted-plusminus",
    "href": "slides/raw_slides/slides_05.html#regularized-adjusted-plusminus",
    "title": "STAT 479: Lecture 5",
    "section": "Regularized Adjusted Plus/Minus",
    "text": "Regularized Adjusted Plus/Minus\n\nExtract \\(\\boldsymbol{\\alpha}(\\hat{\\lambda})\\)Top-10 RAPMDončić vs Davis\n\n\n\nlambda_index &lt;- which(cv_fit$lambda == lambda_min) \nfit &lt;-glmnet(x = X_full, y = Y, alpha = 0,\n         lambda = cv_fit$lambda, \n         standardize = FALSE)\n\nalpha_hat &lt;- fit$beta[,lambda_index] \n\n\n\n\nrapm &lt;- data.frame(id = names(alpha_hat), rapm = alpha_hat) |&gt;\n  dplyr::inner_join(y = player_table |&gt; dplyr::select(id,Name), by = \"id\")\n\n\n\n        id     rapm                    Name\n1  1628983 3.602143 Shai Gilgeous-Alexander\n2  1627827 3.472249     Dorian Finney-Smith\n3  1630596 3.415232             Evan Mobley\n4  1629029 3.352736             Luka Doncic\n5   202699 3.183716           Tobias Harris\n6   203999 2.846327            Nikola Jokic\n7  1631128 2.829463         Christian Braun\n8  1628384 2.813794              OG Anunoby\n9   203507 2.646488   Giannis Antetokounmpo\n10 1626157 2.641075      Karl-Anthony Towns\n\n\n\n\n\nExpect to score 3.4 fewer points per 100 possessions with Davis instead of Dončić\n\n\nluka_rapm &lt;- rapm |&gt; dplyr::filter(Name == \"Luka Doncic\") |&gt; dplyr::pull(rapm)\nad_rapm &lt;- rapm |&gt; dplyr::filter(Name == \"Anthony Davis\") |&gt; dplyr::pull(rapm)\nad_rapm - luka_rapm\n\n[1] -3.417006"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#other-penalties",
    "href": "slides/raw_slides/slides_05.html#other-penalties",
    "title": "STAT 479: Lecture 5",
    "section": "Other Penalties",
    "text": "Other Penalties\n\nFor some \\(a \\in [0,1]\\) glmnet() and cv.glmnet() actually minimize \\[\n\\sum_{i = 1}^{n}{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}} + \\lambda \\times  \\sum_{j = 0}^{p}{\\left[a \\times \\lvert\\alpha_{j}\\rvert + (1-a) \\times \\alpha_{j}^{2}\\right]},\n\\]\nValue of \\(a\\) specified with alpha argument.\nalpha = 0: penalty is \\(\\sum_{j}{\\alpha_{j}^{2}}\\)\n\nPenalty encourage small (but non-zero) \\(\\alpha_{j}\\) values\nRidge regression; \\(\\ell_{2}\\) or Tikohonov regularization\n\nalpha = 1: penalty is \\(\\sum_{j}{\\lvert \\alpha_{j} \\rvert}\\)\n\nPenalty encourages sparsity (i.e., sets many \\(\\alpha_{j} = 0\\))\nLeast Absolute Shrinkage and Selection Operator (LASSO); \\(\\ell_{1}\\) regularization\n\n\\(0 &lt;\\)alpha\\(&lt;1\\): Elastic Net regression"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#the-bootstrap-high-level-idea",
    "href": "slides/raw_slides/slides_05.html#the-bootstrap-high-level-idea",
    "title": "STAT 479: Lecture 5",
    "section": "The Bootstrap (High-Level Idea)",
    "text": "The Bootstrap (High-Level Idea)\n\nSay we compute some statistic \\(T(\\boldsymbol{y})\\) using observed data \\(\\boldsymbol{y}\\)\n\nE.g., RAPM estimate \\(\\hat{\\alpha}_{j}\\) for a single player \\(j\\)\nEstimated difference \\(\\hat{\\alpha}_{j} - \\hat{\\alpha}_{j'}\\) b/w players \\(j\\) and \\(j'\\)\nSomething more exotic: e.g. \\(\\max_{j}\\hat{\\alpha}_{j} - \\min_{j}\\hat{\\alpha_{j}}\\)\n\nThese are estimates and there is always estimation uncertainty\n\n\n\nRepeatedly re-sample data and re-compute statistic\n\nRe-sampled datasets: \\(\\boldsymbol{y}^{(1)}, \\ldots, \\boldsymbol{y}^{(B)}\\)\nCorresponding statistics: \\(T(\\boldsymbol{y}^{(1)}), \\ldots, T(\\boldsymbol{y}^{(B)})\\)\n\nBoostrapped statistics gives a sense of estimate’s variability"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#bootstrapping-rapm-plan",
    "href": "slides/raw_slides/slides_05.html#bootstrapping-rapm-plan",
    "title": "STAT 479: Lecture 5",
    "section": "Bootstrapping RAPM (plan)",
    "text": "Bootstrapping RAPM (plan)\n\nCompute \\(\\hat{\\boldsymbol{\\alpha}}(\\hat{\\lambda})\\) using original dataset\nDraw \\(B\\) re-samples of size \\(n\\)\n\nSample observed stints with replacement\n\nFor each re-sampled dataset, compute \\(\\hat{\\boldsymbol{\\alpha}}(\\hat{\\lambda})\\)\n\nNo need to re-run cross-validation to compute optimal \\(\\lambda\\)\nUse the optimal \\(\\lambda\\) from original dataset\n\nSave the \\(B\\) bootstrap estimates of \\(\\boldsymbol{\\alpha}\\) in an array"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#a-single-iteration",
    "href": "slides/raw_slides/slides_05.html#a-single-iteration",
    "title": "STAT 479: Lecture 5",
    "section": "A Single Iteration",
    "text": "A Single Iteration\n\nRe-sampling DataCompute \\(\\boldsymbol{\\alpha}\\)Bootstrapped Estimate\n\n\n\nset.seed(479)\nn &lt;- nrow(X_full)\np &lt;- ncol(X_full)\nboot_index &lt;- sample(1:n, size = n, replace = TRUE)\n\n\n\n [1]  1  2  3  4  5  6  6  9 10 11 13 14 14 15 16 17 18 18 19 20\n\n\n\n\n\nfit &lt;- glmnet(x = X_full[boot_index,], y = Y[boot_index],\n              lambda = cv_fit$lambda,\n              alpha = 0, standardize = FALSE)\n\n\n\n\n\n                      Name     Orig Bootstrap\n1  Shai Gilgeous-Alexander 3.602143 1.9823702\n2      Dorian Finney-Smith 3.472249 1.8454962\n3              Evan Mobley 3.415232 3.9265593\n4              Luka Doncic 3.352736 2.6188868\n5            Tobias Harris 3.183716 2.3872891\n6             Nikola Jokic 2.846327 3.1117201\n7          Christian Braun 2.829463 2.6171955\n8               OG Anunoby 2.813794 1.9276271\n9    Giannis Antetokounmpo 2.646488 0.8409495\n10      Karl-Anthony Towns 2.641075 4.2211787"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#full-bootstrap",
    "href": "slides/raw_slides/slides_05.html#full-bootstrap",
    "title": "STAT 479: Lecture 5",
    "section": "Full Bootstrap",
    "text": "Full Bootstrap\n\nB &lt;- 500 \nplayer_names &lt;- colnames(X_full) \nboot_rapm &lt;- matrix(nrow = B, ncol = p, dimnames = list(c(), player_names)) \n\nfor(b in 1:B){\n  set.seed(479+b)\n  boot_index &lt;- sample(1:n, size = n, replace = TRUE) \n  \n  fit &lt;- glmnet(x = X_full[boot_index,], y = Y[boot_index], \n                lambda = cv_fit$lambda,\n                alpha = 0,standardize = FALSE)\n  tmp_alpha &lt;- fit$beta[,lambda_index]\n  boot_rapm[b, names(tmp_alpha)] &lt;- tmp_alpha \n}"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#dončić-davis-contrast",
    "href": "slides/raw_slides/slides_05.html#dončić-davis-contrast",
    "title": "STAT 479: Lecture 5",
    "section": "Dončić-Davis Contrast",
    "text": "Dončić-Davis Contrast\n\nBootstrap Dist.Uncertainty Interval\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bootstrap distribution of difference between Doncic & Davis\n\n\n\n\n\n\n\n\ndoncic_id &lt;- player_table |&gt; dplyr::filter(Name == \"Luka Doncic\") |&gt; dplyr::pull(id) \ndavis_id &lt;- player_table |&gt; dplyr::filter(Name == \"Anthony Davis\") |&gt; dplyr::pull(id) \nboot_diff &lt;- boot_rapm[,davis_id] - boot_rapm[,doncic_id]\n\nci &lt;- quantile(boot_diff, probs = c(0.025, 0.975))\nround(ci, digits = 3)\n\n  2.5%  97.5% \n-6.378 -0.484"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#visualizing-rapm-uncertainty",
    "href": "slides/raw_slides/slides_05.html#visualizing-rapm-uncertainty",
    "title": "STAT 479: Lecture 5",
    "section": "Visualizing RAPM Uncertainty",
    "text": "Visualizing RAPM Uncertainty\n\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#uncertainty-in-ranking",
    "href": "slides/raw_slides/slides_05.html#uncertainty-in-ranking",
    "title": "STAT 479: Lecture 5",
    "section": "Uncertainty in Ranking",
    "text": "Uncertainty in Ranking\n\nrank() returns sample ranks of vector elements\n\n\nx &lt;- c(100, -1, 3, 2.5, -2)\nrank(x)\n\n[1] 5 2 4 3 1\n\n\n\n\nrank(-1*x)\n\n[1] 1 4 2 3 5"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#sgas-rapm-ranking-uncertainty",
    "href": "slides/raw_slides/slides_05.html#sgas-rapm-ranking-uncertainty",
    "title": "STAT 479: Lecture 5",
    "section": "SGA’s RAPM Ranking Uncertainty",
    "text": "SGA’s RAPM Ranking Uncertainty\n\nPoint EstimateBootstrapped RanksSGA’s Rank\n\n\n\nrapm &lt;- rapm |&gt; dplyr::mutate(rank_rapm = rank(-1 * rapm))\nrapm |&gt; dplyr::filter(Name == \"Shai Gilgeous-Alexander\")\n\n       id     rapm                    Name rank_rapm\n1 1628983 3.602143 Shai Gilgeous-Alexander         1\n\n\n\n\n\nMARGIN = 1 applies function to each row\n\n\nboot_rank &lt;- apply(-1*boot_rapm, MARGIN = 1, FUN = rank)\nsga_id &lt;- \n  player_table |&gt; dplyr::filter(Name == \"Shai Gilgeous-Alexander\") |&gt; dplyr::pull(id)\nsga_ranks &lt;- boot_rank[sga_id,]\ntable(sga_ranks)[1:10]\n\nsga_ranks\n 1  2  3  4  5  6  7  8  9 10 \n58 59 46 55 36 21 19 12 17 17 \n\n\n\nSGA had\n\nHighest RAPM in 58/500 bootstrap re-samples\n2nd highest RAPM in 59/500 re-samples\n\n\n\n\n\nConsiderable uncertainty in ranks!\n\n\nquantile(sga_ranks, probs = c(0.025, 0.975))\n\n  2.5%  97.5% \n 1.000 51.525"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#looking-ahead",
    "href": "slides/raw_slides/slides_05.html#looking-ahead",
    "title": "STAT 479: Lecture 5",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nLectures 6–8: Building up to wins above replacement in baseball\nProject Check-in: by Friday 26 September email me a short overview of your project\n\nPrecise problem statement & overview of data and analysis plan\nHappy to help brainstorm & narrow down analysis in Office Hours (or by appt.)\nInspiration: exercises in lecture notes & project information page"
  },
  {
    "objectID": "slides/raw_slides/slides_05.html#guest-lecture",
    "href": "slides/raw_slides/slides_05.html#guest-lecture",
    "title": "STAT 479: Lecture 5",
    "section": "Guest Lecture",
    "text": "Guest Lecture\n\nNamita Nandakumar (Director of R&D at Seattle Kraken)\n“Twitter Is Real Life? Translating Student Research To Team Decision-Making”\nGreat perspective on how past public-facing projects inform her current team-centric work\n6pm on Tuesday 30 September in Morgridge Hall 1524 (this room)"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#recap",
    "href": "slides/raw_slides/slides_03.html#recap",
    "title": "STAT 479: Lecture 3",
    "section": "Recap",
    "text": "Recap\n\nIn Lecture 2, fit two XG models\n\nModel 1 accounted for body part\nModel 2 accounted for body part + technique\n\nWhich model is better?\n\nWhich fits the observed data best?\nWhich will predict new data best?"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#our-simple-xg-models",
    "href": "slides/raw_slides/slides_03.html#our-simple-xg-models",
    "title": "STAT 479: Lecture 3",
    "section": "Our Simple XG Models",
    "text": "Our Simple XG Models\n\nModel 1Model 2Concatenating Predictions\n\n\n\nxg_model1 &lt;- \n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\n\n\n\n\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\") \n\n\n\n\nsimple_preds &lt;-\n  wi_shots |&gt;\n  dplyr::select(Y, shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::left_join(y = xg_model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::left_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#qualitative-comparisons",
    "href": "slides/raw_slides/slides_03.html#qualitative-comparisons",
    "title": "STAT 479: Lecture 3",
    "section": "Qualitative Comparisons",
    "text": "Qualitative Comparisons\n\nConsider two shots:\n\nRight-footed half-volley by Beth Mead against Sweden\nRight-footed backheel by Alessia Russo\n\n\n\n\n# A tibble: 2 × 4\n  shot.body_part.name shot.technique.name   XG1   XG2\n  &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Right Foot          Half Volley         0.111 0.089\n2 Right Foot          Backheel            0.111 0.103\n\n\n\n\nModel 2 accounts for more factors\nIntuitively expect it is more accurate"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#setup-notation",
    "href": "slides/raw_slides/slides_03.html#setup-notation",
    "title": "STAT 479: Lecture 3",
    "section": "Setup & Notation",
    "text": "Setup & Notation\n\nData: \\(n\\) shots represented by pairs \\((\\boldsymbol{\\mathbf{x}}_{1}, y_{1}), \\ldots, (\\boldsymbol{\\mathbf{x}}_{n}, y_{n})\\)\n\nOutcomes: \\(y_{i} = 1\\) if shot \\(i\\) results in a goal & 0 otherwise\nFeature vector: \\(\\boldsymbol{\\mathbf{x}}_{i}\\)\n\nAssumption: Data is a representative sample from an infinite super-population of shots \\[\n\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}]\n\\]\n\n\n\n\\(\\hat{p}_{i}\\): predicted \\(\\textrm{XG}\\) for shot \\(i\\) from fitted model\nHow close is \\(\\hat{p}_{i}\\) to \\(y_{i}\\)?"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#misclassification-rate-definition",
    "href": "slides/raw_slides/slides_03.html#misclassification-rate-definition",
    "title": "STAT 479: Lecture 3",
    "section": "Misclassification Rate (Definition)",
    "text": "Misclassification Rate (Definition)\n\n\\(\\hat{p}_{i} &gt; 0.5:\\) model predicts \\(y_{i} = 1\\) more likely than \\(y_{i} = 0\\)\nIdeal: \\(\\hat{p}_{i} &gt; 0.5\\) when \\(y_{i} = 1\\) and \\(\\hat{p}_{i} &lt; 0.5\\) when \\(y_{i} = 0\\)\nIf too many \\(\\hat{p}_{i}\\)’s on wrong-side of 50%, model is badly calibrated.\n\n\n\\[\n\\textrm{MISS} = n^{-1}\\sum_{i = 1}^{n}{\\mathbb{I}(y_{i} \\neq \\mathbb{I}(\\hat{p}_{i} \\geq 0.5))},\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#misclassification-rate-example",
    "href": "slides/raw_slides/slides_03.html#misclassification-rate-example",
    "title": "STAT 479: Lecture 3",
    "section": "Misclassification Rate (Example)",
    "text": "Misclassification Rate (Example)\n\nmisclass &lt;- function(y, phat){ \n  return( mean( (y != 1*(phat &gt;= 0.5))))\n}\nmisclass(simple_preds$Y, simple_preds$XG1)\nmisclass(simple_preds$Y, simple_preds$XG2)\n\n\n\n\nModel 1 misclassification 0.112 \nModel 2 misclassificaiton 0.112 \n\n\n\n\n\nWhy do Models 1 & 2 have the same misclassification rate???\n\n\n\n\nXG1XG2\n\n\n\n\n# A tibble: 3 × 2\n  shot.body_part.name   XG1\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Right Foot          0.111\n2 Left Foot           0.114\n3 Head                0.112\n\n\n\n\n\n\n# A tibble: 14 × 3\n   shot.body_part.name shot.technique.name    XG2\n   &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt;\n 1 Right Foot          Volley              0.0637\n 2 Right Foot          Normal              0.121 \n 3 Right Foot          Half Volley         0.0892\n 4 Left Foot           Overhead Kick       0     \n 5 Left Foot           Normal              0.121 \n 6 Head                Normal              0.113 \n 7 Left Foot           Half Volley         0.0676\n 8 Left Foot           Volley              0.163 \n 9 Head                Diving Header       0     \n10 Right Foot          Lob                 0.208 \n11 Right Foot          Backheel            0.103 \n12 Right Foot          Overhead Kick       0.0714\n13 Left Foot           Lob                 0     \n14 Left Foot           Backheel            0"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#brier-score-definition",
    "href": "slides/raw_slides/slides_03.html#brier-score-definition",
    "title": "STAT 479: Lecture 3",
    "section": "Brier Score (Definition)",
    "text": "Brier Score (Definition)\n\nMISS only cares whether \\(\\hat{p}_{i}\\) is on the wrong-side of 50%\n\nForecasting \\(\\hat{p} = 0.501\\) and \\(\\hat{p} = 0.999\\) have same loss when \\(y = 0\\)\nDoesn’t penalize how far \\(\\hat{p}\\) is from \\(Y\\)\n\n\n\n\nBrier Score penalizes distance b/w forecast \\(\\hat{p}\\) & \\(Y\\) \\[\n\\text{Brier} = n^{-1}\\sum_{i = 1}^{n}{(y_{i} - \\hat{p}_{i})^2}.\n\\]\nJust Mean Square Error applied to binary outcomes"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#brier-score-example",
    "href": "slides/raw_slides/slides_03.html#brier-score-example",
    "title": "STAT 479: Lecture 3",
    "section": "Brier Score (Example)",
    "text": "Brier Score (Example)\n\nbrier &lt;- function(y, phat){\n  return(mean( (y - phat)^2 ))\n}\n\nbrier(simple_preds$Y, simple_preds$XG1)\nbrier(simple_preds$Y, simple_preds$XG2)\n\n\n\nModel 1 Brier Score: 0.1 \n\n\nModel 2 Brier Score: 0.099"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#log-loss-definition",
    "href": "slides/raw_slides/slides_03.html#log-loss-definition",
    "title": "STAT 479: Lecture 3",
    "section": "Log-Loss (Definition)",
    "text": "Log-Loss (Definition)\n\nLike Brier but penalizes extreme mistakes more severely\n\n\\[\n\\textrm{LogLoss} = -1 \\times \\sum_{i = 1}^{n}{\\left[ y_{i} \\times \\log(\\hat{p}_{i}) + (1 - y_{i})\\times\\log(1-\\hat{p}_{i})\\right]}.\n\\]\n\n\nTheoretically, can be infinite (when \\(\\hat{p} = 1-y\\))\nIn practice, truncate \\(\\hat{p}\\) to \\([\\epsilon,1-\\epsilon]\\) to avoid \\(\\log(0)\\)\nAlso known as cross-entropy loss in ML literature"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#log-loss-score-example",
    "href": "slides/raw_slides/slides_03.html#log-loss-score-example",
    "title": "STAT 479: Lecture 3",
    "section": "Log-Loss Score (Example)",
    "text": "Log-Loss Score (Example)\n\nlogloss &lt;- function(y, phat){\n  if(any(phat &lt; 1e-12)) phat[phat &lt; 1e-12] &lt;- 1e-12\n  if(any(phat &gt; 1-1e-12)) phat[phat &gt; 1-1e-12] &lt;- 1-1e-12\n  return(-1 * mean( y * log(phat) + (1-y) * log(1-phat)))\n}\n\nlogloss(simple_preds$Y, simple_preds$XG1)\nlogloss(simple_preds$Y, simple_preds$XG2)\n\n\n\n\nModel 1 Log-Loss: 0.351 \nModel 2 Log-Loss: 0.348"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#in-sample-error",
    "href": "slides/raw_slides/slides_03.html#in-sample-error",
    "title": "STAT 479: Lecture 3",
    "section": "In-sample Error",
    "text": "In-sample Error\n\nRecall our setup:\n\nData is a sample from super-population \\(\\mathcal{P}\\)\nFit model to estimate \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}]\\)\nFitted model returns predictions \\(\\hat{p}(\\boldsymbol{\\mathbf{x}})\\)\n\nComputed \\(\\textrm{MISS}, \\textrm{Brier},\\) and \\(\\textrm{LogLoss}\\) w/ same data used to fit model\nThis only checks whether model fits observed data well\n\n\n\nHow well does model predict new, previously unseen data?"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#out-of-sample-error",
    "href": "slides/raw_slides/slides_03.html#out-of-sample-error",
    "title": "STAT 479: Lecture 3",
    "section": "Out-of-Sample Error",
    "text": "Out-of-Sample Error\n\nSay we had a second dataset \\((\\boldsymbol{\\mathbf{x}}_{1}^{\\star}, y_{1}^{\\star}), \\ldots, (\\boldsymbol{\\mathbf{x}}_{m}^{\\star}, y_{M}^{\\star})\\) from \\(\\mathcal{P}\\)\nCompute predictions \\(\\hat{p}^{\\star}_{m}:= \\hat{p}(\\boldsymbol{\\mathbf{x}}_{m}^{\\star})\\)\n\nImportant: \\((\\boldsymbol{\\mathbf{x}}_{m}^{\\star}, y_{m}^{\\star})\\)’s not used to fit the model\n\n\n\n\nAssess how close \\(\\hat{p}^{\\star}_{m}\\)’s are to \\(y^{\\star}_{m}\\)’s\n\nCould use misclassification rate, Brier score, or log-loss\nResult is the out-of-sample loss"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#cross-validation",
    "href": "slides/raw_slides/slides_03.html#cross-validation",
    "title": "STAT 479: Lecture 3",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nProblem: we don’t have access to a second dataset\n\n\n\nSolution: create random training/testing split\n\nTrain on a random subset containing 75% of the data\nEvaluate using the remaining held-out portion"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#the-trainingtesting-paradigm",
    "href": "slides/raw_slides/slides_03.html#the-trainingtesting-paradigm",
    "title": "STAT 479: Lecture 3",
    "section": "The Training/Testing Paradigm",
    "text": "The Training/Testing Paradigm\n\nCreate SplitFit ModelsEvaluate Models\n\n\n\nn &lt;- nrow(wi_shots)\nn_train &lt;- floor(0.75 * n)\nn_test &lt;- n - n_train\n\nwi_shots &lt;- wi_shots |&gt; dplyr::mutate(id = 1:n)\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train) \ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\")\n\n\n\n\nmodel1 &lt;- \n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nmodel2 &lt;-\n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\ntrain_preds &lt;-\n  train_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\n\n\n\n\ntest_preds &lt;-\n  test_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\nlogloss(train_preds$Y, train_preds$XG1)\nlogloss(test_preds$Y, test_preds$XG1)\n\nlogloss(train_preds$Y, train_preds$XG2)\nlogloss(test_preds$Y, test_preds$XG2)\n\n\n\n\n\nBodyPart train log-loss: 0.357 test log-loss: 0.334 \n\n\nBodyPart+Technique train log-loss: 0.355 test log-loss: 0.351"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#multiple-splits",
    "href": "slides/raw_slides/slides_03.html#multiple-splits",
    "title": "STAT 479: Lecture 3",
    "section": "Multiple Splits",
    "text": "Multiple Splits\n\nRecommend averaging over many train/test splits (e.g., 100)\nSee lecture notes for full code\nEach iteration in a for() loop:\n\nSets new seed & form new train/test split\nRe-trains both XG models and computes train & test error\nSave errors in an array\n\n\n\n\nModel 1 training logloss: 0.351 \nModel 2 training logloss: 0.348 \nModel 1 test logloss: 0.352 \nModel 2 test logloss: 0.356 \n\n\n\n\nSimpler model appaers to have slightly smaller out-of-sample log-loss!"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#motivation-accounting-for-distance",
    "href": "slides/raw_slides/slides_03.html#motivation-accounting-for-distance",
    "title": "STAT 479: Lecture 3",
    "section": "Motivation: Accounting for Distance",
    "text": "Motivation: Accounting for Distance\n\nModel 1 gives same prediction for\n\nA header from 1m away\nA header from 15m away\n\nHow to account for continuous feature like DistToGoal?\n\n\n\nIdea: Divide into discrete bins and then average within bins\n\nProblem: sensitivity to bin sizes (1m , 3m, 10m)\nProblem: potential small sample issues"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#logistic-regression-1",
    "href": "slides/raw_slides/slides_03.html#logistic-regression-1",
    "title": "STAT 479: Lecture 3",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nBinary outcome \\(Y\\) and numerical predictors \\(X_{1}, \\ldots, X_{p}\\): \\[\n\\log\\left(\\frac{\\mathbb{P}(Y= 1 \\vert \\boldsymbol{\\mathbf{X}})}{\\mathbb{P}(Y = 0 \\vert \\boldsymbol{\\mathbf{X}})}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p}.\n\\]\nKeeping all other predictors constant, a one unit change in \\(X_{j}\\) associated with a \\(\\beta_{j}\\) change in the log-odds\nSay \\(\\beta_{j} = 1\\). Increasing \\(X_{j}\\) by 1 unit moves \\(\\mathbb{P}(Y = 1)\\)\n\nFrom 4.7% to 11.2% (log-odds from -3 to -2)\nFrom 37.8% to 62.2% (log-odds from -0.5 to 0.5)\nFrom 73.1% to 88.1% (log-odds from 1 to 2)"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#logistic-inverse-logistic-functions",
    "href": "slides/raw_slides/slides_03.html#logistic-inverse-logistic-functions",
    "title": "STAT 479: Lecture 3",
    "section": "Logistic & Inverse Logistic Functions",
    "text": "Logistic & Inverse Logistic Functions\n\nLogistic function: \\(f(t) = [1 + e^{-t}]^{-1}\\)\nInverse logistic: \\(f^{-1}(p) = \\log{p/(1-p)}\\)\n\n\n\n\n\n\n\n\n\nFigure 1: Logistic and inverse logistic functions"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#an-initial-model",
    "href": "slides/raw_slides/slides_03.html#an-initial-model",
    "title": "STAT 479: Lecture 3",
    "section": "An Initial Model",
    "text": "An Initial Model\n\n\\(X_{1}\\): distance from shot to goal (DistToGoal)\n\\(\\log\\left(\\frac{\\mathbb{P}(Y = 1)}{\\mathbb{P}(Y = 0)} \\right) = \\beta_{0} + \\beta_{1} X_{1}\\)\n\n\nModel FittingModel Summary\n\n\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit1 &lt;- glm(formula = Y~DistToGoal, data = train_data,  \n            family = binomial(\"logit\"))\n\n\n\n\nsummary(fit1)\n\n\nCall:\nglm(formula = Y ~ DistToGoal, family = binomial(\"logit\"), data = train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.134174   0.128410  -1.045    0.296    \nDistToGoal  -0.127023   0.009115 -13.935   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2549.7  on 3569  degrees of freedom\nResidual deviance: 2290.7  on 3568  degrees of freedom\nAIC: 2294.7\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#assessing-model-performance",
    "href": "slides/raw_slides/slides_03.html#assessing-model-performance",
    "title": "STAT 479: Lecture 3",
    "section": "Assessing Model Performance",
    "text": "Assessing Model Performance\n\nUse predict() to make test set predictions\n\n\ntrain_pred1 &lt;- \n  predict(object = fit1,\n          newdata = train_data,\n          type = \"response\") \ntest_pred1 &lt;-\n  predict(object = fit1,\n          newdata = test_data,\n          type = \"response\")\n\nlogloss(train_data$Y, train_pred1)\nlogloss(test_data$Y, test_pred1)\n\n\n\n\nDist training logloss: 0.321 \n\n\nDist testing logloss: 0.305 \n\n\n\n\n\nAveraging across 100 train/test splits: distance-based model better than body-part-based model\n\n\n\nDist*BodyPart training logloss: 0.3167 \n\n\nDist*BodyPart testing logloss: 0.3173"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#including-multiple-predictors",
    "href": "slides/raw_slides/slides_03.html#including-multiple-predictors",
    "title": "STAT 479: Lecture 3",
    "section": "Including Multiple Predictors",
    "text": "Including Multiple Predictors\n\nDistance-based model is more accurate than body-part based model\nWhat if we account for body part and distance?\n\n\n\nBody part is a categorical predictor\n\n\n\n\n      Head  Left Foot Right Foot \n       920       1280       2560 \n\n\n\nConvert to factor type and one-hot encode\n\n\nwi_shots &lt;- wi_shots |&gt;\n  dplyr::mutate(shot.body_part.name = factor(shot.body_part.name))"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#model-specification",
    "href": "slides/raw_slides/slides_03.html#model-specification",
    "title": "STAT 479: Lecture 3",
    "section": "Model Specification",
    "text": "Model Specification\n\\[\n\\beta_{0} + \\beta_{1}\\times \\textrm{DistToGoal} + \\\\ \\beta_{\\textrm{LeftFoot}}\\times \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} \\times \\mathbb{I}(\\textrm{RightFoot})\n\\]\n\nDifferent predictions based on the body part used to attempt the shot\nFor a shot taken at distance \\(d\\):\n\nHeader: log-odds: \\(\\beta_{0} + \\beta_{1}d\\)\nLeft-footed shot: \\(\\beta_{0} + \\beta_{1}d + \\beta_{\\textrm{LeftFoot}}\\)\nRight-footed shot: \\(\\beta_{0} + \\beta_{1}d + \\beta_{\\textrm{RightFoot}}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#fitted-model",
    "href": "slides/raw_slides/slides_03.html#fitted-model",
    "title": "STAT 479: Lecture 3",
    "section": "Fitted Model",
    "text": "Fitted Model\n\nfit &lt;- glm(formula = Y~DistToGoal + shot.body_part.name, \n           data = train_data, family = binomial(\"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal + shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -0.49847    0.15504  -3.215   0.0013 ** \nDistToGoal                    -0.17501    0.01081 -16.187  &lt; 2e-16 ***\nshot.body_part.nameLeft Foot   1.28091    0.17286   7.410 1.26e-13 ***\nshot.body_part.nameRight Foot  1.30351    0.15711   8.297  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2533.3  on 3569  degrees of freedom\nResidual deviance: 2167.4  on 3566  degrees of freedom\nAIC: 2175.4\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#interactions",
    "href": "slides/raw_slides/slides_03.html#interactions",
    "title": "STAT 479: Lecture 3",
    "section": "Interactions",
    "text": "Interactions\n\nModel assumes effect of distance is the same regardless of body part\nInteractions allow the effect of one factor to vary based on the value of another. \\[\n\\begin{align}\n&\\beta_{0} + \\beta_{\\textrm{LeftFoot}} \\times \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} * \\mathbb{I}(\\textrm{RightFoot}) +  \\\\\n&+[\\beta_{\\textrm{Dist}} + \\beta_{\\textrm{Dist:LeftFoot}}*\\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{Dist:RightFoot}}\\mathbb{I}(\\textrm{RightFoot})] \\times \\textrm{Dist}   \n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#fitting-interactive-model",
    "href": "slides/raw_slides/slides_03.html#fitting-interactive-model",
    "title": "STAT 479: Lecture 3",
    "section": "Fitting Interactive Model",
    "text": "Fitting Interactive Model\n\nfit &lt;- \n  glm(formula = Y~DistToGoal * shot.body_part.name, \n      data = train_data, family = binomial(\"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal * shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                               0.87652    0.43891   1.997 0.045822\nDistToGoal                               -0.34477    0.05423  -6.358 2.05e-10\nshot.body_part.nameLeft Foot              0.04336    0.52416   0.083 0.934070\nshot.body_part.nameRight Foot            -0.30481    0.48128  -0.633 0.526517\nDistToGoal:shot.body_part.nameLeft Foot   0.15967    0.05769   2.768 0.005645\nDistToGoal:shot.body_part.nameRight Foot  0.18662    0.05576   3.347 0.000817\n                                            \n(Intercept)                              *  \nDistToGoal                               ***\nshot.body_part.nameLeft Foot                \nshot.body_part.nameRight Foot               \nDistToGoal:shot.body_part.nameLeft Foot  ** \nDistToGoal:shot.body_part.nameRight Foot ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2533.3  on 3569  degrees of freedom\nResidual deviance: 2154.5  on 3564  degrees of freedom\nAIC: 2166.5\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#cross-validation-1",
    "href": "slides/raw_slides/slides_03.html#cross-validation-1",
    "title": "STAT 479: Lecture 3",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nFit & assess models w/ 100 train/test splits\nModel w/ interactions is slightly better than others\n\n\nSimple ModelsDistance OnlyDistance + BodyPartDistance*Body Part\n\n\n\n\nBodyPart training logloss: 0.351 \nBodyPart+Technique training logloss: 0.348 \nBodyPart test logloss: 0.352 \nBodyPart+Technique test logloss: 0.356 \n\n\n\n\n\n\nDist*BodyPart training logloss: 0.3167 \n\n\nDist*BodyPart testing logloss: 0.3173 \n\n\n\n\n\n\nDist*BodyPart training logloss: 0.3069 \n\n\nDist*BodyPart testing logloss: 0.308 \n\n\n\n\n\n\nDist*BodyPart training logloss: 0.3047 \n\n\nDist*BodyPart testing logloss: 0.3066"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#including-even-more-features",
    "href": "slides/raw_slides/slides_03.html#including-even-more-features",
    "title": "STAT 479: Lecture 3",
    "section": "Including Even More Features",
    "text": "Including Even More Features\n\nStatsBomb records many potentially important features\n\n\n\n [1] \"shot.type.name\"      \"shot.technique.name\" \"shot.body_part.name\"\n [4] \"DistToGoal\"          \"DistToKeeper\"        \"AngleToGoal\"        \n [7] \"AngleToKeeper\"       \"AngleDeviation\"      \"avevelocity\"        \n[10] \"density\"             \"density.incone\"      \"distance.ToD1\"      \n[13] \"distance.ToD2\"       \"AttackersBehindBall\" \"DefendersBehindBall\"\n[16] \"DefendersInCone\"     \"InCone.GK\"           \"DefArea\"            \n\n\n\n\nHow much more predictive accuracy can we gain by accounting for these?\nChallenge: hard to specify nonlinearities & interactions correctly in glm()"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#regression-trees-i",
    "href": "slides/raw_slides/slides_03.html#regression-trees-i",
    "title": "STAT 479: Lecture 3",
    "section": "Regression Trees I",
    "text": "Regression Trees I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression trees can elegantly model interactions and non-linearities\nRegression trees are just step-functions"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#regression-trees-ii",
    "href": "slides/raw_slides/slides_03.html#regression-trees-ii",
    "title": "STAT 479: Lecture 3",
    "section": "Regression Trees II",
    "text": "Regression Trees II\n\n\n\nLeft: True function (pink) and a step function approximation (green). Right: representation of the step function as a regression tree"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#regression-trees-iii",
    "href": "slides/raw_slides/slides_03.html#regression-trees-iii",
    "title": "STAT 479: Lecture 3",
    "section": "Regression Trees III",
    "text": "Regression Trees III\n\n\nRegression trees can approximate functions arbitrarily well\nBut often need very deep and complicated trees"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#tree-ensembles",
    "href": "slides/raw_slides/slides_03.html#tree-ensembles",
    "title": "STAT 479: Lecture 3",
    "section": "Tree Ensembles",
    "text": "Tree Ensembles\n\n\nComplicated trees can be written as sums of shallow trees!"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#random-forests-1",
    "href": "slides/raw_slides/slides_03.html#random-forests-1",
    "title": "STAT 479: Lecture 3",
    "section": "Random Forests",
    "text": "Random Forests\n\nApproximate \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}]\\) w/ tree ensemble\nNo need to pre-specify functional form or interactions\nScales nicely even if \\(\\boldsymbol{\\mathbf{X}}\\) is high-dimensional\nWe will use implementation in ranger package"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#fitting-a-random-forests-model",
    "href": "slides/raw_slides/slides_03.html#fitting-a-random-forests-model",
    "title": "STAT 479: Lecture 3",
    "section": "Fitting a Random Forests Model",
    "text": "Fitting a Random Forests Model\n\nData PreparationModel FittingAssessing Predictions\n\n\n\nshot_vars &lt;-\n  c(\"Y\",\n    \"shot.type.name\", \n    \"shot.technique.name\", \"shot.body_part.name\",\n    \"DistToGoal\", \"DistToKeeper\", # dist. to keeper is distance from GK to goal\n    \"AngleToGoal\", \"AngleToKeeper\",\n    \"AngleDeviation\", \n    \"avevelocity\",\"density\", \"density.incone\",\n    \"distance.ToD1\", \"distance.ToD2\",\n    \"AttackersBehindBall\", \"DefendersBehindBall\",\n    \"DefendersInCone\", \"InCone.GK\", \"DefArea\")\n\nwi_shots &lt;-\n  wi_shots |&gt;\n  dplyr::mutate(\n    shot.type.name = factor(shot.type.name),\n    shot.body_part.name = factor(shot.body_part.name),\n    shot.technique.name = factor(shot.technique.name))\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train) |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\", shot_vars)))\n\ny_train &lt;- train_data$Y\ny_test &lt;- test_data$Y\n\ntrain_data &lt;-\n  train_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\ntest_data &lt;-\n  test_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\n\n\n\n\nlibrary(ranger)\nfit &lt;- ranger(formula = Y~., \n  data = train_data, probability = TRUE)\n\n\n\n\ntrain_preds &lt;- \n  predict(object = fit,\n          data = train_data)$predictions[,2] \n\ntest_preds &lt;- \n  predict(object = fit,\n          data = test_data)$predictions[,2]\n\nlogloss(y_train, train_preds)\n\n[1] 0.1135258\n\nlogloss(y_test, test_preds)\n\n[1] 0.2522654"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#average-out-of-sample-performance",
    "href": "slides/raw_slides/slides_03.html#average-out-of-sample-performance",
    "title": "STAT 479: Lecture 3",
    "section": "Average Out-of-Sample Performance",
    "text": "Average Out-of-Sample Performance\n\nAcross 100 random train/test splits random forests model outperforms others\n\n\n\nRandomForest training logloss: 0.1113 \n\n\nRandomForest testing logloss: 0.2705"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#comparison-to-statsbombs-model",
    "href": "slides/raw_slides/slides_03.html#comparison-to-statsbombs-model",
    "title": "STAT 479: Lecture 3",
    "section": "Comparison to StatsBomb’s Model",
    "text": "Comparison to StatsBomb’s Model\n\nFull RF ModelIn-Sample LossComparison\n\n\n\nfull_data &lt;-\n  wi_shots |&gt;\n  dplyr::select(dplyr::all_of(c(shot_vars)))\n\nfull_rf_fit &lt;-\n  ranger::ranger(formula = Y~.,data = full_data, probability = TRUE)\n\npreds &lt;- predict(object = fit, data = full_data)$predictions[,2]\n\n\n\n\n\nRandomForest training logloss: 0.154 \n\n\nStatsBomb training logloss: 0.2642 \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Comparing our random forest predictions to StatsBomb’s proprietary XG"
  },
  {
    "objectID": "slides/raw_slides/slides_03.html#looking-ahead",
    "href": "slides/raw_slides/slides_03.html#looking-ahead",
    "title": "STAT 479: Lecture 3",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nNext week: model-based assessment of NBA player performance\n\nGood idea to read notes & install hoopR\nReview notes on linear regression from prev. course (e.g., STAT 333 or 340)\nChapter 1.6 of Beyond Multiple Linear Regression is a great resource as well\n\nForm projects groups by tomorrow\n\nShould be able to sign up on Canvas\nEmail me if you’re running into issues\nPiazza can help find teammates\n\nI’ll post several project ideas over the weekend\n\nGoal for y’all: form ideas of what y’all want to do by end of next week\nGoal for me: check-in w/ all project teams by 9/26"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#lectures-office-hours",
    "href": "slides/raw_slides/slides_01.html#lectures-office-hours",
    "title": "STAT 479 Lecture 1",
    "section": "Lectures & Office Hours",
    "text": "Lectures & Office Hours\n\nLectures: Tuesdays & Thursday 11am-12:15pm (Morgridge Hall 1524)\nInstructor: Sameer Deshpande\n\nMondays: 11am - 12pm (Morgridge Hall 5586)\nWednesdays: 3pm - 4pm (Morgridge Hall 5586)\nFridays: 3pm - 4:30pm (Morgridge Hall 5618)\n\nTA: Zhexuan Liu\n\nTuesdays & Thursdays 9:15am-10:45am (Morgridge Hall 2515)"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#what-this-course-is-not",
    "href": "slides/raw_slides/slides_01.html#what-this-course-is-not",
    "title": "STAT 479 Lecture 1",
    "section": "What This Course is NOT",
    "text": "What This Course is NOT\n\nNOT a statistical methods course w/ sports applications\nMethods introduced as needed\n\nBut will not cover all technical details\nI’ll point to relevant resources & classes where relevant\nMain goal: answering substantive sports questions\n\n\n\n\nNOT a course on sports betting or fantasy sports\n\nSome material may be relevant …\n… but we won’t explicitly discuss these topics\n\n\n\n\n\nNOT a way to discuss last night’s game for credit\n\nThis is a serious statistics & data science course\n\nGoal: practice skills needed to work w/ sports data (e.g. for a team)\n\nCourse design reflects input from leaders in the industry"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#course-learning-outcomes",
    "href": "slides/raw_slides/slides_01.html#course-learning-outcomes",
    "title": "STAT 479 Lecture 1",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\n\n\nImplement appropriate statistical methods to assess player and team performance\nWork with play-by-play and high-resolution tracking data\nProvide constructive and actionable feedback on your peers’ analytic reports\nBuild a personal portfolio of sports data analyses\n\n\n\n\nAsking and answering substantive sports questions w/ data\n\nWe’ll go well beyond “who’s the best at…”"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#statistics-vs-analytics",
    "href": "slides/raw_slides/slides_01.html#statistics-vs-analytics",
    "title": "STAT 479 Lecture 1",
    "section": "Statistics vs Analytics",
    "text": "Statistics vs Analytics\n\nSports statistics refer to counts and rates that summarize performance:\n\nNumber of rebounds, wins, games played\nFree throw percentage, batting average\n\n\n\n\nAnalytics refers to the use of statistical modeling to help gain competitive advantage\nAnalytics makes use of basic statistics & creates new ones\nAnalytics uses data to answer substantive sports questions\nMany questions can framed in terms of prediction"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#bestnba-shooting-performances",
    "href": "slides/raw_slides/slides_01.html#bestnba-shooting-performances",
    "title": "STAT 479 Lecture 1",
    "section": "“Best”NBA Shooting Performances",
    "text": "“Best”NBA Shooting Performances\n\nWho is the best shooter in NBA history?\n\nWhat do we mean by “best”?\nIt’s not clear that this can be answered w/ data\n\n\n\n\nA more precise question: which player made the most shots in a single season?\n\nCan be answered by data\nWe’ll assess whether “best” = “made most shots” is satisfactory later\n\n\n\n\n\nData: box scores from each game since 2002-03\nAvailable from the hoopR package\n\n\nraw_box &lt;- hoopR::load_nba_player_box(seasons = 2002:(hoopR::most_recent_nba_season()))"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#data-snapshot",
    "href": "slides/raw_slides/slides_01.html#data-snapshot",
    "title": "STAT 479 Lecture 1",
    "section": "Data Snapshot",
    "text": "Data Snapshot\n\nColumnsSingle Game\n\n\n\n\n [1] \"game_id\"                           \"season\"                           \n [3] \"season_type\"                       \"game_date\"                        \n [5] \"game_date_time\"                    \"athlete_id\"                       \n [7] \"athlete_display_name\"              \"team_id\"                          \n [9] \"team_name\"                         \"team_location\"                    \n[11] \"team_short_display_name\"           \"minutes\"                          \n[13] \"field_goals_made\"                  \"field_goals_attempted\"            \n[15] \"three_point_field_goals_made\"      \"three_point_field_goals_attempted\"\n[17] \"free_throws_made\"                  \"free_throws_attempted\"            \n[19] \"offensive_rebounds\"                \"defensive_rebounds\"               \n[21] \"rebounds\"                          \"assists\"                          \n[23] \"steals\"                            \"blocks\"                           \n[25] \"turnovers\"                         \"fouls\"                            \n[27] \"plus_minus\"                        \"points\"                           \n[29] \"starter\"                           \"ejected\"                          \n[31] \"did_not_play\"                      \"active\"                           \n[33] \"athlete_jersey\"                    \"athlete_short_name\"               \n[35] \"athlete_headshot_href\"             \"athlete_position_name\"            \n[37] \"athlete_position_abbreviation\"     \"team_display_name\"                \n[39] \"team_uid\"                          \"team_slug\"                        \n[41] \"team_logo\"                         \"team_abbreviation\"                \n[43] \"team_color\"                        \"team_alternate_color\"             \n[45] \"home_away\"                         \"team_winner\"                      \n[47] \"team_score\"                        \"opponent_team_id\"                 \n[49] \"opponent_team_name\"                \"opponent_team_location\"           \n[51] \"opponent_team_display_name\"        \"opponent_team_abbreviation\"       \n[53] \"opponent_team_logo\"                \"opponent_team_color\"              \n[55] \"opponent_team_alternate_color\"     \"opponent_team_score\"              \n[57] \"reason\"                           \n\n\n\n\n\nraw_box |&gt; dplyr::filter(game_date == \"2011-06-12\") |&gt;\n  dplyr::select(athlete_display_name, \n         field_goals_made, field_goals_attempted)\n\n# A tibble: 30 × 3\n   athlete_display_name field_goals_made field_goals_attempted\n   &lt;chr&gt;                           &lt;int&gt;                 &lt;int&gt;\n 1 Dirk Nowitzki                       9                    27\n 2 Tyson Chandler                      2                     4\n 3 Jason Kidd                          2                     4\n 4 Shawn Marion                        4                    10\n 5 J.J. Barea                          7                    12\n 6 Brian Cardinal                      1                     1\n 7 Caron Butler                       NA                    NA\n 8 Ian Mahinmi                         2                     3\n 9 Rodrigue Beaubois                  NA                    NA\n10 DeShawn Stevenson                   3                     5\n# ℹ 20 more rows"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#extract-regular-season-data",
    "href": "slides/raw_slides/slides_01.html#extract-regular-season-data",
    "title": "STAT 479 Lecture 1",
    "section": "Extract Regular Season Data",
    "text": "Extract Regular Season Data\n\nallstar_dates &lt;- lubridate::date(c(\"2002-02-10\", \"2003-02-09\", \n    \"2004-02-15\",\"2005-02-20\", \"2006-02-19\", \"2007-02-18\", \n    \"2008-02-17\", \"2009-02-15\", \"2010-02-14\",\"2011-02-20\", \n    \"2012-02-26\", \"2013-02-17\", \"2014-02-16\", \"2015-02-15\", \n    \"2016-02-14\",\"2017-02-19\", \"2018-02-18\", \"2019-02-17\",\n    \"2020-02-16\", \"2021-03-07\", \"2022-02-20\",\"2023-02-19\", \n    \"2024-02-18\", \"2025-02-16\"))\nreg_box &lt;- raw_box |&gt;\n  dplyr::filter(season_type == 2 & !did_not_play & !game_date %in% allstar_dates)"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#preprocessing",
    "href": "slides/raw_slides/slides_01.html#preprocessing",
    "title": "STAT 479 Lecture 1",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nRename columns & set defaults for players with no minutes\n\n\nreg_box &lt;-\n  reg_box |&gt;\n  dplyr::rename(\n    Player = athlete_display_name, \n    FGM = field_goals_made, FGA = field_goals_attempted,\n    TPM = three_point_field_goals_made,TPA = three_point_field_goals_attempted,\n    FTM = free_throws_made, FTA = free_throws_attempted) |&gt;\n  dplyr::mutate(\n    FGM = ifelse(is.na(minutes), 0, FGM), FGA = ifelse(is.na(minutes), 0, FGA),\n    TPM = ifelse(is.na(minutes), 0, TPM),TPA = ifelse(is.na(minutes), 0, TPA),\n    FTM = ifelse(is.na(minutes), 0, FTM),FTA = ifelse(is.na(minutes), 0, FTA)) |&gt;\n  tidyr::replace_na(list(minutes = 0))"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#season-totals",
    "href": "slides/raw_slides/slides_01.html#season-totals",
    "title": "STAT 479 Lecture 1",
    "section": "Season Totals",
    "text": "Season Totals\n\nseason_box &lt;-\n  reg_box |&gt;\n  dplyr::group_by(Player, season) |&gt;\n  dplyr::summarise(FGM = sum(FGM),FGA = sum(FGA),\n    TPM = sum(TPM),TPA = sum(TPA),FTM = sum(FTM),FTA = sum(FTA),\n    minutes = sum(minutes), n_games = dplyr::n(),.groups = \"drop\")\nseason_box |&gt; dplyr::filter(Player == \"Dirk Nowitzki\") \n\n\n\n\n# A tibble: 18 × 10\n   Player        season   FGM   FGA   TPM   TPA   FTM   FTA minutes n_games\n   &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n 1 Dirk Nowitzki   2002   600  1258   139   350   440   516    2891      76\n 2 Dirk Nowitzki   2003   690  1489   148   390   483   548    3117      80\n 3 Dirk Nowitzki   2004   605  1310    99   290   371   423    2915      77\n 4 Dirk Nowitzki   2005   663  1445    91   228   615   708    3020      78\n 5 Dirk Nowitzki   2006   751  1564   110   271   539   598    3086      81\n 6 Dirk Nowitzki   2007   673  1341    72   173   498   551    2819      80\n 7 Dirk Nowitzki   2008   630  1314    79   220   478   544    2769      81\n 8 Dirk Nowitzki   2009   774  1616    61   170   485   545    3051      82\n 9 Dirk Nowitzki   2010   720  1496    51   121   536   586    3041      82\n10 Dirk Nowitzki   2011   610  1179    66   168   395   443    2505      82\n11 Dirk Nowitzki   2012   473  1034    78   212   318   355    2078      66\n12 Dirk Nowitzki   2013   332   707    63   151   164   191    1628      52\n13 Dirk Nowitzki   2014   633  1273   131   329   338   376    2625      80\n14 Dirk Nowitzki   2015   487  1062   104   274   255   289    2285      77\n15 Dirk Nowitzki   2016   498  1112   126   342   250   280    2362      75\n16 Dirk Nowitzki   2017   296   678    79   209    98   112    1421      54\n17 Dirk Nowitzki   2018   346   758   138   337    97   108    1901      77\n18 Dirk Nowitzki   2019   135   376    64   205    39    50     794      51"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#who-made-the-most-shots",
    "href": "slides/raw_slides/slides_01.html#who-made-the-most-shots",
    "title": "STAT 479 Lecture 1",
    "section": "Who Made the Most Shots?",
    "text": "Who Made the Most Shots?\n\nseason_box |&gt;\n  dplyr::arrange(dplyr::desc(FGM)) |&gt;\n  dplyr::select(Player, season, FGM, FGA, minutes, n_games) |&gt; \n  dplyr::slice_head(n=5)\n\n# A tibble: 5 × 6\n  Player                  season   FGM   FGA minutes n_games\n  &lt;chr&gt;                    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n1 Kobe Bryant               2006   949  2109    3184      78\n2 LeBron James              2006   875  1823    3361      82\n3 Kobe Bryant               2003   868  1924    3401      82\n4 Shai Gilgeous-Alexander   2025   868  1680    2633      77\n5 LeBron James              2018   857  1580    3024      82\n\n\n\n\nIs Kobe’s 2002-03 really the same as SGA’s 2024-25???\n\n\n\n\nArguably “best” should account for efficiency"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#field-goal-percentage",
    "href": "slides/raw_slides/slides_01.html#field-goal-percentage",
    "title": "STAT 479 Lecture 1",
    "section": "Field Goal Percentage",
    "text": "Field Goal Percentage\n\nseason_box &lt;-\n  season_box |&gt;\n  dplyr::mutate(FGP = ifelse(FGA &gt; 0, FGM/FGA, NA_real_))\n\n\n\nFGP LeadersAnother LookRestricted Leaders\n\n\n\nseason_box |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::slice_head(n=5) |&gt;\n  dplyr::select(Player, season, FGP)\n\n# A tibble: 5 × 3\n  Player           season   FGP\n  &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;\n1 Ahmad Caver        2022     1\n2 Alondes Williams   2025     1\n3 Andris Biedrins    2014     1\n4 Anthony Brown      2018     1\n5 Braxton Key        2023     1\n\n\n\n\n\nseason_box |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::slice_head(n=5) |&gt;\n  dplyr::select(Player, season, FGP, FGA)\n\n# A tibble: 5 × 4\n  Player           season   FGP   FGA\n  &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Ahmad Caver        2022     1     1\n2 Alondes Williams   2025     1     2\n3 Andris Biedrins    2014     1     1\n4 Anthony Brown      2018     1     1\n5 Braxton Key        2023     1     1\n\n\n\n\n\nseason_box |&gt; \n  dplyr::filter(FGA &gt;= 400) |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, FGP, FGA) |&gt;\n  dplyr::slice_head(n = 5)\n\n# A tibble: 5 × 4\n  Player         season   FGP   FGA\n  &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Daniel Gafford   2024 0.725   480\n2 Walker Kessler   2023 0.720   414\n3 DeAndre Jordan   2017 0.714   577\n4 Rudy Gobert      2022 0.713   508\n5 DeAndre Jordan   2015 0.710   534"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#effective-field-goal-percentage",
    "href": "slides/raw_slides/slides_01.html#effective-field-goal-percentage",
    "title": "STAT 479 Lecture 1",
    "section": "Effective Field Goal Percentage",
    "text": "Effective Field Goal Percentage\n\nFGP is arguably a better measure of skill than FGM but\n\nExtreme values for players w/ few attempts\nDoesn’t distinguish 2- and 3-point shots\n\n\n\n\n\\(\\textrm{eFGP} = (\\textrm{FGM} + 0.5 \\times \\textrm{TPM})/\\textrm{FGA}\\)\n\n\nseason_box &lt;- \n  season_box |&gt; \n  dplyr::mutate(eFGP = (FGM + 0.5 * TPM)/FGA) \n\n\n\n\nRaw eFGP LeadersRestricted Leaders\n\n\n\neFGP leaders are mostly centers who don’t shoot 3’s\n\n\n\n# A tibble: 5 × 6\n  Player         season  eFGP   FGP   TPA n_games\n  &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 Daniel Gafford   2024 0.725 0.725     0      74\n2 Walker Kessler   2023 0.721 0.720     3      74\n3 DeAndre Jordan   2017 0.714 0.714     2      81\n4 Rudy Gobert      2022 0.713 0.713     4      66\n5 DeAndre Jordan   2015 0.711 0.710     4      82\n\n\n\n\n\nWe can restrict to players with \\(\\textrm{FGA} &gt; 400\\) and \\(\\textrm{TPA} &gt; 100\\)\n\n\n\n# A tibble: 5 × 6\n  Player          season  eFGP   FGP   TPA n_games\n  &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 Kyle Korver       2015 0.671 0.487   449      75\n2 Duncan Robinson   2020 0.667 0.470   606      73\n3 Obi Toppin        2024 0.660 0.571   260      83\n4 Nikola Jokic      2023 0.660 0.632   149      69\n5 Joe Harris        2021 0.655 0.502   427      69"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#reflection",
    "href": "slides/raw_slides/slides_01.html#reflection",
    "title": "STAT 479 Lecture 1",
    "section": "Reflection",
    "text": "Reflection\n\neFGP is arguably better than FGP and FGM\nBut it is still highly variable\n\n\n\nRefined question: instead of “who’s the best”, we can ask\n\n“what is the probability that a player makes a shot”\n\nLater: methods to estimate these probs. that\n\nAccount for contextual factors (e.g., shot location)\nProduce stable estimates even w/ small sample sizes\nAvoid imposing arbitrary cut-offs (e.g., \\(\\textrm{FGA} &gt; 400\\))\nCalibrate performance against transparent baselines (e.g., “above replacement”)\n\nAnalytics involves motivating and justifying choices"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#requisites",
    "href": "slides/raw_slides/slides_01.html#requisites",
    "title": "STAT 479 Lecture 1",
    "section": "Requisites",
    "text": "Requisites\n\nSTAT 333 or 340\n\nRandom variables, expectations, probability\nFitting linear models & interpreting outputs\n\n\n\n\nPrior experience with R\n\nAssignment, scripting, loops, control flow\nSaving objects, installing & loading packages.\nData manipulation with dplyr and other tidyverse packages.\nCreating visualizations"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#course-website",
    "href": "slides/raw_slides/slides_01.html#course-website",
    "title": "STAT 479 Lecture 1",
    "section": "Course Website",
    "text": "Course Website\n\n\n\n\n\nAlso Canvas & Piazza"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#assignments-grading",
    "href": "slides/raw_slides/slides_01.html#assignments-grading",
    "title": "STAT 479 Lecture 1",
    "section": "Assignments & Grading",
    "text": "Assignments & Grading\n\n3 Group Projects (900 pts): Due on 10/10, 11/7, and 12/5\n\nWritten Report (100 pts) & Presentation (100 pts)\nPeer Reviews (60 pts)\nTeam Accountability Survey (40 pts)\n\n\n\n\nParticipation (100 pts): Assessed holistically\n\nRegularly attend lectures & office hours\nContribute to Piazza discussions\n\n\n\n\n\nFinal grade based on how many of the 1000 points earned"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#projects",
    "href": "slides/raw_slides/slides_01.html#projects",
    "title": "STAT 479 Lecture 1",
    "section": "Projects",
    "text": "Projects\n\nWork in groups of up to 4\n\nSame groups for Projects 1 & 2; can change for Project 3\nForm groups by Friday September 12\nUse Piazza to find teammates & sign up on Canvas\n\nCourse projects can\n\nModify or extend analysis from lecture (e.g., to new sport)\nAnswer a new question not covered in lecture\n…\n\nIdeally jump-start a portfolio that you can show teams"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-report",
    "href": "slides/raw_slides/slides_01.html#project-report",
    "title": "STAT 479 Lecture 1",
    "section": "Project Report",
    "text": "Project Report\n\nExecutive Summary:\n\nNon-technical overview of goals, methods, and results\nAudience: front office executive, coach, or player\n\nTechnical Report:\n\nInclude all code needed to reproduce findings\nTightly integrate code & output w/ written exposition\nAudience: fellow data scientists\n\n\n\nI highly recommend using Quarto or RMarkdown"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-presentation",
    "href": "slides/raw_slides/slides_01.html#project-presentation",
    "title": "STAT 479 Lecture 1",
    "section": "Project Presentation",
    "text": "Project Presentation\n\nRecord & upload a 8–10 minute presentation\nEvery group member must speak\n\n\n\nI’ll select 5-6 groups to present in-class on the last day (12/9)\n\nDetails to be announced\nPresenting on the last day unrelated to course grades\nBut there’ll be prizes"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-peer-review",
    "href": "slides/raw_slides/slides_01.html#project-peer-review",
    "title": "STAT 479 Lecture 1",
    "section": "Project Peer Review",
    "text": "Project Peer Review\n\nProvide feedback on 3 presentations & 3 reports\n\nRubrics will be available\nFill out rubric + leave constructive comments\n\nDue 1 weeks after projects: 10/17, 11/14, and 12/12"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#team-accountability-survey",
    "href": "slides/raw_slides/slides_01.html#team-accountability-survey",
    "title": "STAT 479 Lecture 1",
    "section": "Team Accountability Survey",
    "text": "Team Accountability Survey\n\nAssign score from 0 (least) to 10 (most) for\n\nParticipation, Preparation, and Respectfulness\n\nRate every group member (including yourself)\nRatings and comments will be kept anonymous\nCourse staff may give warnings for low peer score"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#unit-1-quantifying-performances",
    "href": "slides/raw_slides/slides_01.html#unit-1-quantifying-performances",
    "title": "STAT 479 Lecture 1",
    "section": "Unit 1: Quantifying Performances",
    "text": "Unit 1: Quantifying Performances\n\nExpected vs actual performance\nValue of a game state\nPerformance above “replacement” level\nCase Studies:\n\nExpected goals in soccer (Lectures 2 & 3)\nAdjusted plus/minus in the NBA (Lectures 4 & 5)\nRun expectancy & WAR in baseball (Lectures 6–8)\nWAR for the NFL (Lectures 9 & 10)\nPitch Framing in baseball (Lecture 11)"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-1",
    "href": "slides/raw_slides/slides_01.html#project-1",
    "title": "STAT 479 Lecture 1",
    "section": "Project 1",
    "text": "Project 1\n\nUse box-score or play-by-play data\nIntroduce a new metric & show it has favorable properties\n\nSeason-to-season stability\nAbility to predict match- or season-level outcomes\nReveal new insight about player valuation\n\nEvaluate individual or team performance in a new sport\n\nWAR for volleyball or college football?\nExpected goals in hockey?\n\nExtend case study from lecture\n\nConstruct analogs for another sport\nExamine season-to-season variability\n\n…"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#unit-2-rankings-simulation",
    "href": "slides/raw_slides/slides_01.html#unit-2-rankings-simulation",
    "title": "STAT 479 Lecture 1",
    "section": "Unit 2: Rankings & Simulation",
    "text": "Unit 2: Rankings & Simulation\n\nUse models to simulate games, tournaments, drafts, etc.\nCase Studies:\n\nNCAA Volleyball & Hockey Tournaments (Lectures 12 & 13)\nMarkov chain simulations (Lecture 14 & 15)\nBuilding a consensus mock draft (Lecture 16)\nEstimating impact of a rule change (Lecture 17)"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-2",
    "href": "slides/raw_slides/slides_01.html#project-2",
    "title": "STAT 479 Lecture 1",
    "section": "Project 2",
    "text": "Project 2\n\nFit a model to estimate latent team- or player- strength\nUse model estimates to simulate plays, games, tournaments, drafts, etc.\nCompute probabilities based on simulation\n\nProb. of winning tournament or making it past 1st round\nProb. football drive ends in a touchdown\nProb. of winning a cricket test\nChance player available in 2nd round of draft"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#unit-3-tracking-data",
    "href": "slides/raw_slides/slides_01.html#unit-3-tracking-data",
    "title": "STAT 479 Lecture 1",
    "section": "Unit 3: Tracking Data",
    "text": "Unit 3: Tracking Data\n\nCurrently the hottest area of sports analytics\n\nTrackman + Statcast in baseball\nNFL: player positions 10 times per second\nHawkeye for tennis, Hudl for soccer, …\n\nTracking data opens up many possibilities\n\nIncorporating spatial info. into prediction models\nCreating new metrics using tracking data\nSpace ownership & predicting trajectories"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#project-3",
    "href": "slides/raw_slides/slides_01.html#project-3",
    "title": "STAT 479 Lecture 1",
    "section": "Project 3",
    "text": "Project 3\n\nAnalyze tracking data\nNFL’s Big Data Bowl is a great opportunity\n\nBest way into the field (even for non-football sports)\nCash prizes & chance to present at Combine\n\nI highly encourage turning Project 3 into a BDB submission\n\nMore info will be posted on Piazza\nI’ll provide additional feedback/input for teams that do submit"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#course-expectations",
    "href": "slides/raw_slides/slides_01.html#course-expectations",
    "title": "STAT 479 Lecture 1",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nRespect diverse backgrounds\n\nNot everyone may know as much about a sport/method as You\nYou may not know as much about a sport/method as someone else\n\nDon’t hesitate to ask for and to provide help!\nTake care of yourself & each other"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#generative-ai-expectations-1",
    "href": "slides/raw_slides/slides_01.html#generative-ai-expectations-1",
    "title": "STAT 479 Lecture 1",
    "section": "Generative AI Expectations 1",
    "text": "Generative AI Expectations 1\n\nYou have the right to the full benefit of my expertise and engagement in this course.\nI will therefore never use AI to\n\nTo provide feedback on assignments\nTo prepare any course content (e.g., slides, code, assignment flavor-text, etc.)\nTo mediate or assist any communications with you\n\nEverything you see in the course was created by me without the aid of generative AI"
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#generative-ai-expectations-2",
    "href": "slides/raw_slides/slides_01.html#generative-ai-expectations-2",
    "title": "STAT 479 Lecture 1",
    "section": "Generative AI Expectations 2",
    "text": "Generative AI Expectations 2\n\nA core theme of this class is practice\n\nIt is OK to make mistakes or not know an answer\nProcess is more important than results\n\nGenerative AI short-circuits the intended learning process\nIts use is expressly prohibited\n\n\n\n\n\n\n\nPeer Review\n\n\nRespect your classmates enough to review their projects yourself. Uploading someone else’s project report or presentation to a generative AI tool (e.g., for creating summaries) is forbidden and will result in a failing grade."
  },
  {
    "objectID": "slides/raw_slides/slides_01.html#looking-ahead",
    "href": "slides/raw_slides/slides_01.html#looking-ahead",
    "title": "STAT 479 Lecture 1",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nLectures 2 & 3: Expected Goals in Soccer\nWill use public data provided by Hudl\nBe sure to install the StatsBombR & ranger packages\n\n\ndevtools::install_github(\"statsbomb/StatsBombR\")\ninstall.packages(\"ranger\")"
  },
  {
    "objectID": "slides/lecture12.html",
    "href": "slides/lecture12.html",
    "title": "Lecture 12: Bradley-Terry Models I",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 12: Bradley-Terry Models I"
    ]
  },
  {
    "objectID": "slides/lecture10.html",
    "href": "slides/lecture10.html",
    "title": "Lecture 10: NFL WAR",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 10: NFL WAR"
    ]
  },
  {
    "objectID": "slides/lecture08.html",
    "href": "slides/lecture08.html",
    "title": "Lecture 8: Defensive Credit Allocation & WAR",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation & WAR"
    ]
  },
  {
    "objectID": "slides/lecture06.html",
    "href": "slides/lecture06.html",
    "title": "Lecture 6: Expected Runs",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 6: Expected Runs"
    ]
  },
  {
    "objectID": "slides/lecture04.html",
    "href": "slides/lecture04.html",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "slides/lecture02.html",
    "href": "slides/lecture02.html",
    "title": "Lecture 2: Expected Goals",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture15.html#beyond-run-scoring",
    "href": "lectures/lecture15.html#beyond-run-scoring",
    "title": "Lecture 15: Markov Chains II",
    "section": "Beyond Run Scoring",
    "text": "Beyond Run Scoring\n\nGame State After 2 Batters",
    "crumbs": [
      "Lecture 15: Markov Chains II"
    ]
  },
  {
    "objectID": "lectures/lecture15.html#incorporating-team-information",
    "href": "lectures/lecture15.html#incorporating-team-information",
    "title": "Lecture 15: Markov Chains II",
    "section": "Incorporating Team Information",
    "text": "Incorporating Team Information\nOur Markov chain simulations have, so far, assumed that the transition probabilities between states are identical for all teams. This simplified assumption overlooks important differences between batting and pitching abilities.\n\nEmpirical Bayes Shrinkage Estimator",
    "crumbs": [
      "Lecture 15: Markov Chains II"
    ]
  },
  {
    "objectID": "lectures/lecture15.html#team-specific-half-inning-simulation",
    "href": "lectures/lecture15.html#team-specific-half-inning-simulation",
    "title": "Lecture 15: Markov Chains II",
    "section": "Team-specific Half-Inning Simulation",
    "text": "Team-specific Half-Inning Simulation",
    "crumbs": [
      "Lecture 15: Markov Chains II"
    ]
  },
  {
    "objectID": "lectures/lecture13.html",
    "href": "lectures/lecture13.html",
    "title": "Lecture 13: Tournament Simulation",
    "section": "",
    "text": "In Lecture 12, we used a Bradley-Terry model to estimate the latent strength of each Division I women’s ice hockey team. The model posits that the log-odds of one team beating the other is simply the difference in these latent strengths. According to our model estimates, the Wisconsin Badgers and Ohio State Buckeyes were ranked 1st and 2nd among all teams. Using the estimated model parameters, we simulated a “best-of-5” series between the two teams, finding that Wisconsin was predicted to win such a series about 91% of the time.\nOne major drawback of our initial model is that it fails to account for any potential home-court advantage. In other words, it assumes that the probability that Wisconsin beats Ohio State is the same whether the game was played at La Bahn Arena (the home of the Wisconin Badgers), the Ohio State University Ice Rink, or at a neutral site.\nIn this lecture, we fit a refined version of the Bradley-Terry model that accounts for location. We introduce this model in Section 2 and perform the necessary pre-processing in Section 3. Then, in Section 4, we fit the model using the BradleyTerry2 package and examine the resulting power rankings. Finally, in Section 5 we simulate the semi-finals and finals of the 2025 NCAA Tournament under several different scenarios.\nBefore proceeding, we load in the data table containing all regular season Division I women’s ice hockey games from the 2024-25 season that did not end in ties.\n\nload(\"wd1hockey_regseason_2024_2025.RData\")",
    "crumbs": [
      "Lecture 13: Tournament Simulation"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#overview",
    "href": "lectures/lecture13.html#overview",
    "title": "Lecture 13: Tournament Simulation",
    "section": "",
    "text": "In Lecture 12, we used a Bradley-Terry model to estimate the latent strength of each Division I women’s ice hockey team. The model posits that the log-odds of one team beating the other is simply the difference in these latent strengths. According to our model estimates, the Wisconsin Badgers and Ohio State Buckeyes were ranked 1st and 2nd among all teams. Using the estimated model parameters, we simulated a “best-of-5” series between the two teams, finding that Wisconsin was predicted to win such a series about 91% of the time.\nOne major drawback of our initial model is that it fails to account for any potential home-court advantage. In other words, it assumes that the probability that Wisconsin beats Ohio State is the same whether the game was played at La Bahn Arena (the home of the Wisconin Badgers), the Ohio State University Ice Rink, or at a neutral site.\nIn this lecture, we fit a refined version of the Bradley-Terry model that accounts for location. We introduce this model in Section 2 and perform the necessary pre-processing in Section 3. Then, in Section 4, we fit the model using the BradleyTerry2 package and examine the resulting power rankings. Finally, in Section 5 we simulate the semi-finals and finals of the 2025 NCAA Tournament under several different scenarios.\nBefore proceeding, we load in the data table containing all regular season Division I women’s ice hockey games from the 2024-25 season that did not end in ties.\n\nload(\"wd1hockey_regseason_2024_2025.RData\")",
    "crumbs": [
      "Lecture 13: Tournament Simulation"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#sec-bt-home",
    "href": "lectures/lecture13.html#sec-bt-home",
    "title": "Lecture 13: Tournament Simulation",
    "section": "Bradley-Terry Models with Home-Court Advantage",
    "text": "Bradley-Terry Models with Home-Court Advantage\nLike the basic model from Lecture 12, we will associate a latent parameter \\(\\lambda_{j}\\) to each team \\(j = 1, \\ldots, p.\\) We will also fix one \\(\\lambda_{j} = 0\\), corresponding to a pre-specified reference team1. In addition to the latent team strengths \\(\\lambda_{1}, \\ldots, \\lambda_{p},\\) we will introduce an additional parameter \\(\\lambda_{0}\\) that accounts for a systematic home advantage.\nNow suppose that the home team \\(H\\) plays an away team \\(A\\) at team \\(H\\)’s home. Our augmented Bradley-Terry model asserts that the log-odds that \\(H\\) beats \\(A\\) is \\(\\lambda_{0} + \\lambda_{H} - \\lambda_{A}.\\) Equivalently, our augmented model asserts that \\[\n\\mathbb{P}(\\textrm{team H beats team A at H's home}) = \\frac{e^{\\lambda_{H} + \\lambda_{0}}}{e^{\\lambda_{H} + \\lambda_{0}} + e^{\\lambda_{A}}}.\n\\] Compared to the original Bradley-Terry model we considered in Lecture 12, our new model gives a systematic advantage to the home team. For any two teams \\(i\\) and \\(j,\\) our original Bradley-Terry model assumes that the probability of team \\(i\\) beating team \\(j\\) is the same across all locations. Under our new model, the model makes a different prediction depending on location: the log-odds of \\(i\\) beating \\(j\\) are\n\n\\(\\lambda_{0} + \\lambda_{i} - \\lambda_{j}\\): if the game is played at \\(i\\)’s home arena\n\\(-\\lambda_{0} + \\lambda_{i} - \\lambda_{j}\\): if the game is played at \\(j\\)’s home arena\n\\(\\lambda_{i} - \\lambda_{j}\\): if the game is played at a neutral site (i.e., not at \\(i\\)’s or \\(j\\)’s home arena)\n\nTo derive the log-odds of \\(i\\) beating \\(j\\) at \\(j\\)’s home arena, note that according to our model \\[\n\\mathbb{P}(\\textrm{team j beats i at j's home}) = \\frac{e^{\\lambda_{j} + \\lambda_{0}}}{e^{\\lambda_{j} + \\lambda_{0}} + e^{\\lambda_{i}}}.\n\\] Thus \\[\n\\begin{align}\n\\mathbb{P}(\\textrm{team i beats j at j's home}) &= \\frac{e^{\\lambda_{i}}}{e^{\\lambda_{j} + \\lambda_{0}} + e^{\\lambda_{i}}} \\\\\n&= \\frac{1}{e^{\\lambda_{j} + \\lambda_{0} - \\lambda_{i}} + 1} \\\\\n&= \\frac{1}{1 + e^{-1 \\times (-\\lambda_{0} + \\lambda_{i} - \\lambda_{j})}}.\n\\end{align}\n\\]",
    "crumbs": [
      "Lecture 13: Tournament Simulation"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#sec-data-prep",
    "href": "lectures/lecture13.html#sec-data-prep",
    "title": "Lecture 13: Tournament Simulation",
    "section": "Data Preparation",
    "text": "Data Preparation\nUnfortunately, the data table we scraped from USCHO does not record the location of the game. USCHO publishes box scores for individual games on separate webpages with highly structured URLs. For instance, the URL for the webpage with the box score from the championship game between the Badgers and Buckeyes is\nhttps://www.uscho.com/gameday/division-i-women/2024-2025/2025-03-23/game-7949/\nNotice that after the date there is a unique game identifier (7949 in this case). Based on this, it is tempting to write a function that visits each individual site (e.g., by looping over the unique game ID’s). Unfortunately, this strategy requires us to determine the unique game identifiers, which is not entirely straightforward. We instead manually impute the location information with some heuristics.\n\nDetermining Location\nWe will assume all non-tournament conference games and most tournament games are played at the home arena of the listed home team and not at a neutral site. For instance, there were four games in Icebreaker tournament featured four games:\n\nPenn State vs Cornell (boxscore)\nStonehill vs Ohio State (boxscore)\nCornell vs Ohio State (boxscore)\nStonehill vs Penn State (boxscore)\n\n\nno_ties |&gt;\n  dplyr::filter(grepl(\"IceBreaker\", Notes)) |&gt;\n  dplyr::select(Date, Opponent, Home, Notes, Type)\n\n# A tibble: 4 × 5\n  Date       Opponent   Home       Notes      Type \n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;\n1 10/25/2024 Penn State Cornell    IceBreaker NC   \n2 10/25/2024 Stonehill  Ohio State IceBreaker NC   \n3 10/26/2024 Cornell    Ohio State IceBreaker NC   \n4 10/26/2024 Stonehill  Penn State IceBreaker NC   \n\n\nInspecting the box scores pages for all four games, we see that they were all played at Value City Arena. Although this is located at the Ohio State University — and is the home arena of their men’s ice hockey team — it is not the home arena of the women’s ice hockey team. For the purposes of our analysis, we will assume that the games in this tournament are played at a neutral site.\nTo record whether the game was played at the home arena of Home or at a neutral site, we will first identify all tournaments by pulling out the unique values in Notes that do not include the string \"SO\".\n\nunik_notes &lt;-\n  no_ties |&gt;\n  dplyr::filter(!is.na(Notes) & !grepl(\"SO\", Notes)) |&gt;\n  dplyr::pull(Notes) |&gt;\n  unique()\nunik_notes\n\n [1] \"IceBreaker\"          \"Nutmeg Classic\"      \"Smashville Showcase\"\n [4] \"Mayor's Cup\"         \"East/West Classic\"   \"Beanpot\"            \n [7] \"ECAC Tournament\"     \"AHA Tournament\"      \"HEA Tournament\"     \n[10] \"WCHA Tournament\"     \"NEWHA Tournament\"   \n\n\nWe then manually inspect the box scores for each of the tournaments to determine whether they were played at any of the team’s home arena. We find * The East/West Classic was played at Ridder Arena, which is the home arena of Minnesota. So, we will treat the the games between Penn State and Bemidji State and between Brown and Bemidji State as being played at a neutral site. But we will treat the games involving Minneosta as being played at their home arena. * The Nutmeg Classic was played at Martire Family Arena, which is the home arena of Sacred Heart. So, we will treat games from this tournament that do not involve Sacred Heart can be considered a neutral site game. * The Mayor’s Cup game between Brown and Providence was played at Schneider Arena, which is the home arena of Providence. * The Mayor’s Cup game between Union and RPI was played at MVP Arena, which is a neutral site * All games in the Smashville Showcase were played at a neutral site: they were played at the Ford Ice Center, which is not the home arena of any of St. Thomas, Merrimack, Clarkson, or Penn State. * The Beanpot games between Boston University and Harvard and Boston College and Northeastern were played at Matthews Arena, which is the home arena of Northeastern. The games between Boston College and Harvard and Boston University and Northeastern were played at the T.D. Garden, which is a neutral site.\nGenerally speaking, most conference tournament games were played at the home arena of the listed home team. We find that:\n\nAll games in the NEWHA, and AHA, and HEA tournaments were played at the higher-ranked seeds home arena. So, none of these tournament games were played at a neutral site.\nAll but the last three games of the ECAC tournament were held at the listed home team’s home arena. The last three games (St. Lawrence vs Colgate, Clarkson vs Cornell, and Colgate vs Cornell) were held at Cornell’s home arena. So only the March 7, 2025 game between St. Lawrence and Colgate was held at a neutral site.\nAll the but last three games of the WHCA tournament were held at the listed home team’s home arena. The last three (Minnesota vs Ohio State State, Minnesota Duluth vs Wisconsin, and Minnesota vs Wisconsin) were held at Minneosta Duluth’s home arena. So, the March 7, 2025 game between Minnesota and Ohio State and the March 8, 2025 game between Minnesota and Wisconsin were held at a neutral site.\n\nBased on these findings, we create a new variable in no_ties, which records whether the game was played at a neutral site.\n\nno_ties &lt;-\n  no_ties |&gt;\n  dplyr::mutate(\n    neutral = dplyr::case_when(\n      grepl(\"IceBreaker\", Notes) ~ 1,\n      grepl(\"East/West\", Notes) & Home != \"Minnesota\" ~ 1,\n      grepl(\"Nutmeg\", Notes) & Home != \"Sacred Heart\" ~ 1,\n      grepl(\"Mayor\", Notes) & Home == \"Rensselaer\" ~ 1,\n      grepl(\"Smashville\", Notes) ~ 1,\n      grepl(\"Beanpot\", Notes) & Home != \"Northeastern\" ~ 1, \n      Date == \"3/7/2025\" & Home == \"St. Lawrence\" & Opponent == \"Colgate\" ~ 1,\n      Date == \"3/7/2025\" & Home == \"Minnesota\" & Opponent == \"Ohio State\" ~ 1,\n      Date == \"3/8/2025\" & Home == \"Minnesota\" & Opponent == \"Wisconsin\" ~ 1,\n      .default = 0))",
    "crumbs": [
      "Lecture 13: Tournament Simulation"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#sec-fit",
    "href": "lectures/lecture13.html#sec-fit",
    "title": "Lecture 13: Tournament Simulation",
    "section": "Fitting Our Model",
    "text": "Fitting Our Model\nRecall from Lecture 12 that we counted up the number of home and away team wins for every unique combination of home and away teams. Because we now wish to account for potential home advantages, we need to separate these counts based on the game location. To do this, we will subdivide the games in no_ties based on the combination of Home, Opponent, and neutral and count up the numbers of home and away team wins. In the code below, we also rename Home and Opponent.\n\nunik_teams &lt;- sort(unique(c(no_ties$Home, no_ties$Opponent)))\n\nresults &lt;-\n  no_ties |&gt;\n  dplyr::rename(home.team = Home, away.team = Opponent) |&gt;\n  dplyr::group_by(home.team, away.team, neutral) |&gt; \n  dplyr::summarise(\n    home.win = sum(Home_Winner), \n    away.win = sum(Opp_Winner), .groups = 'drop') |&gt; \n  dplyr::mutate(\n    home.team = factor(home.team, levels = unik_teams), \n    away.team = factor(away.team,levels = unik_teams),\n    home.athome = ifelse(neutral == 1, 0, 1),\n1    away.athome = 0)\n\n\n1\n\nThis is a bit redundant but is necessary to set up the call to BTm().\n\n\n\n\nFitting our more elaborate Bradley-Terry model requires somewhat more complicated syntax. Following the example from Section 3.3 of the package vignette “Bradley-Terry Models in R”, we create a temporary data frame tmp_df containing the numbers of home and away team wins for every combination of home.team, away.team, and neutral. We then add two more columns to this data frame, one for the home team (home.team) and one for the away team (away.team). These columns are themselves data frames2 with columns recording team identity and whether the team was playing at home.\n\ntmp_df &lt;- data.frame(home.win = results$home.win, away.win = results$away.win)\ntmp_df$home.team &lt;- data.frame(team = results$home.team, at.home = results$home.athome)\ntmp_df$away.team &lt;- data.frame(team = results$away.team, at.home = results$away.athome)\n\nfit &lt;-\n  BradleyTerry2::BTm(\n    outcome = cbind(home.win, away.win),\n    player1 = home.team, player2 = away.team,\n1    formula = ~ team + at.home,\n2    refcat = \"New Hampshire\",\n3    id = \"team\",\n    data = tmp_df) \n\n\n1\n\nBTm uses a somewhat non-standard formula interface. This specification tells BTm() to estimate a separate latent strength for each team and a parameter for the home advantage.\n\n2\n\nManually specify the reference team (whose \\(\\lambda\\) is set to 0)\n\n3\n\nThe id argument specifies that “team” is the name of the factor used to identity teams\n\n\n\n\nInspecting the model parameters, we see \\(\\hat{\\lambda}_{0} \\approx 0.311.\\) A change of this magnitude on the log-odds scale corresponds to a change of\n\n26% (log-odds of -1) to 33% (log-odds of -0.689)\n46% (log-odds of -0.156) to 54% (log-odds of 0.156)\n73% (log-odds of 1) to 79% (log-odds of 1.311)\n\n\nround(coef(fit), digits = 3)\n\n       teamAssumption     teamBemidji State    teamBoston College \n               -4.746                -0.753                 0.885 \nteamBoston University             teamBrown          teamClarkson \n                1.135                 0.150                 1.225 \n          teamColgate       teamConnecticut           teamCornell \n                2.075                 1.118                 2.636 \n        teamDartmouth   teamFranklin Pierce           teamHarvard \n               -0.794                -3.949                -2.171 \n       teamHoly Cross        teamLindenwood               teamLIU \n               -0.509                -1.474                -3.077 \n            teamMaine        teamMercyhurst         teamMerrimack \n               -0.384                 0.511                -0.781 \n        teamMinnesota  teamMinnesota Duluth   teamMinnesota State \n                2.766                 2.180                 0.782 \n     teamNortheastern        teamOhio State        teamPenn State \n                1.026                 3.274                 2.026 \n             teamPost         teamPrinceton        teamProvidence \n               -4.422                 0.907                 0.660 \n       teamQuinnipiac        teamRensselaer               teamRIT \n                1.071                -0.300                -0.220 \n    teamRobert Morris      teamSacred Heart        teamSt. Anselm \n               -1.614                -3.339                -4.023 \n  teamSt. Cloud State      teamSt. Lawrence     teamSt. Michael's \n                1.288                 1.316                -5.913 \n       teamSt. Thomas         teamStonehill          teamSyracuse \n                0.084                -4.174                -0.394 \n            teamUnion           teamVermont         teamWisconsin \n               -0.057                -0.565                 4.774 \n             teamYale               at.home \n                0.822                 0.311 \n\n\nWe can extract the latent team strengths using the BradelyTerry::BTabilities function\n\nlambda0_hat &lt;- coef(fit)[\"at.home\"]\nlambda_hat &lt;- BradleyTerry2::BTabilities(fit)\nlambda_hat[c(\"Wisconsin\", \"Ohio State\"),]\n\n            ability      s.e.\nWisconsin  4.773946 0.9995582\nOhio State 3.273501 0.7936477\n\n\nAccording to our model, if \\(\\lambda_{WIS}\\) and \\(\\lambda_{OSU}\\) are Wisconsin’s and Ohio States’ latent strength, then the log-odds of Wisconsin beating Ohio State are\n\nIf Wisconsin is at home: \\(\\lambda_{WIS} - \\lambda_{OSU} + \\lambda_{0}\\)\nIf Ohio State is at home: \\(\\lambda_{WIS} - \\lambda_{OSU} - \\lambda_{0}\\)\nIf the game is at a neutral site: \\(\\lambda_{WIS} - \\lambda_{OSU}.\\)\n\nUsing the estimates \\(\\hat{\\lambda}_{0} \\approx 0.311, \\hat{\\lambda}_{WIS} \\approx 4.774,\\) and \\(\\hat{\\lambda}_{OSU} \\approx 3.274,\\) the probabilities that Wisconsin beats Ohio State are\n\n\nAt WIS: 85.95 %\n\n\nAT OSU: 76.67 %\n\n\nAT neutral site: 81.76 %\n\n\nFigure 1 shows the estimated latent strengths of all teams along with approximate 95% confidence intervals. The high degree of overlap between these marginal intervals suggests that there may be considerable uncertainty in the relative rankings of some teams. Precisely quantifying this uncertainty in rankings (e.g., with the bootstrap) is left as an Exercise.\n\n\n\n\n\n\n\n\nFigure 1: Power Rankings",
    "crumbs": [
      "Lecture 13: Tournament Simulation"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#sec-frozen4",
    "href": "lectures/lecture13.html#sec-frozen4",
    "title": "Lecture 13: Tournament Simulation",
    "section": "Frozen 4 Simulation",
    "text": "Frozen 4 Simulation\nWisconsin, Ohio State, Cornell, and Minnesota qualified for the semi-finals of the 2025 NCAA Tournament (known as the Frozen 4). The teams were seeded as: (1) Wisconsin; (2) Ohio State; (3) Cornell; and (4) Minnesota. Following a standard bracket construction, the first semi-final game (which we will label SF1) was played between the 1st and 4th seed (Wisconsin vs Minnesota) and the second semi-final game (which we will label SF2) was played between the 2nd and 3rd seed (Ohio State vs Cornell).\n\nSimulating the Original Tournament\nTo power our simulation, we will first enumerate all possible matchups between the four teams and compute the probability that the higher-seeded team wins. In computing these probabilities, we will account for the fact that all games were played at Ridder Arena, which is the home arena of Minnesota.\nIn the following code block, we first create a table containing the team names and their seeds. Then, we enumerate all possible combinations of teams using expand.grid().\n\nseeds &lt;- data.frame(\n  Team = c(\"Wisconsin\", \"Ohio State\", \"Cornell\", \"Minnesota\"),\n  Seed = 1:4)\n\npossible_matchups &lt;-\n  expand.grid(Hi = seeds$Team, Lo = seeds$Team) |&gt;\n  as.data.frame() |&gt;\n  dplyr::inner_join(y = seeds |&gt; dplyr::rename(Hi = Team, Hi.Seed=Seed), by = \"Hi\") |&gt;\n  dplyr::inner_join(y = seeds |&gt; dplyr::rename(Lo = Team, Lo.Seed=Seed), by = \"Lo\") |&gt;\n  dplyr::filter(Hi.Seed &lt; Lo.Seed) |&gt;\n  dplyr::mutate(neutral = ifelse(Hi == \"Minnesota\" | Lo == \"Minnesota\", 0, 1)) |&gt;\n  dplyr::mutate(\n    diff = lambda_hat[Hi, \"ability\"] - lambda_hat[Lo, \"ability\"],\n    prob = dplyr::case_when(\n      neutral == 1 ~ 1/(1 + exp(-1 * diff)),\n      neutral == 0 & Hi == \"Minnesota\" ~ 1/(1 + exp(-1 * (diff + lambda0_hat))),\n      neutral == 0 & Lo == \"Minnesota\" ~ 1/(1 + exp(-1 * (diff - lambda0_hat)))))\n\n\nsemis &lt;- \n  data.frame(Hi = c(\"Wisconsin\", \"Ohio State\"), Lo = c(\"Minnesota\", \"Cornell\")) |&gt;\n  dplyr::left_join(possible_matchups |&gt; dplyr::select(Hi, Lo, prob), by = c(\"Hi\", \"Lo\"))\n\nset.seed(479)\nsf_winners &lt;- c(NA, NA)\n\n1sf_outcomes &lt;- rbinom(n = nrow(semis), size = 1, prob = semis$prob)\nfor(i in 1:nrow(semis)){\n  if(sf_outcomes[i] == 1) sf_winners[i] &lt;- semis$Hi[i]\n  else sf_winners[i] &lt;- semis$Lo[i]\n}\ncat(\"Semi-final outcomes:\", sf_outcomes, \"\\n\")\ncat(\"Semi-final winners:\", sf_winners, \"\\n\")\n\n\n1\n\nThe \\(i\\)-th entry of sf_outcomes is 1 if the higher-seed in game \\(i\\) wins and 0 if the lower-seed in game \\(i\\) wins\n\n\n\n\nSemi-final outcomes: 0 1 \nSemi-final winners: Minnesota Ohio State \n\n\nThe winners of both semi-final matches play each other in the finals. In the code below, we create a data frame containing the team names and then determine which is the higher seed. We then join the corresponding probability of the higher-seeded team winning the game. Finally, we simulate the outcome of the match and record the winner.\n\nfinals &lt;- \n  data.frame(Team1 = sf_winners[1], Team2 = sf_winners[2]) |&gt;\n  dplyr::left_join(y = seeds |&gt; dplyr::rename(Team1 = Team, Team1.Seed = Seed), by = \"Team1\") |&gt;\n  dplyr::left_join(y = seeds |&gt; dplyr::rename(Team2 = Team, Team2.Seed = Seed), by = \"Team2\") |&gt;\n  dplyr::mutate(\n    Hi = ifelse(Team1.Seed &lt; Team2.Seed, Team1, Team2),\n    Lo = ifelse(Team1.Seed &lt; Team2.Seed, Team2, Team1)) |&gt;\n  dplyr::select(Hi, Lo) |&gt; \n  dplyr::left_join(\n    y = possible_matchups |&gt; dplyr::select(Hi, Lo, prob), by = c(\"Hi\", \"Lo\"))\n\nfinal_outcome &lt;- rbinom(n = 1, size = 1, prob = finals$prob)\nif(final_outcome == 1){\n  final_winner &lt;- finals$Hi[1]\n} else{\n  final_winner &lt;- finals$Lo[1]\n}\n\ncat(\"Finals outcome:\", final_outcome, \"\\n\")\n\nFinals outcome: 1 \n\ncat(\"Finals winner:\", final_winner, \"\\n\")\n\nFinals winner: Ohio State \n\n\nIn this simulation, Minnesota upset Wisconsin the first semi-finals; Ohio State beat Cornell in the second semi-finals; and Ohio State defeated Minnesota in the finals. Of course, if we run the simulation again with a different randomization seed, we might observe a different result. The code below runs simulates the Frozen 4 10,000 times. In each simulation iteration, we will save the winners of the two semi-final games (SF1_winner and SF2_winner); the two teams playing in the finals (Finals_Hi for the higher-seeded team and Finals_Lo for the lower-seeded team); and the eventual champion (Champion). By saving all these results, we can estimate the probabilities of not only simple events (e.g. \\(\\mathbb{P}(\\textrm{Wisconsin wins the finals})\\)) but also joint probabilities like \\[\\mathbb{P}(\\textrm{Wisconsin makes it to and wins the finals})\\] and conditional probabilities like \\[\n\\mathbb{P}(\\textrm{Wisconsin wins the finals} \\vert \\textrm{Wisconsin makes it to the finals}).\n\\] by dividing the number of simulations in which Wisconsin wins the championship by beating Ohio State in the finals by the number of simulations in which Wisconsin plays Ohio State in the finals.\n\nn_sims &lt;- 10000\n\n1results &lt;- data.frame(\n  SF1_Winner = rep(NA, times = n_sims),\n  SF2_Winner = rep(NA, times = n_sims),\n  Finals_Hi = rep(NA, times = n_sims),\n  Finals_Lo = rep(NA, times = n_sims),\n  Champion = rep(NA, times = n_sims))\n\n\nfor(r in 1:n_sims){\n2  set.seed(479+r)\n  sf_winners &lt;- c(NA, NA)\n  sf_outcomes &lt;- rbinom(n = nrow(semis), size = 1, prob = semis$prob)\n  for(i in 1:nrow(semis)){\n    if(sf_outcomes[i] == 1){\n      sf_winners[i] &lt;- semis$Hi[i]\n    } else{\n      sf_winners[i] &lt;- semis$Lo[i]\n    }\n3    results[r, c(\"SF1_Winner\", \"SF2_Winner\")] &lt;- sf_winners\n  }\n  \n  finals &lt;- \n    data.frame(Team1 = sf_winners[1], Team2 = sf_winners[2]) |&gt;\n    dplyr::left_join(\n      y = seeds |&gt; dplyr::rename(Team1 = Team, Team1.Seed = Seed), \n      by = \"Team1\") |&gt;\n    dplyr::left_join(\n      y = seeds |&gt; dplyr::rename(Team2 = Team, Team2.Seed = Seed), \n      by = \"Team2\") |&gt;\n    dplyr::mutate(\n      Hi = ifelse(Team1.Seed &lt; Team2.Seed, Team1, Team2),\n      Lo = ifelse(Team1.Seed &lt; Team2.Seed, Team2, Team1)) |&gt;\n    dplyr::select(Hi, Lo) |&gt; \n    dplyr::left_join(\n      y = possible_matchups |&gt; dplyr::select(Hi, Lo, prob), \n      by = c(\"Hi\", \"Lo\"))\n  \n4  results[r, \"Finals_Hi\"] &lt;- finals$Hi[1]\n  results[r, \"Finals_Lo\"] &lt;- finals$Lo[1]\n  \n  final_outcome &lt;- rbinom(n = 1, size = 1, prob = finals$prob)\n  \n  if(final_outcome == 1){\n    results[r, \"Champion\"] &lt;- finals$Hi[1]\n  } else{\n    results[r, \"Champion\"] &lt;- finals$Lo[1]\n  }\n}\n\n\n1\n\nData frame to hold the simulation results\n\n2\n\nManually setting our randomization seed ensures reproducibility. But we need to use a different seed in every simulation iteration. Otherwise, we will get identical results across replications and that can bias our final probability estimates.\n\n3\n\nSave the winners of the semi-finals\n\n4\n\nSave the high and low seeds of the finals\n\n\n\n\nLooking at the first several rows in results, we find that Wisconsin won the championship in most of the simulations.\n\nresults |&gt; dplyr::slice_head(n = 10)\n\n   SF1_Winner SF2_Winner  Finals_Hi  Finals_Lo   Champion\n1   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n2   Wisconsin Ohio State  Wisconsin Ohio State Ohio State\n3   Wisconsin    Cornell  Wisconsin    Cornell  Wisconsin\n4   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n5   Wisconsin    Cornell  Wisconsin    Cornell  Wisconsin\n6   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n7   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n8   Wisconsin Ohio State  Wisconsin Ohio State  Wisconsin\n9   Wisconsin    Cornell  Wisconsin    Cornell  Wisconsin\n10  Minnesota Ohio State Ohio State  Minnesota  Minnesota\n\n\nIn total, we find that Wisconsin won the championship in 7189 of the 10,000 simulation runs.\n\ntable(results$Champion)\n\n\n   Cornell  Minnesota Ohio State  Wisconsin \n       502        765       1544       7189 \n\n\nAcross the 10,000 simulations, we find that\n\nWisconsin made the finals in 8496 simulations\nWisconsin made and won the finals in 7189 simulations\n\nBased on these numbers, we conclude that the conditional probability of Wisconsin winning the finals given that it made it to the finals is about 84.6%\n\nsum(results$SF1_Winner == \"Wisconsin\" & results$Champion == \"Wisconsin\")/sum(results$SF1_Winner == \"Wisconsin\")\n\n[1] 0.8461629\n\n\n\n\nSimulating an Alternative Tournament\nWhat if, instead of playing the semi-finals and finals at Ridder Arena, each game was held at the home arena of the higher-seeded team? Intuitively, we might expect Wisconsin’s chances of winning the championship to be much higher if they played all their matches at La Bahn Arena. To verify this, we will re-run our simulation. The only change is in the individual matchup probabilities: whereas in our original simulation, we only added the home advantage for Minnesota’s games, in our new simulation, we will add the advantage to all games.\n\nalt_matchups &lt;-\n  expand.grid(Hi = seeds$Team, Lo = seeds$Team) |&gt;\n  as.data.frame() |&gt;\n  dplyr::inner_join(y = seeds |&gt; dplyr::rename(Hi = Team, Hi.Seed=Seed), by = \"Hi\") |&gt;\n  dplyr::inner_join(y = seeds |&gt; dplyr::rename(Lo = Team, Lo.Seed=Seed), by = \"Lo\") |&gt;\n  dplyr::filter(Hi.Seed &lt; Lo.Seed) |&gt;\n  dplyr::mutate(\n1    diff = lambda_hat[Hi, \"ability\"] - lambda_hat[Lo, \"ability\"],\n2    log_odds = diff + lambda0_hat,\n    prob = 1/(1 + exp(-1 * (log_odds))))\n\nsemis &lt;- \n  data.frame(Hi = c(\"Wisconsin\", \"Ohio State\"), Lo = c(\"Minnesota\", \"Cornell\")) |&gt;\n3  dplyr::left_join(alt_matchups |&gt; dplyr::select(Hi, Lo, prob), by = c(\"Hi\", \"Lo\"))\n\n\n1\n\nThis is the log-odds of the higher seed winning if the game were played at a neutral site\n\n2\n\nIn this alternative, the higher seed always plays at a home, hence the additional lambda0_hat factor.\n\n3\n\nRecreate semis but include the new matchup probabilities for the alternative scenario\n\n\n\n\n\nn_sims &lt;- 10000\n\nalt_results &lt;- data.frame(\n  SF1_Winner = rep(NA, times = n_sims),\n  SF2_Winner = rep(NA, times = n_sims),\n  Finals_Hi = rep(NA, times = n_sims),\n  Finals_Lo = rep(NA, times = n_sims),\n  Champion = rep(NA, times = n_sims))\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  sf_winners &lt;- c(NA, NA)\n  sf_outcomes &lt;- rbinom(n = nrow(semis), size = 1, prob = semis$prob)\n  for(i in 1:nrow(semis)){\n    if(sf_outcomes[i] == 1){\n      sf_winners[i] &lt;- semis$Hi[i]\n    } else{\n      sf_winners[i] &lt;- semis$Lo[i]\n    }\n    alt_results[r, c(\"SF1_Winner\", \"SF2_Winner\")] &lt;- sf_winners\n  }\n  \n  finals &lt;- \n    data.frame(Team1 = sf_winners[1], Team2 = sf_winners[2]) |&gt;\n    dplyr::left_join(\n      y = seeds |&gt; dplyr::rename(Team1 = Team, Team1.Seed = Seed), \n      by = \"Team1\") |&gt;\n    dplyr::left_join(\n      y = seeds |&gt; dplyr::rename(Team2 = Team, Team2.Seed = Seed), \n      by = \"Team2\") |&gt;\n    dplyr::mutate(\n      Hi = ifelse(Team1.Seed &lt; Team2.Seed, Team1, Team2),\n      Lo = ifelse(Team1.Seed &lt; Team2.Seed, Team2, Team1)) |&gt;\n    dplyr::select(Hi, Lo) |&gt; \n    dplyr::left_join(\n      y = possible_matchups |&gt; dplyr::select(Hi, Lo, prob), \n      by = c(\"Hi\", \"Lo\"))\n  \n  alt_results[r, \"Finals_Hi\"] &lt;- finals$Hi[1]\n  alt_results[r, \"Finals_Lo\"] &lt;- finals$Lo[1]\n  \n  final_outcome &lt;- rbinom(n = 1, size = 1, prob = finals$prob)\n  \n  if(final_outcome == 1){\n    alt_results[r, \"Champion\"] &lt;- finals$Hi[1]\n  } else{\n    alt_results[r, \"Champion\"] &lt;- finals$Lo[1]\n  }\n}\n\nIn the alternative competition, where the higher-seeded team plays at home, we estimate that Wisconsin would win the championship about 77% of the time, up from the 71% in the version of the tournament played a neutral site.\n\ntable(alt_results$Champion)\n\n\n   Cornell  Minnesota Ohio State  Wisconsin \n       360        418       1533       7689",
    "crumbs": [
      "Lecture 13: Tournament Simulation"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#exercises",
    "href": "lectures/lecture13.html#exercises",
    "title": "Lecture 13: Tournament Simulation",
    "section": "Exercises",
    "text": "Exercises\n\nUse the bootstrap to quantify the uncertainty in relative rankings of each team. To do this, you will need to re-sample individual games, re-compute the numbers of home & away team wins for every combination of teams and neutral, and fit a Bradley-Terry model. Report the number of bootstrap simulations in which Wisconsin was ranked 1st and also report a 95% bootstrap confidence interval for Wisconsin’s overall rank.\nBuild a simulation for the entire NCAA Tournament based on the estimating probabilities from our Bradley-Terry model that accounts for home advantage. In this tournament, the first round and quarterfinals were held on the campuses of the top-4 seeded teams (Wisconsin, Ohio State, Cornell, and Minnesota). Subsequent games were played at Ridder Arena at the University of Minnesota. Your simulation should involve only the 11 teams who actually played in the tournament",
    "crumbs": [
      "Lecture 13: Tournament Simulation"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#footnotes",
    "href": "lectures/lecture13.html#footnotes",
    "title": "Lecture 13: Tournament Simulation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember, this is restriction is needed because the \\(\\lambda_{j}\\)’s are otherwise not identifiable. Without this restriction, the data gives us know way to distinguish between one set of values for the \\(\\lambda_{j}\\)’s and another set that shifts all the values by the same fixed constant. See Lecture 12 for more details.↩︎\nR implements data frames using a list data type, with each column representing a separate list element. So, what we’re really doing here is adding a list-valued element to a list, which is easier to interpret than a data frame-valued column in a data frame.↩︎",
    "crumbs": [
      "Lecture 13: Tournament Simulation"
    ]
  },
  {
    "objectID": "lectures/lecture11.html",
    "href": "lectures/lecture11.html",
    "title": "Lecture 11: Pitch Framing",
    "section": "",
    "text": "In October 2015, the Houston Astros played the New York Yankees in the American League Wild Card game. During and after the game, several Yankees fans took to social media to complain about inconsistencies in the strike zone being called by home plate umpire Eric Cooper. They argued that Cooper was not calling balls and strikes consistently for both teams, putting the Yankees at a distinct disadvantage. Beyond simple fan reaction, individual players took exception to Cooper’s strike zone: after striking out, Yankees catcher Brian McCann complained to Cooper that on similar pitches, Cooper was calling strikes when the Astros were pitching but balls when the Yankees were pitching. Figure 1 show two such pitches.\n\n\n\n\n\n\n\n\n\n\n\n(a) Pitch called a ball\n\n\n\n\n\n\n\n\n\n\n\n(b) Pitch called a strike\n\n\n\n\n\n\n\nFigure 1\n\n\n\nBoth pitches were thrown low and outside1, near the bottom left corner of the strike zone2 shown in the figure. Because no part of the ball passed through the strike zone, by rule, both pitches should have been called balls. But umpire Cooper called the pitch in Figure 1 (a), thrown by Yankees pitcher Masahiro Tanaka, a ball and called the pitch in Figure 1 (b), thrown by Astros pitcher Dallas Keuchel, a strike. During the television broadcast of the game and on social media after the game3, many speculated many speculated that the reason for this discrepancy in strike zones is due Astros catcher Jason Castro’s ability to frame pitches, catching them in a way so as to increase the chances of the umpire calling a strike.\nPitch framing, which has been studied by the sabermetrics community since at least 2008, received lots of coverage in the popular press between 2014 and 2016. A good example is this article about Jonathan Lucroy from ESPN The Magazine, which estimated that Lucroy’s framing accounted for a total of two wins for the Brewers in 2014 and was worth around $14 million. That article went on to say that claim that the most impactful player in baseball today is the game’s 17th highest-paid catcher.”\nIn this lecture, we will estimate how many more runs a catcher saves his team through his framing compared to a replacement-level catcher. Like we did in Lectures 6, 7, and 8, we will work with pitch tracking data scraped from StatCast. We will use the data from 2023 to establish historical strike probabilities (Section 3.1) based on pitch location, batter handedness, and pitcher handedness. We will then include these historical baselines as fixed effects in a multilevel model that also includes random intercepts for the catcher, pitcher, and batter involved in pitches from the 2024 season (Section 3). Using the estimated deviations between catcher intercepts and the grand mean from our multilevel model, we conclude by estimating how many more expected runs catchers save than a replacement-level catcher (Section 4).",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#overview",
    "href": "lectures/lecture11.html#overview",
    "title": "Lecture 11: Pitch Framing",
    "section": "",
    "text": "In October 2015, the Houston Astros played the New York Yankees in the American League Wild Card game. During and after the game, several Yankees fans took to social media to complain about inconsistencies in the strike zone being called by home plate umpire Eric Cooper. They argued that Cooper was not calling balls and strikes consistently for both teams, putting the Yankees at a distinct disadvantage. Beyond simple fan reaction, individual players took exception to Cooper’s strike zone: after striking out, Yankees catcher Brian McCann complained to Cooper that on similar pitches, Cooper was calling strikes when the Astros were pitching but balls when the Yankees were pitching. Figure 1 show two such pitches.\n\n\n\n\n\n\n\n\n\n\n\n(a) Pitch called a ball\n\n\n\n\n\n\n\n\n\n\n\n(b) Pitch called a strike\n\n\n\n\n\n\n\nFigure 1\n\n\n\nBoth pitches were thrown low and outside1, near the bottom left corner of the strike zone2 shown in the figure. Because no part of the ball passed through the strike zone, by rule, both pitches should have been called balls. But umpire Cooper called the pitch in Figure 1 (a), thrown by Yankees pitcher Masahiro Tanaka, a ball and called the pitch in Figure 1 (b), thrown by Astros pitcher Dallas Keuchel, a strike. During the television broadcast of the game and on social media after the game3, many speculated many speculated that the reason for this discrepancy in strike zones is due Astros catcher Jason Castro’s ability to frame pitches, catching them in a way so as to increase the chances of the umpire calling a strike.\nPitch framing, which has been studied by the sabermetrics community since at least 2008, received lots of coverage in the popular press between 2014 and 2016. A good example is this article about Jonathan Lucroy from ESPN The Magazine, which estimated that Lucroy’s framing accounted for a total of two wins for the Brewers in 2014 and was worth around $14 million. That article went on to say that claim that the most impactful player in baseball today is the game’s 17th highest-paid catcher.”\nIn this lecture, we will estimate how many more runs a catcher saves his team through his framing compared to a replacement-level catcher. Like we did in Lectures 6, 7, and 8, we will work with pitch tracking data scraped from StatCast. We will use the data from 2023 to establish historical strike probabilities (Section 3.1) based on pitch location, batter handedness, and pitcher handedness. We will then include these historical baselines as fixed effects in a multilevel model that also includes random intercepts for the catcher, pitcher, and batter involved in pitches from the 2024 season (Section 3). Using the estimated deviations between catcher intercepts and the grand mean from our multilevel model, we conclude by estimating how many more expected runs catchers save than a replacement-level catcher (Section 4).",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#data",
    "href": "lectures/lecture11.html#data",
    "title": "Lecture 11: Pitch Framing",
    "section": "Data",
    "text": "Data\n\nValue of a Called Strike\nLater, in Section 4, we will compute how many expected runs catchers save their teams To do this, we must first determine the value of a called strike using a similar framework as in Lecture 6. Specifically, we will compute the number of runs scored by the batting the team in the remainder of the half-inning following every pitch. Then, for every combination of the count4 and the number of outs, we will compute the average number of runs scored in the remainder of the half-inning after a called ball and after a called strike on taken pitches5. The difference between these quantities captures the value of a called strike in that particular count-out state.\nTo compute this, we start by loading the data frame statcast2024 that we prepared and saved in Lecture 6.\n\nload(\"statcast2024.RData\")\n\nRecall that this data table contains a row for every pitch and that the column RunsRemaining recorded how many runs the batting team scored in the remainder of the half-inning following each pitch. We determine whether a pitch was taken using the description field\n\ntable(statcast2024$description)\n\nball_descriptions &lt;- \n  c(\"ball\", \"blocked_ball\", \"pitchout\", \"hit_by_pitch\")\nswing_descriptions &lt;- \n  c(\"bunt_foul_tip\", \"foul\", \"foul_bunt\", \"foul_tip\",\n    \"hit_into_play\", \"missed_bunt\", \"swinging_strike\",\n    \"swinging_strike_block\")\n\ntaken2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::filter(!description %in% swing_descriptions) |&gt;\n  dplyr::mutate(\n2    Y = ifelse(description == \"called_strike\", 1, 0),\n    Count = paste(balls, strikes, sep = \"-\")) |&gt;\n  dplyr::select(Y, plate_x, plate_z, \n                Count, Outs,\n3                batter, pitcher, fielder_2,\n4                stand, p_throws,\n                RunsRemaining, sz_top, sz_bot) |&gt; \n  dplyr::mutate(\n    Count = factor(Count),\n    batter = factor(batter),\n    pitcher = factor(pitcher),\n    fielder_2 = factor(fielder_2),\n    stand = factor(stand),\n    p_throws = factor(p_throws))\n\n\n1\n\nExtract only the taken pitches\n\n2\n\nAdd columns recording whether taken pitch was called a strike (Y = 1) or a ball (Y = 0) and the count.\n\n3\n\nfielder_2 contains the MLB Advanced Media ID for the catcher\n\n4\n\nstand and p_throws record the handedness of the batter and pitcher.\n\n\n\n\n\n                   ball            blocked_ball           bunt_foul_tip \n                 231032                   14717                      15 \n          called_strike                    foul               foul_bunt \n                 113912                  126012                    1208 \n               foul_tip            hit_by_pitch           hit_into_play \n                   7218                    1979                  121751 \n            missed_bunt                pitchout         swinging_strike \n                    196                      52                   73209 \nswinging_strike_blocked \n                   3834 \n\n\nWe can now compute the expected numbers of runs scored after a called ball or strike for every combination of out and count.\n\ner_balls &lt;-\n  taken2024 |&gt;\n  dplyr::filter(Y == 0) |&gt;\n  dplyr::group_by(Count, Outs) |&gt;\n  dplyr::summarise(er_ball = mean(RunsRemaining), .groups = 'drop')\n\ner_strikes &lt;-\n  taken2024 |&gt;\n  dplyr::filter(Y == 1) |&gt;\n  dplyr::group_by(Count, Outs) |&gt;\n  dplyr::summarise(er_strike = mean(RunsRemaining), .groups = 'drop')\n\ner_taken &lt;-\n  er_balls |&gt;\n  dplyr::left_join(y = er_strikes, by = c(\"Count\", \"Outs\")) |&gt;\n  dplyr::mutate(value = er_ball - er_strike)\n\ner_taken |&gt; dplyr::slice_head(n=5)\n\n# A tibble: 5 × 5\n  Count  Outs er_ball er_strike  value\n  &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 0-0       0   0.731     0.605 0.126 \n2 0-0       1   0.547     0.425 0.122 \n3 0-0       2   0.283     0.193 0.0902\n4 0-1       0   0.660     0.525 0.135 \n5 0-1       1   0.464     0.365 0.0990\n\n\nLooking at er_taken, we find that batting teams score\n\nAbout 0.731 runs following a called ball on a 0-0 pitch with 0 outs\nAbout 0.605 runs following a called strike on a 0-0 pitch with 0 outs So, if a good pitch framer can get a 0-0 pitch with 0 outs called a strike instead of ball, he saves the fielding team about 0.126 runs, on average. From the standpoint of the fielding team, a called strike is most valuable on a 3-2 pitch with 0 outs (when they can expect to save almost 0.8 runs on average) and least value on a 0-1 pitch with 2 outs (when they can expect to save about 0.068 runs on average).\n\n\ner_taken |&gt; dplyr::arrange(dplyr::desc(value)) |&gt; dplyr::slice(c(1, dplyr::n()))\n\n# A tibble: 2 × 5\n  Count  Outs er_ball er_strike  value\n  &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 3-2       0   1.12      0.321 0.800 \n2 0-1       2   0.234     0.166 0.0684\n\n\nSo that we can use it later in Section 4, we will append a column to taken2024 containing the value of a called strike (from the fielding team’s perspective).\n\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::left_join(y = er_taken |&gt; dplyr::select(Count, Outs, value), by = c(\"Count\", \"Outs\"))",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#sec-multilevel",
    "href": "lectures/lecture11.html#sec-multilevel",
    "title": "Lecture 11: Pitch Framing",
    "section": "A Multilevel Model",
    "text": "A Multilevel Model\nWe are ultimately interested in understanding how individual players (i.e., batters, catchers, and pitchers) can influence the called strike probability. Any credible model for these probabilities must account for location. After all, if a pitch is thrown several feet away from the batter, no amount of pitch framing will change the call from a ball to a strike. Similarly, catcher skill is likely n The StatCast variables plate_x and plate_z respectively record the horizontal and vertical coordinate of each pitch as it crosses the front edge of home plate. These variables are measured from the catcher’s perspective so pitches thrown to the left of home plate have negative plate_x values and pitches thrown to the right of home plate have positive plate_x values. Both plate_x and plate_z are measured in feet.\nA natural starting point is to fit a multilevel model with player-specific random intercepts that adjusts for the fixed effects of plate_x and plate_z. Specifically, because we are dealing with a binary outcome, we might start with the model \\[\n\\log \\left( \\frac{\\mathbb{P}(Y_{i} = 1)}{\\mathbb{P}(Y_{i} = 0)} \\right) = B_{b[i]} + C_{c[i]} + P_{p[i]} + \\beta_{x} x_{i} + \\beta_{z}z_{i}\n\\] where \\(b[i], c[i],\\) and \\(p[i]\\) record the identities of the batter, catcher, and pitcher involved in taken pitch \\(i;\\) \\(x_{i}\\) and \\(z_{i}\\) are the plate_x and plate_z measurements; and the \\(B_{b}\\)’s, \\(C_{c}\\)’s, and \\(P_{p}\\)’s are random intercepts for the batters, catchers, and pitchers with \\(B_{b} \\sim N(\\mu_{B}, \\sigma^{2}_{B}),\\) \\(C_{c} \\sim N(\\mu_{C}, \\sigma^{2}_{C})\\), and \\(P_{p} \\sim N(\\mu_{P})\\). This simple model assumes that the log-odds of a called strike is monotonic in plate_x and plate_z. Due to the monotonicity of the inverse logistic transformation6, a positive (resp. negative) \\(\\beta_{x}\\) would imply that the probability of a called strike increases (resp. decreases) as the pitch moves from left to right. Similarly, a positive (resp. negative) \\(\\beta_{z}\\) implies that the probability of a called strike increases (resp. decreases) the higher up a pitch is thrown.\nA cursory glance at the data reveals such monotonicity is not realistic. Specifically, if called strike probability was monotonic in plate_x and plate_z, then we should see progressively more strikes in one of the four corners of the plot7. In the plot, we overlay the “average” rule book strike zone8.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\",\n1     xlim = c(-2.5, 2.5), ylim = c(0, 6),\n     xlab = \"\", ylab = \"\",\n     main = \"Called pitches (2024)\",\n     xaxt = \"n\", yaxt = \"n\", bty = \"n\")\npoints(x = taken2024$plate_x, y = taken2024$plate_z,\n       pch = 16, cex = 0.25,\n2       col = ifelse(taken2024$Y == 1, rgb(1, 0, 0, 0.25), rgb(0,0,1,0.25)))\nrect(xleft = -8.5/12, \n     ybottom = mean(taken2024$sz_bot, na.rm = TRUE),\n     xright = 8.5/12,\n     ytop = mean(taken2024$sz_top, na.rm = TRUE))\n\n\n1\n\nRestrict attention to pitches that are not in the dirt (i.e., plate_z &gt; 0) but not thrown too high (i.e. plate_z &lt; 6), are within 2.5 feet of the center of home plate in either direction.\n\n2\n\nColor strikes (Y = 1) in red and balls in blue (Y = 0). The 0.25 sets the transparency\n\n\n\n\n\n\n\n\n\n\nFigure 2: All called balls and strikes\n\n\n\n\n\nLetting \\(n_{B}, n_{C},\\) and \\(n_{P}\\) be the numbers of batters, catchers, and pitchers, we will instead model \\[\n\\log \\left( \\frac{\\mathbb{P}(Y_{i} = 1)}{\\mathbb{P}(Y_{i} = 0)} \\right) = B_{b[i]} + C_{c[i]} + P_{p[i]} + \\beta_{p} \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})}\\right)\n\\] where \\(\\hat{p}(x,z)\\) is a baseline called strike probability estimated using data from the previous season9 and the random player intercepts satisfy \\[\n\\begin{align}\nB_{b} &= \\mu_{B} + u_{b}^{(B)}; \\quad u^{(B)}_{b} \\sim N(0, \\sigma^{2}_{B}) \\quad \\text{for each}~b = 1, \\ldots, n_{B} \\\\\nC_{c} &= \\mu_{C} + u_{c}^{(C)}; \\quad u^{(C)}_{c} \\sim N(0, \\sigma^{2}_{C}) \\quad \\text{for each}~c = 1, \\ldots, n_{C} \\\\\nP_{p} &= \\mu_{P} + u_{p}^{(P)}; \\quad u^{(P)}_{p} \\sim N(0, \\sigma^{2}_{P}) \\quad \\text{for each}~p = 1, \\ldots, n_{P}. \\\\\n\\end{align}\n\\] Our model accounts for pitch location by including a suitably-transformed baseline called strike probability estimate as a fixed effect. The random player intercepts capture how much each player contributions to the log-odds of a called strike over and above what is determined by pitch location.\n\nHistorical Strike Probabilities\nTo fit our proposed multilevel model, we must first compute the baseline called strike probabilities using data from the 2023 season. We first scrape all StatCast data fr om2023 using the function annual_statcast_query, which we defined in Lecture 6 and is available here.\n\n1source(\"annual_statcast_query.R\")\nraw_statcast2023 &lt;- annual_statcast_query(season = 2023)\nsave(raw_statcast2023, file = \"raw_statcast2023.RData\")\n\n\n1\n\nThis will only work if the file “annual_statcast_query.R” is saved in your R working directory\n\n\n\n\nWe can now apply the same basic pre-processing to raw_statcast2023 that we did to raw_statcast2024 back in Lecture 6.\n\nstatcast2023 &lt;-\n  raw_statcast2023 |&gt; \n  dplyr::filter(game_type == \"R\") |&gt;\n  dplyr::filter(\n    strikes &gt;= 0 & strikes &lt; 3 & \n      balls &gt;= 0 & balls &lt; 4 & \n      outs_when_up &gt;= 0 & outs_when_up &lt; 3) |&gt;\n  dplyr::arrange(game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n    BaseRunner = \n      paste0(1*(!is.na(on_1b)),1*(!is.na(on_2b)),1*(!is.na(on_3b)))) |&gt;\n  dplyr::rename(Outs = outs_when_up)\n\nNow that we have pre-processed the 2023 StatCast data, we will extract all called pitches. We will fit our generalized additive model only to pitches with plate_x values between -1.5 and 1.5 and plate_z values between 1 and 6. Beyond this region there are virtually no called strikes. Including pitches that are too far away from the strike zone — i.e., pitches for which the called strike probability is likely exactly zero — can lead to some numerical instabilities when fitting the GAM.\n\ntaken2023 &lt;-\n  statcast2023 |&gt;\n  dplyr::filter(!description %in% swing_descriptions) |&gt; \n  dplyr::mutate(\n    Y = ifelse(description == \"called_strike\", 1, 0)) |&gt; \n  dplyr::select(Y, plate_x, plate_z, stand, p_throws) |&gt; \n  dplyr::filter(!is.na(plate_x) & !is.na(plate_z)) |&gt;\n  dplyr::filter(abs(plate_x) &lt;= 1.5 & plate_z &gt;= 1 & plate_z &lt;= 6) |&gt;\n  dplyr::mutate(\n    stand = factor(stand),\n    p_throws = factor(p_throws))\n\nWe now fit our generalized additive model.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code takes several minutes to run.\n\n\n\nlibrary(mgcv)\n1hgam_fit &lt;-\n  bam(formula = Y ~ stand + p_throws + s(plate_x, plate_z),\n      family = binomial(link=\"logit\"), \n      data = taken2023)\n\n\n1\n\nThe “h” serves as a reminder that we’re getting historical called strike probabilities\n\n\n\n\nLike we did with our model for the probability of making an out as a function of fielding location in Lecture 8, we can visualize the fitted probabilities along a fine grid of locations. The following code plots the fitted called strike probabilities as a function of location for a right-handed pitcher facing a right-handed batter.\n\ngrid_sep &lt;- 0.05\nx_grid &lt;- seq(-1.5, 1.5, by = grid_sep)\nz_grid &lt;- seq(0, 6, by = grid_sep)\ncol_list &lt;- colorBlindness::Blue2DarkRed18Steps \nplate_grid &lt;- \n  expand.grid(plate_x = x_grid, plate_z = z_grid) |&gt;\n1  dplyr::mutate(stand = \"R\", p_throws = \"R\") |&gt;\n2  dplyr::mutate(stand = factor(stand, levels = c(\"L\", \"R\")),\n                p_throws = factor(p_throws, levels = c(\"L\", \"R\")))\n\ngrid_preds &lt;-\n  predict(object = hgam_fit,\n          newdata = plate_grid,\n          type = \"response\")\ngrid_prob_cols &lt;- \n  rgb(colorRamp(col_list,bias=1)(grid_preds)/255)\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", \n     xlim = c(-1.5, 1.5), ylim = c(0,6),\n     main = \"Historical called strike probabilities\",\n     xlab = \"\", ylab = \"\", \n     xaxt = \"n\", yaxt = \"n\")\nfor(i in 1:nrow(plate_grid)){\n  rect(xleft = plate_grid$plate_x[i] - grid_sep/2, \n       ybot = plate_grid$plate_z[i] - grid_sep/2,\n       xright = plate_grid$plate_x[i] + grid_sep/2,\n       ytop = plate_grid$plate_z[i] + grid_sep/2,\n       col = adjustcolor(grid_prob_cols[i], alpha.f = 0.5),\n       border = NA)\n}\nrect(xleft = -8.5/12, \n     ybottom = mean(taken2024$sz_bot, na.rm = TRUE),\n     xright = 8.5/12,\n     ytop = mean(taken2024$sz_top, na.rm = TRUE))\n\n\n1\n\nChange this line to visualize the fitted probabilities for different combinations of batter and pitcher handedness\n\n2\n\nWe will eventually pass plate_grid to the predict function, which will expect both stand and p_throws to be factor variables\n\n\n\n\n\n\n\n\n\n\nFigure 3: Historical called strike probabilities for a right-handed batter facing a right-handed pitcher.\n\n\n\n\n\n\n\nFitting Our Multilevel Model\nNow that we have fit a GAM model to the historical data, we are ready to compute the baseline log-odds for a called strike for each pitch from 2024. We can do this using the predict function with the argument type = \"link\" instead of type  = \"response\".\n\nbaseline &lt;- \n  predict(object = hgam_fit,\n          newdata = taken2024, \n          type = \"link\")\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::mutate(baseline = baseline) |&gt;\n1  dplyr::filter(abs(plate_x) &lt;= 1.5 & plate_z &gt;= 1 & plate_z &lt;= 6)\n\n\n1\n\nWe focus only on “frameable” pitches which are not throw too far away from home plate in the horizontal direction and are not thrown too high or too low in the vertical direction.\n\n\n\n\nWe’re finally ready to fit our multilevel model\n\nlibrary(lme4)\nmultilevel_fit &lt;-\n1  glmer(formula = Y ~ 1 + (1 | fielder_2) + (1 | batter)  + (1 | pitcher) +  baseline,\n        family = binomial(link = \"logit\"),\n        data = taken2024)\n\n\n1\n\nBecause the outcome is binary, we need to use the function glmer() instead of lmer(). We moreover need to specify the family argument.\n\n\n\n\nAlthough we cannot estimate the actual player-specific random intercepts \\(B_{b}, C_{c}\\) and \\(P_{p},\\) we can estimate the deviations between these the overall global means \\(\\mu_{B}, \\mu_{C},\\) and \\(\\mu_{P}.\\) The following code pulls out the deviations for the catchers (i.e., the deviations \\(u_{c}^{(C)}\\)) and appends these values to\n\ntmp &lt;- ranef(multilevel_fit)\ncatcher_u &lt;- \n  data.frame(\n1    fielder_2 = as.integer(rownames(tmp[[\"fielder_2\"]])),\n    catcher_u = tmp[[\"fielder_2\"]][,1])\n\n\n1\n\nThe MLB Advanced Media IDs are saved as integers but rownames() returns them as a string.",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#sec-frame-war",
    "href": "lectures/lecture11.html#sec-frame-war",
    "title": "Lecture 11: Pitch Framing",
    "section": "Runs Saved Above Replacement",
    "text": "Runs Saved Above Replacement\nThe data table catcher_u estimates of the amount each catcher adds to log-odds of a called strike, after adjusting for the historical baseline and the other players, relative to a global average across a super-population of catchers. As we argued in Lecture 10, it is arguably more useful to compare each individual catcher’s performance to a well-defined replacement level rather than to a nebulously defined global average. Noting that most MLB teams carry two catchers on their active roster, we will sort catchers based on the total number of pitches received in the 2024 season. We define the top-60 as non-replacement level and the remaining catchers as replacement-level.\nThe following code determines the pitch count threshold for defining replacement-level. It then computes the average of the estimated deviations \\(u^{(C)}_{c}\\)’s across all replacement-level catchers. We will denote this average by \\(\\overline{u}^{(C)}_{R}.\\)\n\ncatcher_counts &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarise(count = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(count))\n\ncatcher_threshold &lt;- \n  catcher_counts |&gt;\n  dplyr::slice(60) |&gt;\n  dplyr::pull(count)\n\ncatcher_u &lt;-\n  catcher_u |&gt;\n  dplyr::left_join(y = catcher_counts, by = \"fielder_2\")\n\nrepl_u &lt;-\n  catcher_u |&gt;\n  dplyr::filter(count &lt; catcher_threshold) |&gt;\n  dplyr::pull(catcher_u) |&gt;\n  mean()\n\nFor every pitch thrown in 2024, we can estimate how the called strike probability changes when the original catcher is replaced with a replacement-level catcher. First, observe that the log-odds of a called strike on pitch \\(i\\) is given by \\[\n\\mu_{C} + u_{c[i]}^{(C)} + B_{b[i]} + P_{p[i]} + \\hat{\\beta}_{p} \\times \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})} \\right)\n\\] We can compute these predicted log-odds using the predict() function and append them to the data table taken2024:\n\nml_preds &lt;-\n  predict(object = multilevel_fit,\n          newdata = taken2024,\n1          type = \"link\")\ntaken2024 &lt;- \n  taken2024 |&gt;\n  dplyr::mutate(fitted_log_odds = ml_preds)\n\n\n1\n\nSince we want the fitted log-odds and not the fitted probabilities, we specify type = \"link\".\n\n\n\n\nIf we replace the original catcher \\(c[i]\\) with a replacement-level catcher, then the log-odds of a called strike is now \\[\n\\mu_{C} + \\overline{u}_{R}^{(C)} + B_{b[i]} + P_{p[i]} + \\hat{\\beta}_{p} \\times \\log\\left(\\frac{\\hat{p}(x_{i}, z_{i})}{1 - \\hat{p}(x_{i}, z_{i})} \\right)\n\\] To compute the counter-factual log-odds, we need to take the fitted log-odds, subtract the original catcher’s deviation \\(u_{c[i]}^{(C)}\\), and add the average deviation across all replacement-level catchers (i.e., \\(\\overline{u}_{R}^{(C)}\\)). The following code does this by first appending the estimated \\(u^{(C)}_{c[i]}\\) to each row of taken2024 and then computing the counter-factual log-odds\n\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::left_join(y = catcher_u |&gt; \n                     dplyr::select(fielder_2, catcher_u) |&gt;\n1                     dplyr::mutate(fielder_2 = factor(fielder_2)),\n                   by = \"fielder_2\") |&gt;\n  dplyr::mutate(repl_log_odds = fitted_log_odds - catcher_u + repl_u) \n\n\n1\n\nfielder_2 is a factor variable in taken2024 but a numeric variable in catcher_u. By converting it to a factor, we ensure that the join is successful\n\n\n\n\nThe number of expected runs a catcher saves through his framing relative to a replacement-level catcher is the product of the value of a called strike multiplied by the actual and counter-factual called strike probability.\n\ntaken2024 &lt;-\n  taken2024 |&gt;\n  dplyr::mutate(\n    fitted_prob = 1/(1 + exp(-1 * fitted_log_odds)),\n    repl_prob = 1/(1 + exp(-1 * repl_log_odds)),\n    rsar = value * (fitted_prob - repl_prob))\n\nSumming over all pitches received by a catcher over the season, we can compute his total number of runs saved above replacement. In the following code, we load in the player look-up table we created in Lecture 6\n\nload(\"player2024_lookup.RData\")\n\nrsar &lt;-\n  taken2024 |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarise(rsar = sum(rsar), n = dplyr::n()) |&gt; \n  dplyr::rename(key_mlbam = fielder_2) |&gt;\n  dplyr::left_join(player2024_lookup |&gt;\n                     dplyr::select(key_mlbam, Name) |&gt;\n                     dplyr::mutate(key_mlbam = factor(key_mlbam)), by = \"key_mlbam\")\n\nThere is considerable overlap between the top-10 catchers according to our framing runs saved above replacement metric and the top-10 catchers according to rankings produced by Baseball Savant. Both models, for instance, identify Patrick Bailey, who won a Golden Glove award in 2024, as a particularly strong framer.\n\nrsar |&gt;\n  dplyr::arrange(dplyr::desc(rsar)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::select(Name, rsar, n)\n\n# A tibble: 10 × 3\n   Name               rsar     n\n   &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n 1 Patrick Bailey     30.1  6118\n 2 Cal Raleigh        19.9  6449\n 3 Austin Wells       17.9  5540\n 4 Alejandro Kirk     17.3  4761\n 5 Jake Rogers        16.7  4417\n 6 Christian Vazquez  15.2  4541\n 7 Jose Trevino       14.6  3639\n 8 Francisco Alvarez  13.6  4629\n 9 Bo Naylor          11.6  5639\n10 Yasmani Grandal    11.0  3535",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#footnotes",
    "href": "lectures/lecture11.html#footnotes",
    "title": "Lecture 11: Pitch Framing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThat is, away from the batter.↩︎\nThe strike zone is defined as “the area over home plate from the midpoint between a batter’s shoulders and the top of the uniform pants – when the batter is in his stance and prepared to swing at a pitched ball – and a point just below the kneecap.” See here.↩︎\nSee this blogpost for a compilation of posts.↩︎\nThe numbers of balls and strikes previously called in the at-bat.↩︎\nThese are pitches in which the batter doesn’t swing.↩︎\nIf \\(u = \\log{\\frac{p}{1-p}}\\) for some \\(p \\in [0,1],\\) then \\(p = 1/[1 + e^{-u}]\\), which is monotonically increasing as↩︎\nTry to prove yourself by considering four cases, one for each combination of the sign of \\(\\beta_{x}\\) and \\(\\beta_{z}\\) in the model above.↩︎\nHome plate measures 17 inches across, so we set the horizontal limits at -8.5/12 and 8.5/12. To set the vertical limits, we take the average values of the StatCast variable sz_top and sz_bot, which are manually determined by the StatCast operator on every pitch↩︎\nThe strategy to include a suitably-transformed baseline probability estimates was first used by Judge, Pavlidis, and Brooks in a 2015 Baseball Prospectus article. It was also used by Deshpande and Wyner (2017) (see Section 2.2 of that paper).↩︎",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture09.html",
    "href": "lectures/lecture09.html",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "",
    "text": "Which of the two touchdown plays is more impressive?\n\nThe 86-yard touchdown pass from Justin Herbert to Ladd McConckey (video link)\nThe 64-yard touchdown pass from Cooper Rush to KaVontae Turpin (video link)\n\nWhile these two plays share many similarities — they were both thrown on 3rd down, out of the shotgun formation with no running back, against the Houston Texans, when the team on offense was trailing, and resulted in a score — there are some big differences: Herbert and the Chargers needed 26 yards to gain a first down while the Cowboys need only 10; Herbert threw his pass from a collapsing pocket while Rush threw with essentially no pressure; McConckey broke at least one tackle after catching the pass. Based on these qualitative differences, we might view the first touchdown pass as more impressive. In this lecture, we introduce expected points (EP; Section 3) and expected points added (EPA), which together facilitate a more nuanced, quantitative comparison of plays. Then, using a multilevel model (Section 5), we will identify the players whom we would expect to generate the most expected points per passing attempt.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-intro",
    "href": "lectures/lecture09.html#sec-intro",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "",
    "text": "Which of the two touchdown plays is more impressive?\n\nThe 86-yard touchdown pass from Justin Herbert to Ladd McConckey (video link)\nThe 64-yard touchdown pass from Cooper Rush to KaVontae Turpin (video link)\n\nWhile these two plays share many similarities — they were both thrown on 3rd down, out of the shotgun formation with no running back, against the Houston Texans, when the team on offense was trailing, and resulted in a score — there are some big differences: Herbert and the Chargers needed 26 yards to gain a first down while the Cowboys need only 10; Herbert threw his pass from a collapsing pocket while Rush threw with essentially no pressure; McConckey broke at least one tackle after catching the pass. Based on these qualitative differences, we might view the first touchdown pass as more impressive. In this lecture, we introduce expected points (EP; Section 3) and expected points added (EPA), which together facilitate a more nuanced, quantitative comparison of plays. Then, using a multilevel model (Section 5), we will identify the players whom we would expect to generate the most expected points per passing attempt.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-nflfastr-intro",
    "href": "lectures/lecture09.html#sec-nflfastr-intro",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Play-By-Play Football Data",
    "text": "Play-By-Play Football Data\nWe will use play-by-play data from the NFL available from the nflfastR package, which can be installed using the code:\n\ninstall.packages(\"nflfastR\")\n\nThe package contains several functions for efficiently scraping NFL data and for pre-processing the play-by-play data. In this lecture and in Lecture 10, we will work with pre-processed play-by-play data, which we can load into our R session using the function nflfastR::load_pbp().\n\npbp2024 &lt;- nflfastR::load_pbp(season = 2024)\n\nEach row of the table pbp2024 corresponds to an individual play from the 2024-25 NFL season. The data table contains information about 49492 distinct plays. Some columns of pbp2024 contain game-level information including\n\na unique identifier for the game (game_id);\nthe week in which the game was played (week);\nwhether the play took place in a regular season game (season_type = \"REG\") or play-off game (season_type=\"POST\");\nthe identities of the home and away team (home_team and away_team).\n\nSeveral columns record information about the state of the game just before the start of the play including\n\nwhich team has possession of the ball (posteam) and which team is on defense (defteam)\nthe scores of the two teams (posteam_score and defteam_score)\nthe amount of time left in the quarter (time)\nthe location of the ball at the start of the play (side_of_field and yardline_100)\nthe down and distance to the first down marker (down, ydstogo).\n\nThe table also contains columns recording information about the state of the game as soon as the play ends including the scores of both teams (posteam_score_post and defteam_score_post) and the number of yards gained on the play (ydsgained) It finally contains a large number of columns that record exactly what happened during the play including\n\nthe type of play (e.g., run, pass, kickoff, punt, etc.; play_type)\na narrative description of the play (desc)\nseveral self-explanatory indicators of what happened (e.g., fumble, complete_pass, and passing_yards).\n\nYou can find a full listing of the variableshere.\nnflfastR identifies players using a unique 9-digit identifier provided by Game Statistics and Information Systems (GSIS). We can load a table containing roster information, including player identifier, name, position, and team using the function nflfastR::fast_scraper_roster()\n\nroster2024 &lt;-\n  nflfastR::fast_scraper_roster(seasons = 2024)",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-ep",
    "href": "lectures/lecture09.html#sec-ep",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Expected Points",
    "text": "Expected Points\nParaphrasing Yurko, Ventura, and Horowitz (2019), the expected points framework uses historical data to estimate the average number of points eventually scored by teams in similar situations. Similar to what we did in Lecture 6 with expected runs at the start and end of at-bats in baseball, the difference between the post-snap and pre-snap expected points is known as expected points added (EPA). Generally speaking, plays that result in positive EPA are considered successful while plays resulting in negative EPA are not.\nYurko, Ventura, and Horowitz (2019) introduced an expected points model that focused on predicting the next scoring event in the half after each play. The “next score” responses, ignoring points after touchdown (PAT) attempts, are, with respect to the team in possession:\n\nTouchdown (TD), which is worth 7 points\nField goal (FG), which is worth 3 points\nSafety (SAF), which is worth 2 points\nNo score (NS), which worth 0 points\nOpponent safety (oSAF), which is worth -2 points\nOpponent field goal (oFG), which is worth -3 points\nOpponent touch down (oTD), which is worth -7 points\n\nFormally, for every play, we can collect features summarizing the state of the game in the \\(\\boldsymbol{\\mathbf{Z}}.\\) Typically, these features will include things like the time left in the half, the current score, the down and distance to the first down line, etc.\n\n\n\n\n\n\nDefinition: Drive Outcome Probabilities\n\n\n\nGiven the state of the game at the beginning of a play \\(\\boldsymbol{\\mathbf{z}},\\) within a drive, for each possible outcome \\(k \\in \\left\\{\\textrm{TD}, \\textrm{FG}, \\ldots, \\textrm{oTD}\\right\\}\\) let \\(\\pi_{k}(\\boldsymbol{\\mathbf{z}})\\) conditional probability that the drives containing plays characterized by \\(\\boldsymbol{\\mathbf{z}}\\) end in outcome \\(k.\\) We collect these probabilities into a vector \\(\\boldsymbol{\\pi}(\\boldsymbol{\\mathbf{z}}) = (\\pi_{\\textrm{TD}}(\\boldsymbol{\\mathbf{z}}), \\ldots, \\pi_{\\textrm{oFG}}(\\boldsymbol{\\mathbf{z}})).\\)\n\n\n\n\n\n\n\n\nDefinition: Expected Points\n\n\n\nGiven a game state feature vector \\(\\boldsymbol{\\mathbf{z}}\\) and vector of drive outcome probabilities \\(\\boldsymbol{\\pi}(\\boldsymbol{\\mathbf{z}}),\\) the expected points \\(\\textrm{EP}(\\boldsymbol{\\mathbf{z}})\\) is \\[\n\\begin{align}\n\\textrm{EP}(\\boldsymbol{\\mathbf{z}}) &=  7\\times\\pi_{\\textrm{TD}}(\\boldsymbol{\\mathbf{z}}) +\n3\\times\\pi_{\\textrm{FG}}(\\boldsymbol{\\mathbf{z}}) +\n2\\times\\pi_{\\textrm{SAF}}(\\boldsymbol{\\mathbf{z}}) \\\\\n~&~~-2\\times\\pi_{\\textrm{oSAF}}(\\boldsymbol{\\mathbf{z}}) -\n3\\times\\pi_{\\textrm{oFG}}(\\boldsymbol{\\mathbf{z}})\n- 7\\times\\pi_{\\textrm{oTD}}(\\boldsymbol{\\mathbf{z}})\n\\end{align}\n\\]\n\n\n\nEstimating \\(\\textrm{EP}(\\boldsymbol{\\mathbf{z}})\\)\nnflfastR implements a version of Yurko, Ventura, and Horowitz (2019)’s original EP model. At a high-level, it is a multinomial classification model built using extreme gradient boosting (xgboost). Like random forests, xgboost builds an ensemble of decision trees, each of which map a vector of game state feature \\(\\boldsymbol{\\mathbf{x}}\\) to one of the next score outcomes \\(k.\\) The final prediction is formed using the proportions of trees outputting a certain outcome. More details are about nflfastR’s EP model are available here.\n\n\nBasic EPA Uses\nFor each play in pbp2024, the column epa records the expected points added on the play. It turns out that the Herbert-to-McConckey touchdown introduced in Section 1 was the play with the largest EPA across the entire 2024-25 regular season. At the start of the play, the Chargers were facing 3rd and 26 from their 14 yard line. According to nflfastR’s EP model, teams starting from the same position score about -1.54 points. The Chargers ended the play with a touchdown, which corresponds to a terminal game state with an EP of 7. Thus, their EPA on the play was 7-(-1.54) = 8.54.\n\npbp2024 |&gt;\n1  dplyr::slice_max(epa) |&gt;\n  dplyr::select(ep, desc)\n\n\n1\n\nslice_max() extracts the row with the largest value in a column\n\n\n\n\n# A tibble: 1 × 2\n     ep desc                                                                    \n  &lt;dbl&gt; &lt;chr&gt;                                                                   \n1 -1.54 (10:50) (Shotgun) 10-J.Herbert pass deep middle to 15-L.McConkey for 86…\n\n\nIt turns out that Turpin’s 64-yard touchdown was Dallas Cowboys’ passing play with the largest EPA across the whole season.\n\npbp2024 |&gt;\n  dplyr::filter(posteam==\"DAL\" & play_type == \"pass\") |&gt;\n  dplyr::slice_max(epa) |&gt;\n  dplyr::select(ep, desc)\n\n# A tibble: 1 × 2\n     ep desc                                                                    \n  &lt;dbl&gt; &lt;chr&gt;                                                                   \n1 0.775 (15:00) (Shotgun) 10-C.Rush pass short right to 9-K.Turpin for 64 yards…\n\n\nFacing 3rd down with 10 yards to go from their own 36 yard line, the Cowboys were expected to score about 0.77 points at the start of the play. By scoring a touchdown, they gained 7-0.77 = 6.23. Because the Chargers scored from a much less favorable starting point than the Cowboys in terms of EP, one could argue that the 86-yard touchdown is more impressive than the 64-yard touchdown.\nBeyond comparing individual plays, we can rank different teams’ offenses by their average EPA per play\n\npbp2024 |&gt;\n1  dplyr::group_by(posteam) |&gt;\n2  dplyr::summarize(epa = mean(epa, na.rm = TRUE)) |&gt;\n  dplyr::arrange(desc(epa)) |&gt;\n3  dplyr::slice(c(1:5, (dplyr::n()-4):(dplyr::n())))\n\n\n1\n\nDivide plays by offensive team\n\n2\n\nCompute the average EPA for each team’s offense\n\n3\n\nShow only the top-5 and bottom-5 teams in terms of average EPA per play\n\n\n\n\n# A tibble: 10 × 2\n   posteam     epa\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 BAL      0.143 \n 2 BUF      0.141 \n 3 DET      0.135 \n 4 WAS      0.115 \n 5 TB       0.103 \n 6 NE      -0.0643\n 7 NYG     -0.0829\n 8 TEN     -0.0896\n 9 LV      -0.107 \n10 CLE     -0.151 \n\n\nInterestingly, the top three offenses in terms of average EPA (the Baltimore Ravens, Buffalo Bills, and Detroit Lions) were also the three highest scoring offenses in the 2024 season, each scoring over 30 points per game, on average.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-newpass",
    "href": "lectures/lecture09.html#sec-newpass",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Predicting EPA On A New Pass",
    "text": "Predicting EPA On A New Pass\nBased on the 2024-25 regular season data, how much EPA would we expect to see on a hypothetical future passing play? Without any additional information about the play — i.e., who threw the pass, how long the pass was, whether it was thrown out of the shotgun formation, etc. — it seems reasonable to use the average EPA across all passing plays in our data set. But, if we knew the identity of the passers, we might reasonably expect the average EPA across their passes would yield better predictions.\nTo compute the league-wide and player-specific average EPAs, we first create a data table containing only the passing plays from the regular season. We can do this by filtering on the variables play_type and season_type. We see that there are about 19,000 pass plays in the regular season.\n\ntable(pbp2024[,c(\"play_type\", \"season_type\")])\n\n             season_type\nplay_type      POST   REG\n  extra_point    61  1241\n  field_goal     51  1115\n  kickoff       146  2803\n  no_play       205  4729\n  pass          853 19153\n  punt           73  2046\n  qb_kneel       32   405\n  qb_spike        4    71\n  run           726 14318\n\n\nIn addition to EPA, we also include the unique identifier of the passer (passer_player_id) and a narrative description of the play (desc).\n\npass2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(play_type == \"pass\" & season_type == \"REG\") |&gt; \n1  dplyr::filter(!grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc) &\n                  !grepl(\"sacked\", desc)) |&gt;\n  dplyr::select(epa, passer_player_id, desc)\npass2024 |&gt; dplyr::slice_head(n = 2)\n\n\n1\n\nFor now, we will ignore 2-point conversion passing attempts. We also ignore sacks, which nflfastR sometimes treat as passing plays\n\n\n\n\n# A tibble: 2 × 3\n    epa passer_player_id desc                                                   \n  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;                                                  \n1 2.03  00-0035228       (14:27) 1-K.Murray pass short left to 6-J.Conner to BU…\n2 0.754 00-0035228       (13:43) (Shotgun) 1-K.Murray pass short middle to 6-J.…\n\n\nAfter excluding passes on two-point conversion attempts and sacks, we have data for 17727 passes thrown by 103 players. Across all passes and passers, the average EPA per pass is about 0.159.\n\nmean(pass2024$epa)\n\n[1] 0.1589677\n\n\n\nPlayer-Specific EPA/Pass\nGiven the vast spread of EPAs in our dataset (roughly -12.7 to 8.5), using the league-wide average of 0.159 to predict the EPA on a future pass is rather unsatisfactory. One avenue for improvement is to condition on additional information like the identity of the passer, which is recorded in the column passer_player_id. Intuitively, we might expect a superstar quarterback like Patrick Mahomes to generate more EPA per pass than Daniel Jones, who was benched and released by his team mid-way through the season.\nWe will model the observed EPA on each passing play as a noisy observation of an underlying average EPA specific to each passer. So, the EPA’s on passes thrown by Dak Prescott, for instance, are treated as noisy measurements of his average EPA. We can then predict the EPA on a future pass thrown by Prescott using an estimate of his per-pass EPA. We can further rank passers by their per-pass EPAs.\nFormally, suppose that we have data for \\(I\\) unique passers1 and for each passer \\(i = 1, \\ldots, I,\\) let \\(n_{i}\\) be the total number of passes thrown by passer \\(i.\\) Further, for each \\(j = 1, \\ldots, n_{i},\\) let \\(Y_{ij}\\) be the EPA on the the \\(j\\)-th pass thrown by passer \\(i.\\) Let \\(\\alpha_{i}\\) be the underlying — and as yet unknown — average EPA for passer \\(i.\\) We model \\(Y_{ij} = \\alpha_{i} + \\epsilon_{ij}\\) where the random errors \\(\\epsilon_{ij}\\) have mean zero and are assumed to be independent across passers and passes.\nWhile we can certainly run this calculation by grouping plays in pass2024 by passer_player_id and then computing the average EPA of the plays involving each passer, for reasons that will hopefully become clear soon, we will instead estimate the \\(\\alpha_{i}\\)’s by fitting a linear regression model with ordinary least squares. Specifically, we will regress epa onto the categorical variable passer_player_id, which we will represent as a factor variable. When run with a single categorical predictor, function lm() does not directly estimate the average outcome within each level of the predictor. Instead, it returns the average outcome within one reference level of the predictor and the average difference in outcomes between the non-reference levels and the reference. We will, rather arbitrarily, use Aaron Rodgers as the reference player.\n\nrodgers_id &lt;- \n  roster2024 |&gt; \n  dplyr::filter(full_name == \"Aaron Rodgers\") |&gt; \n1  dplyr::pull(gsis_id)\n\npass2024 &lt;-\n  pass2024 |&gt;\n  dplyr::mutate(\n    passer_player_id = factor(passer_player_id),\n2    passer_player_id = relevel(passer_player_id, ref = rodgers_id))\n\n\n1\n\nPull out Aaron Rodgers’ id number\n\n2\n\nSet the reference level to Rodgers\n\n\n\n\nWe can now fit our linear model and collect the estimated parameters in the vector ols_betas. Notice that the names of all but the first element of ols_betas contain the string “passer_player_id” and a player identifier.\n\nols_fit &lt;-\n1  lm(formula = epa ~ 1 + passer_player_id,\n2     data = pass2024)\nols_betas &lt;- coefficients(ols_fit)\n3ols_betas[1:5]\n\n\n1\n\nThe 1+ is not strictly necessary (since R usually includes an intercept by default).\n\n2\n\nBecause the formula argument only involves epa and passer_player_id, none of the other variables in pass2024 are included as outcomes or predictors in the model\n\n3\n\nPrint out a handful of parameter estimates\n\n\n\n\n               (Intercept) passer_player_id00-0026158 \n                0.11303340                 0.04427535 \npasser_player_id00-0026300 passer_player_id00-0026498 \n               -0.32316791                 0.05621013 \npasser_player_id00-0027973 \n               -0.19838613 \n\n\nIn our simple EPA model, the estimated intercept is exactly the average EPA on all passes thrown by Aaron Rodgers.\n\nmean( pass2024$epa[pass2024$passer_player_id == rodgers_id])\n\n[1] 0.1130334\n\n\nTo get the average EPA on all passes thrown by a different quarterback (e.g., Dak Prescott whose id is 00-0033077), we need to add the corresponding entry in ols_beta to the estimated intercept2.\n\nmean(pass2024$epa[pass2024$passer_player_id == \"00-0033077\"]) \n\n[1] 0.05691811\n\nols_betas[\"passer_player_id00-0033077\"] + ols_betas[\"(Intercept)\"]\n\npasser_player_id00-0033077 \n                0.05691811 \n\n\nWe will repeat this calculation for each of the 101 unique passers in our data set and build a table alphas with columns containing the player’s identifier, name, and average EPA on passing plays. In the following code, we strip out the string “passer_player_id” from the names of ols_beta.\n\n1names(ols_betas)[1] &lt;- paste0(\"passser_player_id\", rodgers_id)\nnames(ols_betas) &lt;- \n2  stringr::str_remove(string = names(ols_betas), pattern = \"passer_player_id\")\nalphas &lt;- \n3  data.frame(gsis_id = names(ols_betas), ols = ols_betas) |&gt;\n4  dplyr::mutate(ols = ifelse(gsis_id == rodgers_id, ols, ols + dplyr::first(ols))) |&gt;\n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\")\n\n\n1\n\nRename first element of ols_beta so its name follows the same format as the names of all other elements\n\n2\n\nRemove the string “passer_player_id” from all names of ols_beta\n\n3\n\nFacilitate a later join with roster2024, which records player identifiers as gsis_id\n\n4\n\nSince the first row of alphas corresponds to Aaron Rodgers, we need to add the first element in the column ols to all other elements in the column.\n\n\n\n\nSomewhat surprisingly, the top- and bottom-five passers based on average EPA per play are not quarterbacks but include kickers. What’s going on??\n\nalphas |&gt;\n  dplyr::arrange(dplyr::desc(ols)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n())) |&gt;\n  dplyr::select(full_name, ols)\n\n          full_name       ols\n1           AJ Cole  4.333918\n2  Courtland Sutton  3.745369\n3          Jack Fox  3.637429\n4  Justin Jefferson  3.050907\n5      Stefon Diggs  2.762919\n6   Miles Killebrew -2.677720\n7        J.K. Scott -2.706606\n8       Bryan Anger -2.843511\n9     Johnny Hekker -2.856017\n10     Keenan Allen -5.911163\n\n\n\n\nIssues\nBy this point in the course, you probably already suspect what’s going on: the extreme average EPA estimates are artifacts of small sample sizes. To verify that this is so, we’ll add a column to alphas recording the number of passes thrown by each player.\n\nn_passes &lt;-\n  pass2024 |&gt;\n  dplyr::group_by(passer_player_id) |&gt; \n1  dplyr::summarise(n = dplyr::n()) |&gt;\n2  dplyr::rename(gsis_id = passer_player_id)\n\nalphas &lt;-\n  alphas |&gt;\n  dplyr::inner_join(y = n_passes, by = \"gsis_id\")\n\nalphas |&gt;\n  dplyr::arrange(dplyr::desc(ols)) |&gt;\n3  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n\n1\n\nCounts the number of passes thrown by each player in our dataset\n\n2\n\nSo that we can join the counts to alphas, we need to rename the column recording the player identifiers\n\n3\n\nLook at the top-5 and bottom-5 rows\n\n\n\n\n      gsis_id       ols        full_name n\n1  00-0035190  4.333918          AJ Cole 1\n2  00-0034348  3.745369 Courtland Sutton 2\n3  00-0035156  3.637429         Jack Fox 1\n4  00-0036322  3.050907 Justin Jefferson 1\n5  00-0031588  2.762919     Stefon Diggs 1\n6  00-0032399 -2.677720  Miles Killebrew 1\n7  00-0034162 -2.706606       J.K. Scott 1\n8  00-0029692 -2.843511      Bryan Anger 2\n9  00-0028872 -2.856017    Johnny Hekker 2\n10 00-0030279 -5.911163     Keenan Allen 1\n\n\nAs anticipated, the passers with the top- and bottom-5 per-pass EPA all threw 1 or 2 passes. For instance, AJ Cole is a punter who threw a single pass that ultimately gained 34 yards and resulted in a very large EPA of around 4.33.\n\npass2024 |&gt;\n  dplyr::filter(passer_player_id == \"00-0035190\") |&gt;\n  dplyr::select(desc, epa)\n\n── nflverse play by play data ──────────────────────────────────────────────────\n\n\nℹ Data updated: 2025-04-30 02:39:06 EDT\n\n\n# A tibble: 1 × 2\n  desc                                                                       epa\n  &lt;chr&gt;                                                                    &lt;dbl&gt;\n1 (8:33) (Punt formation) 6-A.Cole pass short right to 5-D.Deablo to DEN …  4.33\n\n\nPlotting each player’s average EPA against the number of pass attempts reveals that there is a lot of (resp. very little) variation in EPA-per-pass among passers who threw a very small (resp. very large) number of passes.\n\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(alphas$n, alphas$ols, \n     xlab = \"Number of passes\",\n     ylab = \"Avg. EPA per pass\",\n     main = \"EPA per pass\",\n     pch = 16, cex = 0.5, col = oi_colors[1])\nabline(h = mean(pass2024$epa), col = oi_colors[3])\n\n\n\n\n\n\n\nFigure 1: The variability in per-pass decreases dramatically as the number of passes increases.\n\n\n\n\n\n\n\nPartial Pooling\nLet’s return to the problem of forecasting EPA on a single new pass based solely on the identity of the passer. One option is to ignore the passer identity and just use the overall mean of about 0.159. Doing so completely ignores differences between passers, which is fairly unreasonable given the large gaps in talent between NFL quarterbacks. At the other extreme, we could use the player-specific average EPA estimates. While this approach acknowledges differences between players, it can sometimes lead to very extreme estimates. Neither option seems particularly compelling.\nAs with most things, an ideal solution lies somewhere between the two extremes. For players who threw a large number of passes, we may be more comfortable using the their average EPA value since we are more certain about its value3. But for players who threw a very small number of passes, it’s probably safe to assume that they’ll perform closer to the league-average rather than to rely on their potentially very noisy average EPA estimate4. And for players who threw a moderate number of passes, we might prefer a value somewhere between the league-wide average and the player-specific estimate.\nFormally, we can make such predictions with a weighted average: letting \\(\\overline{y}\\) be the overall league-wide average, we can predict the EPA on a new pass thrown by player \\(i\\) using \\[\nw_{i}\\times \\hat{\\alpha}_{i} + (1 - w_{i}) \\times \\overline{y},\n\\] where the weight \\(w_{i}\\) depends on the number of passes thrown by player \\(i\\) and how far \\(\\hat{\\alpha}_{i}\\) differs from \\(\\overline{y}.\\) Ideally, if a player \\(i\\) throws a large number of passes the weight \\(w_{i}\\) assigned to his empirical average EPA would be close to 1 and and if he threw very few passes, the weight would be set much closer to 0. The hope is, in other words, that we could differentially shrink every player’s raw average per-pass EPA towards the grand mean.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-multilevel-model",
    "href": "lectures/lecture09.html#sec-multilevel-model",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Multilevel Models",
    "text": "Multilevel Models\nMultilevel models provide a principled way to compute these weights automatically. Like in our initial analysis, our first multilevel will model the observed EPA’s as noisy measurements of some underlying player-specific parameter. However, we make an additional model assumption: that the \\(\\alpha_{i}\\)’s are themselves normally distributed around some grand mean \\(\\alpha_{0}\\) with standard deviation \\(\\sigma_{\\alpha}.\\) We can specify our model in two levels: \\[\n\\begin{align}\n\\textrm{Level 1}&: &\\quad  Y_{ij} &= \\alpha_{i} + \\epsilon_{ij}; \\epsilon_{ij} \\sim N(0, \\sigma^{2}) \\quad \\textrm{for all}\\ j = 1, \\ldots, n_{i},\\ i = 1, \\ldots, I \\\\\n\\textrm{Level 2}&: &\\quad \\alpha_{i} &= \\alpha_{0} + u_{i}; u_{i} \\sim N(0, \\sigma^{2}_{\\alpha}) \\quad \\textrm{for all}\\ i = 1, \\ldots, I\n\\end{align}\n\\]\nThe Level 1 model says that for each passer \\(i,\\) the observed EPAs on his passing plays are normally distributed around his latent averge EPA per play \\(\\alpha_{i}.\\) The parameter \\(\\sigma\\) quantifies the amount by which we expect each passer’s EPAs to deviate from his latent average. In our model, we assume that this “within-passer” variability is the same across passers.\nIn Level 2 of our model, the responses (\\(\\alpha_{i}\\)) are not observed quantities but rather unknown regression coefficients that appear in the Level 1 model. Level 2 of our model assumes that the passers constitute a random sample from some super-population of passers in which latent per-pass EPAs follow a normal distribution. The parameter \\(\\alpha_{0}\\) represents the global average per-pass EPA value across this super-population. The parameter \\(\\sigma_{\\alpha}\\) captures the average deviation between each passer’s latent EPA per-play and the global average.\nTo see why a multilevel model might make sense here, imagine trying to make a prediction about the EPA on a pass thrown by a new player not already in our original dataset. If we fit the usual multiple linear regression model (i.e., only the Level 1 model and ignored the grouping structure & Level 2), there is really no way for us to make a principled prediction about his latent EPA per play. But with a multilevel model, if we’re willing to assume that the player is also drawn from the same super-population of passers, we can predict his latent EPA per play by drawing from a \\(N(\\hat{\\alpha}_{0}, \\hat{\\sigma}^{2}_{\\alpha})\\) distribution. Effectively, multilevel models allow us to “borrow information” across different groups.\nIn general, multilevel models are used when observations in a dataset display hierarchical grouping structure (e.g., passes grouped by quarterbacks, students in classrooms in schools, etc.). Often in these settings, the grouping variables are believed to be relevant to the outcome of interest and can induce correlation between observations within the same group. Moreover, we might expect that outcomes from the same group to be more tightly clustered around a group-level average than around the overall grand mean. For instance, if a particular medical practice uses a new, highly effective treatment strategy, knowing the outcome of one patient from that practice who received the treatment provides a lot of information about the potential outcome of another patient from that practice who also received the treatment. As we will soon see, the standard multiple linear regression model with which you are already familiar tends not to perform well in the presence of grouping structure, largely because it assumes observations are independent.\n\n\n\n\n\n\nWarning\n\n\n\nThe above is by no means a comprehensive review of multilevel models, which is beyond the scope of this course. I highly recommend Chapter 8 of the book Beyond Multiple Linear Regression. I also recommend this blog post, which discusses how the R syntax maps onto the multilevel model notation.\n\n\n\nFitting Multilevel Models\nWe can fit multilevel models using the lme4 package, which can be installed using the code\n\ninstall.packages(\"lme4\")\n\nThis cheatsheet provides a really nice overview of the underlying concepts and the relevant R syntax.\n\nlibrary(lme4)\nmultilevel_fit &lt;-\n1  lmer(formula = epa ~ 1 + (1|passer_player_id),\n       data = pass2024)\n\n\n1\n\nThe term (1 | passer_play_id) signals to lmer() that there should be a separate random intercept for every passer\n\n\n\n\nThe model summary contains a lot of useful information about our model\n\nsummary(multilevel_fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: epa ~ 1 + (1 | passer_player_id)\n   Data: pass2024\n\nREML criterion at convergence: 65351.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.3587 -0.5309 -0.1400  0.5547  5.4923 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n passer_player_id (Intercept) 0.01518  0.1232  \n Residual                     2.33009  1.5265  \nNumber of obs: 17723, groups:  passer_player_id, 101\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.1297     0.0207   6.265\n\n\nFirst, we find the estimate of the overall grand mean EPA per play in the “Fixed effects” section: the average EPA across all passers and passes is estimated to be \\(\\hat{\\alpha}_{0} \\approx 0.1294.\\) The “Random effects” section reports our estimates \\(\\hat{\\sigma} \\approx 1.526\\) and \\(\\hat{\\sigma}_{\\alpha} = 0.123\\) of, respectively, the variability in EPA that we’d expect to see between passes thrown by the same player and the variability in latent EPA per pass across the different passers. The fact that \\(\\hat{\\sigma}_{\\alpha}\\) is so much smaller than \\(\\hat{\\sigma}\\) suggests that most of the variability in observed passing play EPA is driven more by noise than differences in latent player ability.\nWe can use the function ranef() to extract the estimates of the \\(u_{i}\\)’s, the function fixef to extract the estimate of \\(\\alpha_{0},\\) and the function coef() to extract the estimates of \\(\\alpha_{i} = \\alpha_{0} + u_{i}.\\) The function coef() returns a named list with one element per grouping variable5. Each element of coef()’s output is a data frame with named rows\n\ntmp &lt;- coef(multilevel_fit)\n1tmp[[\"passer_player_id\"]] |&gt; dplyr::slice_head(n = 5)\n\n\n1\n\nFor brevity, we only display the first 5 rows\n\n\n\n\n           (Intercept)\n00-0023459   0.1164973\n00-0026158   0.1467385\n00-0026300   0.1231416\n00-0026498   0.1601689\n00-0027973   0.0199101\n\n\nWe will append our multilevel estimates to our data table alpha\n\n1lmer_alpha &lt;-\n  data.frame( \n2    lmer = tmp[[\"passer_player_id\"]][,1],\n3    gsis_id = rownames(tmp[[\"passer_player_id\"]]))\n\nalphas &lt;-\n  alphas |&gt;\n  dplyr::inner_join(y = lmer_alpha, by = \"gsis_id\")\n\n\n1\n\nWe’ll create a data frame with columns containing the player identifier and their corresponding parameter estimate\n\n2\n\nIn this case, the output of coef is a matrix with 1 column whose row names are just the levels of the grouping variable\n\n3\n\nFor our eventual join, we will store the player’s identifier as gsis_id\n\n\n\n\nOur multilevel model returns very high per-pass EPA estimates for several top quarterbacks and very low estimates for quarterbacks widely regarded to have struggled in 2024-25.\n\nalphas |&gt;\n  dplyr::arrange(dplyr::desc(lmer)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n())) |&gt;\n  dplyr::select(full_name, lmer, n)\n\n                  full_name        lmer   n\n1             Lamar Jackson  0.36827643 469\n2                Jared Goff  0.33318327 536\n3                Josh Allen  0.27346854 482\n4                Joe Burrow  0.27175401 651\n5            Baker Mayfield  0.27066068 569\n6               Andy Dalton  0.01991010 160\n7                 Drew Lock  0.01613596 180\n8        Anthony Richardson  0.01212565 261\n9           Spencer Rattler -0.04063512 227\n10 Dorian Thompson-Robinson -0.13685136 118\n\n\nWe see that our multilevel estimates for players who threw very few passes are aggressively shrunk towards to the overall grand-mean.\n\n1alpha0_hat &lt;- fixef(multilevel_fit)[\"(Intercept)\"]\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(alphas$n, alphas$ols, \n     ylim = c(-4.5, 4.5),\n     xlab = \"Number of passes\",\n     ylab = \"Avg. EPA per pass\",\n     main = \"EPA per pass\",\n     pch = 16, cex = 0.5, col = oi_colors[9])\npoints(alphas$n, alphas$lmer, pch = 15, col = oi_colors[2], cex = 0.5)\nabline(h = alpha0_hat, col = oi_colors[3])\n\n\n1\n\nThe function fixef() returns all the fixed effects (see below) in a named vector. As this is an intercept-only model, we use the name “(Intercept)” to access the parameter estimate.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Although our multilevel model heavily shrink the raw per-pass EPA values (black dots) towards the overall sample mean (blue line), there is still player-to-player variability in the resulting estimates (orange).",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-covariates",
    "href": "lectures/lecture09.html#sec-covariates",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Adjusting For Additional Covariates",
    "text": "Adjusting For Additional Covariates\nUp to this point, we’ve worked with a rather toy problem, predicting EPA given only the identity of the passer. But what if we include additional information about the pass (e.g., whether it was throw by the home or away team, whether it was thrown out of the shotgun formation, etc.)? Do we still run into the small sample size issues when determining how much EPA an individual player adds, after accounting for this other information?\nTo answer these questions, we re-build our data table pass2024 but now include:\n\nair_yards: the distance traveled by the pass through the air6\nshotgun: whether the pass was thrown out of the shotgun formation\nqb_hit: whether the quarterback was hit while throwing\nno_huddle: whether the play was run without a huddle\nposteam_type: whether the home team had possession (i.e., was on offense)\npass_location: whether the pass was thrown to the left, right, or center of the field\n\n\npass2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(play_type == \"pass\" & season_type == \"REG\") |&gt; \n  dplyr::filter(!grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc) &\n                  !grepl(\"sacked\", desc)) |&gt;\n  dplyr::select(epa, passer_player_id, \n                air_yards, \n                posteam_type, shotgun, no_huddle, qb_hit,\n                pass_location,\n                desc) |&gt;\n  dplyr::mutate(posteam_type = factor(posteam_type),\n                pass_location = factor(pass_location))\npass2024 |&gt; dplyr::slice_head(n = 2)\n\n# A tibble: 2 × 9\n    epa passer_player_id air_yards posteam_type shotgun no_huddle qb_hit\n  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 2.03  00-0035228              -3 away               0         0      0\n2 0.754 00-0035228               2 away               1         0      0\n# ℹ 2 more variables: pass_location &lt;fct&gt;, desc &lt;chr&gt;\n\n\nFor pass \\(j\\) thrown by passer \\(i,\\) let \\(\\boldsymbol{\\mathbf{x}}_{ij}\\) be a vector containing the features air_yards, posteam_type, shotgun, no_huddle, qb_hit, and pass_location. We can elaborate our initial single-level regression model, which doesn’t account for the grouping structure, to adjust for the effects of these factors: \\[\nY_{ij} = \\alpha_{i} + \\boldsymbol{\\mathbf{x}}_{ij}^{\\top}\\boldsymbol{\\beta} + \\epsilon_{ij}\n\\] Under this model, \\(\\alpha_{i}\\) represents the amount of EPA that can be attributable to the passer over and above what can be explained by these factors. When we fit this single-level linear model using lm, we find that the estimated passer-specific intercepts display the exact same small sample size artifacts as our initial intercept-only model.\n\nols_fit_full &lt;- \n1  lm(epa ~ passer_player_id + air_yards + posteam_type + shotgun + no_huddle + qb_hit + pass_location,\n     data = pass2024)\nols_betas_full &lt;- coefficients(ols_fit_full)\n\nnames(ols_betas_full)[1] &lt;- paste0(\"passser_player_id\", rodgers_id) \nnames(ols_betas_full) &lt;- \n  stringr::str_remove(string = names(ols_betas_full), pattern = \"passer_player_id\") \nalphas_full &lt;- \n  data.frame(gsis_id = names(ols_betas_full), ols = ols_betas_full) |&gt; \n  dplyr::mutate(ols = ifelse(gsis_id == rodgers_id, ols, ols + dplyr::first(ols))) |&gt; \n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::inner_join(y = n_passes, by = \"gsis_id\")\n\nalphas_full |&gt;\n  dplyr::arrange(dplyr::desc(ols)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n())) |&gt;\n  dplyr::select(full_name, ols, n)\n\n\n1\n\nUse all the available information, including passer identity, and the features to predict epa\n\n\n\n\n          full_name       ols n\n1           AJ Cole  4.337054 1\n2  Courtland Sutton  3.702885 2\n3  Justin Jefferson  3.316711 1\n4          Jack Fox  3.264767 1\n5      Stefon Diggs  2.994487 1\n6        J.K. Scott -2.586283 1\n7   Miles Killebrew -2.913268 1\n8       Bryan Anger -2.969681 2\n9     Johnny Hekker -3.151710 2\n10     Keenan Allen -6.093688 1\n\n\nLuckily, we can similarly elaborate our multilevel model to account for the covariate effects. \\[\n\\begin{align}\n\\textrm{Level 1}&: &\\quad  Y_{ij} &= \\alpha_{i} + \\boldsymbol{\\mathbf{x}}_{ij}^{\\top}\\boldsymbol{\\beta} +  \\epsilon_{ij}; \\quad \\epsilon_{ij} \\sim N(0, \\sigma^{2}) \\quad \\textrm{for all}\\ j = 1, \\ldots, n_{i},\\ i = 1, \\ldots, I \\\\\n\\textrm{Level 2}&: &\\quad \\alpha_{i} &= \\alpha_{0} + u_{i}; \\quad u_{i} \\sim N(0, \\sigma^{2}_{\\alpha}) \\quad \\textrm{for all}\\ i = 1, \\ldots, I\n\\end{align}\n\\] Like our original multilevel model, Level 2 of our new model assumes that each player’s latent per-pass EPA is normally distributed around some grand mean \\(\\alpha_{0}.\\) However, it no longer assumes that the EPA on all passes thrown by passer \\(i\\) are clustered around the latent \\(\\alpha_{i}.\\) Instead, the average EPA on a pass thrown by passer \\(i\\) depends on several covariates and the player’s latent intercept. In our new model, the covariate effects are assumed to be constant across passers. So, for instance, keeping all other covariates the same, the difference in average EPA on 5 yard and 10 yard passes is the same whether the pass was thrown by Patrick Mahomes or Daniel Jones. Within the multilevel modeling literature, these effects are often called “fixed effects” while effects that can vary across groups (e.g., passers) are termed “random effects.” See Section 8.5.2 of Beyond Multiple Linear Regression and here for some discussion about the distinction.7\nIt is fairly easy to fit our elaborated multilevel model that accounts for fixed covariate effects:\n\nml_fit_full &lt;-\n  lme4::lmer(formula = epa ~ 1 + (1|passer_player_id) + \n               air_yards + posteam_type + shotgun +\n               no_huddle + qb_hit + pass_location, data = pass2024)\n\nLike with our initial random intercept model, we can read off several important parameter estimates using the model summary.\n\nsummary(ml_fit_full)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: epa ~ 1 + (1 | passer_player_id) + air_yards + posteam_type +  \n    shotgun + no_huddle + qb_hit + pass_location\n   Data: pass2024\n\nREML criterion at convergence: 65016.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.5225 -0.5478 -0.0841  0.5538  5.4631 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n passer_player_id (Intercept) 0.01344  0.1159  \n Residual                     2.28043  1.5101  \nNumber of obs: 17727, groups:  passer_player_id, 103\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)          0.133559   0.039314   3.397\nair_yards            0.016728   0.001132  14.772\nposteam_typehome    -0.010065   0.022883  -0.440\nshotgun             -0.109698   0.032192  -3.408\nno_huddle            0.021179   0.034703   0.610\nqb_hit              -0.470851   0.039760 -11.842\npass_locationmiddle  0.135755   0.030724   4.418\npass_locationright  -0.053875   0.025769  -2.091\n\nCorrelation of Fixed Effects:\n            (Intr) ar_yrd pstm_t shotgn n_hddl qb_hit pss_lctnm\nair_yards   -0.207                                             \npostm_typhm -0.280  0.012                                      \nshotgun     -0.676 -0.005 -0.011                               \nno_huddle   -0.039 -0.009 -0.024 -0.105                        \nqb_hit      -0.078 -0.067  0.008  0.005  0.011                 \npss_lctnmdd -0.240 -0.060 -0.013 -0.041 -0.002 -0.019          \npss_lctnrgh -0.348  0.007 -0.010  0.010 -0.003 -0.019  0.441   \n\n\nLooking first at the “Random effects” panel, we that our estimated residual standard deviation is \\(\\hat{\\sigma} \\approx 1.5101.\\) The estimate \\(\\hat{\\sigma}_{\\alpha} \\approx 0.1159\\) indicates that even after adjusting for constant covariate effects, there is little variability in the passer-to-passer latent per-pass EPA. The “Fixed effects” panel contains estimates for the global mean intercept \\(\\alpha_{0}\\) as well as the fixed covariate effects. We see, for instance, that passes thrown from the shotgun formation are expected to generate about 0.11 fewer expected points than passes thrown from under center, all else being equal.\nThe following code extracts the estimates of the player-specific latent per-pass EPAs and appends them to the data table alphas_full.\n\ntmp &lt;- coef(ml_fit_full)\nlmer_alpha_full &lt;- \n  data.frame( \n    lmer = tmp[[\"passer_player_id\"]][,1],\n    gsis_id = rownames(tmp[[\"passer_player_id\"]])) \nalphas_full &lt;-\n  alphas_full |&gt;\n  dplyr::inner_join(y = lmer_alpha_full, by = \"gsis_id\")\nalphas_full |&gt;\n  dplyr::arrange(dplyr::desc(lmer)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))  |&gt;\n  dplyr::select(full_name, lmer, n)\n\n                  full_name        lmer   n\n1             Lamar Jackson  0.34106738 469\n2                Jared Goff  0.32096595 536\n3                Joe Burrow  0.29762396 651\n4            Tua Tagovailoa  0.26988653 397\n5                Josh Allen  0.26577678 482\n6              Bailey Zappe  0.03321274  31\n7               Andy Dalton  0.02969282 160\n8        Anthony Richardson -0.01605454 261\n9           Spencer Rattler -0.03511495 227\n10 Dorian Thompson-Robinson -0.08925603 118\n\n\nWe can further visualize the shrinkage effect by comparing the multiple linear regression a multilevel model estimates of the passer-specific per-pass EPAs to the number of passing attempts.\n\nalpha0_hat &lt;- fixef(ml_fit_full)[\"(Intercept)\"]\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(alphas_full$n, alphas_full$ols, ylim = c(-0.75, 0.75), \n     xlab = \"Number of passes\",\n     ylab = \"EPA\",\n     main = \"EPA per pass\",\n     pch = 16, cex = 0.5, col = oi_colors[1])\npoints(alphas_full$n, alphas_full$lmer, pch = 15, col = oi_colors[2], cex = 0.5)\nabline(h = alpha0_hat, col = oi_colors[3])\n\n\n\n\n\n\n\nFigure 3: Even after adjusting for covariate effects, players with fewer passing attempts display much more variation in per-pass EPA than players with more passing attempts.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#sec-next-time",
    "href": "lectures/lecture09.html#sec-next-time",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nUsing Expected Points Added (EPA), we can assess individual plays based on how the probability of different scoring outcomes changed over the course of the play. Armed with EPA, it is very tempting to rank passers in terms of the average EPA on their passing plays. But, as we have seen many times in the course, doing so can lead to extreme conclusions. Multilevel models offer a way to overcome these issues while also accounting for the inherent grouping structure.\nUsing a multilevel model, we estimated the average EP per pass each passer provides over and above what could be explained by covariates like air_yards and the formation from which the pass was thrown. In Lecture 10, we will fit a series of multilevel models to divide the EPA on a play between the different offensive players. We will use the model estimates to compute a version of wins above replacement for offensive players in the NFL.",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#exercises",
    "href": "lectures/lecture09.html#exercises",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Exercises",
    "text": "Exercises\n\nArguably, a better measure of a play success than EPA is whether a play increased the offensive team’s chances of winning the game. nflfastR provides estimates of the win probability and win probability added. Repeat the modeling but with WPA instead of EPA. What is the relationship between per-pass WPA and per-pass EPA after adjusting for the same fixed effects from our full model?",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#footnotes",
    "href": "lectures/lecture09.html#footnotes",
    "title": "Lecture 9: Multilevel Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn our dataset \\(I=\\) 103.↩︎\nYou can prove that computing group-level averages is mathematically equivalent to fitting a linear regression model with a single categorical predictor encoding group membership. Try doing this yourself or come by office hours if you’d like to see the derivation.↩︎\nMathematically, we know that the average of 100 independent samples from some population has much more variance than the average of 10,000 independent samples from the same population.↩︎\nIndeed, if AJ Cole, who is a punter, were to throw many more passes, do we really think he’d consistently add around 4 expected points every time?↩︎\nFor now, we are only using one grouping variable. But later in Lecture 10, we will have multiple grouping variables.↩︎\nYurko, Ventura, and Horowitz (2019) defines air yards as “the perpendicular distance in yards from the line of scrimmage to the yard line at which the receiver was targeted or caught the ball.↩︎\nPersonally, I find the “fixed effects” and “random effects” terminology unhelpful. For one thing, the terms mean different things to different people! And, since I mostly work within the Bayesian paradigm, in which we use probability to model all of our uncertainty and not just to random noise or sampling variable, I could argue that everything is a “random.” I find it much more useful to think carefully about whether a covariate’s effect might vary across groups or whether it could be assumed to be constant. But, since you will likely see this terms in future courses or later in your careers, we’ll use them.↩︎",
    "crumbs": [
      "Lecture 9: Multilevel Modeling"
    ]
  },
  {
    "objectID": "lectures/lecture07.html",
    "href": "lectures/lecture07.html",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "",
    "text": "In Lecture 6, we computed the run value created by the offensive team in each at-bat of the 2024 MLB regular season. Run value is the sum of (i) the number of runs scored in the at-bat and (ii) the change in the number of runs the batting team is expected to score in the remainder of the half-inning. This change in expected runs is driven by the change in the combination of the number of outs and baserunner configuration. We then ranked players based on their run value totals, aggregating over all their at-bats. While the resulting rankings did appear to pass the “eye test” — both Aaron Judge and Shohei Ohtani created some of the largest run values — the metric implicitly gives batters all the credit for creating run value.\nOver the next two lectures, we will develop our own version of wins above replacement. Our development largely follows that of Baumer, Jensen, and Matthews (2015) but with some important differences.\n\n\nThe central idea — what (Baumer, Jensen, and Matthews 2015) call the “conservation of runs” framework — is that if the batting team gains \\(\\delta_{i}\\) units of run value during an at-bat, the fielding team gains \\(-\\delta_{i}\\) units of run value during that same at-bat. In this lecture, we will apportion \\(\\delta_{i}\\) between the batters (Section 4) and the baserunners involved in at-bat \\(i\\) (Section 5) In Lecture 8, we will apportion \\(-\\delta_{i}\\) between the pitcher and fielders involved in at-bat \\(i.\\)",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#overview",
    "href": "lectures/lecture07.html#overview",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "",
    "text": "In Lecture 6, we computed the run value created by the offensive team in each at-bat of the 2024 MLB regular season. Run value is the sum of (i) the number of runs scored in the at-bat and (ii) the change in the number of runs the batting team is expected to score in the remainder of the half-inning. This change in expected runs is driven by the change in the combination of the number of outs and baserunner configuration. We then ranked players based on their run value totals, aggregating over all their at-bats. While the resulting rankings did appear to pass the “eye test” — both Aaron Judge and Shohei Ohtani created some of the largest run values — the metric implicitly gives batters all the credit for creating run value.\nOver the next two lectures, we will develop our own version of wins above replacement. Our development largely follows that of Baumer, Jensen, and Matthews (2015) but with some important differences.\n\n\nThe central idea — what (Baumer, Jensen, and Matthews 2015) call the “conservation of runs” framework — is that if the batting team gains \\(\\delta_{i}\\) units of run value during an at-bat, the fielding team gains \\(-\\delta_{i}\\) units of run value during that same at-bat. In this lecture, we will apportion \\(\\delta_{i}\\) between the batters (Section 4) and the baserunners involved in at-bat \\(i\\) (Section 5) In Lecture 8, we will apportion \\(-\\delta_{i}\\) between the pitcher and fielders involved in at-bat \\(i.\\)",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-data-prep",
    "href": "lectures/lecture07.html#sec-data-prep",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo divide up offensive run value, we need to create a data table whose rows correspond to individual at-bats. This data table must, at a minimum, contain the starting and ending outs and baserunner configurations as well as the identities of the baserunners at the start and end of the at-bat. We will also want to include the columns event and des, which record the events and a narrative description of what happened in the at-bat.\n\nload(\"statcast2024.RData\")\nload(\"runValue2024.RData\")\nload(\"player2024_lookup.RData\")\nraw_atbat2024 &lt;- \n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n2    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner),\n    next_on_1b = dplyr::lead(on_1b),\n    next_on_2b = dplyr::lead(on_2b),\n    next_on_3b = dplyr::lead(on_3b)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n3    end_Outs = dplyr::last(next_Outs),\n    end_BaseRunner = dplyr::last(next_BaseRunner), # \n4    end_on_1b = dplyr::last(next_on_1b),\n    end_on_2b = dplyr::last(next_on_2b), \n    end_on_3b = dplyr::last(next_on_3b),  \n5    end_events = dplyr::last(events)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(end_bat_score = bat_score + RunsScored, end_fld_score = fld_score,\n         end_Outs = ifelse(is.na(end_Outs), 3, end_Outs)) |&gt;\n  dplyr::select(game_date, game_pk, at_bat_number, inning, inning_topbot,\n         Outs, BaseRunner, batter, on_1b, on_2b, on_3b, bat_score, fld_score, \n         end_Outs, end_BaseRunner, end_on_1b, end_on_2b, end_on_3b, end_bat_score, end_fld_score,\n         end_events, des) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\"))\n\n\n1\n\nDivide by game and half-inning\n\n2\n\nGets the value of several variables from the next pitch in the half-inning\n\n3\n\nGoes to last pitch of each at bat and gets next value of variable. That is, the starting value of the first pitch in the next at-bat.\n\n4\n\nFor instance, this looks up who’s on first at end of the current at-bat/start of the next at-bat.\n\n5\n\nThe variable events tells us what happened during the plate-appearance\n\n\n\n\n\nDealing with Missing Events\nThe column end_events in our data table raw_atbat2024 records what happened as a result of the at-bat. There are 308 rows with a missing entry.\n\ntable(raw_atbat2024$end_events)\n\n\n                                     catcher_interf                    double \n                      308                        97                      7608 \n              double_play               field_error                 field_out \n                      336                      1093                     72233 \n          fielders_choice       fielders_choice_out                 force_out \n                      373                       306                      3408 \ngrounded_into_double_play              hit_by_pitch                  home_run \n                     3152                      1977                      5326 \n                 sac_bunt                   sac_fly       sac_fly_double_play \n                      446                      1222                        13 \n                   single                 strikeout     strikeout_double_play \n                    25363                     40145                       107 \n                   triple               triple_play              truncated_pa \n                      685                         1                       304 \n                     walk \n                    14029 \n\n\nThe column des includes a much more detailed description of what happened during the plate appearance. A cursory look through the values of des corresponding to rows with missing end_events reveals that several of these at-bats ended with a walk, involved an automatic strike1, or an inning-ending pick off2\n\nraw_atbat2024 |&gt;\n  dplyr::filter(end_events == \"\") |&gt;\n  dplyr::slice_head(n = 15) |&gt;\n  dplyr::select(Outs, end_Outs, des)\n\n# A tibble: 15 × 3\n    Outs end_Outs des                                                           \n   &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                                                         \n 1     0        0 Mookie Betts walks.                                           \n 2     2        2 Freddie Freeman walks.                                        \n 3     2        3 Xander Bogaerts strikes out on automatic strike.              \n 4     2        2 Héctor Neris intentionally walks Wyatt Langford.              \n 5     2        2 Logan Webb intentionally walks Ha-Seong Kim.                  \n 6     2        2 Cole Ragans intentionally walks Carlos Santana.               \n 7     1        2 Andrew Vaughn strikes out on automatic strike.                \n 8     2        3 Oneil Cruz strikes out on automatic strike.                   \n 9     2        3 Pitcher Bryce Miller picks off Wilyer Abreu at on throw to sh…\n10     1        1 Yohan Ramírez intentionally walks Christian Yelich.           \n11     1        2 Alex Kirilloff strikes out on automatic strike.               \n12     1        1 Tony Kemp walks. James McCann to 2nd.                         \n13     2        3 With Anthony Rendon batting, Zach Neto picked off and caught …\n14     2        3 Miguel Sanó strikes out on automatic strike.                  \n15     2        3 With Vinnie Pasquantino batting, Bobby Witt Jr. picked off an…\n\n\nThe following code manually corrects the missing values for end_events\n\natbat2024 &lt;-\n  raw_atbat2024 |&gt;\n  dplyr::mutate(\n    end_events = dplyr::case_when(\n      end_events == \"\" & grepl(\"walk\", des) ~ \"walk\",\n      end_events == \"\" & grepl(\"strikes out\", des) ~ \"strikeout\",\n1      end_events == \"\" & end_Outs == 3 ~ \"truncated_pa\",\n2      end_events == \"\" & grepl(\"flies out\", des) ~ \"field_out\",\n      .default = end_events))\n\n\n1\n\nAfter accounting for the walks and strike outs on automatic strikes, all but one of the at-bats that still had a missing end_events value involved a pick-off that ended the inning\n\n2\n\nThe remaining at-bat involved a fly out that was caught in foul territory.",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-adjusted-rv",
    "href": "lectures/lecture07.html#sec-adjusted-rv",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Adjusted Run Values",
    "text": "Adjusted Run Values\nWe want to give credit to the batter and base runners for creating value over and above what would been expected given the game state and the actual outcome of the at-bat. More precisely, recall that \\(\\delta_{i}\\) is the run value created in at-bat \\(i.\\) We will denote the game state at the beginning of the at-bat with \\(\\textrm{g}_{i}\\) and the ending event with \\(\\textrm{e}_{i}.\\) We form the game state variable \\(\\textrm{g}\\) by concatenating the Outs and BaseRunners and separating them with a period so that \\(\\textrm{g} = \"0.101\"\\) corresponds to a situation with no outs and runners on first and third base.\n\natbat2024 &lt;-\n  atbat2024 |&gt;\n  dplyr::mutate(GameState = paste(Outs, BaseRunner, sep = \".\"))\n\nWe will assume that the run value created in each at-bat beginning in state \\(\\textrm{g}\\) and ending with event \\(\\textrm{e}\\) is equal to the average run value created in all at-bats with the same beginning and end plus some mean-zero error. That is, for each at-bat \\(i\\), \\[\n\\delta_{i} = \\mathbb{E}[\\delta \\vert \\textrm{g} = \\textrm{g}_{i}, \\textrm{e} = \\textrm{e}_{i}] + \\varepsilon_{i},\n\\] The average run value \\(\\mu:=\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}]\\) represents the average run value created in at-bats that begin in state \\(\\textrm{g}\\) and end with the event \\(\\textrm{e}.\\)\nIt is tempting to compute the expectation \\(\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}]\\) using the “binning-and-averaging” approach we took when developing our initial XG models back in Lecture 2. Unfortunately, such a procedure is liable to yield extreme and erratic answers as the number of bins is quite large. To wit, there are 24 distinct game states (i.e., combinations of outs and base runners) and 21 different events.\nThe 2024 dataset contains only 373 of the 504 total combinations of game state and ending event. Of the observed combinations, there is a huge disparity in the relative frequencies. Some combinations (e.g., triples with no outs and runners on second and third) occurred just once while others (e.g., striking out with no outs and nobody on) occurred close to 10,000 times.\n\natbat2024 |&gt;\n  dplyr::count(Outs, BaseRunner, end_events) |&gt;\n  dplyr::arrange(n) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n# A tibble: 10 × 4\n    Outs BaseRunner end_events                n\n   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;\n 1     0 001        catcher_interf            1\n 2     0 001        fielders_choice_out       1\n 3     0 011        triple                    1\n 4     0 101        strikeout_double_play     1\n 5     0 101        triple_play               1\n 6     1 000        strikeout              7490\n 7     0 000        strikeout              9814\n 8     2 000        field_out             11563\n 9     1 000        field_out             14573\n10     0 000        field_out             20148\n\n\nInstead of “binning and averaging”, like we did with our distance-based XG models in Lecture 3, we will fit a statistical model. A natural starting model asserts that there are numbers \\(\\alpha_{0.000}, \\ldots, \\alpha_{2.111}\\) and \\(\\alpha_{\\textrm{catcher\\_interf}}, \\ldots, \\alpha_{\\textrm{walk}}\\) such that for all game states \\(\\textrm{g}\\) and ending events \\(\\textrm{e},\\) \\[\n\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}] = \\alpha_{\\textrm{g}} + \\alpha_{\\textrm{e}}.\n\\]\nUnder the assumed model, the average run value created by hitting a single when there are two outs and no runners on is \\(\\alpha_{\\textrm{2.000}} + \\alpha_{\\textrm{single}}\\) while the average run value created by hitting a single when there are no outs and runners on first and second is \\(\\alpha_{\\textrm{0.110}} + \\alpha_{\\textrm{single}}.\\)\nBecause we do not know the exact values of the \\(\\alpha_{g}\\)’s and \\(\\alpha_{e}\\)’s, we need to estimate them using our data. Perhaps the simplest way is by solving a least squares minimization problem \\[\n\\hat{\\boldsymbol{\\alpha}} = \\textrm{argmin} \\sum_{i = 1}^{n}{(\\delta_{i} - \\alpha_{g_{i}} - \\alpha_{e_{i}})^2},\n\\] where \\(g_{i}\\) and \\(e_{i}\\) record the game state and event of at-bat \\(i.\\)\nSolving this problem is equivalent to fitting a linear regression model without an intercept3. We can do this in R using the lm() function and including -1 in the formula argument4. In the following code, we create a temporary data frame that extracts just the run values \\(\\delta\\), game states \\(\\textrm{g}\\), and ending events \\(\\textrm{e}\\) from atbats2024 and convert the game state and event variables into factors.\n\ntmp_df &lt;-\n  atbat2024 |&gt;\n  dplyr::select(RunValue, GameState, end_events) |&gt;\n  dplyr::mutate(\n    GameState = factor(GameState),\n    end_events = factor(end_events))\n\nstate_event_fit &lt;-\n  lm(RunValue ~ -1 + GameState + end_events, data = tmp_df)\n\nUsing the summary() function, we can take a quick look at the \\(\\alpha\\)’s.\n\nsummary(state_event_fit)\n\n\nCall:\nlm(formula = RunValue ~ -1 + GameState + end_events, data = tmp_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.65924 -0.10891  0.01957  0.08867  2.24020 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \nGameState0.000                       0.344318   0.024078  14.300  &lt; 2e-16 ***\nGameState0.001                       0.240733   0.027701   8.690  &lt; 2e-16 ***\nGameState0.010                       0.392981   0.024448  16.074  &lt; 2e-16 ***\nGameState0.011                       0.272952   0.026280  10.386  &lt; 2e-16 ***\nGameState0.100                       0.391419   0.024160  16.201  &lt; 2e-16 ***\nGameState0.101                       0.237505   0.025347   9.370  &lt; 2e-16 ***\nGameState0.110                       0.407975   0.024500  16.652  &lt; 2e-16 ***\nGameState0.111                       0.369993   0.025731  14.379  &lt; 2e-16 ***\nGameState1.000                       0.348466   0.024094  14.463  &lt; 2e-16 ***\nGameState1.001                       0.253997   0.024902  10.200  &lt; 2e-16 ***\nGameState1.010                       0.354195   0.024327  14.560  &lt; 2e-16 ***\nGameState1.011                       0.265455   0.025049  10.597  &lt; 2e-16 ***\nGameState1.100                       0.396792   0.024121  16.450  &lt; 2e-16 ***\nGameState1.101                       0.332078   0.024655  13.469  &lt; 2e-16 ***\nGameState1.110                       0.412645   0.024311  16.974  &lt; 2e-16 ***\nGameState1.111                       0.359217   0.024860  14.449  &lt; 2e-16 ***\nGameState2.000                       0.356264   0.024096  14.785  &lt; 2e-16 ***\nGameState2.001                       0.346744   0.024549  14.124  &lt; 2e-16 ***\nGameState2.010                       0.318496   0.024256  13.131  &lt; 2e-16 ***\nGameState2.011                       0.321323   0.024888  12.911  &lt; 2e-16 ***\nGameState2.100                       0.351540   0.024132  14.567  &lt; 2e-16 ***\nGameState2.101                       0.363065   0.024474  14.835  &lt; 2e-16 ***\nGameState2.110                       0.354515   0.024269  14.608  &lt; 2e-16 ***\nGameState2.111                       0.338946   0.024651  13.750  &lt; 2e-16 ***\nend_eventsdouble                     0.398473   0.024206  16.461  &lt; 2e-16 ***\nend_eventsdouble_play               -1.293011   0.027320 -47.328  &lt; 2e-16 ***\nend_eventsfield_error                0.097087   0.025100   3.868  0.00011 ***\nend_eventsfield_out                 -0.589994   0.024071 -24.510  &lt; 2e-16 ***\nend_eventsfielders_choice            0.342555   0.027015  12.680  &lt; 2e-16 ***\nend_eventsfielders_choice_out       -0.963020   0.027678 -34.794  &lt; 2e-16 ***\nend_eventsforce_out                 -0.713014   0.024403 -29.219  &lt; 2e-16 ***\nend_eventsgrounded_into_double_play -1.195243   0.024442 -48.901  &lt; 2e-16 ***\nend_eventshit_by_pitch              -0.001119   0.024636  -0.045  0.96376    \nend_eventshome_run                   1.043303   0.024271  42.985  &lt; 2e-16 ***\nend_eventssac_bunt                  -0.443274   0.026600 -16.664  &lt; 2e-16 ***\nend_eventssac_fly                   -0.341349   0.025120 -13.589  &lt; 2e-16 ***\nend_eventssac_fly_double_play       -0.887624   0.070048 -12.672  &lt; 2e-16 ***\nend_eventssingle                     0.115000   0.024099   4.772 1.83e-06 ***\nend_eventsstrikeout                 -0.618027   0.024083 -25.662  &lt; 2e-16 ***\nend_eventsstrikeout_double_play     -1.041349   0.033231 -31.337  &lt; 2e-16 ***\nend_eventstriple                     0.704276   0.025700  27.404  &lt; 2e-16 ***\nend_eventstriple_play               -2.133826   0.238222  -8.957  &lt; 2e-16 ***\nend_eventstruncated_pa              -0.625189   0.026806 -23.323  &lt; 2e-16 ***\nend_eventswalk                      -0.024163   0.024135  -1.001  0.31675    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2369 on 178488 degrees of freedom\nMultiple R-squared:  0.7682,    Adjusted R-squared:  0.7682 \nF-statistic: 1.344e+04 on 44 and 178488 DF,  p-value: &lt; 2.2e-16\n\n\nWe estimate \\(\\hat{\\alpha}_{2.000} \\approx 0.356\\) and \\(\\hat{\\alpha}_{single} \\approx 0.115.\\) So, according to our fitted model the average run value created by singling when there are two outs and no runners on is about \\(0.471.\\)\n\n\n\n\n\n\nStatistical Significance & Model Assumptions\n\n\n\nYou’ll notice that summary() returns a lot of inferential output (e.g., standard errors, p-values). These are computed under an additional assumption that the true errors \\(\\varepsilon_{i}\\) are independent and following a mean-zero normal distribution with constant variance. Since our main interest is prediction, we’re really not interested in checking whether, say, \\(\\alpha_{0:010}\\) is statistically significantly different than zero. So, we will not check whether the usual multiple linear model assumptions nor will we attempt. If you did want to make inferential statements about our model parameters, you would need to first check that the multiple linear multiple assumptions are not grossly violated.\n\n\nEquipped with our estimated model parameters, for each at-bat \\(i,\\) let \\(\\hat{\\mu}_{i} = \\hat{\\alpha}_{\\textrm{g}_{i}} + \\hat{\\alpha}_{\\textrm{e}_{i}}\\) and let \\(\\eta_{i} = \\delta_{i} - \\hat{\\mu}_{i}.\\) In terms of dividing credit between the batter and the base runner, we will follow Baumer, Jensen, and Matthews (2015) and attribute \\(\\hat{\\mu}_{i}\\) to the batter’s hitting in at-bat \\(i\\) and \\(\\eta_{i}\\) to the base running in that at-bat. We will add columns to atbat2024 holding the values of \\(\\hat{\\mu}\\) (mu) and \\(\\eta\\) (eta).\n\natbat2024$eta &lt;- state_event_fit$residuals\natbat2024$mu &lt;- state_event_fit$fitted.values\nsave(atbat2024, file = \"atbat2024.RData\")",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-baserunner",
    "href": "lectures/lecture07.html#sec-baserunner",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Baserunning Run Value",
    "text": "Baserunning Run Value\nOhtani’s second hit against the Padres on March 20, 2024 was a single with runners on first and second and no outs. While Ohtani and the runner originally on first advanced one base, the runner originally on second scored. Because this latter runner advanced more than what might have been otherwise expected, it makes sense to give him a larger share of the \\(\\eta_{i}\\) than to the first two runners, who only advanced one base on a single. Following (Baumer, Jensen, and Matthews 2015, sec. 3.2), the amount of base running run value \\(\\eta_{i}\\) that we assign to base runner \\(j\\) in at-bat \\(i\\) will be proportional to \\(\\kappa_{ij} = \\mathbb{P}(K &lt; k_{ij} \\vert \\textrm{e}_{i}),\\) where \\(k_{ij}\\) is the number of bases actually advanced by the base runner.\nEssentially, \\(\\kappa_{ij}\\) is the probability that a typical base runner advanced at most the \\(k_{ij}\\) bases advanced by base runner \\(j\\) in at-bat \\(i\\) following event \\(\\textrm{e}_{i}.\\) If the base runner does worse than expected (e.g., not advancing from second on a single), then \\(\\kappa_{ij}\\) will be very small. But if the base runner does better than expected (e.g., scoring from second on a single), then \\(\\kappa_{ij}\\) will be larger. When computing \\(\\kappa_{ij}\\) it is crucial that we condition on the actual ending event \\(\\textrm{e}_{i}.\\) After all, while we may want to penalize a runner for not advancing from second on a single, we definitely don’t want to penalize a runner for not advancing from second following a strike out!\n\nBaserunner Advancement\nUnfortunately, StatCast does not compute the number of bases that each runner advances during each at-bat. The following code implements a function that determines the number of bases advanced by the runner on first (if any). It works by first checking whether there is anyone on 1b at the start of the at-bat. If so, it checks whether that player is on first, second, or third base at the end of the at-bat. If not, it parses the at-bat description contained in des and looks for a sentence containing the player’s name. If that sentence contains the words “out” or “caught stealing”, it sets the number of bases advanced to 0. But, if the sentence contains the word “score”, it sets the number of bases advanced to 3, since the runner scored from first.\n\nload(\"player2024_lookup.RData\")\n#| label: mvt-1b-function\nmvt_1b &lt;- function(on_1b, Outs, bat_score,\n                   end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_1b)){\n    # there was someone on 1st base at the start of the at-bat\n1    if(!is.na(end_on_1b) & on_1b == end_on_1b) mvt &lt;- 0\n2    if(!is.na(end_on_2b) & on_1b == end_on_2b) mvt &lt;- 1\n3    if(!is.na(end_on_3b) & on_1b == end_on_3b) mvt &lt;- 2\n\n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on first\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_1b)]\n      # Start by splitting it a string\n      play_split &lt;- \n        stringr::str_split_1(\n          string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n          pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      \n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 3 # player scored from 1st\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      } \n    }\n  } \n  return(mvt)\n}\n\n\n1\n\nRunner remained on first, so they advanced 0 bases\n\n2\n\nRunner advanced 1 base (first to second)\n\n3\n\nRunner advanced 2 bases (first to third)\n\n\n\n\nWe similarly define functions to track the number of bases advanced by the runners on second and third base and by the batter. For brevity, we have folded the code.\n\n\nShow code for computing the number of bases advanced by the batter and the runners on 2nd and 3rd base.\nmvt_2b &lt;- function(on_2b, Outs, bat_score,\n                   end_on_2b, end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_2b)){\n    # there was someone on 2nd base at the start of the at-bat\n    if(!is.na(end_on_2b) & on_2b == end_on_2b) mvt &lt;- 0 # runner remained on 2nd\n    if(!is.na(end_on_3b) & on_2b == end_on_3b) mvt &lt;- 1 # runner advanced to 3rd\n    \n    #if(end_Outs == 3) mvt &lt;- 0 # inning ended ; there may be some edge cases here\n    # e.g., in last at-bat there may be a wild pitch\n    # https://www.espn.com/mlb/playbyplay/_/gameId/401568474 where runner scores and then batter gets out to end the inning\n    \n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on second\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_2b)]\n      # Start by splitting it a string\n      play_split &lt;- \n        stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                    pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      \n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 2 # player scored from 2nd\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      } \n    }\n  } \n  return(mvt)\n}\n\n\nmvt_3b &lt;- function(on_3b, Outs, bat_score,\n                   end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_3b)){\n    if(!is.na(end_on_3b) & on_3b == end_on_3b) mvt &lt;- 0 # runner remained on 3rd\n    \n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on second\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_3b)]\n      play_split &lt;- \n        stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                    pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 1 # player scored from 3rd\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      }\n    }\n  } \n  return(mvt)\n}\n\n\nmvt_batter &lt;- function(batter, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score, des)\n{\n  mvt &lt;- NA\n  if(!is.na(end_on_1b) & batter == end_on_1b) mvt &lt;- 1 # batter advanced to 1st\n  else if(!is.na(end_on_2b) & batter == end_on_2b) mvt &lt;- 2 # batter advanced to 2nd\n  else if(!is.na(end_on_3b) & batter == end_on_3b) mvt &lt;- 3 # batter advanced to 3rd\n  else{\n    # batter is not on base\n    # look up player name\n    player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == batter)]\n    \n    play_split &lt;-\n      stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                           pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n    \n    check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n    if(any(check)){\n      # found something with player name in it\n      play &lt;- play_split[check]\n      if( any(grepl(pattern = \"out\", x = play))) mvt &lt;- 0 # player got out\n      else if(any(grepl(pattern = \"score\", x = play) | grepl(pattern = \"home\", x = play))) mvt &lt;- 4 # batter scored\n      else if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      else mvt &lt;- NA\n    }\n  }\n  return(mvt)\n}\n\n\nWe can now apply these functions to every row of our data frame.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code takes a few minutes to run.\n\n\n\nbaserunning &lt;-\n  atbat2024 |&gt;\n  dplyr::rowwise() |&gt;\n  dplyr::mutate(\n    mvt_batter = mvt_batter(batter, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score, des),\n    mvt_1b = mvt_1b(on_1b, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b,end_Outs, end_bat_score, des),\n    mvt_2b = mvt_2b(on_2b, Outs, bat_score, end_on_2b, end_on_3b, end_Outs, end_bat_score, des),\n    mvt_3b = mvt_3b(on_3b, Outs, bat_score, end_on_3b, end_Outs, end_bat_score, des)) |&gt;\n  dplyr::ungroup() \n\n\n\nCumulative Baserunning Probabilities\nNow that we have computed the \\(k_{ij}\\)’s — that is, the number of bases each base runner advanced in each at-bat — we are ready to compute the cumulative base running probabilities \\(\\mathbb{P}(K \\leq k  \\vert \\textrm{e}).\\) In the following code, we first group at-bats by the ending event and then compute the proportion of times that the baserunner advances at most \\(k\\) bases. We also set the cumulative probability to zero for situations when there isn’t a runner on a particular base.\n\nbr_batter_probs &lt;-\n  baserunning |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_batter &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_batter &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_batter &lt;= 2, na.rm = TRUE),\n    kappa_3 = mean(mvt_batter &lt;= 3, na.rm = TRUE),\n    kappa_4 = mean(mvt_batter &lt;= 4, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_batter\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_batter\") |&gt;\n  dplyr::mutate(\n    mvt_batter = ifelse(mvt_batter == \"NA\", NA, mvt_batter),\n    mvt_batter = as.numeric(mvt_batter))\nbr_1b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_1b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_1b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_1b &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_1b &lt;= 2, na.rm = TRUE),\n    kappa_3 = mean(mvt_1b &lt;= 3, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_1b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_1b\") |&gt;\n  dplyr::mutate(mvt_1b = ifelse(mvt_1b == \"NA\", NA, mvt_1b),\n                mvt_1b = as.numeric(mvt_1b))\n\nbr_2b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_2b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_2b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_2b &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_2b &lt;= 2, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_2b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_2b\") |&gt;\n  dplyr::mutate(mvt_2b = ifelse(mvt_2b == \"NA\", NA, mvt_2b),\n                mvt_2b = as.numeric(mvt_2b))\n\nbr_3b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_3b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_3b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_3b &lt;= 1, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_3b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_3b\") |&gt;\n  dplyr::mutate(mvt_3b = ifelse(mvt_3b == \"NA\", NA, mvt_3b),\n                mvt_3b = as.numeric(mvt_3b))\n\nThe table br_1b_probs contains the cumulative base running probabilities for runners who start on first base broken down by ending event. We find that in about 64.7% of singles, the runner on first advances one base or fewer while in 95.7% of singles, the runner on first advances two bases or fewer.\n\nbr_1b_probs |&gt;\n  dplyr::filter(end_events == \"single\")\n\n# A tibble: 5 × 3\n  end_events mvt_1b kappa_1b\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 single          0   0.0194\n2 single          1   0.647 \n3 single          2   0.957 \n4 single          3   1     \n5 single         NA   0     \n\n\n\n\nBaserunning Runs Above Average\nNow that we have the cumulative base running probabilities, we’re (finally) ready to compute \\(\\kappa_{ij}.\\) To do so, we will use inner_join()’s to add columns to our baserunning data table with columns for the batter and runners on first, second, and third. Note, whenever there is no baserunner on first base (i.e., on_1b = NA), we will set the corresponding \\(\\kappa\\) to 0. Because we want to divide all of \\(\\eta_{i}\\) amongst the base runners, we need to normalize the \\(\\kappa_{ij}\\) values to sum to 1 within each at-bat. The columns norm_batter, norm_1b, norm_2b, and norm_3b contain these normalized weights.\n\nbaserunning &lt;-\n  baserunning |&gt;\n  dplyr::inner_join(y = br_batter_probs, by = c(\"end_events\", \"mvt_batter\")) |&gt;\n  dplyr::inner_join(y = br_1b_probs, by = c(\"end_events\", \"mvt_1b\")) |&gt;\n  dplyr::inner_join(y = br_2b_probs, by = c(\"end_events\", \"mvt_2b\")) |&gt;\n  dplyr::inner_join(y = br_3b_probs, by = c(\"end_events\", \"mvt_3b\")) |&gt;\n  dplyr::mutate(\n    total_kappa = kappa_batter + kappa_1b + kappa_2b + kappa_3b,\n    norm_batter = kappa_batter/total_kappa,\n    norm_1b = kappa_1b/total_kappa,\n    norm_2b = kappa_2b/total_kappa,\n    norm_3b = kappa_3b/total_kappa)\n\nTo illustrate our calculations so far, let’s look at Ohtani’s at-bats from that game against the Padres. First, we see that Ohtani reached first base in all except his fourth at-bat. So, for these four at-bats, his mvt_batter value is 1.\n\nload(\"player2024_lookup.RData\")\nohtani_id &lt;- \n  player2024_lookup |&gt;\n  dplyr::filter(FullName == \"Shohei Ohtani\") |&gt;\n  dplyr::pull(key_mlbam)\n\nbaserunning |&gt;\n  dplyr::filter(game_pk == 745444 & batter == ohtani_id) |&gt;\n  dplyr::select(at_bat_number, mvt_batter, des)\n\n# A tibble: 5 × 3\n  at_bat_number mvt_batter des                                                  \n          &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                                                \n1             2          1 Shohei Ohtani grounds into a force out, shortstop Ha…\n2            18          1 Shohei Ohtani singles on a sharp line drive to right…\n3            37          1 Shohei Ohtani grounds into a force out, third basema…\n4            52          0 Shohei Ohtani grounds out softly, pitcher Wandy Pera…\n5            65          1 Shohei Ohtani singles on a line drive to left fielde…\n\n\nIn his second at-bat, Ohtani singled with no runners on. So, he should get credit for creating all the base running run value above average on that at-bat. In contrast, we argued earlier that when he drove in a run in his fifth at-bat, the runner who scored from second should get a bit more credit than Ohtani and the runner on first, who only advanced one base. Looking at the weights norm_1b, norm_2b, and norm_batter for this at-bat, we see that indeed, we’re assigning a bit more weight to the runner on second than the runner on first.\n\nbaserunning |&gt;\n  dplyr::filter(game_pk == 745444 & batter == ohtani_id) |&gt;\n  dplyr::select(at_bat_number, mvt_1b, mvt_2b, mvt_3b, mvt_batter, norm_1b, norm_2b, norm_3b, norm_batter)\n\n# A tibble: 5 × 9\n  at_bat_number mvt_1b mvt_2b mvt_3b mvt_batter norm_1b norm_2b norm_3b\n          &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1             2      0     NA     NA          1   0.491   0           0\n2            18     NA     NA     NA          1   0       0           0\n3            37      0     NA     NA          1   0.491   0           0\n4            52     NA     NA     NA          0   0       0           0\n5            65      1      2     NA          1   0.247   0.382       0\n# ℹ 1 more variable: norm_batter &lt;dbl&gt;\n\n\nRecall that \\(\\eta_{i}\\) represents the run value above average generated in at-bat \\(i\\) due to base running. Whenever there is a runner on first at the start of the at-bat, the quantity \\(\\kappa_{i,\\textrm{1b}}/\\sum_{j}{\\kappa_{ij}} \\times \\eta_{i}\\) reflects the run value above average generated in the at-bat due to the base running of the runner initially on first. For each player, we can aggregate these values across all at-bats in which they are on first.\n\nraa_1b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_1b)) |&gt;\n  dplyr::mutate(RAA_1b = norm_1b * eta) |&gt;\n  dplyr::group_by(on_1b) |&gt;\n  dplyr::summarise(RAA_1b = sum(RAA_1b)) |&gt;\n  dplyr::rename(key_mlbam = on_1b)\n\nWe can similarly compute the run value above average created by the runners on second base and third base as well as by the batter.\n\n\nCompute the baserunning runs above average created by batter and the runners on second and third bases\nraa_2b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_2b)) |&gt;\n  dplyr::mutate(RAA_2b = norm_2b * eta) |&gt;\n  dplyr::group_by(on_2b) |&gt;\n  dplyr::summarise(RAA_2b = sum(RAA_2b)) |&gt;\n  dplyr::rename(key_mlbam = on_2b)\n\nraa_3b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_3b)) |&gt;\n  dplyr::mutate(RAA_3b = norm_3b * eta) |&gt;\n  dplyr::group_by(on_3b) |&gt;\n  dplyr::summarise(RAA_3b = sum(RAA_3b)) |&gt;\n  dplyr::rename(key_mlbam = on_3b)\n\nraa_batter &lt;-\n  baserunning |&gt;\n  dplyr::mutate(RAA_batter = norm_batter * eta) |&gt;\n  dplyr::group_by(batter) |&gt;\n  dplyr::summarise(RAA_batter = sum(RAA_batter)) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\n\nFinally, we can aggregate the total run value above average that each player creates from their base running, which we can \\(\\textrm{RAA}^{\\textrm{br}}.\\)\n\nraa_br &lt;-\n  raa_batter |&gt;\n  dplyr::full_join(y = raa_1b, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_2b, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_3b, by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(list(RAA_batter = 0, RAA_1b = 0, RAA_2b = 0, RAA_3b = 0)) |&gt;\n  dplyr::mutate(RAA_br = RAA_batter + RAA_1b + RAA_2b + RAA_3b) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_br, RAA_batter, RAA_1b, RAA_2b, RAA_3b)\n\nInterestingly, the player with the largest \\(\\textrm{RAA}^{\\textrm{br}}\\), Jose Ramirez, is known for his aggressive baserunning5.",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-batting",
    "href": "lectures/lecture07.html#sec-batting",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Crediting Batters",
    "text": "Crediting Batters\nWe will credit \\(\\hat{\\mu}_{i}\\) to each batter. The following code block totals the \\(\\hat{\\mu}_{i}\\) for each batter.\n\nbatting &lt;- \n  atbat2024 |&gt;\n  dplyr::select(batter, mu) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\nraa_b &lt;-\n  batting |&gt;\n  dplyr::group_by(key_mlbam) |&gt;\n  dplyr::summarise(RAA_b = sum(mu)) |&gt;\n  dplyr::left_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_b)\n\nWe find a lot of very good batters among the players with highest \\(\\textrm{RAA}^{\\textrm{b}}\\)’s\n\nraa_b |&gt;\n  dplyr::arrange(dplyr::desc(RAA_b)) |&gt;\n  dplyr::slice_head(n=10)\n\n# A tibble: 10 × 2\n   Name              RAA_b\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Aaron Judge        84.6\n 2 Juan Soto          64.3\n 3 Shohei Ohtani      59.6\n 4 Bobby Witt         59.0\n 5 Vladimir Guerrero  45.0\n 6 Gunnar Henderson   42.7\n 7 Ketel Marte        41.3\n 8 Jose Ramirez       35.9\n 9 Brent Rooker       34.8\n10 William Contreras  33.5",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#sec-looking-ahead",
    "href": "lectures/lecture07.html#sec-looking-ahead",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nWe’ve distributed the run value \\(\\delta_{i}\\) created in each at-bat between the batter and base runners and computed season total runs values above average based on batting \\(\\textrm{RAA}^{\\textrm{b}}\\) and base running \\(\\textrm{RAA}^{\\textrm{br}}.\\) Next lecture, we will distribute \\(-\\delta_{i}\\) between the pitcher and fielders involved in each at-bat. So that we don’t have to repeat our earlier calculations, we will save raa_br and raa_b\n\nsave(raa_br, raa_b, file = \"raa_offensive2024.RData\")",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#footnotes",
    "href": "lectures/lecture07.html#footnotes",
    "title": "Lecture 7: Offensive Credit Allocation in Baseball",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStarting in 2023, Major League Baseball implemented a pitch timer. Batters who were not in the batter’s box and alert to the pitcher by the 8-second mark of the timer are penalized with an automatic strike. See the rules here.↩︎\nWhen this happens, Statcast usually records it as a truncated plate appearance (truncated_pa).↩︎\nTry proving this mathematically!↩︎\nCheck out the documentation for R’s formula interface here. Specifically, look at the bullet point about the - operations under “Details”↩︎\nSee this article about his base running from earlier this year.↩︎",
    "crumbs": [
      "Lecture 7: Offensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture05.html",
    "href": "lectures/lecture05.html",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "",
    "text": "In Lecture 4, we introduced adjusted plus/minus, which attempts to estimate how much value an NBA player creates after adjusting for the quality of his teammates and opponents. As noted in that lecture, using the method of least squares to estimate APM values is problematic because it assumes that all baseline players have the exact same partial effect on their team’s average point differential per 100 possessions. In this lecture, we solve a penalized version of the least squares problem (Section 2) to fit a regularized adjusted plus/minus model that avoids the need to specify baseline players (Section 3). We then use the bootstrap (Section 4) to quantify uncertainty about differences between players and the overall RAPM rankings.\n\n\nWe begin by reviewing the notation and set-up from Lecture 4. First, let \\(n\\) denote the number of stints and \\(p\\) the total number of players. For each stint \\(i = 1, \\ldots, n\\) and player \\(j = 1, \\ldots, p,\\) let \\(x_{ij}\\) be the signed on-court indicator for player \\(j\\) during stint \\(i.\\) That is, \\(x_{ij} = 1\\) (resp. \\(x_{ij} = -1)\\) if player \\(j\\) is on the court and playing at home (resp. on the road) during stint \\(i\\) and \\(x_{ij} = 0\\) if player \\(j\\) is not on the court during stint \\(i.\\) We can arrange these indicators into a large \\(n \\times p\\) matrix \\(\\boldsymbol{\\mathbf{X}}\\) whose rows (resp. columns) correspond to stints (resp. players). Finally, let \\(Y_{i}\\) be the point differential per 100 possessions observed during shift \\(i.\\)\nThe original APM model introduced a single number \\(\\alpha_{j}\\) for each player \\(j = 1, \\ldots, p,\\) and modeled \\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\] where \\(h_{1}(i), \\ldots, h_{5}(i)\\) and \\(a_{1}(i), \\ldots, a_{5}(i)\\) are, respectively, the indices of the homes and away team players on the court during stint \\(i\\); \\(\\alpha_{0}\\) captures the average home court advantage in terms of point-differential per 100 possessions across all teams; and the \\(\\epsilon_{i}\\)’s are independent random errors drawn from a distribution with mean zero.\nIn Lecture 4, we formed the \\(n \\times (p+1)\\) matrix \\(\\boldsymbol{\\mathbf{Z}}\\) by appending a column of 1’s to the matrix \\(\\boldsymbol{\\mathbf{X}}\\) and let \\(\\boldsymbol{\\alpha} = (\\alpha_{0}, \\alpha_{1}, \\ldots, \\alpha_{p})^{\\top}\\) be the vector of length \\(p+1\\) containing the home court advantage \\(\\alpha_{0}\\) and all the player-specific parameters \\(\\alpha_{j}.\\) Letting \\(\\boldsymbol{\\mathbf{z}}_{i}\\) be the \\(i\\)-th row of \\(\\boldsymbol{\\mathbf{Z}},\\) we can write the APM model in a more compact form. \\[\nY_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}.\n\\]\nAt the end of Lecture 4, we saved a copy of the full matrix \\(\\boldsymbol{\\mathbf{X}}\\) (which we called X_full) of signed on-court indicators, a vector of point differentials per 100 possessions \\(\\boldsymbol{Y}\\) (Y), and a look-up table containing player id numbers and names. We load those objects into our environment.\n\nload(\"lecture04_05_data.RData\")",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#sec-overview",
    "href": "lectures/lecture05.html#sec-overview",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "",
    "text": "In Lecture 4, we introduced adjusted plus/minus, which attempts to estimate how much value an NBA player creates after adjusting for the quality of his teammates and opponents. As noted in that lecture, using the method of least squares to estimate APM values is problematic because it assumes that all baseline players have the exact same partial effect on their team’s average point differential per 100 possessions. In this lecture, we solve a penalized version of the least squares problem (Section 2) to fit a regularized adjusted plus/minus model that avoids the need to specify baseline players (Section 3). We then use the bootstrap (Section 4) to quantify uncertainty about differences between players and the overall RAPM rankings.\n\n\nWe begin by reviewing the notation and set-up from Lecture 4. First, let \\(n\\) denote the number of stints and \\(p\\) the total number of players. For each stint \\(i = 1, \\ldots, n\\) and player \\(j = 1, \\ldots, p,\\) let \\(x_{ij}\\) be the signed on-court indicator for player \\(j\\) during stint \\(i.\\) That is, \\(x_{ij} = 1\\) (resp. \\(x_{ij} = -1)\\) if player \\(j\\) is on the court and playing at home (resp. on the road) during stint \\(i\\) and \\(x_{ij} = 0\\) if player \\(j\\) is not on the court during stint \\(i.\\) We can arrange these indicators into a large \\(n \\times p\\) matrix \\(\\boldsymbol{\\mathbf{X}}\\) whose rows (resp. columns) correspond to stints (resp. players). Finally, let \\(Y_{i}\\) be the point differential per 100 possessions observed during shift \\(i.\\)\nThe original APM model introduced a single number \\(\\alpha_{j}\\) for each player \\(j = 1, \\ldots, p,\\) and modeled \\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\] where \\(h_{1}(i), \\ldots, h_{5}(i)\\) and \\(a_{1}(i), \\ldots, a_{5}(i)\\) are, respectively, the indices of the homes and away team players on the court during stint \\(i\\); \\(\\alpha_{0}\\) captures the average home court advantage in terms of point-differential per 100 possessions across all teams; and the \\(\\epsilon_{i}\\)’s are independent random errors drawn from a distribution with mean zero.\nIn Lecture 4, we formed the \\(n \\times (p+1)\\) matrix \\(\\boldsymbol{\\mathbf{Z}}\\) by appending a column of 1’s to the matrix \\(\\boldsymbol{\\mathbf{X}}\\) and let \\(\\boldsymbol{\\alpha} = (\\alpha_{0}, \\alpha_{1}, \\ldots, \\alpha_{p})^{\\top}\\) be the vector of length \\(p+1\\) containing the home court advantage \\(\\alpha_{0}\\) and all the player-specific parameters \\(\\alpha_{j}.\\) Letting \\(\\boldsymbol{\\mathbf{z}}_{i}\\) be the \\(i\\)-th row of \\(\\boldsymbol{\\mathbf{Z}},\\) we can write the APM model in a more compact form. \\[\nY_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}.\n\\]\nAt the end of Lecture 4, we saved a copy of the full matrix \\(\\boldsymbol{\\mathbf{X}}\\) (which we called X_full) of signed on-court indicators, a vector of point differentials per 100 possessions \\(\\boldsymbol{Y}\\) (Y), and a look-up table containing player id numbers and names. We load those objects into our environment.\n\nload(\"lecture04_05_data.RData\")",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#sec-ridge",
    "href": "lectures/lecture05.html#sec-ridge",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nUnfortunately, a unique least squares estimator of \\(\\boldsymbol{\\alpha}\\) does not exist. This is because the matrix \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) does not have a unique inverse[^rankdeficient]. Instead, there are infinitely many different vector \\(\\boldsymbol{\\alpha}\\) that minimize the quantity \\[\n\\sum_{i = 1}^{n}{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}},\n\\] Complicating matters further, the data do not provide any guidance from picking between these minimizers as they all yield the same sum of squared errors. The original developers of adjusted plus/minus overcame this issue by (i) specifying a group of baseline-level players; (ii)\nRather than minimize the sum of the squared errors, we will instead try to minimize the quantity \\[\n\\sum_{i = 1}^{n}{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}} + \\lambda \\times \\sum_{j = 0}^{p}{\\alpha_{j}^{2}},\n\\] where \\(\\lambda &gt; 0\\) is a fixed number1.\nThe first term is the familiar sum of squared errors \\(\\sum{(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha})^{2}}.\\) The second term \\(\\sum_{j}{\\alpha_{j}^{2}}\\) is known as the shrinkage penalty because it is small whenever all the \\(\\alpha_{j}\\)’s are close to zero (i.e., when they are shrunk towards zero). Minimizing the penalized sum of squares trades off finding \\(\\alpha_{j}\\)’s that best fit the data (i.e., minimize the squared error) and finding \\(\\alpha_{j}\\)’s that are not too far away from zero. When \\(\\lambda\\) is very close to zero, the shrinkage penalty tends to exert little influence but as \\(\\lambda \\rightarrow \\infty,\\) the penalty dominates and the minimizer converges to the vector of all zeros. Consequently, it is imperative that we select a good value of \\(\\lambda.\\)\nBefore discussing how we set the tuning parameter \\(\\lambda\\) in practice, it is worth pointing out that for every \\(\\lambda &gt; 0,\\) a unique solution to the penalized sum of squares problems exists and is available in closed-form: \\[\n\\hat{\\boldsymbol{\\alpha}}(\\lambda) = \\left(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}} + \\lambda I \\right)^{-1}\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Y}},\n\\] Looking at this formula, we notice that the expression is very nearly identical to the general formula for ordinary least squares. The only difference is that we’ve added \\(\\lambda\\) to the diagonals of \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\), which guarantees that the matrix \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}} + \\lambda I\\) has a unique inverse. This technique of adding a small perturbation to \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) in the least squares formula — which is equivalent to solving the penalized sum of squares problem — is known as ridge regression.2\n\nPicking \\(\\lambda\\) with Cross-Validation\nIn practice, we typically pick a value of \\(\\lambda\\) using cross-validation The basic idea is to select the value of \\(\\lambda\\) that results in the smallest out-of-sample prediction error. Since we don’t have any out-of-sample data, we will instead approximate it using the technique from Lecture 3 in which we repeatedly held out a subset of the data when training our model and then assessed the error on the held-out data. This process is known as “cross-validation.”\nMore specifically, we begin by specifying a large grid of potential \\(\\lambda\\) values. Then, for each \\(\\lambda\\) in the grid, we repeatedly iterate through the following steps. 1. Form a random training/testing split 2. Fit a ridge regression model with the current value of \\(\\lambda\\) using only the training subset to obtain an estimate \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda)\\) 3. Compute the average of the squared errors \\(\\left(Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\hat{\\boldsymbol{\\alpha}}(\\lambda) \\right)\\) in the testing subset. After iterating through these steps many times for a single \\(\\lambda,\\) we compute the average out-of-sample errors (across the different training/testing splits) before moving onto the next \\(\\lambda\\) value in the grid.\nAt the end of this process, we have an estimate for the out-of-sample mean square error for every \\(\\lambda\\) and can pick the \\(\\lambda\\) that yields the smallest one. Then, we can fit a ridge regression model using all of the data to obtain our final solution.",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#sec-rapm",
    "href": "lectures/lecture05.html#sec-rapm",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Regularized Adjusted Plus/Minus",
    "text": "Regularized Adjusted Plus/Minus\nTo recap, by fitting a ridge regression model instead of an ordinary linear model, we are able to estimate an adjusted plus/minus value for all players, thereby avoiding having to specify an arbitrary baseline-level. Although this is conceptually easy, finding a suitable \\(\\lambda\\) via cross-validation is quite an involved process. Luckily for us, the function cv.glmnet() from the glmnet package automates the process (including specifying the initial grid).\nThe following code shows how to perform the cross-validation, plots the cross-validation error for each value of \\(\\lambda\\), and selects the \\(\\lambda\\) with smallest error.\n\n1set.seed(479)\n2library(glmnet)\ncv_fit &lt;-\n3  cv.glmnet(x = X_full, y = Y,\n4            alpha = 0,\n5            standardize = FALSE)\n6lambda_min &lt;- cv_fit$lambda.min\n\n\n1\n\nBecause cross-validation involves forming random training/testing splits, setting the randomization seed ensures our results are reproducible.\n\n2\n\nLoad the glmnet package into our R environment\n\n3\n\nUnlike lm, glmnet functions do not use a formula argument. Instead, you must manually pass the design matrix (without intercept) and output vector.\n\n4\n\nThe argument alpha = 0 tells glmnet to use a squared error penalty.\n\n5\n\nIt is extremely important to set standardize = FALSE. If you don’t, you end up over-shrinking the APM estimates of players with lots of playing time and under-shrinking APM estimates for players with little playing time\n\n6\n\nExtracts the value of \\(\\lambda\\) with smallest out-of-sample error\n\n\n\n\nWe can plot the out-of-sample error as a function of \\(\\lambda\\):\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(cv_fit)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nPlots the out-of-sample error as a function of \\(\\lambda.\\)\n\nWe can now fit a ridge regression model with the cross-validated choice of \\(\\lambda\\) using data from all our stints. Because we do not need to run cross-validation again, we use the function glmnet instead of cv.glmnet.\nIn the following code, we actually fit a ridge regression model using all values of \\(\\lambda\\) from the grid constructed by cv.glmnet(). This is because the function glmnet can behave a little strangely when only one value of \\(\\lambda\\) is specified3. Then, we pull out the estimated coefficients corresponding to the optimal \\(\\lambda\\) value.\n\n1lambda_index &lt;- which(cv_fit$lambda == lambda_min)\nfit &lt;-\n  glmnet(x = X_full, y = Y,\n         alpha = 0,\n2         lambda = cv_fit$lambda,\n         standardize = FALSE)\n\n3alpha_hat &lt;- fit$beta[,lambda_index]\n\n4rapm &lt;-\n  data.frame(\n    id = names(alpha_hat),\n    rapm = alpha_hat) |&gt;\n  dplyr::inner_join(y = player_table |&gt; dplyr::select(id,Name), by = \"id\")\n\n\n1\n\nDetermines where in the grid of \\(\\lambda\\) values specified by cv_glmnet() the optimal one lies.\n\n2\n\nSolves the ridge regression problem at all values of \\(\\lambda\\) along the grid specified by the initial cv_glmnet() fit.\n\n3\n\nglmnet() returns a matrix of parameter estimates where the columns correspond to the different \\(\\lambda\\) values. Here, we pull out the estimates for the optimal \\(\\lambda\\), which was the second value provided.\n\n4\n\nLike we did in Lecture 4, we create a data frame with every players’ regularized adjusted plus/minus value and name\n\n\n\n\nWe recognize many super-star-level players (e.g., Gilgeous-Alexander, Dončić, Jokić, Giannis) among those with the highest regularized adjusted plus/minus values.\n\nrapm |&gt;\n  dplyr::arrange(dplyr::desc(rapm)) |&gt;\n  dplyr::slice_head(n = 10)\n\n        id     rapm                    Name\n1  1628983 3.602143 Shai Gilgeous-Alexander\n2  1627827 3.472249     Dorian Finney-Smith\n3  1630596 3.415232             Evan Mobley\n4  1629029 3.352736             Luka Doncic\n5   202699 3.183716           Tobias Harris\n6   203999 2.846327            Nikola Jokic\n7  1631128 2.829463         Christian Braun\n8  1628384 2.813794              OG Anunoby\n9   203507 2.646488   Giannis Antetokounmpo\n10 1626157 2.641075      Karl-Anthony Towns\n\n\nRecall that in our regularized adjusted plus/minus model, the individual \\(\\alpha_{j}\\)’s correspond to a somewhat absurd counter-factual. Specifically, \\(\\alpha_{j}\\) represents the amount by which a team’s point differential per 100 possession increases if they go from playing 4-on-5 to playing 5-on-5 with player \\(j\\) on the court. Differences of the form \\(\\alpha_{j} - \\alpha_{j'}\\) are much more useful than the single \\(\\alpha_{j}\\) values. By replacing player \\(j'\\) with player \\(j,\\) a team can expect to score \\(\\alpha_{j} - \\alpha_{j'}\\) more points per 100 possessions.\nFor instance, by replacing Luka Dončić with Anthony Davis, teams are expected to score an additional -3.41 points per possession4.\n\nluka_rapm &lt;- rapm |&gt; dplyr::filter(Name == \"Luka Doncic\") |&gt; dplyr::pull(rapm)\nad_rapm &lt;- rapm |&gt; dplyr::filter(Name == \"Anthony Davis\") |&gt; dplyr::pull(rapm)\n\nad_rapm - luka_rapm\n\n[1] -3.417006",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#sec-bootstrap",
    "href": "lectures/lecture05.html#sec-bootstrap",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Uncertainty Quantification with the Bootstrap",
    "text": "Uncertainty Quantification with the Bootstrap\nSo far, we have only focused on obtaining point estimates. What is the range of our uncertainty about the relative impact of players.\nThe bootstrap is an elegant way to quantify uncertainty about the output particular statistical method. At a high-level, the bootstrap works by repeatedly sampling observations from the original dataset and applying the method to the re-sampled dataset. In the context of our RAPM analysis, we will repeat the following steps many, many times (e.g., \\(B = 100\\) times):\n\nRandomly draw a sample of \\(n\\) stints with replacement\nUsing the random re-sample, compute an estimate of the RAPM parameters \\(\\hat{\\boldsymbol{\\alpha}}(\\hat{\\lambda})\\) by fitting a ridge regression model with the optimal \\(\\lambda\\) found by our initial cross-validation.\nSave the estimates in an array\n\nRunning this procedure, we obtain \\(B\\) different estimates \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda)^{(1)}, \\ldots, \\hat{\\boldsymbol{\\alpha}}(\\lambda)^{(B)}\\) where \\(\\hat{\\boldsymbol{\\alpha}}(\\lambda)^{(b)}\\) is the estimate obtained during the \\(b\\)-th time through the loop. Importantly, each bootstrap estimate of the whole vector \\(\\boldsymbol{\\alpha}\\) yields a bootstrap estimate of the constrast \\(\\alpha_{j} - \\alpha_{j'}.\\) By examining the\nAs a concrete illustration, the following code computes 500 bootstrap estimates of the RAPM parameter vector. It saves the bootstrap estimates of \\(\\boldsymbol{\\alpha}\\) in a large matrix whose rows correspond to bootstrap samples and whose columns correspond to players.\n\n\n\n\n\n\nImportant\n\n\n\n##Warning The following code takes several minutes to run!\n\n\n\n1B &lt;- 500\n2n &lt;- nrow(X_full)\np &lt;- ncol(X_full)\n3player_names &lt;- colnames(X_full)\nboot_rapm &lt;- \n4  matrix(nrow = B, ncol = p,\n5         dimnames = list(c(), player_names))\n\nfor(b in 1:B){\n  if(b %% 10 == 0) cat(b, \" \")\n6  set.seed(479+b)\n7  boot_index &lt;- sample(1:n, size = n, replace = TRUE)\n  \n  fit &lt;- \n8    glmnet(x = X_full[boot_index,], y = Y[boot_index],\n           alpha = 0,\n           lambda = cv_fit$lambda,\n           standardize = FALSE)\n  tmp_alpha &lt;- fit$beta[,lambda_index]\n9  boot_rapm[b, names(tmp_alpha)] &lt;- tmp_alpha\n}\n\n\n1\n\nSet number of bootstrap iterations\n\n2\n\nGet the number of stints and players\n\n3\n\nGet the list of player id’s, which are also the column names of X_full\n\n4\n\nBuild a \\(B \\times p\\) matrix to store bootstrap estimates of \\(\\boldsymbol{\\alpha}.\\)\n\n5\n\nUse the players’ IDs as column names for boot_rapm\n\n6\n\nWe set the seed for reproducibility. But we need to vary the seed with every bootstrap re-sample. Otherwise, we’d be drawing the exact same sample every time!\n\n7\n\nThis is a list of row-indices for our re-sampled stints.\n\n8\n\nTakes advantage of R’s indexing capabilities to create our re-sampled design matrix and outcome vector\n\n9\n\nSaves the bootstrap estimate of \\(\\boldsymbol{\\alpha}\\) in the \\(b\\)-th row of boot_ramp\n\n\n\n\n10  20  30  40  50  60  70  80  90  100  110  120  130  140  150  160  170  180  190  200  210  220  230  240  250  260  270  280  290  300  310  320  330  340  350  360  370  380  390  400  410  420  430  440  450  460  470  480  490  500  \n\n\nWe can now compute the difference between Dončić’s and Davis’ estimated RAPM values for every bootstrap estimate. We find that the overwhelming majority of these samples are negative, suggesting that that we can be virtually certain that a team is expected to score score fewer points per 100 possessions by replacing Dončić by Davis.\n\n1doncic_id &lt;- player_table |&gt; dplyr::filter(Name == \"Luka Doncic\") |&gt; dplyr::pull(id)\ndavis_id &lt;- player_table |&gt; dplyr::filter(Name == \"Anthony Davis\") |&gt; dplyr::pull(id) \n\n2boot_diff &lt;- boot_rapm[,davis_id] - boot_rapm[,doncic_id]\n\n3hist(boot_diff,\n4     breaks = 20,\n     main = \"Bootstrap Distribution of Davis-Doncic\", \n     xlab = \"Difference\")\n\n\n1\n\nGet Dončić’s and Davis’ ids\n\n2\n\nGet the vectors of Dončić’s and Davis’ bootstrapped RAPM values by extracting the relevant columns from boot_rapm and compute the difference\n\n3\n\nMake a histogram of the bootstrapped differences between Davis’ and Dončić’s RAPM values\n\n4\n\nSet the number of bins in the histogram to 20\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bootstrap distribution of difference between Doncic & Davis\n\n\n\n\n\nWe can form an uncertainty interval using the sample quantiles of the bootstrapped differences between Davis’ and Dončić’s RAPM values.\n\n1quantile(boot_diff, probs = c(0.025, 0.975))\n\n\n1\n\nA 95% uncertainty interval can be formed using the 2.5% and 97.5% quantiles of the bootstrapped estimates\n\n\n\n\n      2.5%      97.5% \n-6.3776674 -0.4839716 \n\n\nWe conclude with 95% confidence that by replacing Dončić by Davis, the Mavericks are expected to score between 0.48 and 6.3 fewer points per 100 possessions.\n\nBootstrap Intervals for Ranks\nRecall that in our original analysis, Shai Gilgeous-Alexander had the highest RAPM value. How certain should we be about his overall ranking?\nJust like we computed the difference between Dončić’s and Davis’ RAPM estimates for every bootstrap re-sample, we can compute the rank of SGA’s RAPM estimate in every bootstrap sample. Then, we can look at the bootstrap distribution of these ranks to get an idea about the uncertainty in the rank.\nThe rank() function returns the sample ranks of the values in vector. For instance,\n\nx &lt;- c(100, -1, 3, 2.5, -2)\nrank(x)\n\n[1] 5 2 4 3 1\n\n\nNotice that the smallest value in x (-2) is ranked 1 and the largest value in x (100) is ranked 5. This demonstrates that rank() assigns ranks in increasing order. If we wanted the largest value to be ranked 1, the second largest ranked 2, etc., then we need to call rank on the negative values:\n\nrank(-x)\n\n[1] 1 4 2 3 5\n\n\nThe following code confirms that SGA ranked first in terms of the original RAPM estimate.\n\nrapm &lt;-\n  rapm |&gt;\n  dplyr::mutate(rank_rapm = rank(-1 * rapm))\n\nrapm |&gt; dplyr::filter(Name == \"Shai Gilgeous-Alexander\")\n\n       id     rapm                    Name rank_rapm\n1 1628983 3.602143 Shai Gilgeous-Alexander         1\n\n\nTo quantify the uncertainty in SGA’s RAPM ranking, we need to rank the values within every bootstrap estimate of \\(\\boldsymbol{\\alpha}.\\) In other words, we need to rank the values within every row of boot_rapm. We can do this using the apply() function. For our purposes, apply() runs a function (specified using the FUN argument) on every row or column of a two-dimensional array. To run the function along every row, one specifies the argument MAR = 1 and to run the function along every column, one specifies the argument MAR = 2. Notice that the output has \\(p\\) rows (one for every player) and \\(B\\) columns (one for every bootstrap iteration).\n\nboot_rank &lt;- \n  apply(-1*boot_rapm, MARGIN = 1, FUN = rank)\ndim(boot_rank)\n\n[1] 569 500\n\n\nTo get the bootstrap distribution of SGA’s ranking, we need to extract the values from the corresponding row of boot_rank. In the following code, we first look up SGA’s player ID and then get the values from the row of boot_rank that is named with SGA’s ID.\n\nsga_id &lt;- \n  player_table |&gt;\n  dplyr::filter(Name == \"Shai Gilgeous-Alexander\") |&gt;\n  dplyr::pull(id)\n\nsga_ranks &lt;- boot_rank[sga_id,]\ntable(sga_ranks)\n\nsga_ranks\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18  19  20  21 \n 58  59  46  55  36  21  19  12  17  17  13  10   8  12   8   5   2   9   6   5 \n 22  23  24  25  26  27  28  29  30  31  32  33  34  36  37  38  39  40  41  44 \n  4   5   4   5   5   2   2   4   5   2   2   5   4   3   2   1   2   1   1   2 \n 45  47  48  51  52  56  57  66  70  84  85  86  91 105 106 176 \n  5   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1 \n\n\nIt turns out that SGA had the highest RAPM value in just 58 of our 500 bootstrap samples and the second-highest RAPM value in 59 of our bootstrap samples. We can form a 95% bootstrap uncertainty interval around his overall ranking by computing the sample quantiles of these rankings.\n\nquantile(sga_ranks, probs = c(0.025, 0.975))\n\n  2.5%  97.5% \n 1.000 51.525 \n\n\nOur analysis reveals quite a bit of uncertainty about SGA’s overall rank in terms of RAPM!",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#exercises",
    "href": "lectures/lecture05.html#exercises",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Exercises",
    "text": "Exercises\n\nPick several players and compare the bootstrap distribution of the difference in their RAPM estimates.\nUsing the same 100 training/testing splits constructed in Exercise 4.5, estimate the out-of-sample mean square error for RAPM based on the 2024-25 data. Instead of computing an optimal \\(\\lambda\\) for each training split, you may use the optimal \\(\\lambda\\) value computed on the whole dataset. Is RAPM a better predictive model than APM?\nFit RAPM models to data from (i) the 2023-24 and 2024-25 regular seasons and (ii) the 2022-23, 2023-24, and 2024-25 regular season. Use the bootstrap to quantify how uncertainty about the difference in player RAPM values and the overall rankings changes as you include data from more seasons.\nJust like lm, the functions cv.glmnet() and glmnet() accept a weights argument. Using the weighting schemes you considered in Exercise 4.2, fit regularized weighted adjusted plus/minus models.",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#footnotes",
    "href": "lectures/lecture05.html#footnotes",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn practice, \\(\\lambda\\) is often determined using the data. See Section 2.1.↩︎\nSee Section 6.2 of An Introduction to Statistical Learning for a good review of ridge regression.↩︎\nIn fact, the glmnet() documentation explicitly warns users from running with only a single value of \\(\\lambda.\\)↩︎\nOnce again: Fire Nico.↩︎",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture03.html",
    "href": "lectures/lecture03.html",
    "title": "Lecture 3: Estimating XG",
    "section": "",
    "text": "In Lecture 2, we fit two very basic models for XG. The first used only body part to predict the probability of the shot resulting in a goal while the second used both body part and shot technique. By looking at a few of Beth Mead’s shots from EURO 2022, we decided that the second model was a better because it assigned a higher XG to her one-on-one lob than the shot through multiple defenders.\nIn this lecture, we review several quantitative measures of predictive performance (Section 2) and introduce a principled way to estimate how model’s perform out-of-sample (Section 3). Then, we fit introduce a logistic regression model that accounts for continuous features like distance to the goal (Section 4) before fitting a more flexible, nonparametric model that can simultaneously account for many more features (Section 5).\n\n\nWe will continue to work with all shot event data from women’s international competitions that StatsBomb makes publicly available. Using code from Lecture 2, we can load in all the shot event data and create a binary outcome recording whether the shot resulted in a goal.\n\nwi_shots &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international)|&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam() |&gt;\n6  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n7  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\n1\n\nGets the table of all available competitions\n\n2\n\nIdentifies all women’s international competitions\n\n3\n\nGets match-level data from all women’s international competitions\n\n4\n\nGets event-level data\n\n5\n\nRuns StatsBomb’s recommend pre-processing\n\n6\n\nSubsets to shot-event data taken with either the foot or head\n\n7\n\nCreates a column Y\n\n\n\n\nWe will also re-construct the two simple XG models, one that conditions only on body part and the other that conditions on body part and technique. We then create a data table with columns containing the body part, technique, and outcome of the shot as well as the XG predictions from both models.\n\nxg_model1 &lt;- \n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\") \n\nsimple_preds &lt;-\n  wi_shots |&gt;\n  dplyr::select(Y, shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::left_join(y = xg_model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::left_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-overview",
    "href": "lectures/lecture03.html#sec-overview",
    "title": "Lecture 3: Estimating XG",
    "section": "",
    "text": "In Lecture 2, we fit two very basic models for XG. The first used only body part to predict the probability of the shot resulting in a goal while the second used both body part and shot technique. By looking at a few of Beth Mead’s shots from EURO 2022, we decided that the second model was a better because it assigned a higher XG to her one-on-one lob than the shot through multiple defenders.\nIn this lecture, we review several quantitative measures of predictive performance (Section 2) and introduce a principled way to estimate how model’s perform out-of-sample (Section 3). Then, we fit introduce a logistic regression model that accounts for continuous features like distance to the goal (Section 4) before fitting a more flexible, nonparametric model that can simultaneously account for many more features (Section 5).\n\n\nWe will continue to work with all shot event data from women’s international competitions that StatsBomb makes publicly available. Using code from Lecture 2, we can load in all the shot event data and create a binary outcome recording whether the shot resulted in a goal.\n\nwi_shots &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international)|&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam() |&gt;\n6  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n7  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\n1\n\nGets the table of all available competitions\n\n2\n\nIdentifies all women’s international competitions\n\n3\n\nGets match-level data from all women’s international competitions\n\n4\n\nGets event-level data\n\n5\n\nRuns StatsBomb’s recommend pre-processing\n\n6\n\nSubsets to shot-event data taken with either the foot or head\n\n7\n\nCreates a column Y\n\n\n\n\nWe will also re-construct the two simple XG models, one that conditions only on body part and the other that conditions on body part and technique. We then create a data table with columns containing the body part, technique, and outcome of the shot as well as the XG predictions from both models.\n\nxg_model1 &lt;- \n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\") \n\nsimple_preds &lt;-\n  wi_shots |&gt;\n  dplyr::select(Y, shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::left_join(y = xg_model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::left_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-model-comparison",
    "href": "lectures/lecture03.html#sec-model-comparison",
    "title": "Lecture 3: Estimating XG",
    "section": "Model Comparison",
    "text": "Model Comparison\nOur first XG model conditions only on the body part used to take the shot while our second model additionally conditions on the technique. Consequently, the first model returns exactly the same predicted XG for a right-footed half-volley (e.g., this Beth Mead goal against Sweden in EURO 2022) as it does for a right-footed backheel shot (e.g., this Alessia Russo goal from the same game). In contrast, our second model, which accounts for both the body part and the shot technique, can distinguish between these two shots and return different XG predictions. Intuitively, we might expect the second model’s predictions to be more accurate because it leverages more information.\nTo make this more concrete, suppose we have observed a shots dataset of pairs \\((\\boldsymbol{\\mathbf{x}}_{1}, y_{1}), \\ldots (\\boldsymbol{\\mathbf{x}}_{n}, y_{n})\\) of feature vectors \\(\\boldsymbol{\\mathbf{X}}\\) and binary indicators \\(Y\\) recording whether the shot resulted in a goal or not. Recall from Lecture 2 that a key assumption of XG models is the observed dataset comprises a sample from some infinite super-population of shots. For each feature combination \\(\\boldsymbol{\\mathbf{x}},\\) the corresponding \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is defined to be the conditional expectation \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) := \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}].\\) Because \\(Y\\) is a binary indicator, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) can be interpreted as the probability that a shot with features \\(\\boldsymbol{\\mathbf{x}}\\) results in a goal.\nNow, suppose we have used our data to fit an XG model. We can go back to every shot \\(i\\) in our dataset and use each fitted model to obtain the predicted XG, which we denote1 as \\(\\hat{p}_{i}.\\) We are ultimately interested in assessing how close \\(\\hat{p}_{i}\\) is to the actual observed \\(Y_{i}.\\)\nIn Statistics and Machine Leaning, there are three common ways to assess the discrepancy between a probability forecast \\(\\hat{p}_{i}\\) and a binary outcome: misclassification rate, Brier score, and log-loss.\n\nMisclassification Rate\nMisclassification rate is the coarsest measure of discrepancy between a predicted probability and a binary outcome. A forecast \\(\\hat{p}_{i} &gt; 0.5\\) (resp. \\(\\hat{p}_{i} &lt; 0.5\\)) reflects the fact that we believe it more likely than not that \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\)). So, intuitively, we should expect a highly accurate model to return forecasts \\(\\hat{p}_{i}\\) that were greater (resp. less) than 0.5 whenever \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\)). A model’s misclassification rate is the proportion of times that the model deviates from this expectation (i.e., when it returns forecasts greater than 50% when \\(y = 0\\) and returns forecasts less than 50% when \\(y = 1.\\))\nFormally, given a binary observations \\(y_{1}, \\ldots, y_{n}\\) and predicted probabilities \\(\\hat{p}_{1}, \\ldots, \\hat{p}_{n},\\) the misclassification rate is defined as \\[\n\\textrm{MISS} = n^{-1}\\sum_{i = 1}^{n}{\\mathbb{I}(y_{i} \\neq \\mathbb{I}(\\hat{p}_{i} \\geq 0.5))},\n\\] where \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) equals 1 when \\(\\hat{p}_{i} \\geq 0.5\\) and equals 0 otherwise and \\(\\mathbb{I}(y_{i} \\neq \\mathbb{I}(\\hat{p}_{i} \\geq 0.5))\\) equals 1 when \\(y_{i}\\) is not equal to \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) and equals 0 otherwise. On the view that misclassification rate captures the number of times our predictions are on the wrong side of the 50% boundary, we prefer models with smaller misclassification rate.\nIn the context of XG, misclassification rate measures the proportion of times we predict an XG higher than 0.5 for a non-goal or an XG lower than 0.5 for a goal.\n\n1misclass &lt;- function(y, phat){\n  return( mean( (y != 1*(phat &gt;= 0.5))))\n}\n\n2cat(\"BodyPart misclassification\",\n    round(misclass(simple_preds$Y, simple_preds$XG1), digits = 3), \"\\n\")\ncat(\"BodyPart+Technique misclassificaiton\", \n    round(misclass(simple_preds$Y, simple_preds$XG2), digits = 3), \"\\n\")\n\n\n1\n\nA helper function that computes misclassification rate.\n\n2\n\ncat() prints output to the R console and it is usually good to round numeric output to 2 or 3 decimal points.\n\n\n\n\nBodyPart misclassification 0.112 \nBodyPart+Technique misclassificaiton 0.112 \n\n\nOur two simple models have identical misclassification rates of about 10.7%. At first glance this is a bit surprising because the two models return different predicted XG values\n\ncat(\"Unique XG1 values:\", sort(unique(simple_preds$XG1)), \"\\n\")\n\nUnique XG1 values: 0.1113281 0.1119565 0.1140625 \n\ncat(\"Unique XG2 values:\", sort(unique(simple_preds$XG2)), \"\\n\")\n\nUnique XG2 values: 0 0.06372549 0.06756757 0.07142857 0.08920188 0.1034483 0.1131868 0.1207729 0.1214361 0.1632653 0.2083333 \n\n\nOn closer inspection, we find that the two simple models both output predicted XG values that are all less than 0.5. So, the thresholded values \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) are identical for both models’ \\(\\hat{p}_{i}\\)’s. Just for comparison’s sake, let’s compute the misclassification rate of the proprietary StatsBomb XG model\n\ncat(\"StatsBomb misclassificaiton\", \n    round(misclass(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 3), \"\\n\")\n\nStatsBomb misclassificaiton 0.095 \n\n\nWe see that StatsBomb’s model has a slightly smaller misclassification rate than our two simple models, but the gap is not very large.\n\n\nBrier Score & Log-Loss\nA major drawback of misclassification rate is that it only penalizes predictions for being on the wrong side of 50% but does not penalize predictions based on how far away they are from 0 or 1. For instance, if one model returns \\(\\hat{p} = 0.999\\) and another model returns \\(\\hat{p} = 0.501,\\) the thresholded forecasts \\(\\mathbb{I}(\\hat{p} &gt; 0.5)\\) are identical.\nBrier score and log-loss both take into account how far \\(\\hat{p}\\) is from \\(Y,\\) albeit in slightly different ways. The Brier score is defined as \\[\n\\text{Brier} = n^{-1}\\sum_{i = 1}^{n}{(y_{i} - \\hat{p}_{i})^2}.\n\\] The quantity \\((y_{i} - \\hat{p}_{i})^{2}\\) is small whenever (i) \\(y_{i} = 1\\) and \\(\\hat{p}_{i}\\) is very close to 1 or (ii) \\(y_{i} = 0\\) and \\(\\hat{p}_{i}\\) is very close to 0. The quantity is very large when \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\) and \\(\\hat{p}\\) is very close to 0 (resp. very closer to 1).\nLog-loss, which is also known as cross-entropy loss in the Machine Learning literature, more aggressively penalizes wrong forecasts. It is defined as \\[\n\\textrm{LogLoss} = -1 \\times \\sum_{i = 1}^{n}{\\left[ y_{i} \\times \\log(\\hat{p}_{i}) + (1 - y_{i})\\times\\log(1-\\hat{p}_{i})\\right]}.\n\\] Notice that if \\(y_{i} = 1\\) as \\(\\hat{p}_{i} \\rightarrow 0,\\) the term \\(y_{i} \\times \\log{\\hat{p}_{i}} \\rightarrow -\\infty.\\) So, while the Brier score is mathematically bounded between 0 and 1, the average log-loss can be arbitrarily large. Essentially, log-loss heavily penalizes predictions that are confident (i.e., very close to 0 or 1) but wrong (i.e., different than \\(y_{i}\\).) Like we did for the misclassification rate, we define a helper function for computing the Brier score and then apply that function to predictions from our two simple XG models and StatsBomb’s XG model.\n\nbrier &lt;- function(y, phat){\n  return(mean( (y - phat)^2 ))\n}\n\ncat(\"BodyPart Brier\", \n    round(brier(simple_preds$Y, simple_preds$XG1), digits = 4), \"\\n\")\n\nBodyPart Brier 0.0996 \n\ncat(\"BodyPart+Technique Brier\", \n    round(brier(simple_preds$Y, simple_preds$XG2) , digits = 4), \"\\n\")\n\nBodyPart+Technique Brier 0.0991 \n\ncat(\"StatsBomb Brier\",\n    round(brier(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 4), \"\\n\")\n\nStatsBomb Brier 0.0753 \n\n\nWhen computing log-loss in practice, it’s common to truncate probabilities close to 0 or 1, which ensures the final answer is finite (i.e., avoids trying to compute log(0)).\n\nlogloss &lt;- function(y, phat){\n  \n1  if(any(phat &lt; 1e-12)) phat[phat &lt; 1e-12] &lt;- 1e-12\n  if(any(phat &gt; 1-1e-12)) phat[phat &gt; 1-1e-12] &lt;- 1-1e-12\n  return(-1 * mean( y * log(phat) + (1-y) * log(1-phat)))\n}\n\ncat(\"BodyPart LogLoss:\", \n    round(logloss(simple_preds$Y, simple_preds$XG1), digits = 3), \"\\n\")\ncat(\"BodyPart+Technique LogLoss:\", \n    round(logloss(simple_preds$Y, simple_preds$XG2) , digits = 3), \"\\n\")\ncat(\"StatsBomb LogLoss:\",\n    round(logloss(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 3), \"\\n\")\n\n\n1\n\nSince \\(\\log(0) = -\\infty,\\) we often truncate very small or very large predictions when computing average log-loss\n\n\n\n\nBodyPart LogLoss: 0.351 \nBodyPart+Technique LogLoss: 0.348 \nStatsBomb LogLoss: 0.264",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-train-test",
    "href": "lectures/lecture03.html#sec-train-test",
    "title": "Lecture 3: Estimating XG",
    "section": "Estimating Out-of-Sample Performance",
    "text": "Estimating Out-of-Sample Performance\nBased on the preceding calculations, it is tempting to conclude that our second, more complex model, is more accurate than the first model. After all, it achieved a slightly smaller Brier score and log-loss. There is, however, a small catch: we assessed the model’s performance using exactly the same data that we used to fit the model.\nGenerally speaking, if we used the data \\((\\boldsymbol{\\mathbf{x}}_{1}, y_{1}), \\ldots, (\\boldsymbol{\\mathbf{x}}_{n}, y_{n})\\) to produce an estimate \\(\\hat{f}\\) of the conditional expectation function \\(f(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}].\\) We can use this estimate to compute predictions \\(\\hat{y}_{i} = \\hat{f}(\\boldsymbol{\\mathbf{x}})\\) of the originally observed outcomes \\(y_{i}.\\) While we would like each \\(\\hat{y}_{i} \\approx y_{i},\\) we are often more interested in determining whether \\(\\hat{f}(\\boldsymbol{\\mathbf{x}}^{\\star})\\) is approximately equal to \\(y^{\\star}\\) for some new pair \\((\\boldsymbol{\\mathbf{x}}^{\\star}, y^{\\star})\\) not included in the original dataset. That is, we are often interested in knowing how well the fitted model can predict previously unseen data from the same infinite super-population. Moreover, between two models, we tend to prefer the one that yields smaller out-of-sample or testing error. That is, in the context of our XG models, we would prefer the one that yielded smaller misclassification rate, Brier score, and/or log-loss on shots not used to fit our initial models.\nIn practice, we rarely have access to a separate set of data that we can hold-out of training. Instead, we often (i) divide our data into two parts; (ii) fit our model on one part (the training data); and (iii) assess the predictive performance on the second part (the testing data). Commonly, we use 75% or 80% of the data to train a model and set the remaining part aside as testing data. Moreover, we often create the training/testing split randomly.\nThe next codeblock illustrates this workflow. To help create training/testing splits, we start by giving every row its own unique ID. Then, we randomly select a fixed number of rows to form our training dataset.\n\nn &lt;- nrow(wi_shots)\n1n_train &lt;- floor(0.75 * n)\nn_test &lt;- n - n_train\n\nwi_shots &lt;-\n  wi_shots |&gt;\n2  dplyr::mutate(id = 1:n)\n\n3set.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n4  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n5  dplyr::anti_join(y = train_data, by = \"id\")\n\n\n1\n\nWe will use about 75% of the data for training and the remainder for testing\n\n2\n\nAssign a unique ID number to each row in wi_shots\n\n3\n\nWe often manually the randomization seed, which dictates how R generates (pseudo)-random numbers (see here), to ensure full reproducibility of our analyses. Another user running this code should get the same results.\n\n4\n\nThe function dplyr::slice_sample() returns a random subset of rows.\n\n5\n\nThe function dplyr::anti_join() extracts the rows in wi_shots whose IDs are not contained in train_data\n\n\n\n\nAs a sanity check, we can check whether any of the id’s in test_data are also in train_data:\n\nany(train_data$id %in% test_data$id)\n\n[1] FALSE\n\n\nNow that we have a single training and testing split, let’s fit our two simple XG models only using the training data and then computing the average training and testing losses:\n\nmodel1 &lt;- \n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nmodel2 &lt;-\n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\ntrain_preds &lt;-\n  train_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ntest_preds &lt;-\n  test_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ncat(\"BodyPart train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG1), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG1), digits = 3), \"\\n\")\n\nBodyPart train log-loss: 0.357 test log-loss: 0.334 \n\ncat(\"BodyPart+Technique train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG2), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG2), digits = 3), \"\\n\")\n\nBodyPart+Technique train log-loss: 0.355 test log-loss: 0.351 \n\n\nFor this training/testing split, we see that the more complex model, which conditioned on both body-part and technique, produced slightly smaller log-loss than the simple model, which only conditioned on body-part. But this may not necessarily always hold. For instance, if we drew a different training/testing split (e.g., by initially setting a different randomization seed), we might observe the opposite:\n\nset.seed(478)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\")\n\nmodel1 &lt;- \n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nmodel2 &lt;-\n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\ntrain_preds &lt;-\n  train_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ntest_preds &lt;-\n  test_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ncat(\"BodyPart train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG1), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG1), digits = 3), \"\\n\")\n\nBodyPart train log-loss: 0.359 test log-loss: 0.327 \n\ncat(\"BodyPart+Technique train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG2), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG2), digits = 3), \"\\n\")\n\nBodyPart+Technique train log-loss: 0.356 test log-loss: 0.346 \n\n\nOn this second random split, the simpler model had better in-sample and out-of-sample performance.\nInstead of relying on a single training/testing split, we generally average the in- and out-of-sample losses across several different random splits. In the code below, we repeat the above exercise using 100 random training/testing splits. Notice that in our for loop, we manually set the seed in a predictable way to ensure reproducibility.\n\nn_sims &lt;- 100\n1train_logloss &lt;-\n  matrix(nrow = 2, ncol = n_sims,\n         dimnames = list(c(\"XG1\",\"XG2\"), c()))\ntest_logloss &lt;- \n  matrix(nrow = 2, ncol = n_sims,\n         dimnames = list(c(\"XG1\",\"XG2\"), c()))\n\nfor(r in 1:n_sims){\n2  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train) \n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n  \n  model1 &lt;- \n    train_data |&gt;\n    dplyr::group_by(shot.body_part.name) |&gt;\n    dplyr::summarise(XG1 = mean(Y))\n  \n  model2 &lt;-\n    train_data |&gt;\n    dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n    dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\n  train_preds &lt;-\n    train_data |&gt;\n    dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n    dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\n  test_preds &lt;-\n    test_data |&gt;\n    dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n    dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n  \n3  train_logloss[\"XG1\", r] &lt;- logloss(train_preds$Y, train_preds$XG1)\n  train_logloss[\"XG2\",r] &lt;- logloss(train_preds$Y, train_preds$XG2)\n  \n  test_logloss[\"XG1\", r] &lt;- logloss(test_preds$Y, test_preds$XG1)\n  test_logloss[\"XG2\",r] &lt;- logloss(test_preds$Y, test_preds$XG2)\n}\n\n4cat(\"XG1 training logloss:\", round(mean(train_logloss[\"XG1\",]), digits = 3), \"\\n\")\ncat(\"XG2 training logloss:\", round(mean(train_logloss[\"XG2\",]), digits = 3), \"\\n\")\n\ncat(\"XG1 test logloss:\", round(mean(test_logloss[\"XG1\",]), digits = 3), \"\\n\")\ncat(\"XG2 test logloss:\", round(mean(test_logloss[\"XG2\",]), digits = 3), \"\\n\")\n\n\n1\n\nWe will save the training and testing log-loss for each training/testing split in a matrix\n\n2\n\nWe create each split using a different randomization seed. Many other choices are possible but simply adding the fold number r to some base seed (479 in this case) is perhaps the simplest approach.\n\n3\n\nWe write the training and testing log-losses for the r-th training/testing split to the r-th column of the matrices train_logloss and test_logloss\n\n4\n\nGet the average loss within each row\n\n\n\n\nXG1 training logloss: 0.351 \nXG2 training logloss: 0.348 \nXG1 test logloss: 0.352 \nXG2 test logloss: 0.356 \n\n\nBased on this analysis, we conclude that the simpler model provides better out-of-sample predictions than the more complex model. In other words, the more complex model appears to over-fit the data!",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-logistic",
    "href": "lectures/lecture03.html#sec-logistic",
    "title": "Lecture 3: Estimating XG",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nAlthough the model that conditions only on body-part has smaller out-of-sample loss than the model that additionally accounts for shot technique, the model is still not wholly satisfactory. This is because it fails to account for important spatial information that we intuitively believe affects goal probability. For instance, we might intuitively expect a model that accounts for the distance between the shot and goal to be more accurate. Unfortunately, it is not easy to apply the same “binning and averaging” approach that we used for our first two models to condition on distance. This is because, with finite data, we will not have much data (if any) for all possible values of distance. As we saw in Lecture 2, averaging outcomes within bins with very small sample sizes can lead to extreme and erratic estimates. One way to overcome this challenge is to fit a statistical model.\nLogistic regression is the canonical statistical model for predicting a binary outcome \\(Y\\) using a vector \\(\\boldsymbol{\\mathbf{X}}\\) of \\(p\\) numerical features \\(X_{1}, \\ldots, X_{p}\\)2.\nThe model asserts that the log-odds that \\(Y = 1\\) is a linear function of the features. That is, \\[\n\\log\\left(\\frac{\\mathbb{P}(Y= 1 \\vert \\boldsymbol{\\mathbf{X}})}{\\mathbb{P}(Y = 0 \\vert \\boldsymbol{\\mathbf{X}})}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p}.\n\\]\n\nLogistic Regression with One Feature\nWe begin by fitting a model that only conditions on distance to the goal using the function glm(). This function works a lot like lm in that we specify the model using R’s formula interface. Whereas lm() is used only to fit linear models via the method least squares, glm() can be used to fit a much wider range of models3. So, we use the family argument to tell glm() exactly what model we want to fit.\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\n1fit1 &lt;- glm(formula = Y~DistToGoal,\n2            data = train_data,\n3            family = binomial(\"logit\"))\n\n\n1\n\nIn the formula argument, the outcome goes on the left-hand side of the ~ and predictors go on the right-hand side\n\n2\n\nLike lm, the function glm takes a data argument. Every term appearing in the formula must appear as a column in the data frame passed via data.\n\n3\n\nThe specification binomial(\"logit\") signals to glm that we want to fit a logistic regression model.\n\n\n\n\nThe summary function provides a snapshot about the fitted model, showing the individual parameter estimates, their associated standard errors, and whether or not they are statistically significantly different than zero.\n\nsummary(fit1)\n\n\nCall:\nglm(formula = Y ~ DistToGoal, family = binomial(\"logit\"), data = train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.134174   0.128410  -1.045    0.296    \nDistToGoal  -0.127023   0.009115 -13.935   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2549.7  on 3569  degrees of freedom\nResidual deviance: 2290.7  on 3568  degrees of freedom\nAIC: 2294.7\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe see that the estimate of \\(\\beta_{1},\\) the coefficient capturing DistToGoal’s effect on the log-odds of a goal, is negative (-0.11) and that this is statistically significantly different than 0 (the associated p-value is &lt;2e-16). This is reassuring as we might intuitively expect the probability of a goal to decrease the further away a shot is from the goal. Recall that the intercept term \\(\\beta_{0}\\) quantifies the log-odds of a goal when DistToGoal = 0 (i.e., when a shot is taken from the middle of the goal line). Our model estimates this intercept to about -0.43, which, on the probability scale is about 39%. On the face of it, this doesn’t make a ton of sense: surely shots taken essentially from the middle of the goal line should go in much more frequently.\nWe can resolve this apparent contradiction by taking a closer look at the range of DistToGoal values in our dataset:\n\nrange(train_data$DistToGoal)\n\n[1]  1.860108 92.800862\n\n\nSince our model was trained on shots that came between 1.8 and 92 yards away, the suspiciously low 39% forecast for goal line shots represents a fairly significant extrapolation.\n\n\n\n\n\n\nExtrapolation\n\n\n\nExercise caution when using predictions at inputs that are far away, systematically different, or otherwise not well-represented in the training data in any downstream analysis.\n\n\nCrucially, we can use our fitted model to make predictions about shots not seen in our training data. In R, we make predictions using the predict() function. The function takes three arguments:\n\nobject: this is the output from our glm call (i.e., the fitted model object), which we have here saved as fit1\nnewdata: this is a data table containing the features of the observations for which we would like predictions. The table that we pass here needs to have columns for every feature used in the model.\ntype: When object is the result from fitting a logistic regression model, predict will return predictions on the log-odds scale (i.e, \\(\\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{1} + \\cdots + \\hat{\\beta}_{p}X_{p}\\)) by default. To get predictions on the probability scale, we need to set type = \"response\"\n\nIn the code below, we obtain predictions for both the training and testing observations and then compute the average log-loss for this training/testing split.\n\ntrain_pred1 &lt;- \n  predict(object = fit1,\n          newdata = train_data,\n          type = \"response\") \ntest_pred1 &lt;-\n  predict(object = fit1,\n          newdata = test_data,\n          type = \"response\")\n\ncat(\"Dist training logloss:\", round(logloss(train_data$Y, train_pred1), digits = 3), \"\\n\")\n\nDist training logloss: 0.321 \n\ncat(\"Dist testing logloss:\", round(logloss(test_data$Y, test_pred1), digits = 3), \"\\n\")\n\nDist testing logloss: 0.305 \n\n\nAt least for this training/testing split, the logistic regression model that conditions on distance is a bit more accurate than the simpler models. Before concluding that this model indeed is more accurate than the body-part based model, we will repeat the above calculations using 100 training/testing splits.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal, data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist training logloss:\", round(mean(train_logloss), digits = 3), \"\\n\")\n\nDist training logloss: 0.317 \n\ncat(\"Dist testing logloss:\", round(mean(test_logloss), digits = 3), \"\\n\")\n\nDist testing logloss: 0.317 \n\n\nSo, accounting for the shot distance appears to yield more accurate predictions than account for just body-part. This immediately raises the question “how much more accurate would predictions be if we accounted for shot distance, body-part, and technique?”\n\n\nAccounting for Multiple Predictors\nIt turns out to be relatively straightforward to answer this question with a logistic regression model fit using glm(). Specifically, we can include more variables in the formula argument. Note that we first convert the variables shot.body_part.name to a factor.\n\nwi_shots &lt;-\n  wi_shots |&gt;\n  dplyr::mutate(\n    shot.body_part.name = factor(shot.body_part.name))\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit &lt;- \n  glm(formula = Y~DistToGoal + shot.body_part.name, \n      data = train_data, family = binomial(\"logit\"))\n\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal + shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -0.48741    0.14831  -3.287  0.00101 ** \nDistToGoal                    -0.15839    0.01032 -15.348  &lt; 2e-16 ***\nshot.body_part.nameLeft Foot   1.02210    0.16747   6.103 1.04e-09 ***\nshot.body_part.nameRight Foot  1.03793    0.15106   6.871 6.38e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2549.7  on 3569  degrees of freedom\nResidual deviance: 2234.3  on 3566  degrees of freedom\nAIC: 2242.3\n\nNumber of Fisher Scoring iterations: 6\n\n\nLooking at the output of summary(fit), we see that the model includes several more parameters. There is a slope associated with left-footed and right-footed shots. Internally, glm() has one-hot-encoded the categorical predictor shot.body_part.name and decomposed the log-odds of a goal as \\[\n\\beta_{0} + \\beta_{1}\\times \\textrm{DistToGoal} + \\beta_{\\textrm{LeftFoot}}\\times \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} \\times \\mathbb{I}(\\textrm{RightFoot})\n\\]\nTo understand what’s going on, suppose that a shot is taken from distance \\(d.\\) The model makes different predictions based on the body part used to attempt the shot:\n\nIf the shot was a header then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d\\)\nIf the shot was taken with the left foot, then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d + \\beta_{\\textrm{LeftFoot}}\\)\nIf the shot was taken with the right foot, then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d + \\beta_{\\textrm{RightFoot}}\\)\n\nReassuringly, the estimates for \\(\\beta_{\\textrm{RightFoot}}\\) and \\(\\beta_{\\textrm{LeftFoot}}\\) are positive, indicating that for a fixed distance, the log-odds of scoring a goal on a left- or right-footed shot is higher than headers.\nBy repeatedly fitting and assessing our new model using 100 random training/testing splits, we discover that accounting for both body part and shot distance results in even more accurate predictions.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal + shot.body_part.name, \n             data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist+BodyPart training logloss:\", round(mean(train_logloss), digits = 3), \"\\n\")\n\nDist+BodyPart training logloss: 0.307 \n\ncat(\"Dist+BodyPart testing logloss:\", round(mean(test_logloss), digits = 3), \"\\n\")\n\nDist+BodyPart testing logloss: 0.308 \n\n\n\n\nInteractions\nOur most accurate model so far accounts for both shot distance and the body part used to attempt the shot. Taking a closer look at the assumed log-odds of a goal, however, reveals a potentially important limitation: the effect of distance is assumed to be the same for all shot-types. That is, the model assume that moving one yard further away decreases the log-odds of a goal of a right-footed shot by exactly the same amount as it would a header.\nInteractions between features in a regression model allow the effect of one factor to vary based on the value of another features. Using R’s formula interface, we can introduce interactions between two variables using *.\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit &lt;- \n  glm(formula = Y~DistToGoal * shot.body_part.name, \n      data = train_data, family = binomial(\"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal * shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                               1.04701    0.41889   2.499  0.01244\nDistToGoal                               -0.34422    0.05092  -6.761 1.37e-11\nshot.body_part.nameLeft Foot             -0.21195    0.50881  -0.417  0.67700\nshot.body_part.nameRight Foot            -0.82193    0.46059  -1.785  0.07434\nDistToGoal:shot.body_part.nameLeft Foot   0.16418    0.05457   3.009  0.00263\nDistToGoal:shot.body_part.nameRight Foot  0.20871    0.05233   3.988 6.66e-05\n                                            \n(Intercept)                              *  \nDistToGoal                               ***\nshot.body_part.nameLeft Foot                \nshot.body_part.nameRight Foot            .  \nDistToGoal:shot.body_part.nameLeft Foot  ** \nDistToGoal:shot.body_part.nameRight Foot ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2549.7  on 3569  degrees of freedom\nResidual deviance: 2214.5  on 3564  degrees of freedom\nAIC: 2226.5\n\nNumber of Fisher Scoring iterations: 6\n\n\nMathematically, the formula specification Y ~ DistToGoal * shot.body_part.name tells R to fit the model that expresses the log-odds of a goal as \\[\n\\begin{align}\n\\beta_{0} + \\beta_{\\textrm{LeftFoot}} * \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} * \\mathbb{I}(\\textrm{RightFoot}) &+ ~ \\\\\n[\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:LeftFoot}}*\\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{DistToGoal:RightFoot}}\\mathbb{I}(\\textrm{RightFoot})] \\times \\textrm{DistToGoal} &  \n\\end{align}\n\\] where \\(\\mathbb{I}(RightFoot)\\) is an indicator of whether or not the shot was taken with the right foot.\nNow for a given shot taken at distance \\(d\\), the log-odds of a goal are:\n\n\\(\\beta_{0} + \\beta_{\\textrm{DistToGoal} * d\\) is the shot was a header.\n\\(\\beta_{0} + \\beta_{\\textrm{LeftFoot}} + (\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:LeftFoot}}) * d\\) if the shot was taken with the left foot.\n\\(\\beta_{0} + \\beta_{\\textrm{RightFoot}} + (\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:RightFoot}}) * d\\) if the shot was taken with the right foot.\n\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal*shot.body_part.name, \n             data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist*BodyPart training logloss:\", round(mean(train_logloss), digits = 4), \"\\n\")\n\nDist*BodyPart training logloss: 0.3047 \n\ncat(\"Dist*BodyPart testing logloss:\", round(mean(test_logloss), digits = 4), \"\\n\")\n\nDist*BodyPart testing logloss: 0.3066 \n\n\nInterestingly, adding the interaction between distance and body part did not seem to improve the test-set log-loss by much.",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-rf",
    "href": "lectures/lecture03.html#sec-rf",
    "title": "Lecture 3: Estimating XG",
    "section": "Random Forests",
    "text": "Random Forests\nRemember how we used the function StatsBombR::allclean() to apply some of StatsBomb’s suggested pre-processing? It turns out that this function creates several features related to shot attempt including4:\n\nDistances from the shot the center of the goal line (DistToGoal); from the shot to the goal keeper (DistSGK); from the goalkeeper to the center of the goal line (DistToGK)\nThe horizontal angles between the shot and goalkeeper to the center of the goal line (AngleToGoal & AngleToKeeper) and the difference between these angles (AngleDeviation)\nThe distances between the shot and the two nearest defenders (distance.ToD1 and distance.ToD2)\nThe number of defenders and whether the goal keeper is in the triangular area defined by the shot location and the two goal posts (the “cone”;DefendersInCone and InCone.GK)\nThe sum of the inverse distances between the shot location the locations of all defenders (density) and those defenders in the cone. A small density indicates that defenders are very far from the shot location.\nThe area of the smallest square that covers the locations of all center-backs and full-backs (DefArea)\n\nThe code below pulls out several of these features and creates a training/testing split\n\nshot_vars &lt;-\n  c(\"Y\",\n    \"shot.type.name\", \n    \"shot.technique.name\", \"shot.body_part.name\",\n    \"DistToGoal\", \"DistToKeeper\", # dist. to keeper is distance from GK to goal\n    \"AngleToGoal\", \"AngleToKeeper\",\n    \"AngleDeviation\", \n    \"avevelocity\",\"density\", \"density.incone\",\n    \"distance.ToD1\", \"distance.ToD2\",\n    \"AttackersBehindBall\", \"DefendersBehindBall\",\n    \"DefendersInCone\", \"InCone.GK\", \"DefArea\")\n\nwi_shots &lt;-\n  wi_shots |&gt;\n  dplyr::mutate(\n    shot.type.name = factor(shot.type.name),\n    shot.body_part.name = factor(shot.body_part.name),\n    shot.technique.name = factor(shot.technique.name))\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train) |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\", shot_vars)))\n\ny_train &lt;- train_data$Y\ny_test &lt;- test_data$Y\n\ntrain_data &lt;-\n  train_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\ntest_data &lt;-\n  test_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\n\nHow much more predictive accuracy can we squeeze out of an XG model when we account for all these variables? While it is tempting to continue fitting logistic regression models, these models make fairly strong assumptions about how the log-odds of a goal change as we vary the features. Moreover, it becomes increasingly difficult to specify interactions, especially between continuous/numerical factors.\nRandom forests is a powerful regression approach that avoids specifying functional forms and allows for high-order interactions. At a high-level, a random forest model approximates a regression function \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}}]\\) using the sum of several regression trees (which are just piece-wise constant step functions).\n\nRegression Trees & Ensembles\nFigure 1 (a) shows an example of a regression tree defined over the space \\([0,1]^{2}\\). Regression trees represent piece-wise constant step-functions (Figure 1 (b)). To see this, imagine taking a point \\(\\boldsymbol{\\mathbf{x}} = (x_{1}, \\ldots, x_{p})\\) and tracing a path from the root of the tree — that is, the node at the very top — down to one of the terminal leaf nodes by following the decision rules. Specifically, when the path encounters an interior node associated with the rule \\(\\{X_{j} &lt; c\\}\\), the path proceeds to the left child if \\(x_{j} &lt; c\\) and the right child otherwise. When the path reaches a leaf \\(\\ell\\), we output the associated scalar \\(\\mu_{\\ell}.\\) For instance, in the case of Figure 1 (b), the path associated with every point in the yellow rectangle terminates in the yellow leaf node in Figure 1 (a).\n\n\n\n\n\n\n\n\n\n\n\n(a) Regression tree\n\n\n\n\n\n\n\n\n\n\n\n(b) Step function represented by the tree\n\n\n\n\n\n\n\nFigure 1: Regression trees are piecewise-constant step-functions\n\n\n\nIt turns out that regression trees are universal function approximators: for just about every function with \\(p\\) inputs, there is a regression tree that can approximate the function to an arbitrarily small tolerance. The catch is one often needs a very deep tree, with lots of splits and leaf nodes to obtain a highly accurate approximation. Figure 2 and Figure 3 illustrate this phenomenon. The tree on the right-hand side of Figure 2 has seven leaves and encodes a step function (green curve on the left) Although this step function captures certain macro trends (e.g., the increase around \\(x = 0.4\\) and subsequent decrease around \\(x = 0.6\\)) of the true function (pink), it misses a lot of local structure (e.g., the local optima between \\(x = 0.2\\) and \\(x = 0.4\\)). But, as the tree gets deeper and number of leaves increases, the approximation improves (Figure 3).\n\n\n\n\n\n\nFigure 2: Left: True function (pink) and a step function approximation (green). Right: representation of the step function as a regression tree\n\n\n\n\n\n\n\n\n\nFigure 3: As the number of leaves increases (i.e., tree becomes deeper), we obtain more accurate approximations\n\n\n\nSo, at least conceptually, this makes regression trees a very powerful tool in Statistics: rather than trying to pre-specify the functional form of \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}]\\) — that is, manually specifying non-linearities or high-order interactions in a (generalized) linear model — we can instead focus on learning a regression trees5 that fits the data well.\nAs suggested by Figure 3, we often need a very deep tree with lots of leaves to get a good approximation. Luckily, it turns out that very complicated trees can be decomposed into sums of simpler trees. In Figure 4, we write the somewhat complex tree on the left as a sum of three simpler trees on the right6. We typically refer to a collection of simple regression trees as an ensemble. Methods that make predictions using ensembles of regression trees often out-perform more complicated deep-learning and neural-network models7\n\n\n\n\n\n\nFigure 4: Complex trees can be represented as sums of simpler trees\n\n\n\n\n\nFitting Random Forests Models\nBreiman (2001) introduced the random forests algorithm for estimating a collection of regression trees that, when added up, provided a good approximation to an unknown regression function8. The technical details of this algorithm are beyond the score of this course9\nIn this course, we will fit random forests models using the ranger package. The main function of that package is called ranger(). Its syntax is somewhat similar to glm() insofar as both functions require us to specify a formula and pass our data in as a data frame (or tibble). Unlike glm(), which required us to specify the argument family = binomial(\"logit\") to fit binary outcomes, ranger() requires us to specify the argument probability=TRUE. This signals to the function that we want predictions on the probability scale and not the \\(\\{0,1\\}\\)-outcome scale.\nWe can also use predict() to get predictions from a model fitted with ranger(). But this also involves slightly different syntax: instead of using the argument newdata, we need to use the argument data. Additionally, when making predictions based on a ranger model with probability = TRUE, the function predict returns a named list. One element of that list is called predictions and it is a matrix whose rows correspond to the rows of data and whose columns contain the probabilities \\(\\mathbb{P}(Y = 0)\\) and \\(\\mathbb{P}(Y = 1).\\) So, to get the predicting XG values, we need to look at the 2nd column of this matrix.\n\nfit &lt;-\n1  ranger::ranger(formula = Y~.,\n                 data = train_data, \n                 probability = TRUE)\ntrain_preds &lt;- \n  predict(object = fit,\n2          data = train_data)$predictions[,2]\n\ntest_preds &lt;- \n  predict(object = fit,\n3          data = test_data)$predictions[,2]\n\nlogloss(y_train, train_preds)\nlogloss(y_test, test_preds)\n\n\n1\n\nThe . is short-hand for “all of the columns in data except for what’s on the left-hand side of the ~”\n\n2\n\nWhen making a prediction using the output of ranger, we pass the data in using the data argument instead of newdata.\n\n3\n\nAs noted earlier, predict returns a list, one of whose elements is a two-column matrix containing the fitted probability. This matrix is stored in the list element predictions. The second column of this matrix contains the fitted probabilities of a goal (\\(Y = 1\\)).\n\n\n\n\n[1] 0.1135258\n[1] 0.2522654\n\n\nAt least for this training/testing split, our in-sample log-loss is much smaller than our out-of-sample log-loss. This is not at all surprising: flexible regression models like random forests are trained to predict the in-sample data and so we should expect to see smaller training errors than testing errors.\nRepeating these calculations over 100 training/testing splits, we can conclude that our more flexible random forest model, which accounts for many more features and can accommodate many more interaction terms than our simple logistic regression model, out-performs the logistic regression models.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train) |&gt;\n    dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") |&gt;\n    dplyr::select(dplyr::all_of(c(\"id\", shot_vars)))\n\n  y_train &lt;- train_data$Y\n  y_test &lt;- test_data$Y\n\n  train_data &lt;-\n    train_data |&gt;\n    dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n    dplyr::select(-id)\n  test_data &lt;-\n    test_data |&gt;\n    dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n    dplyr::select(-id)\n  \n  fit &lt;- ranger::ranger(\n    formula = Y~.,\n    data = train_data, \n    probability = TRUE)\n  train_preds &lt;- \n    predict(object = fit, data = train_data)$predictions[,2]\n\n  test_preds &lt;- \n    predict(object = fit, data = test_data)$predictions[,2]\n\n  train_logloss[r] &lt;- logloss(y_train, train_preds)\n  test_logloss[r] &lt;- logloss(y_test, test_preds)\n}\ncat(\"RandomForest training logloss:\", round(mean(train_logloss), digits = 4), \"\\n\")\n\nRandomForest training logloss: 0.1113 \n\ncat(\"RandomForest testing logloss:\", round(mean(test_logloss), digits = 4), \"\\n\")\n\nRandomForest testing logloss: 0.2705 \n\n\n\n\nComparison to StatsBomb’s XG Model\nNow that we have a much more predictive model, we can compare our model estimates to those from StatsBomb’s proprietary model. In the code below, we re-fit a random forests model to the whole dataset. We then plot our XG estimates against StatsBomb’s.\n\nfull_data &lt;-\n  wi_shots |&gt;\n  dplyr::select(dplyr::all_of(c(shot_vars)))\n\nfull_rf_fit &lt;-\n  ranger::ranger(formula = Y~.,data = full_data, probability = TRUE)\n\npreds &lt;- predict(object = fit, data = full_data)$predictions[,2]\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(wi_shots$shot.statsbomb_xg, preds,\n     pch = 16, cex = 0.5, col = rgb(0,0,0, 0.25),\n     xlim = c(0, 1), ylim = c(0,1),\n     xlab = \"StatsBomb's XG\", ylab = \"Our XG\", main = 'Comparison of XG estimates')\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nIf our model perfectly reproduced StatsBomb’s, we would expect to see all the points in the figure line up on the 45-degree diagonal. In fact, we observe some fairly substantial deviations from StatsBomb’s estimates. In particular, there are some shots for which StatsBomb’s model returns an XG of about 0.8. Our random forests model, on the other hand, returns a range of XG values for those shots. We also see that there are some shots for which StatsBomb’s model returns very small XG;s but our model returns XG’s closer to 0.5\nUltimately, the fact that our model estimates do not match StatsBomb’s is not especially concerning. For one thing, we trained our model using only about 3800 shots from women’s international matches contained in the public data. StatsBomb, on the other hand, trained their model using their full corpus of match data.\nMore substantively, StatsBomb’s model also conditions on many more features than ours. If our model estimates exactly matched StatsBomb’s, that would indicate that these extra features offered no additional predictive value. Considering that StatsBomb’s XG models condition on the height of the ball when the shot was attempted, the differences seem less surprising.",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#exercises",
    "href": "lectures/lecture03.html#exercises",
    "title": "Lecture 3: Estimating XG",
    "section": "Exercises",
    "text": "Exercises\n\nCreate a new feature ConeAngle that measures the angle formed by connecting the shot location to the goal posts (whose coordinates are (120,36) and c(120,44)). What sort of relationship would you expect between ConeAngle and XG?\nCreate a new feature ConeArea that measures the area of the triangle formed by connecting the shot location to the goal posts. What sort of relationship would you expect between ConeArea and XG?\nUsing 100 random training/testing splits, how much more predictive accuracy is gained (if any) by including ConeAngle, ConeArea, and both ConeAngle & ConeArea in our random forests model.\nIn lecture, we trained our XG models using data only from women’s international competitions. Train an XG model using data from at least 5 different competitions/seasons and assess how similar its predications are to the ones provided by StatsBomb",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#footnotes",
    "href": "lectures/lecture03.html#footnotes",
    "title": "Lecture 3: Estimating XG",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOur notation nods to the fact that predicted XG is just a predicted probability↩︎\nWe often convert categorical features like shot.body_part.name or shot.technique.name into several numerical features using one-hot encoding.↩︎\nThese are known as genearlized linear models and include things like logistic regression and Poisson regression. See Chatper 5 of Beyond Multiple Linear Regression for a nice overview.↩︎\nsee this issue in the StatsBomb’s opendata GitHub repo for more information about the variable definitions. The code to compute these values is available here. Note that in the StatsBomb coordinate system, the center of the goal line is at (120,4).↩︎\nThat is, learn the number and arrangement of the interior and leaf nodes as well as the decision rules and leaf outputs↩︎\nTo verify this decomposition is correct, imagine stacking the three partitions on top of each other.↩︎\nThe canonical reference is Grinsztajn, Oyallaon, and Varoquaux (2022) (link).↩︎\nYou can read the full paper here↩︎\nIf you want to learn more about regression trees, consider taking STAT 443.↩︎",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture00.html",
    "href": "lectures/lecture00.html",
    "title": "Lecture 0: Boxscore Metrics",
    "section": "",
    "text": "Who is the best shooter in the NBA? How do we determine this using data?\nIn this note, we will practice using functions from the tidyverse suite of packages (especially dplyr) to manipulate tables of NBA box score data. Hopefully, much of the functionality we encounter in this lecture will be familiar to you. But, if you need a high-level refresher, I highly recommend the following resources:\n\nChapter 3 and Chapter 5 of R for Data Science.\nSection 1.9 and Chapter 3 of *Data Science: A First Introduction.",
    "crumbs": [
      "Lecture 0: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture00.html#motivation-the-best-shooting-season-in-the-nba",
    "href": "lectures/lecture00.html#motivation-the-best-shooting-season-in-the-nba",
    "title": "Lecture 0: Boxscore Metrics",
    "section": "",
    "text": "Who is the best shooter in the NBA? How do we determine this using data?\nIn this note, we will practice using functions from the tidyverse suite of packages (especially dplyr) to manipulate tables of NBA box score data. Hopefully, much of the functionality we encounter in this lecture will be familiar to you. But, if you need a high-level refresher, I highly recommend the following resources:\n\nChapter 3 and Chapter 5 of R for Data Science.\nSection 1.9 and Chapter 3 of *Data Science: A First Introduction.",
    "crumbs": [
      "Lecture 0: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture00.html#sec-basic-box-score",
    "href": "lectures/lecture00.html#sec-basic-box-score",
    "title": "Lecture 0: Boxscore Metrics",
    "section": "Basic Box Score Statistics",
    "text": "Basic Box Score Statistics\nWe will use the package hoopR to scrape NBA boxscore data. You should install the package using the code\n\ninstall.packages(\"hoopR\")\n\nThe function hoopr::load_nba_player_box() loads season-level box-score data:\n\nraw_box &lt;-\n  hoopR::load_nba_player_box(seasons = 2002:(hoopR::most_recent_nba_season()))\n\nThe data table raw_box contains 813246 rows and 57 columns. Checking the column names, we see that there are columns for the numbers of field goals, three point shots, and free throws made and attempted.\n\ncolnames(raw_box)\n\n [1] \"game_id\"                           \"season\"                           \n [3] \"season_type\"                       \"game_date\"                        \n [5] \"game_date_time\"                    \"athlete_id\"                       \n [7] \"athlete_display_name\"              \"team_id\"                          \n [9] \"team_name\"                         \"team_location\"                    \n[11] \"team_short_display_name\"           \"minutes\"                          \n[13] \"field_goals_made\"                  \"field_goals_attempted\"            \n[15] \"three_point_field_goals_made\"      \"three_point_field_goals_attempted\"\n[17] \"free_throws_made\"                  \"free_throws_attempted\"            \n[19] \"offensive_rebounds\"                \"defensive_rebounds\"               \n[21] \"rebounds\"                          \"assists\"                          \n[23] \"steals\"                            \"blocks\"                           \n[25] \"turnovers\"                         \"fouls\"                            \n[27] \"plus_minus\"                        \"points\"                           \n[29] \"starter\"                           \"ejected\"                          \n[31] \"did_not_play\"                      \"active\"                           \n[33] \"athlete_jersey\"                    \"athlete_short_name\"               \n[35] \"athlete_headshot_href\"             \"athlete_position_name\"            \n[37] \"athlete_position_abbreviation\"     \"team_display_name\"                \n[39] \"team_uid\"                          \"team_slug\"                        \n[41] \"team_logo\"                         \"team_abbreviation\"                \n[43] \"team_color\"                        \"team_alternate_color\"             \n[45] \"home_away\"                         \"team_winner\"                      \n[47] \"team_score\"                        \"opponent_team_id\"                 \n[49] \"opponent_team_name\"                \"opponent_team_location\"           \n[51] \"opponent_team_display_name\"        \"opponent_team_abbreviation\"       \n[53] \"opponent_team_logo\"                \"opponent_team_color\"              \n[55] \"opponent_team_alternate_color\"     \"opponent_team_score\"              \n[57] \"reason\"                           \n\n\nNotice as well that there are columns for the game date (game_date), game id (game_id), and player (e.g., athlete_display_name). This suggests that each row corresponds to a unique combination of game and player and records the players individual statistics in that game.\nFor instance, here are the box score statistics for several players from a single game in 2011.\n\nraw_box |&gt;\n  dplyr::filter(game_date == \"2011-06-12\") |&gt;\n  dplyr::select(athlete_display_name, \n         field_goals_made, field_goals_attempted,\n         three_point_field_goals_made, three_point_field_goals_attempted,\n         free_throws_made, free_throws_attempted)\n\n── ESPN NBA Player Boxscores from hoopR data repository ───────── hoopR 2.1.0 ──\n\n\nℹ Data updated: 2025-07-31 06:39:25 CDT\n\n\n# A tibble: 30 × 7\n   athlete_display_name field_goals_made field_goals_attempted\n   &lt;chr&gt;                           &lt;int&gt;                 &lt;int&gt;\n 1 Dirk Nowitzki                       9                    27\n 2 Tyson Chandler                      2                     4\n 3 Jason Kidd                          2                     4\n 4 Shawn Marion                        4                    10\n 5 J.J. Barea                          7                    12\n 6 Brian Cardinal                      1                     1\n 7 Caron Butler                       NA                    NA\n 8 Ian Mahinmi                         2                     3\n 9 Rodrigue Beaubois                  NA                    NA\n10 DeShawn Stevenson                   3                     5\n# ℹ 20 more rows\n# ℹ 4 more variables: three_point_field_goals_made &lt;int&gt;,\n#   three_point_field_goals_attempted &lt;int&gt;, free_throws_made &lt;int&gt;,\n#   free_throws_attempted &lt;int&gt;\n\n\nAs a sanity check, we can cross-reference the data in our table with the box score from ESPN. Luckily, these numbers match up!\nIt turns out that raw_box contains much more data than we need. Specifically, it includes statistics from play-in and play-off games as well as data from some (but not all) All-Star games. Since we’re ultimately interested in identifying the best player-seasons in terms of shooting performance, we need to remove all play-off, play-in, and All-Star games from the dataset. Additionally, the column did_not_play contains a Boolean (i.e., logical) variable that is TRUE is the player did not play in the game and is FALSE if the player did not play in the game\n\nallstar_dates &lt;-\n  lubridate::date(c(\"2002-02-10\", \"2003-02-09\", \"2004-02-15\",\n    \"2005-02-20\", \"2006-02-19\", \"2007-02-18\", \n    \"2008-02-17\", \"2009-02-15\", \"2010-02-14\",\n    \"2011-02-20\", \"2012-02-26\", \"2013-02-17\", \n    \"2014-02-16\", \"2015-02-15\", \"2016-02-14\",\n    \"2017-02-19\", \"2018-02-18\", \"2019-02-17\",\n    \"2020-02-16\", \"2021-03-07\", \"2022-02-20\",\n    \"2023-02-19\", \"2024-02-18\", \"2025-02-16\"))\nreg_box &lt;-\n  raw_box |&gt;\n  dplyr::filter(\n    season_type == 2 & \n      !did_not_play & \n      !game_date %in% allstar_dates)\n\nLooking at the data table reg_box, we see that in about 9% of rows, the number of minutes played is missing. These likely correspond to players who were active but did not play or logged only a few seconds (generally at the end of games). We will replace these NA values with 0’s and, while doing so, rename some of the columns in reg_box.\n\nreg_box &lt;-\n  reg_box |&gt;\n1  dplyr::rename(\n    Player = athlete_display_name,\n    FGM = field_goals_made,\n    FGA = field_goals_attempted,\n    TPM = three_point_field_goals_made,\n    TPA = three_point_field_goals_attempted,\n    FTM = free_throws_made, \n    FTA = free_throws_attempted) |&gt;\n  dplyr::mutate(\n2    FGM = ifelse(is.na(minutes), 0, FGM),\n    FGA = ifelse(is.na(minutes), 0, FGA),\n    TPM = ifelse(is.na(minutes), 0, TPM),\n    TPA = ifelse(is.na(minutes), 0, TPA),\n    FTM = ifelse(is.na(minutes), 0, FTM),\n    FTA = ifelse(is.na(minutes), 0, FTA)) |&gt;\n3  tidyr::replace_na(list(minutes = 0))\n\n\n1\n\nRename several variables\n\n2\n\nFor those rows where minutes is NA, set the numbers of makes and attempts to 0\n\n3\n\nReplace missing minutes values with 0\n\n\n\n\nAt this point, every row of reg_box corresponds to a player-game combination. We ultimately wish to sum up the number of makes and misses of each shot type across an entire season for each player. To illustrate this, let’s focus on Dirk Nowitzki’s performance in the 2006-07 season when he won the league MVP award. Conceptually, we can accomplish this by first dividing the full data table into several smaller tables, one for each combination of player and season. Then, we can sum the number of field goals, three point shots, and free throws attempted and made by each player in each of their season. This is an example of the split-apply-combine strategy in which you “break up a big problem into manageable pieces, operate on each piece independently, and then put all the pieces back together” (Wickham 2011). This functionality is implemented using dplyr::group_by()\n\nseason_box &lt;-\n  reg_box |&gt;\n  dplyr::group_by(Player, season) |&gt;\n  dplyr::summarise(\n    FGM = sum(FGM),\n    FGA = sum(FGA),\n    TPM = sum(TPM),\n    TPA = sum(TPA),\n    FTM = sum(FTM),\n    FTA = sum(FTA),\n    minutes = sum(minutes),\n    n_games = dplyr::n(),\n    .groups = \"drop\")\n\nThe data table season_box contains 11920 rows, each of corresponds to a single player-season combination. Here is a quick snapshot of some of the data for Dirk Nowitzki\n\nseason_box |&gt;\n  dplyr::filter(Player == \"Dirk Nowitzki\") |&gt;\n  dplyr::select(season, FGM, FGA, TPM, TPA, FTM, FTA)\n\n# A tibble: 18 × 7\n   season   FGM   FGA   TPM   TPA   FTM   FTA\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   2002   600  1258   139   350   440   516\n 2   2003   690  1489   148   390   483   548\n 3   2004   605  1310    99   290   371   423\n 4   2005   663  1445    91   228   615   708\n 5   2006   751  1564   110   271   539   598\n 6   2007   673  1341    72   173   498   551\n 7   2008   630  1314    79   220   478   544\n 8   2009   774  1616    61   170   485   545\n 9   2010   720  1496    51   121   536   586\n10   2011   610  1179    66   168   395   443\n11   2012   473  1034    78   212   318   355\n12   2013   332   707    63   151   164   191\n13   2014   633  1273   131   329   338   376\n14   2015   487  1062   104   274   255   289\n15   2016   498  1112   126   342   250   280\n16   2017   296   678    79   209    98   112\n17   2018   346   758   138   337    97   108\n18   2019   135   376    64   205    39    50",
    "crumbs": [
      "Lecture 0: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture00.html#sec-percentages",
    "href": "lectures/lecture00.html#sec-percentages",
    "title": "Lecture 0: Boxscore Metrics",
    "section": "From Totals to Percentages",
    "text": "From Totals to Percentages\nIn order to determine which player-season was the best in terms of shooting, we need to first define “best”. Perhaps the simplest definition is to find the player-season with the most made shots. We can identify this player-season by sorting the data in season_box by FGM in descending order with the dplyr::arrange() function\n\nseason_box |&gt;\n  dplyr::arrange(dplyr::desc(FGM))\n\n# A tibble: 11,920 × 10\n   Player             season   FGM   FGA   TPM   TPA   FTM   FTA minutes n_games\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n 1 Kobe Bryant          2006   949  2109   179   506   675   788    3184      78\n 2 LeBron James         2006   875  1823   127   379   601   814    3361      82\n 3 Kobe Bryant          2003   868  1924   124   324   601   713    3401      82\n 4 Shai Gilgeous-Ale…   2025   868  1680   165   444   604   673    2633      77\n 5 LeBron James         2018   857  1580   149   406   388   531    3024      82\n 6 Dwyane Wade          2009   854  1739    88   278   590   771    3048      82\n 7 Kevin Durant         2014   849  1688   192   491   703   805    3118      81\n 8 James Harden         2019   843  1909   378  1028   754   858    2870      78\n 9 Giannis Antetokou…   2024   837  1369    34   124   514   782    2573      73\n10 Tracy McGrady        2003   829  1813   173   448   576   726    2954      75\n# ℹ 11,910 more rows\n\n\nWhen we look at the ten “best” shooting seasons, we immediately recognize a lot of superstar players! On this basis, we might be satisfied evaluating shooting performances based only on the total number of shots. But taking a closer look, should we really consider Kobe Bryant’s 2002-03 and Shai Gilgeous-Alexander’s 2024-25 seasons to be equally impressive when Kobe took attempted 242 more shots than Shai in order to make 868 shots? Arguably, Shai’s 2024-25 season should rank higher than Kobe’s 2002-03 season because Shai was more efficient.\nThis motivates us to refine our definition of “best” by focusing on the percentage of field goals made rather than total number of field goals made.\n\nseason_box &lt;-\n  season_box |&gt;\n1  dplyr::mutate(FGP = ifelse(FGA &gt; 0, FGM/FGA, NA_real_))\nseason_box |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, FGP)\n\n\n1\n\nFor players who attempted no field goals (i.e., FGA = 0), their field goal percentage is undefined.\n\n\n\n\n# A tibble: 11,920 × 3\n   Player           season   FGP\n   &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;\n 1 Ahmad Caver        2022     1\n 2 Alondes Williams   2025     1\n 3 Andris Biedrins    2014     1\n 4 Anthony Brown      2018     1\n 5 Braxton Key        2023     1\n 6 Chris Silva        2023     1\n 7 Dajuan Wagner      2007     1\n 8 DeAndre Liggins    2014     1\n 9 Donnell Harvey     2005     1\n10 Eddy Curry         2009     1\n# ℹ 11,910 more rows\n\n\nSorting the players by their \\(\\textrm{FGP},\\) we find that several players made 100% of their field goals. But very few of these players are immediately recognizable — and, indeed, none of them have been in the MVP conversation, despite the fact that they made all their shots!\nTo understand what’s going on, let’s take a look at the number of attempts.\n\nseason_box |&gt; \n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, FGP, FGA)\n\n# A tibble: 11,920 × 4\n   Player           season   FGP   FGA\n   &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ahmad Caver        2022     1     1\n 2 Alondes Williams   2025     1     2\n 3 Andris Biedrins    2014     1     1\n 4 Anthony Brown      2018     1     1\n 5 Braxton Key        2023     1     1\n 6 Chris Silva        2023     1     1\n 7 Dajuan Wagner      2007     1     1\n 8 DeAndre Liggins    2014     1     1\n 9 Donnell Harvey     2005     1     2\n10 Eddy Curry         2009     1     2\n# ℹ 11,910 more rows\n\n\nGiven the very low number of shots attempted in any of these player-season, claiming that any of these player-seasons are among the best ever would strain credulity! So, in order to determine the best shooting performance, we will need to threshold our data to players who took a minimum number of shots. For simplicity, let’s focus our attention on those players who attempted at least 400 field goals in a season (i.e., they attempted, on average, at least 5 shots per game).\n\nseason_box |&gt;\n  dplyr::filter(FGA &gt;= 400) |&gt;\n  dplyr::arrange(dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, FGP,FGA)\n\n# A tibble: 4,987 × 4\n   Player         season   FGP   FGA\n   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Daniel Gafford   2024 0.725   480\n 2 Walker Kessler   2023 0.720   414\n 3 DeAndre Jordan   2017 0.714   577\n 4 Rudy Gobert      2022 0.713   508\n 5 DeAndre Jordan   2015 0.710   534\n 6 Jarrett Allen    2025 0.706   640\n 7 Nic Claxton      2023 0.705   587\n 8 DeAndre Jordan   2016 0.703   508\n 9 Daniel Gafford   2025 0.702   403\n10 Daniel Gafford   2022 0.693   411\n# ℹ 4,977 more rows\n\n\nDo we really believe that these performances, all of which were made centers who mostly shoot at or near the rim, represent some of the best shooting performances of all time?\n\nEffective Field Goal Percentage\nA major limitation of FGP is that it treats 2-point shots the same as 3-point shots. As a result, the league-leader in FGP every season is usually a center whose shots mostly come from near the rim. Effective Field Goal Percentage (eFGP) adjusts FGP to account for the fact that a made 3-point shots is worth 50% more than a made 2-point shot. The formula for eFGP is \\[\n\\textrm{eFGP} = \\frac{\\textrm{FGM} + 0.5 \\times \\textrm{TPM}}{\\textrm{FGA}}\n\\]\n\nseason_box &lt;-\n  season_box |&gt;\n  dplyr::mutate(\n    TPP = ifelse(TPA &gt; 0, TPM/TPA,NA_real_),\n    eFGP = (FGM + 0.5 * TPM)/FGA) \nseason_box |&gt;\n  dplyr::filter(FGA &gt;= 400) |&gt;\n  dplyr::arrange(dplyr::desc(eFGP), dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, eFGP, FGP, TPP, TPA, n_games)\n\n# A tibble: 4,987 × 7\n   Player         season  eFGP   FGP    TPP   TPA n_games\n   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Daniel Gafford   2024 0.725 0.725 NA         0      74\n 2 Walker Kessler   2023 0.721 0.720  0.333     3      74\n 3 DeAndre Jordan   2017 0.714 0.714  0         2      81\n 4 Rudy Gobert      2022 0.713 0.713  0         4      66\n 5 DeAndre Jordan   2015 0.711 0.710  0.25      4      82\n 6 Jarrett Allen    2025 0.706 0.706  0         5      82\n 7 Nic Claxton      2023 0.705 0.705  0         2      76\n 8 DeAndre Jordan   2016 0.703 0.703  0         1      77\n 9 Daniel Gafford   2025 0.702 0.702 NA         0      57\n10 Daniel Gafford   2022 0.693 0.693  0         1      72\n# ℹ 4,977 more rows\n\n\nWe see again that some of the best seasons, according to eFGP, were from centers, many of whom attempt few few three point shots. When filter out players who took at least 100 three point shots, we start to see other positions in the top-10.\n\nseason_box |&gt;\n  dplyr::filter(FGA &gt;= 400 & TPA &gt;= 100) |&gt;\n  dplyr::arrange(dplyr::desc(eFGP), dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, eFGP, FGP, TPP, TPA, n_games)\n\n# A tibble: 3,658 × 7\n   Player             season  eFGP   FGP   TPP   TPA n_games\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Kyle Korver          2015 0.671 0.487 0.492   449      75\n 2 Duncan Robinson      2020 0.667 0.470 0.446   606      73\n 3 Obi Toppin           2024 0.660 0.571 0.404   260      83\n 4 Nikola Jokic         2023 0.660 0.632 0.383   149      69\n 5 Joe Harris           2021 0.655 0.502 0.478   427      69\n 6 Joe Ingles           2021 0.652 0.489 0.451   406      67\n 7 Grayson Allen        2024 0.649 0.499 0.461   445      75\n 8 Michael Porter Jr.   2021 0.646 0.541 0.447   374      61\n 9 Mikal Bridges        2021 0.643 0.543 0.425   315      72\n10 Al Horford           2024 0.640 0.511 0.419   258      65\n# ℹ 3,648 more rows\n\n\n\n\nTrue Shooting Percentage #{sec-tsp}\nBoth FGP and eFGP totally ignore free throws. Intuitively, we should expect the best shooter to be proficient at making two-and three-point shots as well as their free throws. One metric that accounts for all field goals, three pointers, and free throws is true shooting percentage (\\(\\textrm{TSP}\\)), whose formula is given by \\[\n\\textrm{TSP} = \\frac{\\textrm{PTS}}{2 \\times \\left(\\textrm{FGA} + (0.44 \\times \\textrm{FTA})\\right)},\n\\] where \\(\\textrm{PTS} =  \\textrm{FTM} + 2 \\times \\textrm{FGM} + \\textrm{TPM}\\) is the total number of points scored.\n\nseason_box &lt;-\n  season_box |&gt;\n  dplyr::mutate(\n    PTS = FTM + 2 * FGM + TPM,\n    TSP = PTS/(2 * (FGA + 0.44 * FTA)))\nseason_box |&gt;\n  dplyr::filter(FGA &gt;= 400 & TPA &gt;= 100) |&gt;\n  dplyr::arrange(dplyr::desc(TSP), dplyr::desc(eFGP), dplyr::desc(FGP)) |&gt;\n  dplyr::select(Player, season, TSP, eFGP, FGP, TPP, n_games)\n\n# A tibble: 3,658 × 7\n   Player          season   TSP  eFGP   FGP   TPP n_games\n   &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Nikola Jokic      2023 0.701 0.660 0.632 0.383      69\n 2 Kyle Korver       2015 0.699 0.671 0.487 0.492      75\n 3 Austin Reaves     2023 0.687 0.616 0.529 0.398      64\n 4 Duncan Robinson   2020 0.684 0.667 0.470 0.446      73\n 5 Dwight Powell     2019 0.682 0.637 0.597 0.307      77\n 6 Grayson Allen     2024 0.679 0.649 0.499 0.461      75\n 7 Kevin Durant      2023 0.677 0.614 0.560 0.404      47\n 8 Moritz Wagner     2024 0.676 0.636 0.601 0.330      80\n 9 Stephen Curry     2018 0.675 0.618 0.495 0.423      51\n10 Obi Toppin        2024 0.675 0.660 0.571 0.404      83\n# ℹ 3,648 more rows",
    "crumbs": [
      "Lecture 0: Boxscore Metrics"
    ]
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Project 2 Information",
    "section": "",
    "text": "For your first project, I would like you to use simulation to estimate probabilities of game-, season-, tournament-, or draft-level outcomes. Ideally, you will go beyond computing probabilities of events like “Team A beats team B” and will focus on more complex events like “Team A makes it to the third-round of March Madness tournament” or “Team A and Team B make it to the finals and Team wins”. Obviously, the types of events of interest will depend on the specific sport you choose and the level of outcome you are modeling. Whatever your choice, the simulation must be based on a probabilistic model fitted using publicly available box-score, play-by-play, or event-level data. See Section 3 for an overview of the types of models that may be useful for this project.\nThe project report and recorded presentation are due on Friday November 7 at 12:00pm (noon)."
  },
  {
    "objectID": "project2.html#overivew",
    "href": "project2.html#overivew",
    "title": "Project 2 Information",
    "section": "",
    "text": "For your first project, I would like you to use simulation to estimate probabilities of game-, season-, tournament-, or draft-level outcomes. Ideally, you will go beyond computing probabilities of events like “Team A beats team B” and will focus on more complex events like “Team A makes it to the third-round of March Madness tournament” or “Team A and Team B make it to the finals and Team wins”. Obviously, the types of events of interest will depend on the specific sport you choose and the level of outcome you are modeling. Whatever your choice, the simulation must be based on a probabilistic model fitted using publicly available box-score, play-by-play, or event-level data. See Section 3 for an overview of the types of models that may be useful for this project.\nThe project report and recorded presentation are due on Friday November 7 at 12:00pm (noon)."
  },
  {
    "objectID": "project2.html#simulation-requirements",
    "href": "project2.html#simulation-requirements",
    "title": "Project 2 Information",
    "section": "Simulation Requirements",
    "text": "Simulation Requirements\nRegardless of the sport you choose and the underlying probabilistic model, you need to run enough simulations to ensure that the estimates of the probabilities of interest are reliable. I recommend at least 10,000 simulation replications. But if you are simulating fairly complex events (e.g., the entire March Madness Tournament or every point in the entire NCAA Women’s Volleyball season), you may only have time to run fewer simulation replications. In those cases, I would request that you run at least 500 simulation replications. One way to mitigate the computational burden is to divide simulation replications across different team members’ computers (making sure to set different random seeds for each replication)."
  },
  {
    "objectID": "project2.html#sec-models",
    "href": "project2.html#sec-models",
    "title": "Project 2 Information",
    "section": "Model Choice",
    "text": "Model Choice\nDepending on the sport and type of event you choose, you may find it useful to fit a Bradley-Terry model, Markov chain model, or a Plackett-Luce model.\n\nBradley-Terry Models\nBradley-Terry models are a very natural model for paired competitions. A perfectly acceptable project would involve identifying a particular sport, fitting a Bradley-Terry model to a season’s worth of match-level outcomes, and then simulating the entire season or a season-ending tournament using the fitted model probabilities.\nBradley-Terry models can also be used to model more granular in-game events. For instance, in racket sports (e.g., tennis, badminton, table tennis, squash, and racquetball) and volleyball, in which one competitor serves the ball to another and competitors exchange a series of shots until a point is scored, one could use a Bradley-Terry model to estimate the probability that the serving team wins a point. Using such a model, one can simulate events at the individual game-level (e.g., whether the match lasts 3 or 4 sets, overall winner, whether team wins a set by more than 5 points), and season-level outcomes (e.g., whether a team wins at least 5 games or wins a championship) by simulating each point of each game.\nOne could even build a Bradley-Terry model to model matchups between individual players in team sports. For instance, one could fit a Bradley-Terry model to estimate the probability that any given batter “wins” a matchup (i.e., gets a hit) against any given pitcher or that a particular wide receiver wins a matchup against a particular defensive back1\n\n\nExtending Bradley-Terry Models\nIn Lecture 12 and Lecture 13, we fit Bradley-Terry models in which every team was assigned a latent strength parameter \\(\\lambda.\\) In principle, these strengths could depend on certain covariates. For instance, in basketball, one might obtain more accurate predictions by allowing each team’s \\(\\lambda\\) to vary with their offensive and defensive rating. Or, in tennis, the latent strength of each player might vary systematically with player characteristics like height.\nThe BradelyTerry2 package allows one to fit such models in which the the latent strength for team \\(j\\) can be decomposed as \\[\n\\lambda_{j} = u_{j} + \\boldsymbol{\\mathbf{x}}_{j}^{\\top}\\beta,\n\\] where \\(\\boldsymbol{\\mathbf{x}}_{j}\\) is a vector of team-level covariates and \\(u_{j}\\) is a team-specific intercept, capturing all parts of team strength not already explained by the covariates.\nIf you do a project that uses a Bradley-Terry model, I encourage you to investigate the possibility that the latent strengths might vary with respect to covariates."
  },
  {
    "objectID": "project2.html#markov-chain-models",
    "href": "project2.html#markov-chain-models",
    "title": "Project 2 Information",
    "section": "Markov chain Models",
    "text": "Markov chain Models\nIf you choose to analyze a sport in which games (or discrete components of a game) invovles a series of state transitions, a Markov chain model can be used to an simulate entire game (or portions of a game). As we saw in Lecture 14, Markov chains can be used to simulate the progression of at-bats within a half-inning. One could take this a step further and model the transition of game-states at a pitch-by-pitch level. Beyond baseball and softball, one can also study American football with a Markov chain in which the states are indexed by factors including, but certainly not limited to, down, distance, and field position. One could also build Markov chains for volleyball and other racquet sports to simulate the trajectories of individual scores (e.g., 0-0, 15-0, 15-15, 30-15, 40-15, game). It may even be possible to use a Markov chain model to simulate a cricket innings at the ball-by-ball or over-by-over level.\nIf you choose to base your simulation around a Markov chain model, you need to carefully define your state-space, identify any potential absorbing states, and estimate the transition probabilities. Depending on the amount of available data, these probabilities may be estimated with a simple binning-or-averaging procedure. But if there is not much data, you may need to fit a multinomial logistic regression model or use an multilevel model to estimate these probabilities.\n\nPlackett-Luce Models\nPlackett-Luce models derive a consensus ranking of several items based on several partial rankings of those same items. This makes them a natural way to aggregate multiple mock drafts. But they can also be used to derive power rankings in situations where games are not head-to-head. For instance, one could derive a power ranking of F1 drivers or teams, runners, cyclists, or swimmers by aggregating finishing time results across multiple races with a Plackett-Luce model. One could also derive a power ranking of golfers based on their finishing positions in multiple tour events."
  },
  {
    "objectID": "project2.html#deliverables",
    "href": "project2.html#deliverables",
    "title": "Project 2 Information",
    "section": "Deliverables",
    "text": "Deliverables\nThe deliverables for Project 2 are the same as for Project 1 and carry similar requirements. The main difference is that you must clearly state and motivate the events of interest (i.e., those whose probabilities you are estimating via simulation) in both your written report and presentation.\n\nWritten Report\nThe written report consists of a non-technical executive summary and a technical report. The executive summary, which should not exceed 500 words, should describe the overall goals, analytic approach, and main conclusions in non-technical language. The executive summary should be free from jargon, code listings, figures, tables, and charts. It should be written to be read and understood by a front office executive, coach, player, or fan with little data science experience. The rest of written report should\n\nClearly state the problem being studied and provide sufficient background details and to motivate why the problem is important and interesting.\nDescribe the data and major steps of the analysis\nPresents the main results within the context of the relevant sport(s) and supports the results with figures, tables, charts, and other statistical software output as appropriate.\nDiscusses the limitations of the analysis and outlines concrete steps for further development.\n\nThe technical section of the report should contain enough detail and code that another data scientist could replicate your analysis verify its soundness. Code listings and output (e.g., figures, tables, charts, and numerical summaries) should be tightly integrated with the written exposition. A good example of such integration is here. Pay particular attention to the way the author complements the numerical results with detailed examples of individual performances.\n\n\nPresentation\nEach team will also record an 8–10 minute presentation (e.g., using Zoom) that provides an overview of their analysis. Each presentation should include the following elements\n\nBackground (2–4 slides): clearly motivate and state the main problem being studied. Explain why it is interesting and important. Present just enough background to motivate the problem, while taking care not to overwhelm the audience with extraneous details. If appropriate, comment on the limitations of existing solutions to the problem or closely-related problems\nAnalysis overview (2–4 slides): present only the main steps of your analysis. Be sure to explain why each step was necessary and how these steps contribute to the overall solution. Focus more on the high-level ideas and motivation for each step rather than the specific implementation or software syntax\nMain results (2–3 slides): distill your results into a few key points. Use figures, tables, charts, and other statistical software output to support your findings.\nConclusion (1 slide): briefly summarize your analysis and findings and outline between 1 and 3 specific directions for future development, improvement or refinement."
  },
  {
    "objectID": "project2.html#footnotes",
    "href": "project2.html#footnotes",
    "title": "Project 2 Information",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you choose to pursue this option, you may need to use tracking data from previous year’s Big Data Bowl competitions to extract specific match-up information.↩︎"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Illustrates the use of statistical modeling and data science techniques to derive actionable insights from sports data. Emphasizes not only technical calculation of advanced metrics but also on written and oral communication to other data scientists and to non-technical audience. Topics may include: deriving team rankings from paired competitions; measuring an individual player’s contribution to their team’s overall success; assessing player performance and team strategy in terms of expected outcomes; forecasting the impact of new rule changes using simulation; and creating new metrics using high-resolution player tracking data."
  },
  {
    "objectID": "syllabus.html#sec-description",
    "href": "syllabus.html#sec-description",
    "title": "Syllabus",
    "section": "",
    "text": "Illustrates the use of statistical modeling and data science techniques to derive actionable insights from sports data. Emphasizes not only technical calculation of advanced metrics but also on written and oral communication to other data scientists and to non-technical audience. Topics may include: deriving team rankings from paired competitions; measuring an individual player’s contribution to their team’s overall success; assessing player performance and team strategy in terms of expected outcomes; forecasting the impact of new rule changes using simulation; and creating new metrics using high-resolution player tracking data."
  },
  {
    "objectID": "syllabus.html#sec-learning-outcomes",
    "href": "syllabus.html#sec-learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nThroughout the course you will\n\nImplement appropriate statistical methods to assess player and team performance\nWork with play-by-play and high-resolution tracking data\nProvide constructive and actionable feedback on your peers’ analytic reports\nBuild a personal portfolio of sports data analyses"
  },
  {
    "objectID": "syllabus.html#sec-course-requisites",
    "href": "syllabus.html#sec-course-requisites",
    "title": "Syllabus",
    "section": "Requisites",
    "text": "Requisites\nThis course will make extensive use of the R programming language through the RStudio integrated development environment (IDE). Because the formal pre-requisites for this course are STAT 333 or 340, you are expected to have previous experience using the R programming language.\n\n\n\n\n\n\nWarning\n\n\n\nIf you do not meet the formal course prerequisites and/or have not used R in a previous course, this is not the right course for you. Exceptions will not made to the formal course requisites.\n\n\nI will assume fluency with basic R functionality (e.g., assignment, writing and executing scripts, saving data objects, setting environments, installing and loading packages), data manipulation with dplyr and other tidyverse packages, and visualization using either base R graphics or ggplot2. I will additionally assume some familiarity with fitting statistical models in R and interpreting their output (e.g., using lm and glm). Here is an example of the type of R coding with which I will assume you are familiar. If you need a refresher on some of the data wrangling in that document, I strongly recommend reviewing your old course notes as well as\n\nChapter 3 and Chapter 5 of R for Data Science.\nSection 1.9 and Chapter 3 of Data Science: A First Introduction."
  },
  {
    "objectID": "syllabus.html#sec-course-staff",
    "href": "syllabus.html#sec-course-staff",
    "title": "Syllabus",
    "section": "Course Staff & Office Hours",
    "text": "Course Staff & Office Hours\nInstructor: Sameer Deshpande (sameer.deshpande@wisc.edu). Instructor office hours will take place on:\n\nMondays from 11:00am to 12:00pm in Morgridge Hall 5586 (beginning 9/8/25).\nWednesdays from 3:00pm to 4:00pm in Morgridge Hall 5586 (beginning 9/3/25)\nFridays from 3:00pm to 4:30pm in Morgridge Hall 5618 (beginning 9/5/25)\n\nOffice hours on Mondays and Wednesdays are designed for one-on-one meetings with the instructor. While the main purpose of Monday and Wednesday office hours is to discuss your specific experience and learning in the course, feel free to stop by to chat about your educational and career goals and/or any personal sports analytic projects you might be pursuing.\nFriday office hours are intended for collaborative and small group work. During Friday office hours, small groups can step through analyses from lecture, work on the exercises, and on their team projects with the instructor and other students. Friday office hours are also a great way to brainstorm project ideas with and to get feedback from other teams.\nIf you have specific questions about the course content (e.g., parsing a particular bit of syntax or understanding a step in an analysis), I encourage you to ask the question on Piazza and to attend Friday office hours. If you cannot make the Monday or Wednesday office hours, please email me and suggest some times at which you are free.\nTeaching Assistant: Zhexuan Liu (zhexuan.liu2@wisc.edu). TA office hours will be held 9:15am - 10:45am on Tuesday and Thursdays in Morgridge Hall 2515. TA office hours will begin on 9/16/25."
  },
  {
    "objectID": "syllabus.html#sec-assignments",
    "href": "syllabus.html#sec-assignments",
    "title": "Syllabus",
    "section": "Assignments & Grading",
    "text": "Assignments & Grading\nYour final grade will be based on your performance on three group projects and your overall participation. For each project, groups can earn up to 100 points for their written report and up to 100 points for their recorded presentation. After each project, every student will peer review three reports and three presentations and complete a team accountability survey. You can earn a total of 10 points for each peer review by (i) completing the provided rubric and (ii) leaving a constructive comment that identities strengths and areas for potential improvement. Students will earn up to 20 points per survey: 10 for completing the survey and 10 based on the feedback from their teammates and their own self-assessment.\nWe will use Canvas to handle report and presentation submissions, peer reviews, and team accountability surveys.\n\nGroup Projects\nThere will be three group projects. Each project consists of two parts, a written report and a group presentation. The project due dates are October 10, November 7, and December 5.\n\nWritten Report\nThe written report consists of a non-technical executive summary and a technical report. The executive summary, which should not exceed 500 words, should describe the overall goals, analytic approach, and main conclusions in non-technical language. The executive summary should be free from jargon, code listings, figures, tables, and charts. It should be written to be read and understood by a front office executive, coach, player, or fan with little data science experience. The rest of written report should\n\nClearly state the problem being studied and provide sufficient background details and to motivate why the problem is important and interesting.\nDescribe the data and major steps of the analysis\nPresents the main results within the context of the relevant sport(s) and supports the results with figures, tables, charts, and other statistical software output as appropriate.\nDiscusses the limitations of the analysis and outlines concrete steps for further development.\n\nThe technical section of the report should contain enough detail and code that another data scientist could replicate your analysis verify its soundness. Code listings and output (e.g., figures, tables, charts, and numerical summaries) should be tightly integrated with the written exposition. The use of Quarto or RMarkdown is highly recommended for preparing the written report.\n\n\nPresentation\nEach team will also record an 8–10 minute presentation (e.g., using Zoom) that provides an overview of their analysis. Each presentation should include the following elements\n\nBackground (2–4 slides): clearly motivate and state the main problem being studied. Explain why it is interesting and important. Present just enough background to motivate the problem, while taking care not to overwhelm the audience with extraneous details. If appropriate, comment on the limitations of existing solutions to the problem or closely-related problems\nAnalysis overview (2–4 slides): present only the main steps of your analysis. Be sure to explain why each step was necessary and how these steps contribute to the overall solution. Focus more on the high-level ideas and motivation for each step rather than the specific implementation or software syntax\nMain results (2–3 slides): distill your results into a few key points. Use figures, tables, charts, and other statistical software output to support your findings.\nConclusion (1 slide): briefly summarize your analysis and findings and outline between 1 and 3 specific directions for future development, improvement or refinement.\n\n\n\n\nPeer Reviews\nAfter every group project, every student is expected to peer-review the written report and presentations of 3 other teams. The primary purpose of this peer review is to practice providing constructive feedback on both technical and non-technical writing to your fellow data scientists. Peer reviews will be completed on Canvas and a structured evaluation rubric will be provided for each assignment. The peer review process will be single-blinded so teams will not know the identities of their reviewers. Students can earn a total of 20 points for each peer review by (i) completing the provided rubric and (ii) leaving a constructive comment that identities strengths and areas for potential improvement. Peer reviews are due one week after the project assignment due dates; that is, they are due on October 17, November 14, and December 12. A penalty will be assessed for submitting late reviews.\n\n\nTeam Accountability Survey\nAfter each project submission, you will fill out a team accountability survey. You can earn up to 40 points per project submission: 10 for completing the survey and 10 based on the feedback from their teammates and their own self-assessment. In total, students can earn up to 120 points based on their level of participation in their project groups.\n\n\nParticipation\nAn additional 100 points will be awarded based on participation during class, office hours, and online discussions on Piazza.\n\n\nFinal Grades\nYour final grade will be based on how many of the 1000 total points you earn during the semester. Over 925 points is at least an A, over 875 points is at least an AB, over 800 points is at least a B, over 700 is at least a BC, over 600 points is at least a C, and over 500 point is at least a D. Final grade boundaries will be announced no later than December 5, 2025."
  },
  {
    "objectID": "syllabus.html#sec-course-sites",
    "href": "syllabus.html#sec-course-sites",
    "title": "Syllabus",
    "section": "Canvas & Piazza",
    "text": "Canvas & Piazza\nWe will use Canvas for course announcement, assignment submission, and grading. The Canvas course page is accessible via this link. We will also use Piazza for specific discussions about course content (e.g., sharing additional information/resources related to material discussed in lecture; answering questions about data, method, and code; etc.) and more general discussions about sports analytics (e.g., sharing job postings, open data competition opportunities, popular press articles, and other analyses you might find online etc.) The Piazza page is accessible via this link.\n\n\n\n\n\n\nAccess to Canvas and Piazza\n\n\n\nThe Canvas and Piazza sites associated with this course are limited to students enrolled in the course and the course teaching staff. Although they can freely access the course notes and code, auditors, UW–Madison students not currently enrolled in the course, and anyone not affiliated with UW–Madison will not have access to Canvas and Piazza sites. Requests to access these sites will be ignored."
  },
  {
    "objectID": "syllabus.html#sec-resources",
    "href": "syllabus.html#sec-resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThe course will make extensive use of R. The following are excellent references and I highly encourage you to read and consult them as needed:\n\nR for Data Science by Wickham, Çentikaya-Rundel, & Grolemud\nThe Quarto guide\nRMarkdown: The Definitive Guide by Xie, Allaire, & Grolemud\nAn Introduction to Statistical Learning by James, Wittin, Hastie, and Tibshirani\nData Science: A First Introduction by Timbers, Campbell, and Lee\nBeyond Linear Modeling: Applied Generalized Linear Models and Multilevel Models in R by Roback and Leger\n\nYou may also find the following websites, blogs, and podcasts useful as sources of inspiration as you develop analyses of your own.\n\nThe online textbook Analyzing Baseball Data with R and the associated blog, which contains lots of helpful resources for analyzing tabular box score data to high-resolution ball tracking data and everything in between\nRon Yurko’s Substack [``Statistical Thinking in Sports Analytics’’] (https://statthinksportsanalytics.substack.com)\nThe : the premiere venue for academic sports analytics articles. You should have access to all articles through the UW Libraries. Each issue also features a publicly available ``Editor’s Choice’’ article.\nThe [Open Source Sports podcast] (https://open.spotify.com/show/3vTtH2JJXbjrzOtEfjrqc4): Each episode of this podcast focuses on a single academic research paper featuring authors as guests, with discussions about the statistical methodology, relevance and future directions of the research.\nHockey Graphs: a blog that’s developed really innovative public-facing hockey analytics.\nThe [Wharton Moneyball Post Game Podcast] (https://knowledge.wharton.upenn.edu/shows/moneyball-highlights/): a podcast version of the popular Sirius XM radio show hosted by several Wharton professors and featuring interviews with sport analytics thought leaders"
  },
  {
    "objectID": "syllabus.html#sec-academic-integrity",
    "href": "syllabus.html#sec-academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nI take academic integrity extremely seriously. While I encourage you to collaborate with your classmates on the problem sets, you are expected to write up your own solutions. You must acknowledge any and all sources that you consulted, whether it be a reference book, online resource, or another person (in the class or otherwise). You may not post, share, or upload any course material, including lecture slides, code, project reports, and presentation recordings, to websites like StackOverflow, Quora, Chegg, and Reddit or to ChatGPT or a similar generative AI service."
  },
  {
    "objectID": "syllabus.html#sec-ai",
    "href": "syllabus.html#sec-ai",
    "title": "Syllabus",
    "section": "Policy on the Use of Generative Artificial Intelligence Models",
    "text": "Policy on the Use of Generative Artificial Intelligence Models\nYou have the right to the full benefits of my expertise and engagement in this course. So, I will never use AI to - Provide feedback on assignments - Prepare any course content (e.g., slides, code, assignments, etc.) - Mediate or assist communications with you In other words, everything you see in this course was created by me without the aid of generative AI.\nWhile the Statistics Department recognizes the potential benefits of AI, its use in academic work can be problematic. Insofar as this course is really about process and not final results, I believe strongly that the use of generative AI is at odds with the learning goals. So, three rules regarding the use of ChatGPT and other generative AI models will be enforced:\n\nPassing off AI-generated responses as original student work constitutes plagiarism and is strictly prohibited. Any students found to be engaging in this practice will be cited for academic misconduct.\nUnless explicitly authorized by the instructor to do so, any use of AI-generated responses as sources of information, even with documentation and attribution, is prohibited.\nYou may not, under any circumstances, upload any course material to a generative AI model or agent. This includes lecture slides and notes and reports or videos uploaded by other student groups.\n\n\n\n\n\n\n\nPeer Review\n\n\n\nRespect your classmates enough to review their projects yourself. Uploading someone else’s project report or presentation to a generative AI tool (e.g., for creating summaries) is forbidden and will result in a failing grade."
  },
  {
    "objectID": "syllabus.html#additional-information",
    "href": "syllabus.html#additional-information",
    "title": "Syllabus",
    "section": "Additional Information",
    "text": "Additional Information\n\nHow credit hours are met\nThis class meets for two 75-minute class periods each week over the semester and carries the expectation that students will work on course learning activities (reading, writing, problem sets, studying, etc.) for about three hours out of the classroom for every class period.\n\n\nRegular and substantive student-instructor interaction.\nSubstantive interaction occurs via two channels: (i) regular class meetings in which students are encouraged to engage in discussion and (ii) weekly office hours.\n\n\nEthics\nThe members of the faculty of the Department of Statistics at UW–Madison uphold the highest ethical standards of teaching, data, and research. We expect our students to uphold the same standards of ethical conduct. The American Statistical Association’s standards for ethical conduct in data analysis and data privacy are available at and include:\n\nUse methodology and data that are relevant and appropriate; without favoritism or prejudice; and in a manner intended to produce valid, interpretable, and reproducible results.\nBe candid about any known or suspected limitations, defects, or biases in the data that may affect the integrity or the reliability of the analysis. Obviously, never modify or falsify data\nProtect the privacy and confidentiality of research subjects and data concerning them, whether obtained from the subjects directly, other persons, or existing records.\n\n\n\nAccommodations for students with disabilities.\nThe University of Wisconsin–Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute 36.12, and UW–Madison policy UW-855 require the university to provide reasonable accommodations to students with disabilities to access and participate in its academic programs and educational services. Faculty and students share responsibility in the accommodation process. Students are expected to inform faculty of their need for instructional accommodations during the beginning of the semester, or as soon as possible after being approved for accommodations. Faculty will work either directly with the student or in coordination with the McBurney Resource Center to provide reasonable instructional and course-related accommodations. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA.\n\n\nDiversity & inclusion\nDiversity is a source of strength, creativity, and innovation for UW–Madison. We value the contributions of each person and respect the profound ways their identity, culture, background, experience, status, abilities, and opinion enrich the university community. We commit ourselves to the pursuit of excellence in teaching, research, outreach, and diversity as inextricably linked goals. The University of Wisconsin–Madison fulfills its public mission by creating a welcoming and inclusive community for people from every background – people who as students, faculty, and staff serve Wisconsin and the world.\n\n\nPrivacy of student records & the use of audio recorded lectures.\nLecture materials and recordings for this course are protected intellectual property at UW–Madison. Students in courses may use the materials and recordings for their personal use related to participation in class. Students may also take notes solely for their personal use. If a lecture is not already recorded, students are not authorized to record lectures without permission unless they are considered by the university to be a qualified student with a disability who has an approved accommodation that includes recording ( Regents Policy Document 4-1).\nStudents may not copy or have lecture materials and recordings outside of class, including posting on internet sites or selling to commercial entities, with the exception of sharing copies of personal notes as a notetaker through the McBurney Disability Resource Center. Students are otherwise prohibited from providing or selling their personal notes to anyone else or being paid for taking notes by any person or commercial firm without the instructor’s express written permission. Unauthorized use of these copyrighted lecture materials and recordings constitutes copyright infringement and may be addressed under the university’s policies, UWS Chapters 14 and 17, governing student academic and non-academic misconduct.\n\n\nReligious observances\nStudents are responsible for notifying the instructor within the first two weeks of classes about any need for flexibility due to religious observances.\n\n\nFurther information and policies}.\nPlease visit this link for additional information about student privacy, course evaluations, and student rights and responsibilities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "",
    "text": "Welcome to STAT 479 (Special Topics in Statistics)! This iteration of the course will focus on sports analytics.\nAll course material including lecture slides, code to reproduce the analyses discussed in lecture, and information about the projects will be posted on this website. So, please bookmark this page and check it regularly throughout the course.\nA PDF copy of the course syllabus is available here and an HTML version of the syllabus is available here Below, you will find more details about course logistics and important information about setting up R."
  },
  {
    "objectID": "index.html#sec-course-logistics",
    "href": "index.html#sec-course-logistics",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nLectures\nLectures will be held in-person Tuesdays and Thursdays from 11:00am to 12:15pm in Morgridge Hall 1524. Slides and code for each lecture will be posted on this website. Lectures will not be recorded. You are not permitted to record the lectures either.\n\n\nCourse Staff & Office Hours\nInstructor: Sameer Deshpande (sameer.deshpande@wisc.edu). Instructor office hours will take place on:\n\nMondays from 11:00am to 12:00pm in Morgridge Hall 5586 (beginning 9/8/25).\nWednesdays from 3:00pm to 4:00pm in Morgridge Hall 5586 (beginning 9/3/25)\nFridays from 3:00pm to 4:30pm in Morgridge Hall 5618 (beginning 9/5/25)\n\nOffice hours on Mondays and Wednesdays are designed for one-on-one meetings with the instructor. While the main purpose of Monday and Wednesday office hours is to discuss your specific experience and learning in the course, feel free to stop by to chat about your educational and career goals and/or any personal sports analytic projects you might be pursuing.\nFriday office hours are intended for collaborative and small group work. During Friday office hours, small groups can step through analyses from lecture, work on the exercises, and on their team projects with the instructor and other students. Friday office hours are also a great way to brainstorm project ideas with and to get feedback from other teams.\nIf you have specific questions about the course content (e.g., parsing a particular bit of syntax or understanding a step in an analysis), I encourage you to ask the question on Piazza and to attend Friday office hours. If you cannot make the Monday or Wednesday office hours, please email me and suggest some times at which you are free.\nTeaching Assistant: Zhexuan Liu (zhexuan.liu2@wisc.edu). TA office hours will be held 9:15am - 10:45am on Tuesday and Thursdays in Morgridge Hall 2515. TA office hours will begin on 9/16/25."
  },
  {
    "objectID": "index.html#sec-R-setup",
    "href": "index.html#sec-R-setup",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "System Setup",
    "text": "System Setup\nAs noted in the syllabus, this course will make extensive use of R. Even if you have prior experience, I strongly recommend installing the latest version of both R and RStudio at the beginning of the course. As of the time of this writing, that is R version 4.5.1 and RStudio version 2025.05.\n\nInstalling R & RStudio\nYou can download a version of R specific to your operating system from this website. After installing R, you should download and re-install RStudio from this website.\n\n\n\n\n\n\nTip\n\n\n\nWhenever you update your version of R, you need to re-install the packages; this is a perennial source of frustration for many R users and some good-natured humor from others1.\n\n\n\n\nRequired Packages\nThroughout the course, we will make extensive use of several tidyverse packages, primarily for data loading, pre-processing, and manipulation. We will also make extensive use of the packages glmnet, lme4, mgcv, and ranger for model fitting. As the course progresses, we will introduce and install new package as required. For the most part, these packages will be specific to a particular sport. Every package that we will use in this class is available through either (i) the Comprehensive R Archive Network (CRAN) or (ii) a public GitHub repository maintained by the packager developer. We typically install CRAN packages using the install.packages() command. To install packages hosted on GitHub, we will use the install_github() function in the devtools package, which is itself available on CRAN.\nFor visualizations, we will try to use colorblind-friendly palettes2 as much as possible. The package colorBlindness contains several palettes for diverging and qualitative data. We will use the Blue2DarkRed18Steps palette from colorBlindness for making heatmaps (e.g., this one)\nPrior to Lecture 2, please make sure you install the required packages.\n\ninstall.packages(c(\"devtools\", \"tidyverse\", \"glmnet\", \"lme4\", \"mgcv\", \"ranger\", \"colorBlindness\"))\n\nI am also personally partial to the palette developed by Okabe & Ito, which is available using the palette.colors() function. We will often use this palette to annotate scatter plots (e.g., this one in Lecture 9). Figure 1 shows the 9 colors contained in the Okabe-Ito palette.\n\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n1par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\n2plot(1:9, 1:9, pch = 16, cex = 4, col = oi_colors)\n\n\n1\n\nThe mar argument reserves 3 margin lines for the bottom and left sides of the plot, 2 for the top, and 1 for the right. The mgp argument specifies how the axis title, label, and lines are placed.\n\n2\n\nPlots large, filled circles with the palette colors\n\n\n\n\n\n\n\n\n\n\nFigure 1: The 9 colors in the Okabe-Ito colorblind-friendly palette."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEven Hadley Wickham, who leads the tidyverse team at Posit, manually re-installs packages after every update↩︎\nSee these notes by Paul Tol for a lot of useful information about colorblindness and how different palettes are perceived by different people.↩︎"
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Project 1 Information",
    "section": "",
    "text": "For your first project, I would like you to use publicly available play-by-play, event-level, or tracking data to assess player or team performance or decision-making. That is, your analysis should be based on data that is more granular than match- or game-level (e.g., box score, tables of seasonal totals, etc.) The project report and recorded presentation are due on Friday October 10 at 12:00pm (noon)."
  },
  {
    "objectID": "project1.html#overivew",
    "href": "project1.html#overivew",
    "title": "Project 1 Information",
    "section": "",
    "text": "For your first project, I would like you to use publicly available play-by-play, event-level, or tracking data to assess player or team performance or decision-making. That is, your analysis should be based on data that is more granular than match- or game-level (e.g., box score, tables of seasonal totals, etc.) The project report and recorded presentation are due on Friday October 10 at 12:00pm (noon)."
  },
  {
    "objectID": "project1.html#deliverables",
    "href": "project1.html#deliverables",
    "title": "Project 1 Information",
    "section": "Deliverables",
    "text": "Deliverables\n\nWritten Report\nThe written report consists of a non-technical executive summary and a technical report. The executive summary, which should not exceed 500 words, should describe the overall goals, analytic approach, and main conclusions in non-technical language. The executive summary should be free from jargon, code listings, figures, tables, and charts. It should be written to be read and understood by a front office executive, coach, player, or fan with little data science experience. The rest of written report should\n\nClearly state the problem being studied and provide sufficient background details and to motivate why the problem is important and interesting.\nDescribe the data and major steps of the analysis\nPresents the main results within the context of the relevant sport(s) and supports the results with figures, tables, charts, and other statistical software output as appropriate.\nDiscusses the limitations of the analysis and outlines concrete steps for further development.\n\nThe technical section of the report should contain enough detail and code that another data scientist could replicate your analysis verify its soundness. Code listings and output (e.g., figures, tables, charts, and numerical summaries) should be tightly integrated with the written exposition. A good example of such integration is here. Pay particular attention to the way the author complements the numerical results with detailed examples of individual performances.\n\n\nPresentation\nEach team will also record an 8–10 minute presentation (e.g., using Zoom) that provides an overview of their analysis. Each presentation should include the following elements\n\nBackground (2–4 slides): clearly motivate and state the main problem being studied. Explain why it is interesting and important. Present just enough background to motivate the problem, while taking care not to overwhelm the audience with extraneous details. If appropriate, comment on the limitations of existing solutions to the problem or closely-related problems\nAnalysis overview (2–4 slides): present only the main steps of your analysis. Be sure to explain why each step was necessary and how these steps contribute to the overall solution. Focus more on the high-level ideas and motivation for each step rather than the specific implementation or software syntax\nMain results (2–3 slides): distill your results into a few key points. Use figures, tables, charts, and other statistical software output to support your findings.\nConclusion (1 slide): briefly summarize your analysis and findings and outline between 1 and 3 specific directions for future development, improvement or refinement.\n\nEvery group member must speak during the presentation. I encourage you practice a few times to ensure smooth transitions between speakers."
  },
  {
    "objectID": "project1.html#potential-topics",
    "href": "project1.html#potential-topics",
    "title": "Project 1 Information",
    "section": "Potential Topics",
    "text": "Potential Topics\nFor your first project, you could extend or modify an analysis presented in class or one of the Exercises listed in the lecture notes. Here are some other potential project ideas. Note, these are merely suggestions and you are free to develop projects outside this list. If you would like additional inspiration, check out the papers from the Reproducible Research Competition from last year’s CMU Sports Analytics Conference. The only requirement is that you (i) carefully assess player or team performance and (ii) utilize data at the play- or event-level (or finer).\n\nDevelop a New Metric\nUsing play-by-play or event-level data, construct a new measure of player or team skill. If you pursue this option, please take care to motivate the development of your new metric and to state, precisely, what the metric aims to quantify. You should explore its operating characteristics including (but certainly not limited to) its stability across games or seasons; its ability to predict season-level outcomes; the extent to which you are measuring a latent skill or ability; and its relationship to existing metrics. You should also explain how your new metric overcomes any limitations of existing metrics and discuss any drawbacks your metric might have. Finally, you should discuss how players, teams, or fans might use your new metric. Note: simply creating a new measure and ranking players according is not sufficient.\n\n\nA More Refined XG Model for Soccer\nIn Lecture 3, we built a random forests model to estimate the probability of a shot resulting in a goal using many features created by StatsBomb. While this model seemed much more accurate than simpler, parametric models, there is still much room for improvement. For your project, you can continue to develop more complex XG model. Here are some potential directions\n\nIntuitively, we might expect XG to be monotonic in certain features (e.g., the further away a shot is from the goal, the lower the XG). Unfortunately, random forests does not allow for monotonic constraints. XGBoost is another tree ensemble method that allows for monotonicity. Explore the use of XGBoost to estimate expected goals.\nOur random forest model did not use the exact player locations as features. Intuitively, we might expect XG to vary smoothly in shot location. But if you simply include location.x and location.y as features, the resulting XG is not a smooth function. Explore the use of generalized additive models for creating smoother XG models.\n\nIf you pursue this option, you should build at least 3 different XG models, compare them both quantitatively and qualitatively, and use the “best” model predictions to make a conclusion about player or team performance. At a minimum, you should establish which model is most predictive. But a more ambitious project will draw insights from differences between each models’ predctios1. If you pursue this option, you should StatsBomb data from at least 3 competitions with more than a handful of match data available.\n\n\nExpected Goals in Hockey\nThe package hockeyR provides functions for scraping play-by-play data from the National Hockey League. Among many other things, these data include shot distance, shot angle, and the coordinates of each shot. Using ideas from Lecture 3, develop an XG model for hockey. If you puruse this option, you should build at least 3 different XG models, compare them both quantaitively and qualitatively, and use the “best” model to make conclusions about player or team performance. You can also compare your model predictions to those provided by the package itself.\n\n\nNBA Heatmaps & Expected Points\nIn Lectures 4 and 5, we used the hoopR package to scrape NBA play-by-play data, which we then used to estimate adjusted plus/minus. This package also includes shot location data. Using these data (and possibly other contextual information about the game state and player), build a model to predict the probability that a player makes a shot. Once built, use the model to compare player performance. For instance, you can convert these probabilities to expected points and explore how closely a player’s total expected points tracks the actual points score. Or you could identify players who score substantially more or less points than what a league-average or replacement-level player might be expected to score given the same shot selection.\nIf you puruse this option, you should build and compare at least 3 different models of shot selection. You should then select the “best” model using both qualitative and quantitative considerations.\n\n\nWAR For College Football and Volleyball\nIn Lecture 10, we used the nflfastR package to scrap play-by-play data from the National Football League. We then used these data to develop a version of Wins Above Replacement for offensive players in the NFL. The package cfbfastR provides similar functinoality for college football. You could try to develop a version of WAR for college football. If you pursue this option, you must carefully deal with the differences between conferences when estimating individual player skills and defining replacement-level.\nThe package ncaavolleyballr provides functions to scrape NCAA Volleyball data2, including play-by-play logs. Each row in the play-by-play data table correspond to individual events within a point (e.g., a serve, set, attack, kill, etc.) Using these data, develop a version of wins above replacement. To do this, you must first define and estimate an analog of runs value and expected points. Then, you must carefully divide this “currency” among players of different positions (e.g., middle blocker, libero, outside hitter, etc.). If you pursue this option, I highly recommend using the pre-scraped play-by-play data available here and then accessing player- and team-specific information (e.g., rosters, schedules, season-level statistics) using the package functions.\n\n\nAdjusted Plus/Minus for College Basketball\nThe ncaahoopR package provides functions for scraping play-by-play data from college basketball. Using a workflow similar to the one presented in Lecture 4, convert these data into stint-level data and develop a version of weighted and/or regularized adjusted plus/minus for college basketball.\n\n\nExtending Ideas from Baseball to Cricket\nThe package cricketdata provides functions for scraping match-level data from (ESPNCricinfo)[https://www.espncricinfo.com]. Himanish Ganjoo also provides very detailed ball-by-ball data for a large number of test, one-day international, and T20 matches here. You could consider building analogs of expected runs (e.g., as a function of wickets in hand and overs left) or building a model to predict the outcome of an individual ball given its line and length information. An even more ambitous project would develop a version of WAR for batting performance."
  },
  {
    "objectID": "project1.html#footnotes",
    "href": "project1.html#footnotes",
    "title": "Project 1 Information",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the section “Benefiting from training several variants of the model” in this blogpost from Hudl for some inspiration.↩︎\nNote that there is another package with similar functionality and an almost identical name (note the uppercase “R”).↩︎"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Project 3 Information",
    "section": "",
    "text": "The amount, granularity, and quality of sports data has increased exponentially over the last two decades. To draw insights and gain competitive advantages from these data, teams and leagues have invested heavily in building analytics departments. Teams are increasingly looking for analysts with the ability to ask and answer substantively interesting sports questions. Identifying such analysts is complicated by at least two factors. First, most undergraduate and graduate data science programs do not specifically train students to work with the types of data emerging in sports. And second, most of the cutting-edge data is not publicly available, making it very difficult for aspiring analysts to demonstrat their ability to analyze that data.\nPublic data competitions have become an increasingly popular means of identifying talent in sports analytics. Generally speaking, these competitions involve a team or league releasing a small amount of their data and inviting students and members of the public to answer substantively interesting questions using the data. The earliest competitions were hackathons organized by the NBA in the mid-2010’s1. By far the most prominent competition these days is the NFL Big Data Bowl, an annual competition in which the NFL releases a small amount of its Next Gen Stats and invites students and members of the public to use these data to draw new insight about the game. Past competitions have resulted in new ways to\n\nPredict where a running play will end based on initial player positions (link)\nDetermine the optimal path for punt returners to follow (link)\nGrade pass-rushers (link)\nQuantify the leverage created by offense when they use pre-snap motion (link)\nPredict what might happen if the quarterback targeted a different receiver (link)\n\nBig Data Bowl finalists receive a cash prize and are given a chance to present their work at the NFL Combine in Indianapolis. Over the last 8 years, over 50 people have been hired by professional teams and sports analytics companies based on their Big Data Bowl performance.\nSomewhat more recently, the Connecticut Sports Analytics Symposium began organizing an open data competition specifically targeted at undergraduate and graduate students. Past competitions have involved * Determining the optimal rosters for the US Olympic gymnastics teams (link) * Determining how batter swing speed and swing length are related to batter discipline and whether pitchers can “dictate” swings (link)\nIn addition to presenting their work at the conference, past winners of the CSAS Data Challenge have been invited to share their finding with the US Olympic Committee.\nFor the third project, I would like you work with data from either the 2026 NFL Big Data Bowl or the 2026 CSAS Data Challenge. My hope is that you develop your course projects into contest submissions; the submission deadlines are December 17, 2025 for the Big Data Bowl and January 17, 2026 for the CSAS Data Challenge.\nDetails about both competitions and the relevant data is available below."
  },
  {
    "objectID": "project3.html#overview",
    "href": "project3.html#overview",
    "title": "Project 3 Information",
    "section": "",
    "text": "The amount, granularity, and quality of sports data has increased exponentially over the last two decades. To draw insights and gain competitive advantages from these data, teams and leagues have invested heavily in building analytics departments. Teams are increasingly looking for analysts with the ability to ask and answer substantively interesting sports questions. Identifying such analysts is complicated by at least two factors. First, most undergraduate and graduate data science programs do not specifically train students to work with the types of data emerging in sports. And second, most of the cutting-edge data is not publicly available, making it very difficult for aspiring analysts to demonstrat their ability to analyze that data.\nPublic data competitions have become an increasingly popular means of identifying talent in sports analytics. Generally speaking, these competitions involve a team or league releasing a small amount of their data and inviting students and members of the public to answer substantively interesting questions using the data. The earliest competitions were hackathons organized by the NBA in the mid-2010’s1. By far the most prominent competition these days is the NFL Big Data Bowl, an annual competition in which the NFL releases a small amount of its Next Gen Stats and invites students and members of the public to use these data to draw new insight about the game. Past competitions have resulted in new ways to\n\nPredict where a running play will end based on initial player positions (link)\nDetermine the optimal path for punt returners to follow (link)\nGrade pass-rushers (link)\nQuantify the leverage created by offense when they use pre-snap motion (link)\nPredict what might happen if the quarterback targeted a different receiver (link)\n\nBig Data Bowl finalists receive a cash prize and are given a chance to present their work at the NFL Combine in Indianapolis. Over the last 8 years, over 50 people have been hired by professional teams and sports analytics companies based on their Big Data Bowl performance.\nSomewhat more recently, the Connecticut Sports Analytics Symposium began organizing an open data competition specifically targeted at undergraduate and graduate students. Past competitions have involved * Determining the optimal rosters for the US Olympic gymnastics teams (link) * Determining how batter swing speed and swing length are related to batter discipline and whether pitchers can “dictate” swings (link)\nIn addition to presenting their work at the conference, past winners of the CSAS Data Challenge have been invited to share their finding with the US Olympic Committee.\nFor the third project, I would like you work with data from either the 2026 NFL Big Data Bowl or the 2026 CSAS Data Challenge. My hope is that you develop your course projects into contest submissions; the submission deadlines are December 17, 2025 for the Big Data Bowl and January 17, 2026 for the CSAS Data Challenge.\nDetails about both competitions and the relevant data is available below."
  },
  {
    "objectID": "project3.html#big-data-bowl",
    "href": "project3.html#big-data-bowl",
    "title": "Project 3 Information",
    "section": "Big Data Bowl",
    "text": "Big Data Bowl\nThis year’s Big Data Bowl focuses on how players move during passing plays. Specifically, it tries to use tracking data between the time that the ball is snapped and the time it is thrown to predict how different players move when the ball is in the air. For the first, the NFL is running two separate competitions:\n\nA prediction competition, in which one must predict the actual \\((x,y)\\) coordinates of several players at several time-steps using just their pre-throw trajectories\nAn analytics competition in which you must create a metric, video, or broadcast tool that helps describe player movement when the ball is in the air in a way that is accessible to coaches and fans.\n\nFor the purposes of this course, I strongly recommend focusing on the analytics competition, as it is a better opportunity to showcase your creativity and skills. The analytics competition has two tracks:\n\nThe University track: undergraduate and graduate students are tasked to analyze player movement, create player or team metrics for either the offensive or defensive players, and/or evaluate other aspects of the passing play informed by the available data\nThe Broadcast Visualization Track: the goal here is to create an accessible, interesting, and novel visualization that uses the movement of players when the ball in the area. For this track, participation are encouraged to overlay their visualizations over game film (available, e.g., from the NFL YouTube channel)\n\n\nData\nYou may download the data from the Big Data Bowl’s Kaggle website. Downloading the data from Kaggle requires registering for the competition. It is also available for download directly from this shared Box folder.\nA full description of the data fields is available here. There are two CSV files for each week of the 2023 regular season. The files input_2023_w[0-18].csv contain tracking data from before the ball was thrown. Each row records where a single player is located on the field in one 0.1-second time frame2. In addition to game, play, frame, and player ID’s (game_id, play_id, frame_id and nfl_id), each row records\n\nInformation about the specific player: player_height, player_weight, player_birth_date, player_position, player_side (offense or defense), and player_role (defensive coverage, targeted receiver, passer or other route runner)\nPlayer location and movement: their x and y coordinates, the orientation (o) and direction of player movement (dir), speed (s), acceleration (a)\nPlay-specific information: the yardline at the start of the play (absolute_yardline_number), where the ball lands (ball_land_x and ball_land_y)\n\nThe NFL removed tracking data for offensive and defensive linemen. They also removed data from very short passes.\n\n\nData Example\nHere, we visualize a single play from the Big Data Bowl dataset. We begin by reading in data from a single week and then extracting all tracking data from a single play.\n\n1raw_w1_input &lt;- readr::read_csv(\"input_2023_w01.csv\")\n2raw_w1_output &lt;- readr::read_csv(\"output_2023_w01.csv\")\n\n3gid &lt;- raw_w1_input$game_id[1]\npid &lt;- raw_w1_input$play_id[1]\n\ntmp_in &lt;-\n  raw_w1_input |&gt;\n  dplyr::filter(game_id == gid & play_id == pid)\n\ntmp_out &lt;-\n  raw_w1_output |&gt;\n  dplyr::filter(game_id == gid & play_id == pid)\n\n4ball_land_x &lt;- tmp_in$ball_land_x[1]\nball_land_y &lt;- tmp_in$ball_land_y[1]\n5all_players &lt;- unique(c(tmp_in$nfl_id, tmp_out$nfl_id))\n\n\n1\n\nPre-throw tracking data for week 1 in 2023\n\n2\n\nTracking data when ball is in the air for week 1 in 2023\n\n3\n\nExtract all tracking data for a single play (indexed by game_id and play_id)\n\n4\n\nCoordinates of where the ball is caught or lands (if it is not caught)\n\n5\n\nID number of all players tracked in this play\n\n\n\n\n\noi_colors &lt;- palette.colors(palette = \"Okabe-Ito\")\n\npar(mar = c(1,1,1,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", xlim = c(0,120), ylim = c(0, 54),\n     xaxt = \"n\", yaxt = \"n\", xlab = \"\", ylab = \"\")\n\npoints(ball_land_x, ball_land_y, pch = 4, cex = 2, col = oi_colors[8])\nlines(x = c(tmp_in$absolute_yardline_number[1], tmp_in$absolute_yardline_number[1]),\n      y = c(par(\"usr\")[3], par(\"usr\")[4]))\nfor(player in all_players){\n  player_in &lt;- \n    tmp_in |&gt;\n    dplyr::filter(nfl_id == player)\n  role &lt;- player_in$player_role[1]\n  \n  if(role %in% c(\"Passer\", \"Targeted Receiver\", \"Other Route Runner\")){\n    points(player_in$x, player_in$y, pch = 16, cex = 0.5,\n           col = adjustcolor(oi_colors[9], alpha.f = 0.5))\n  } else{\n    points(player_in$x, player_in$y, pch = 15, cex = 0.5,\n           col = adjustcolor(oi_colors[9], alpha.f = 0.5))\n  }\n  \n  if(player_in$player_to_predict[1]){\n    player_out &lt;- \n      tmp_out |&gt;\n      dplyr::filter(nfl_id == player)\n    if(role %in% c(\"Passer\", \"Targeted Receiver\", \"Other Route Runner\")){\n      points(player_out$x, player_out$y, pch = 16, cex = 0.5,\n             col = adjustcolor(oi_colors[2], alpha.f = 0.75))\n    } else{\n        points(player_out$x, player_out$y, pch = 15, cex = 0.5,\n               col = adjustcolor(oi_colors[3], alpha.f = 0.75))\n    }\n  } \n} \n\nlegend(\"topleft\", legend = c(\"Offense\", \"Defense\"), pch = c(15, 16), col = oi_colors[c(2,3)], bty = \"n\")\nlegend(\"bottomright\", legend = c(\"Pre-throw\", \"Ball in air\", \"Ball end location\"), pch = c(15, 16, 4),\n       col = oi_colors[c(9,9, 8)], bty = \"n\")\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "project3.html#csas-data-challenge",
    "href": "project3.html#csas-data-challenge",
    "title": "Project 3 Information",
    "section": "CSAS Data Challenge",
    "text": "CSAS Data Challenge\nThis year’s CSAS Data Challenges is about mixed double curling. In all forms of curling, teams take turns sliding stones on a sheet of ice towards a target in multiple rounds, known as “ends.” Whenever a play shoots (i.e., slides the stone), their teammates follow the stone and sweep a path along the ice with brooms to direct its motion. Teams score by having more stones closer to the center of a target than the their opponent. A considerable amount of strategy goes into shot selection: players can try to move a stone into a potential scoring position (draws); block their opponent from reaching scoring position (guards); or push already-thrown stones out of the target area (take-outs). See here for a primer on the sport.\nMixed doubles curling differs from the most common form of curling in a few important ways. First, teams consist of just two people, unlike the four-person teams in traditional curling. More substantively, at the beginning of each end in mixed doubles, two stones (one from each team) are placed in pre-determined positions along the ice: one stone will be placed within the target area and one will be placed as a guard in front of the target. In this way, one team starts with an advantage in each round.\nTeams have the option to exercise a “power play” once per game during which the pre-placed stones are removed. The goal of the CSAS Data Challenge is to determine optimal strategy for the power play using data from several international competitions.\n\nData\nThe data is available for download directly from the Data Challenge GitHub. It is also available for this shared Box folder. The data is organized across multiple CSV files, which are detailed on the Data Challenge website. In addition to information about the competition and players, the data information about * The type/purpose of shot (Task) * The direction in which the stone was turned on release (Handle) * The horizontal and vertical coordinates of each stone3. * An assessment of shot quality (Points). Note this is not the number of points scored by the shot.\nThe data are pulled from Game Books like this one, which include images showing the locations of all stones after each shot.\nBelow, we will visualize the stone locations at the conclusion of a single end in the dataset4. First, we extract all the data from a particular end in a given phase of the match, which is uniquely identified by a combination of Competition_ID, SessionID, and GameID.\n\nraw_shots &lt;-\n  readr::read_csv(file = \"Stones.csv\")\ntmp &lt;-\n  raw_shots |&gt;\n  dplyr::filter(CompetitionID == 0 & SessionID == 1 & GameID == 1) |&gt;\n  dplyr::filter(EndID == 1) |&gt;\n  dplyr::arrange(ShotID)\n\ntmp |&gt;\n  dplyr::select(\n    ShotID, TeamID,\n    stone_1_x, stone_1_y, stone_2_x, stone_2_y, \n    stone_7_x, stone_7_y, stone_8_x, stone_8_y)\n\nThe code below visualizes the locations of all stones left in play at the conclusion of a single end. Stones are colored by team.\n\npar(mar = c(1,1,1,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", xlim = c(0,1500), ylim = c(0, 3000), \n       xaxt = \"n\", yaxt = \"n\", xlab = \"\", ylab = \"\")\nfor(s in 1:12){\n1  x &lt;- tmp[10, paste0(\"stone_\", s, \"_x\")]\n  y &lt;- tmp[10, paste0(\"stone_\", s, \"_y\")]\n  \n2  if(s &lt;= 6){\n    if(x &gt; 0 & x &lt; 4095 & y &gt; 0 & y &lt; 4095){\n      points(x, y, pch = 16, cex = 2, col = oi_colors[3])\n    }\n  } else{\n    if(x &gt; 0 & x &lt; 4095 & y &gt; 0 & y &lt; 4095){\n      points(x,y, pch = 16, cex = 2, col = oi_colors[4])\n    }\n  }\n  \n  \n}\n\n\n1\n\nCoordinates of the stone\n\n2\n\nThe columns stone_[1-6]_x and stone_[1-6]_y are for the stones thrown by the team throwing first in the end. The columns stone_[7-12]_x and stone_[7-12]_y are for the team throwing second.\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\nPotential Topics\nThe Data Challenge Organizers have provided a non-exhaustive list of potential topics including\n\nThe optimal time (e.g., at which score differential, in which end, etc.) to use the power play\nComparing teams’ effectiveness as using the power play and customizing strategies based on opponent tendencies\nIdentifying the most effective opening sequences of shots\nDetermining the relationship between shot quality and scoring"
  },
  {
    "objectID": "project3.html#deliverables",
    "href": "project3.html#deliverables",
    "title": "Project 3 Information",
    "section": "Deliverables",
    "text": "Deliverables\nThe deliverables for Project 3 are the same as for Projects and 1 & 2 and carry similar requirements.\n\nWritten Report\nThe written report consists of a non-technical executive summary and a technical report. The executive summary, which should not exceed 500 words, should describe the overall goals, analytic approach, and main conclusions in non-technical language. The executive summary should be free from jargon, code listings, figures, tables, and charts. It should be written to be read and understood by a front office executive, coach, player, or fan with little data science experience. The rest of written report should\n\nClearly state the problem being studied and provide sufficient background details and to motivate why the problem is important and interesting.\nDescribe the data and major steps of the analysis\nPresents the main results within the context of the relevant sport(s) and supports the results with figures, tables, charts, and other statistical software output as appropriate.\nDiscusses the limitations of the analysis and outlines concrete steps for further development.\n\nThe technical section of the report should contain enough detail and code that another data scientist could replicate your analysis verify its soundness. Code listings and output (e.g., figures, tables, charts, and numerical summaries) should be tightly integrated with the written exposition. A good example of such integration is here. Pay particular attention to the way the author complements the numerical results with detailed examples of individual performances.\n\n\nPresentation\nEach team will also record an 8–10 minute presentation (e.g., using Zoom) that provides an overview of their analysis. Each presentation should include the following elements\n\nBackground (2–4 slides): clearly motivate and state the main problem being studied. Explain why it is interesting and important. Present just enough background to motivate the problem, while taking care not to overwhelm the audience with extraneous details. If appropriate, comment on the limitations of existing solutions to the problem or closely-related problems\nAnalysis overview (2–4 slides): present only the main steps of your analysis. Be sure to explain why each step was necessary and how these steps contribute to the overall solution. Focus more on the high-level ideas and motivation for each step rather than the specific implementation or software syntax\nMain results (2–3 slides): distill your results into a few key points. Use figures, tables, charts, and other statistical software output to support your findings.\nConclusion (1 slide): briefly summarize your analysis and findings and outline between 1 and 3 specific directions for future development, improvement or refinement."
  },
  {
    "objectID": "project3.html#footnotes",
    "href": "project3.html#footnotes",
    "title": "Project 3 Information",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee this promotional video and read about some participants experiences here and here.↩︎\nSo, a single play is comprised of multiple rows.↩︎\nThe value 4095 is used to signal that a stone has been knocked off the sheet and the value 0 indicates that a stone has not yet been thrown.↩︎\nThe locations of the target circles in the given coordinate system are not immediately obvious. If you figure them out, please share on Piazza and I’ll update this page!↩︎"
  },
  {
    "objectID": "lectures/lecture02.html",
    "href": "lectures/lecture02.html",
    "title": "Lecture 2: Expected Goals",
    "section": "",
    "text": "During the EURO2022 football tournament, English player Beth Mead scored 6 goals. Below are links to videos for three of her goals. Which is most impressive to you?\n\nGoal against Austria (link)\nGoal against Norway link\nGoal against Sweden link\n\nThere several important qualitative differences between these shots that affect our subjective comparison. For instance, because Mead scored against Austria in a one-on-one situation but had to shoot through multiple defenders against Norway and Sweden, we might view the former goal as easier and less impressive. On the other hand, lobbing the ball so it does not go over the bar takes a considerable amount of skill.\nWe could argue endlessly about the qualitative differences between these shots. To make our discussion more precise, it is helpful to quantify these differences. One way — but certainly not the only way — is to ask what might happen if Mead were to repeat these three shots over and over again. In this thought experiment, we could compare the shots based on the relative proportion of times that the shots resulted in a goal. Comparing the these hypothetical long-run proportions to the actual observed shot outcomes allows us to how impressive the outcome was. For instance, the lob shot against Austria might look a lot less impressive if we knew that such a shot would very often result in a goal if repeated over and over again. Of course, Mead can’t actually repeat these shots over and over again. In this lecture, we will introduce the expected goals framework, which allows us to estimate those long-run goal frequencies.\n\n\n\nIn Section 2, we discuss how to access soccer event data. We then formally define expected goals and introduce some simple models for estimating XG (Section 3 and Section 5). Finally, we compare Mead’s actual performance in EURO 2022 to her expected performance (Section 6).",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#overview",
    "href": "lectures/lecture02.html#overview",
    "title": "Lecture 2: Expected Goals",
    "section": "",
    "text": "During the EURO2022 football tournament, English player Beth Mead scored 6 goals. Below are links to videos for three of her goals. Which is most impressive to you?\n\nGoal against Austria (link)\nGoal against Norway link\nGoal against Sweden link\n\nThere several important qualitative differences between these shots that affect our subjective comparison. For instance, because Mead scored against Austria in a one-on-one situation but had to shoot through multiple defenders against Norway and Sweden, we might view the former goal as easier and less impressive. On the other hand, lobbing the ball so it does not go over the bar takes a considerable amount of skill.\nWe could argue endlessly about the qualitative differences between these shots. To make our discussion more precise, it is helpful to quantify these differences. One way — but certainly not the only way — is to ask what might happen if Mead were to repeat these three shots over and over again. In this thought experiment, we could compare the shots based on the relative proportion of times that the shots resulted in a goal. Comparing the these hypothetical long-run proportions to the actual observed shot outcomes allows us to how impressive the outcome was. For instance, the lob shot against Austria might look a lot less impressive if we knew that such a shot would very often result in a goal if repeated over and over again. Of course, Mead can’t actually repeat these shots over and over again. In this lecture, we will introduce the expected goals framework, which allows us to estimate those long-run goal frequencies.\n\n\n\nIn Section 2, we discuss how to access soccer event data. We then formally define expected goals and introduce some simple models for estimating XG (Section 3 and Section 5). Finally, we compare Mead’s actual performance in EURO 2022 to her expected performance (Section 6).",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-statsbomb-data",
    "href": "lectures/lecture02.html#sec-statsbomb-data",
    "title": "Lecture 2: Expected Goals",
    "section": "Soccer Event Data",
    "text": "Soccer Event Data\nWe will make use of high-resolution tracking data provided by the company StatsBomb, which was recently acquired by Hudl. StatsBomb extracts player locations from game film using some pretty interesting computer vision techniques. To their great credit, StatsBomb releases a small snapshot of their data for public use1 We can access this data directly in R using the StatsBombR package.\nStatsBomb provides their public data with the package StatsBombR, which you can install using\n\ndevtools::install_github(\"statsbomb/StatsBombR\")\n\nStatsBomb organizes its free data by competition/tournament. The screenshot below shows a table of all the available competitions. We can load this table into our R environment using the function StatsBombR::FreeCompetitions(). Figure 1 shows a snapshot of the available competitions.\n\n\n\n\n\n\nFigure 1: Screenshot of the competitions covered in the public data\n\n\n\nEach competition and season have unique id and we can also see whether it was a men’s or women’s competition. To see which matches from selected competitions have publicly available data, we can pass the corresponding rows of this table to the function StatsBombR::FreeMatches(). For instance, here are the first few rows and some selected columns from the matches from EURO 2022. StatsBomb graciously provided data for all the matches in the tournament. Here is a table of matches from the 2022 EURO Competition; StatsBomb graciously provided data for all matches from the tournament, which can be obtained using the code below.\n\nStatsBombR::FreeCompetitions() |&gt;\n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt;\n  dplyr::select(match_id, home_team.home_team_name, away_team.away_team_name, home_score, away_score)\n\n[1] \"Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please credit StatsBomb as your data source when using the data and visit https://statsbomb.com/media-pack/ to obtain our logos for public use.\"\n[1] \"Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please credit StatsBomb as your data source when using the data and visit https://statsbomb.com/media-pack/ to obtain our logos for public use.\"\n\n\n# A tibble: 31 × 5\n   match_id home_team.home_team_n…¹ away_team.away_team_…² home_score away_score\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt;                       &lt;int&gt;      &lt;int&gt;\n 1  3835331 Sweden Women's          Switzerland Women's             2          1\n 2  3835324 Netherlands Women's     Sweden Women's                  1          1\n 3  3844384 England Women's         Spain Women's                   2          1\n 4  3847567 England Women's         Germany Women's                 2          1\n 5  3845506 England Women's         Sweden Women's                  4          0\n 6  3835335 Northern Ireland        England Women's                 0          5\n 7  3835323 Portugal Women's        Switzerland Women's             2          2\n 8  3835325 France Women's          Italy Women's                   5          1\n 9  3835320 Norway Women's          Northern Ireland                4          1\n10  3845507 Germany Women's         France Women's                  2          1\n# ℹ 21 more rows\n# ℹ abbreviated names: ¹​home_team.home_team_name, ²​away_team.away_team_name\n\n\nTo access the raw event-level data from a subset of matches, we need to pass the table above to the function StatsBombR::free_allevents(). StatsBomb also recommends running some basic pre-processing, all of which is nicely packaged together in the functions StatsBombR::allclean() and StatsBombR::get.opposingteam().\nAs an example, the code chunk below pulls out publicly available event data for every women’s international match.\n\nwi_events &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international) |&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam()\n\n\n1\n\nGet table of all available competitions\n\n2\n\nFind all women’s international competition\n\n3\n\nGet table of matches\n\n4\n\nGet all events\n\n5\n\nallclean() and get.opposingteam() run several pre-processing scripts that StatsBomb recommends.\n\n\n\n\n\n\n\n\n\n\nDeveloping Complex Pipelines\n\n\n\nIt is not easy to codepipelines like the above in a single attempt. In fact, I had to build the code line-by-line. For instance, I initially ran just the first line and manually inspected the table of free competitions (using View()) to figure out which variables I needed to filter() on in the second line. It is very helpful to develop pipelines incrementally and to check intermediate results before putting everything together in one block of code.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-init-xg",
    "href": "lectures/lecture02.html#sec-init-xg",
    "title": "Lecture 2: Expected Goals",
    "section": "Estimating Expected Goals",
    "text": "Estimating Expected Goals\nSuppose we observe a dataset consisting of \\(n\\) shots. For each shot \\(i = 1, \\ldots, n,\\) let \\(Y_{i}\\) be a binary indicator of whether the shot resulted in a goal (\\(Y_{i} = 1\\)) or not (\\(Y_{i} = 0\\)). From the high-resolution tracking data, we can extract a potentially huge number of features about the shot at the moment of it was taken. Possible features include, but are certainly not limited to, the player taking the shot, the body part and side of the body used, the positions of the defenders and goal keepers, and contextual information like the score. Mathematically, we can collect all these features into a (potentially large) vector \\(\\boldsymbol{\\mathbf{X}}_{i}.\\)\nExpected Goals (XG) models work by (i) positing an infinite super-population of shots represented by pairs \\((\\boldsymbol{\\mathbf{X}}, Y)\\) of feature vector \\(\\boldsymbol{\\mathbf{X}}\\) and binary outcome \\(Y\\); and (ii) assuming that the shots in our dataset constitute a random sample \\((\\boldsymbol{\\mathbf{X}}_{1}, Y_{1}), \\ldots, (\\boldsymbol{\\mathbf{X}}_{n}, Y_{n})\\) from that population.\n\n\n\n\n\n\nConditional Expectations\n\n\n\nFor each combination of features \\(\\boldsymbol{\\mathbf{x}}\\), the expect goals given \\(\\boldsymbol{\\mathbf{x}},\\) which we will denote by \\(\\textrm{XG}(\\boldsymbol{\\mathbf{X}})\\) is just the average value of \\(Y\\) among the (assumed infinite) sub-population of shots with features \\(\\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}.\\) Mathematically, XG is conditional expectation: \\[\n\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}],\n\\]\n\n\nBecause the shot outcome \\(Y\\) is binary, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is the proportion of goals scored within the sub-population of shots defined by the feature combinations \\(\\boldsymbol{\\mathbf{x}}.\\) In other words, it is the conditional probability of a goal given the shot features \\(\\boldsymbol{\\mathbf{x}}.\\) On this view, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) provides a quantitative answer to our motivating question “If we were to replay a particular shot over and over again, what fraction of the time does it result in a goal?”\nThe StatsBomb variable shot.body_part.name records the body part with which each shot was taken. Within our dataset of women’s international matches, we can see the breakdown of these body parts.\n\ntable(wi_events$shot.body_part.name)\n\n\n      Head  Left Foot      Other Right Foot \n       920       1280         27       2560 \n\n\nFor this analysis, we will focus on fitting XG models using data from shots taken with a player’s feet or head.\n\nwi_shots &lt;-\n  wi_events |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;  \n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\nLater, it will be useful for us to focus only on the shots from EURO2022, so we will also create a table euro2022_shots of all shots from that competition using similar code.\n\n\nCode\neuro2022_shots &lt;-\n  StatsBombR::FreeCompetitions() |&gt; \n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt; \n  StatsBombR::free_allevents() |&gt; \n  StatsBombR::allclean() |&gt; \n  StatsBombR::get.opposingteam() |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\nNow suppose we only include the body part in \\(\\boldsymbol{\\mathbf{X}}\\). If we had full access to the infinite super-population of women’s international shots, then we could compute \\[\\textrm{XG}(\\text{right-footed shot}) = \\mathbb{P}(\\text{goal} \\vert \\text{right-footed shot})\\] by (i) forming a sub-group containing only those right-footed shots and then (ii) calculating the proportion of goals scored within that sub-group. We could similarly compute \\(\\textrm{XG}(\\text{left-footed shot})\\) and \\(\\textrm{XG}(\\text{header})\\) by calculating the proportion of goals scored within the sub-groups containing, respectively, only left-footed shots and only headers.\nOf course, we don’t have access to the infinite super-population of shots. However, on the assumption that our observed data constitute a sample from that super-population, we can estimate \\(\\textrm{XG}\\) by mimicking the idealized calculations described above:\n\nBreak the dataset of all observed shots in women’s international matches into several groups based on the body part\nWithin these two groups, compute the proportion of goals\n\nTo keep things simple, we dropped the 23 shots that were taken with a body part other than the feet or the head.\n\nxg_model1 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y), n = dplyr::n())\nxg_model1\n\n# A tibble: 3 × 3\n  shot.body_part.name   XG1     n\n  &lt;chr&gt;               &lt;dbl&gt; &lt;int&gt;\n1 Head                0.112   920\n2 Left Foot           0.114  1280\n3 Right Foot          0.111  2560\n\n\n\n\n\n\n\n\nGeneralization\n\n\n\nA key assumption of all XG models is that the observed data is a random sample drawn from the super-population. The only women’s internationals matches for which StatsBomb data were from the 2019 and 2023 World Cup and the 2022 and 2025 EURO tournaments. These matches are arguably not highly representative of all women’s international matches, meaning that we should exercise some caution when using models fitted to these data to analyze matches from other competitions (e.g., an international friendly or a match in a domestic league).\n\n\nWe can now create a table of just Beth Mead’s shots from EURO 2022 and add a column with the XG for each shot. To do this, we first filter our table wi_shots using the player name (note, StatsBomb uses her full name!). Then, for every left-footed shot Mead attempted, we want to copy over the corresponding value from the table xg_model1, which in this case is 0.114. Similarly, we want to copy over the corresponding values for right-footed shots and headers from xg_model1 into our table for Mead’s shots. We can do this using an left join. In the code below, we actually create a temporary version of xg_model1 that drops the column recording the overall counts of the body part used for the shots in wi_shots. This way, when we perform the join, we don’t create a new column with these counts.\n\nmead_shots &lt;-\n  euro2022_shots |&gt;\n  dplyr::filter(player.name == \"Bethany Mead\") |&gt;\n1  dplyr::left_join(y = xg_model1 |&gt; dplyr::select(shot.body_part.name, XG1),\n                   by = c(\"shot.body_part.name\"))\n\n\n1\n\nNo need to include n when joining the tables\n\n\n\n\nWe can now look at the what our model says about the three goals from above. The first, against Austria in the 15th minute; the second, against Norway in the 37th minute, and the third against Sweden in the 33rd minute. These turn out to be in rows 1, 4, and 14 of the table mead_shots\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, Y, XG1) |&gt;\n  dplyr::slice(c(1, 4, 14))\n\n# A tibble: 3 × 5\n  OpposingTeam    minute shot.body_part.name     Y   XG1\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Austria Women's     15 Right Foot              1 0.111\n2 Norway Women's      37 Left Foot               1 0.114\n3 Sweden Women's      33 Right Foot              1 0.111\n\n\nAccording to our first model, the “impressiveness” of these goals is pretty similar: our model put the respective chances of each shot resulting in a goal at about 11%. But, watching the videos a bit more closely, this conclusion is not especially satisfying: Mead scored the first goal in a one-on-one situation but had to shoot through several defenders on the second and third goal. The discrepancy between our qualitative comparisons and our quantitative modeling results stems from the fact that we only conditioned on the body part and did not account for the other ways that the shots are different. In other words, our initial XG model is much too coarse to quantify the differences between the three chances that we believe are important.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-addtl-features",
    "href": "lectures/lecture02.html#sec-addtl-features",
    "title": "Lecture 2: Expected Goals",
    "section": "Conditioning On Additional Features",
    "text": "Conditioning On Additional Features\nFor a more refined comparison, we need an XG model that conditions on more features, including ones that differ between the two shots. To this end, notice that Mead uses a different technique on the three shots: she lobs the ball into the net on the first goal; shoots the ball from the ground on the second goal; and scores the third goal off of a half-volley, striking the ball as it bounces up off the ground. The StatsBomb variable shot.technique.name records the technique of each shot type\n\ntable(wi_shots$shot.technique.name)\n\n\n     Backheel Diving Header   Half Volley           Lob        Normal \n           35            10           648            28          3720 \nOverhead Kick        Volley \n           17           302 \n\n\nBy conditioning on both body part and technique, we can begin to build a more refined XG model. The code to do this is almost identical to the code used in our first model. The only difference is that we now group by two variables shot.body_part.name and shot.technique_name. Because we are grouping by two variables, specify the argument .groups=\"drop\" argument when calling summarize; this prevents a (mostly innocuous) warning message2. We additionally append our new XG estimates to the table containing all of Mead’s shots.\n\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarize(XG2 = mean(Y), n = dplyr::n(), .groups = \"drop\")\n\nmead_shots &lt;-\n  mead_shots |&gt;\n  dplyr::inner_join(\n    y = xg_model2 |&gt; dplyr::select(-n), \n    by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y, XG2) |&gt;\n  dplyr::slice(c(1, 4, 14))\n\n# A tibble: 3 × 6\n  OpposingTeam    minute shot.body_part.name shot.technique.name     Y    XG2\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n1 Austria Women's     15 Right Foot          Lob                     1 0.208 \n2 Norway Women's      37 Left Foot           Normal                  1 0.121 \n3 Sweden Women's      33 Right Foot          Half Volley             1 0.0892",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-sophisticated-xg",
    "href": "lectures/lecture02.html#sec-sophisticated-xg",
    "title": "Lecture 2: Expected Goals",
    "section": "More Sophisticated XG Models",
    "text": "More Sophisticated XG Models\nAccording to our new XG model, the right-footed lob against Austria has a much higher XG than the other shots against Norway and Sweden, which seems much more reasonable than our previous model. But are we fully satisfied with this model?\nOne could credibly argue that even though our model returns somewhat more sensible XG estimates, it is still too coarse for to meaningfully compare the shots above. After all, because it does not condition on distance, our model would return exactly the same XG for right-footed volleys taken one meter and 15 meters away from the goal. Similarly, we could try to account for the number of defenders between the shot and the goal and the position of the keeper.\nIf we had access to the infinite super-population of shots, conditioning on even more features is conceptually straightforward: we look at the corresponding sub-group of the super-population defined by a particular combination of features and compute the average \\(Y.\\) Unfortunately, with finite data, trying to “bin-and-average” using lots of features can lead to erratic estimates. For instance, here are the five largest and five smallest XG estimates based on body-part and shot technique.\n\nxg_model2 |&gt; \n  dplyr::arrange(dplyr::desc(XG2)) |&gt; \n  dplyr::filter(dplyr::row_number() %in% c(1:5, (dplyr::n()-4):dplyr::n()))\n\n# A tibble: 10 × 4\n   shot.body_part.name shot.technique.name    XG2     n\n   &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt; &lt;int&gt;\n 1 Right Foot          Lob                 0.208     24\n 2 Left Foot           Volley              0.163     98\n 3 Left Foot           Normal              0.121    947\n 4 Right Foot          Normal              0.121   1863\n 5 Head                Normal              0.113    910\n 6 Right Foot          Volley              0.0637   204\n 7 Head                Diving Header       0         10\n 8 Left Foot           Backheel            0          6\n 9 Left Foot           Lob                 0          4\n10 Left Foot           Overhead Kick       0          3\n\n\nBecause none of the 3 left-footed lobs in our dataset led to goals, our model estimates \\(\\textrm{XG}(\\text{left-footed lob})\\) as 0. Similarly, the rather large \\(\\textrm{XG}(\\text{right-footed lob})\\) of 33% is based on only 12 shots. Attempting to condition on even more variables would result in estimates based on even smaller sample sizes3.\nSo, it would appear that we’re stuck between a rock and a hard place. On the one hand, our XG model with two features is still too coarse to quantify important differences between the motivating shots. But, on the other hand, binning and averaging with even more features carries the risk of producing highly erratic, extreme, and somewhat nonsensical estimates4.\nStatistical models offer a principled approach to overcome these issues. We will explore several such models in Lecture 3. But for now, we will rely on a model developed by StatsBomb that accounts for a large number of features based on player locations (in two dimensions), the ball location (in three dimension), and other factors like the body part, shot technique, and the actions leading up to the shot (e.g., whether shot was taken off dribble or first touch). You can read more about their model here. Luckily for us, they include XG estimates for each shot in the public data, under the column shot.statsbomb_xg.\nFor instance, here are the XG estimates from StatsBomb’s model for all of Beth Mead’s goals\n\nmead_shots |&gt;\n  dplyr::filter(Y == 1) |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, \n                shot.technique.name, Y, shot.statsbomb_xg)\n\n# A tibble: 6 × 6\n  OpposingTeam     minute shot.body_part.name shot.technique.name     Y\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;\n1 Austria Women's      15 Right Foot          Lob                     1\n2 Norway Women's       33 Head                Normal                  1\n3 Norway Women's       37 Left Foot           Normal                  1\n4 Norway Women's       80 Left Foot           Volley                  1\n5 Northern Ireland     43 Left Foot           Normal                  1\n6 Sweden Women's       33 Right Foot          Half Volley             1\n# ℹ 1 more variable: shot.statsbomb_xg &lt;dbl&gt;\n\n\nRecall that XG quantifies a certain hypothetical long-term frequency of scoring a goal: if the shot was replayed under exactly the conditions quantified by the feature vector \\(\\boldsymbol{\\mathbf{x}}\\), \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is the proportion of times a goal is scored. So, according to StatsBomb’s proprietary model, if Mead repeatedly attempted the three shots introduced in Section 1.1, we should expect them to result in goals 36%, 44%, and 9% of the time. In other words, according to StatsBomb’s model, Meads goal against Sweden is much more impressive than her goals against Austria and Norway. One could argue, further, that this goal was somewhat lucky.\nWe can also look at the XG’s of the shots Mead took that didn’t result in goals.\n\nmead_shots |&gt;\n  dplyr::filter(Y == 0) |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, \n                shot.technique.name, Y, shot.statsbomb_xg)\n\n# A tibble: 9 × 6\n  OpposingTeam     minute shot.body_part.name shot.technique.name     Y\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;\n1 Norway Women's       29 Right Foot          Normal                  0\n2 Norway Women's       52 Right Foot          Volley                  0\n3 Northern Ireland      5 Head                Normal                  0\n4 Northern Ireland     15 Right Foot          Half Volley             0\n5 Northern Ireland     56 Right Foot          Normal                  0\n6 Northern Ireland     83 Right Foot          Normal                  0\n7 Sweden Women's        4 Head                Normal                  0\n8 Sweden Women's       19 Left Foot           Normal                  0\n9 Sweden Women's       46 Left Foot           Normal                  0\n# ℹ 1 more variable: shot.statsbomb_xg &lt;dbl&gt;\n\n\nWe see that most of Mead’s misses were on shots with very low XG values, indicating that none of these misses were especially unlucky.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-goe",
    "href": "lectures/lecture02.html#sec-goe",
    "title": "Lecture 2: Expected Goals",
    "section": "Goals Over Expected",
    "text": "Goals Over Expected\nBy summing the differences \\(Y_{i} - \\textrm{XG}_{i}\\) across all of her shots, we can quantify the degree to which Mead under- or over-performed the model expectations.\n\nsum(mead_shots$Y - mead_shots$shot.statsbomb_xg)\n\n[1] 2.896323\n\n\nWe conclude that during EURO 2022, Beth Mead scored 2.9 more goals than what StatsBomb’s XG model expected based on the contexts in which she attempted shots. We can repeat this calculation — summing over the difference between shot outcome \\(Y\\) and \\(\\textrm{XG}\\) — for all players in EURO 2022 to find the players that most over-performed and most under-performed the model expectations.\n\ngoe &lt;- \n  euro2022_shots |&gt;\n  dplyr::mutate(diff = Y - shot.statsbomb_xg) |&gt;\n  dplyr::group_by(player.name) |&gt;\n  dplyr::summarise(GOE = sum(diff),n = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(GOE))\ngoe\n\n# A tibble: 200 × 3\n   player.name               GOE     n\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Alexandra Popp          3.34     16\n 2 Bethany Mead            2.90     15\n 3 Alessia Russo           1.79     12\n 4 Francesca Kirby         1.79      5\n 5 Lina Magull             1.70     14\n 6 Ingrid Filippa Angeldal 1.37     10\n 7 Romée Leuchter          1.19      2\n 8 Hanna Ulrika Bennison   0.952     1\n 9 Nicole Anyomi           0.896     1\n10 Julie Blakstad          0.879     1\n# ℹ 190 more rows\n\n\nIt turns out that Alexandra Popp, the German captain, outperformed StatsBomb’s XG model expectations by an even wider margin than Beth Mead. Like Mead, Popp scored 6 goals during the tournament off a similar number of shots (16 for Popp and 15 for Mead). Interestingly, Mead won the Golden Boot because she had one more assist…",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-exercises",
    "href": "lectures/lecture02.html#sec-exercises",
    "title": "Lecture 2: Expected Goals",
    "section": "Exercises",
    "text": "Exercises\n\nFor each team in EURO 2022, sum the \\(\\textrm{XG}_{i}\\) values and sum the residuals \\(Y_{i} - \\textrm{XG}_{i}.\\) Which teams most over- or under-performed expectations? What relationship, if any, do you observe between teams’ cumulative XG and their performance relative to the StatsBomb XG model’s expectations?\nHudl recently released data from EURO 2025. Repeat the analysis from Section 6 to identify the players with the most extreme goals over expected values from the tournament.\nRepeat Exercise 2.1 using data from EURO 2025\nHudl also release several full seasons worth of data from professional leagues. Pick one such season and explore the relationships between (i) the cumulative XG the team generates on offense; (ii) the cumulative XG the team gives up on defense; and (iii) the win-loss record across the season.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#footnotes",
    "href": "lectures/lecture02.html#footnotes",
    "title": "Lecture 2: Expected Goals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd props to Hudl for continuing to make this data available!↩︎\nSee this StackOverflow post and the documentation for summarise for details.↩︎\nTry to convince yourself why this is the case!↩︎\nIndeed, it seems absurd to claim that, at least in women’s international soccer, players will never score off left-footed lobs! As the Statistician Dennis Lindley put it, we must “never believe in anything absolutely” and we should “leave a little probability for the moon being made of green cheese; it can be as small as 1 in a million, but have it there since otherwise an army of astronauts returning with samples of the said cheese will leave you unmoved” (Lindley 1985, sec. 6.7, available here). Lindley termed this principle “Cromwell’s Rule”, a reference to Oliver Cromwell’s quote “I beseech you, in the bowels of Christ, think it possible that you may be mistaken” from his letter to the Church of Scotland.↩︎",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture04.html",
    "href": "lectures/lecture04.html",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "",
    "text": "Which NBA players do the most to help their teams win? And how might we quantify that contribution? One option is to rely on box score statistics like points, rebounds, assists, steals, blocks, and turnovers. Underlying this approach is the assumption that the “best” or players with the most impact are the ones that record extreme values of these statistics. That is, they are the ones who score the most points, grab the most rebounds, commit the fewest turnovers, etc?\nWhile this approach seems natural, it quickly runs into problems. As Ilardi 2007 notes, it is not obvious whether all of these statistics should count the same or whether some should be weighted more heavily than others. It is also difficult to compare players who play different roles: is a guard who records 10 assists per game more valuable than a center who grabs 10 rebounds per game? More subtley, players do many things during the course of a game (e.g., setting screens, rotating on defense, dive for loose balls, etc.) that impact the outcome of a play or the game but do not appear in the box score. How should those contributions be valued?\nIn today’s lecture, we introduce two now-classical approaches to player evaluation: plus/minus (Section 3) and adjusted plus/minus (Section 4). At a high-level, plus/minus is simply the total point differential that a player’s team accrues when he is on the court. Adjusted plus/minus measures how much more a player contributes to his team’s point differential per 100 after accounting than a baseline-level player, after accounting for the relative contributions of his teammates and opponents.",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-overview",
    "href": "lectures/lecture04.html#sec-overview",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "",
    "text": "Which NBA players do the most to help their teams win? And how might we quantify that contribution? One option is to rely on box score statistics like points, rebounds, assists, steals, blocks, and turnovers. Underlying this approach is the assumption that the “best” or players with the most impact are the ones that record extreme values of these statistics. That is, they are the ones who score the most points, grab the most rebounds, commit the fewest turnovers, etc?\nWhile this approach seems natural, it quickly runs into problems. As Ilardi 2007 notes, it is not obvious whether all of these statistics should count the same or whether some should be weighted more heavily than others. It is also difficult to compare players who play different roles: is a guard who records 10 assists per game more valuable than a center who grabs 10 rebounds per game? More subtley, players do many things during the course of a game (e.g., setting screens, rotating on defense, dive for loose balls, etc.) that impact the outcome of a play or the game but do not appear in the box score. How should those contributions be valued?\nIn today’s lecture, we introduce two now-classical approaches to player evaluation: plus/minus (Section 3) and adjusted plus/minus (Section 4). At a high-level, plus/minus is simply the total point differential that a player’s team accrues when he is on the court. Adjusted plus/minus measures how much more a player contributes to his team’s point differential per 100 after accounting than a baseline-level player, after accounting for the relative contributions of his teammates and opponents.",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-data-prep",
    "href": "lectures/lecture04.html#sec-data-prep",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Stint-Level NBA Data",
    "text": "Stint-Level NBA Data\n\n\n\n\n\n\nDefinition: Stint\n\n\n\nA stint is a period of play between substitutions during which the same 10 players remain on the court.\n\n\nTo compute plus/minus and adjusted plus/minus, we need to build a data table in which rows correspond to stints within individual games. In addition to recording the identities of the home and away team players on the court during each stint, the data table needs to include game-state information about each stint including both teams’ scores and the amount of time left at the start and end of each stint and the number of possessions in each stint. Such a table can be built using play-by-play data.\nThe NBA posts a play-by-play for every game containing the time-stamp and a short description of important events during a game. Basically, whenever a player does something tracked by the scorekeepers or reported in the box-score (e.g., attempt a shot, steal the ball, commit a foul, etc.), an entry is added to the play-by-play log. Here is an example from a game from a few years ago between the Dallas Mavericks and Minnesota Timberwolves1.\nWe can scrap play-by-play data into R using the [hoopR] package, which can be installed using the code:\n\ninstall.packages(\"hoopR\")\n\nThe following function, which is based on a script written by Ron Yurko, extracts stint information from a single game’s play-by-play.\n\n\n\n\n\n\nWarning\n\n\n\nIt takes about 30 minutes to download all single season’s worth of play-by-play data.\n\n\n\n\nShow code to get stint data\n1get_game_stint_data &lt;- function(game_i) {\n  \n  game_data &lt;- hoopR::nba_pbp(game_id = game_i)\n  \n2  game_data$home_score[1] &lt;- 0\n  game_data$away_score[1] &lt;- 0\n  game_data$score_margin[1] &lt;- 0\n  \n  game_data &lt;- \n    game_data |&gt;\n3    tidyr::fill(home_score, away_score, score_margin)\n  \n  home_lineups &lt;- \n    game_data |&gt;\n    dplyr::select(home_player1, home_player2, home_player3, home_player4, home_player5) |&gt;\n4    apply(MARGIN = 1, FUN = function(x){paste(sort(x), collapse = \"_\")})\n  away_lineups &lt;-\n    game_data |&gt;\n    dplyr::select(away_player1, away_player2,away_player3, away_player4, away_player5) |&gt;\n5    apply(MARGIN = 1, FUN = function(x){paste(sort(x), collapse = \"_\")})\n  \n  game_data$home_lineup &lt;- home_lineups\n6  game_data$away_lineup &lt;- away_lineups\n  \n  # Now there are a couple of ways to figure out the stints, but I think the\n  # best way is to use the substitution events - where a stint changes once\n  # a substitution takes places (but only if the previous event was NOT a substitution)\n  \n  # Start by making an indicator for the stint change:\n  game_data &lt;- \n    game_data |&gt;\n    dplyr::mutate(is_sub = ifelse(event_type == 8, 1, 0),\n                  new_stint_start = ifelse((is_sub == 1) & (dplyr::lead(is_sub) != 1),1, 0))\n  \n  # For substitutions that take place\n  # during a free throw window, then the new stint should start after the free throw\n  \n  # Easiest way to track if substitution takes places before final free throw:\n  game_data &lt;- \n    game_data |&gt;\n    dplyr::mutate(sub_during_free_throw = dplyr::case_when(\n      (stringr::str_detect(visitor_description, \"Free Throw 1 of 1\") |\n         stringr::str_detect(visitor_description, \"Free Throw 2 of 2\") |\n         stringr::str_detect(visitor_description, \"Free Throw 3 of 3\")) &\n        (dplyr::lag(is_sub) == 1) ~ 1,\n      (stringr::str_detect(home_description, \"Free Throw 1 of 1\") |\n         stringr::str_detect(home_description, \"Free Throw 2 of 2\") |\n         stringr::str_detect(home_description, \"Free Throw 3 of 3\")) &\n        (dplyr::lag(is_sub) == 1) ~ 1,\n      .default = 0),\n      # Now if the sub is followed by this, then set new_stint_start to 0, but\n      # if this set a new stint to start post the final free throw:\n      new_stint_start = ifelse(is_sub == 1 & dplyr::lead(sub_during_free_throw) == 1, 0, new_stint_start),\n      new_stint_start = ifelse(lag(sub_during_free_throw) == 1,1, new_stint_start),\n      new_stint_start = ifelse(is.na(new_stint_start), 0, new_stint_start)\n    )\n  \n  # Filter out subs that are not new stints, and then just use\n  # the cumulative sum of the new stint start to effectively create a stint ID:\n  game_data &lt;- game_data |&gt;\n    dplyr::filter(!(is_sub == 1 & new_stint_start == 0)) |&gt;\n    dplyr::mutate(stint_id = cumsum(new_stint_start) + 1)\n  \n  # Toughest part - need to count the number of possessions for each team during\n  # the stint... will rely on this for counting when a possession ends:\n  # https://squared2020.com/2017/09/18/deep-dive-on-regularized-adjusted-plus-minus-ii-basic-application-to-2017-nba-data-with-r/\n  # \"Recall that a possession is ended by a converted last free throw, made field goal, defensive rebound, turnover, or end of period\"\n  \n  game_data &lt;- game_data |&gt;\n    dplyr::mutate(pos_ends = dplyr::case_when(\n      stringr::str_detect(home_description, \" PTS\") &\n        stringr::str_detect(home_description, \"Free Throw 1 of 2\", negate = TRUE) &\n        stringr::str_detect(home_description, \"Free Throw 2 of 3\",negate = TRUE) ~ 1, # made field goals or free throws\n      stringr::str_detect(visitor_description, \" PTS\") & \n        stringr::str_detect(visitor_description, \"Free Throw 1 of 2\",negate = TRUE) & \n        stringr::str_detect(visitor_description, \"Free Throw 2 of 3\",negate = TRUE) ~ 1, \n      stringr::str_detect(tolower(visitor_description), \"rebound\") &\n        stringr::str_detect(tolower(lag(home_description)), \"miss \") ~ 1,\n      stringr::str_detect(tolower(home_description), \"rebound\") &\n        stringr::str_detect(tolower(lag(visitor_description)), \"miss \") ~ 1,\n      stringr::str_detect(tolower(home_description), \" turnover\") ~ 1,\n      stringr::str_detect(tolower(visitor_description), \" turnover\") ~ 1,\n      stringr::str_detect(neutral_description, \"End\") ~ 1,.default = 0))\n  \n  # Now the final part - compute the stint level summaries:\n  game_data |&gt;\n    dplyr::group_by(stint_id) |&gt;\n    dplyr::summarize(\n      home_lineup = dplyr::first(home_lineup),\n      away_lineup = dplyr::first(away_lineup),\n      n_home_lineups = length(unique(home_lineup)),\n      n_away_lineups = length(unique(away_lineup)),\n      start_home_score = dplyr::first(home_score),\n      end_home_score = dplyr::last(home_score),\n      start_away_score = dplyr::first(away_score),\n      end_away_score = dplyr::last(away_score),\n      start_minutes = dplyr::first(minute_game),\n      end_minutes = dplyr::last(minute_game),\n      n_pos = sum(pos_ends),\n      .groups = \"drop\") |&gt;\n    dplyr::mutate(\n7      home_points = end_home_score - start_home_score,\n      away_points = end_away_score - start_away_score,\n      minutes = end_minutes - start_minutes,\n      pts_diff = home_points - away_points,\n      margin = 100 * (pts_diff) / n_pos,\n      game_id = game_i) |&gt;\n    dplyr::select(game_id, stint_id, \n                  start_home_score, start_away_score,\n                  start_minutes, \n                  end_home_score, end_away_score, end_minutes,\n                  home_lineup, away_lineup, n_pos,\n                  home_points, away_points, minutes, pts_diff, margin) |&gt;\n8    dplyr::filter(n_pos != 0)\n}\n\nposs_get_game_stints &lt;- \n9  purrr::possibly(.f = get_game_stint_data, otherwise = NULL)\n\n\n\n1\n\nFunction to extract the stints from a single game’s play-by-play\n\n2\n\nInitialize the starting score and margin (i.e., point differential per 100 possessions) at 0\n\n3\n\ntidyr::fill() fills in missing values with the previous entry.\n\n4\n\nGet the 5 home team players and a create a string of the numeric IDs separated by underscores (“_“)\n\n5\n\nGet the 5 away team players and a create a string of the numeric IDs separated by underscores (“_“)\n\n6\n\nAdd the strings recording who is on the court to game_data\n\n7\n\nCompute number of points scored by each team and other game-state information\n\n8\n\nRemove stints with 0 possessions.\n\n9\n\nWe will loop over many games. To prevent an error from prematurely terminating the loop (e.g., from a game with missing or corrupted play-by-play), we create a version that returns NULL when it hits an error.\n\n\n\n\nThe following code downloads play-by-play logs for every game in the 2024-25 season and applies the function defined above to create our stint-level data table.\n\nraw_game_log &lt;- hoopR::nba_leaguegamelog(season = 2024)\nnba_game_log &lt;- raw_game_log$LeagueGameLog\n\nnba_season_games &lt;- \n  nba_game_log |&gt;\n  dplyr::pull(GAME_ID) |&gt;\n1  unique()\n\n\nseason_stint_data &lt;- \n2  purrr::map(nba_season_games, ~poss_get_game_stints(.x)) |&gt;\n3  dplyr::bind_rows()\n\n4game_stint_context &lt;-\n  season_stint_data |&gt;\n  dplyr::select(\n    game_id, stint_id, n_pos,\n    start_home_score, start_away_score,start_minutes, \n    end_home_score, end_away_score, end_minutes,\n    home_points, away_points, minutes, \n    pts_diff, margin)\n\n5home_players_data &lt;-\n  season_stint_data |&gt;\n  dplyr::select(game_id, stint_id, home_lineup)|&gt;\n  tidyr::separate_rows(home_lineup, sep = \"_\") |&gt;\n  dplyr::mutate(on_court = 1) |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(\"game_id\", \"stint_id\"),\n    names_from = home_lineup,\n    values_from = on_court,\n    values_fill = 0)\nhome_players_cols &lt;- colnames(home_players_data)[3:ncol(home_players_data)]\n\n6away_players_data &lt;-\n  season_stint_data |&gt;\n  dplyr::select(game_id, stint_id, away_lineup) |&gt;\n  tidyr::separate_rows(away_lineup, sep = \"_\") |&gt;\n  dplyr::mutate(on_court = -1) |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(\"game_id\", \"stint_id\"),\n    names_from = away_lineup,\n    values_from = on_court,\n    values_fill = 0)\naway_players_cols &lt;- colnames(away_players_data)[3:ncol(away_players_data)]\n\ngame_stint_players_data &lt;- \n  home_players_data |&gt;\n7  dplyr::bind_rows(away_players_data) |&gt;\n  dplyr::group_by(game_id, stint_id) |&gt;\n  dplyr::summarize(\n    dplyr::across(dplyr::everything(), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\")\n\n8rapm_data &lt;-\n  game_stint_context |&gt;\n  dplyr::left_join(game_stint_players_data,by = c(\"game_id\", \"stint_id\"))\n\n\n1\n\nGet the ID number for every game\n\n2\n\nLoop over all game IDs and extract stint information\n\n3\n\npurrr::map() stores the stint data table for each game as a separate element of a list. Here we concatenate each of these tables into a single table.\n\n4\n\nPulls out the game-state/contextual information about each stint\n\n5\n\nCreates a column for every home team player’s on-court indicator\n\n6\n\nCreates a column for every away team player’s on-court indicator\n\n7\n\nConcatenates the home team indicators and away team indicators\n\n8\n\nJoins the player on-court indicators with the contextual information about the stint\n\n\n\n\nThe first 14 columns of rapm_data record:\n\nNumeric identifiers for the game (game_id) and stint (stint_id).\nBoth teams’ scores and the amount of time remaining at the start of a stint: start_home_score, start_away_score, and start_minutes.\nBoth teams’ scores and the amount of time remaining at the end of each stint: end_home_score, end_away_score, and end_minutes.\nThe length of a stint (minutes), the number of points scored by the home (home_points) and away team (away_points), and the number of possessions (n_pos).\nThe score differential pts_diff = home_points - away_points.\nThe score differential per 100 possessions, margin = pts_diff/n_pos * 100.\n\nThe remaining columns record whether individual players are not on the court (0), on the court and playing at home (+1), or on the court and playing on the road (-1) in each stint. The names of these columns are just identifiers assigned by hoopR. The function hoopR::nba_commonallplayers() returns a table with all players identifiers and names. The code below uses the function hoopR::nba_commonallplayers() to make a look-up table containing player names and identifiers. It also creates a column where all accents and special characters have been removed, which makes it easier to look up player ids by name (i.e., we can type “Luka Doncic” instead of “Luka Dončić”).\n\nplayer_table &lt;-\n1  hoopR::nba_commonallplayers()[[\"CommonAllPlayers\"]] |&gt;\n  dplyr::select(PERSON_ID, DISPLAY_FIRST_LAST) |&gt;\n  dplyr::rename(id = PERSON_ID, FullName = DISPLAY_FIRST_LAST) |&gt;\n  dplyr::mutate(\n2    Name = stringi::stri_trans_general(FullName, \"Latin-ASCII\"))\n\n\n1\n\nThe function returns a list with a single element named “CommonAllPlayers”. The [[ ]] bit pulls out the table.\n\n2\n\nTransliterate player names to ASCII. Effectively this strips out the accents",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-pm",
    "href": "lectures/lecture04.html#sec-pm",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Plus/Minus",
    "text": "Plus/Minus\nIntuitively, we expect a team to perform better when a “good” player is on the court than when he is not on the court. Similarly, we expect a team to perform worse when a “bad” player is on the court than when he is not on the court. Plus/Minus (hereafter +/-) attempts to quantify this intuition by computing the total number of points by which a player’s team outscores their opponents when that player is on the floor. If a player has a large, positive +/- value, that means that the player’s team outscored its opponents by a large margin Similarly, a large, negative +/- value indicates that a player’s team was outscored by a wide margin when the player was on the court.\n\nComputing Individual +/-’s\nAs an example, let’s compute the plus/minus values for Shai Gilgeous-Alexander (SGA), who was recognized as the league’s Most Valuable Player (MVP) in the 2024-25 regular season. To compute SGA’s +/-, we need to\n\nSum the home team point differentials (i.e., the values in the column pts_diff) for all stints where SGA was on the court and playing at home.\nSum the negative of the home team point differentials for all stints where SGA was on the court and playing on the road.\nAdd the two totals from Steps 1 and 2.\n\nMathematically, for every stint \\(i\\) in our data table, let \\(\\Delta_{i}\\) be the home team’s point differential during that stint. Additionally, let \\(x_{i, \\textrm{SGA}}\\) be the signed on-court indicator that is equal to 1 if SGA is on the court and playing at home during stint \\(i\\); -1 if Shai is on the court and playing on the road during stint \\(i\\); and 0 if SGA is not on the court during stint \\(i.\\) Then, SGA’s +/- is equal to \\[\n\\sum_{i = 1}^{n}{x_{i,\\textrm{SGA}} \\times \\Delta_{i}}.\n\\] The data table rapm_data contains a column with the signed on-court indicators for every player. The names of these columns are just the hoopR identifiers, which were recorded in the id column of our look-up table player_table. The following code computes Shai’s +/- and uses the function [dplyr::pull()](https://dplyr.tidyverse.org/reference/pull.html) to extract values from specific columns of rapm_data.\n\nshai_id &lt;-\n  player_table |&gt;\n1  dplyr::filter(Name == \"Shai Gilgeous-Alexander\") |&gt;\n2  dplyr::pull(id)\n3shai_x &lt;- rapm_data |&gt; dplyr::pull(shai_id)\n4delta &lt;- rapm_data |&gt; dplyr::pull(pts_diff)\n\n5sum(shai_x * delta)\n\n\n1\n\nFind the row in our look-up table corresponding to SGA\n\n2\n\nSince there is only one row corresponding to SGA, this saves SGA’s identifier as shai_id\n\n3\n\nExtracts the vector of SGA’s signed on-court indicators from the corresponding column of rapm_data\n\n4\n\nExtracts the vector of home team point differentials across all stints from rapm_data\n\n5\n\nComputes SGA’s +/-\n\n\n\n\n[1] 888\n\n\nWe now repeat this calculation for Nikola Jokic, who finished second in the MVP voting in the 2024-25 regular season. His overall +/- is substantially smaller than SGA’s.\n\njokic_id &lt;-\n  player_table |&gt;\n  dplyr::filter(Name == \"Nikola Jokic\") |&gt;\n  dplyr::pull(id)\n\njokic_x &lt;- rapm_data |&gt; dplyr::pull(jokic_id) \nsum(jokic_x * delta) \n\n[1] 452\n\n\n\n\nMatrix-Based Computation\nRepeating this calculation for all players one at a time is exceptionally tedious. Luckily, with a bit of matrix algebra, we can compute the +/- for all players in one shot.\nTo this end, let \\(n\\) be the total number of stints in our data table and let \\(p\\) be the total number of players. Just like we did for SGA and Jokic in Section 3.1, for each stint \\(i = 1, \\ldots, n\\) and for each player \\(j = 1, \\ldots, p,\\) let \\(x_{ij}\\) be the signed on-court indicator for player \\(j\\) during stint \\(i.\\) So, \\(x_{ij}\\) is equal to 0 if player \\(j\\) is not on the court during stint \\(i\\) and is equal to 1 (resp. -1) if player \\(j\\) is on the court and playing at home (resp. on the road) during stint \\(i.\\) Player \\(j\\)’s overall +/- is given by \\[\n\\sum_{i = 1}^{n}{x_{ij}\\Delta_{i}}.\n\\] We can arrange the \\(x_{ij}\\) values into an \\(n \\times p\\) matrix \\(\\boldsymbol{\\mathbf{X}}\\) so that \\(x_{ij}\\) appears in in the \\(j-th\\) column of the \\(i\\)-th row (i.e., \\(x_{ij}\\) is the \\((i,j)\\) entry of \\(\\boldsymbol{\\mathbf{X}}\\)2). This way, the rows (resp. columns) of \\(\\boldsymbol{\\mathbf{X}}\\) correspond to stints (resp. players). Let \\(\\boldsymbol{\\Delta}\\) be the vector of length \\(n\\) whose \\(i\\)-th entry is \\(\\Delta_{i},\\) the home-team point differential in stint \\(i.\\)\nIt turns out3 that player \\(j\\)’s +/- is the \\(j\\)-th entry of the vector obtained when we multiply the transpose[^transpose] of \\(\\boldsymbol{\\mathbf{X}}\\) by the vector \\(\\boldsymbol{\\Delta}.\\) That is, if \\[\n\\boldsymbol{\\mathbf{pm}} = \\boldsymbol{\\mathbf{X}}^{\\top}\\boldsymbol{\\Delta},\n\\] then the vector \\(\\boldsymbol{\\mathbf{pm}}\\) contains \\(p\\) entries and its \\(j\\)-th entry is given by \\[\n\\textrm{pm}_{j} = \\sum_{i = 1}^{n}{x_{ij}\\Delta_{i}}.\n\\] [^transpose]: The transpose of a matrix \\(\\boldsymbol{A}\\), which is denoted as \\(\\boldsymbol{A}^{\\top}\\), is obtained by switching the rows and columns indices. For instance, \\(\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}^{\\top} = \\begin{pmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{pmatrix}.\\) See this Wikipedia article for more information.\nThe first 14 columns of the data table rapm_data contain contextual information about the stint. The remaining columns contain the values of each player’s signed on-court indicators. So, to form the matrix \\(\\boldsymbol{\\mathbf{X}},\\) we just need to drop the contextual columns.\n\n1context_vars &lt;-\n  c(\"game_id\", \"stint_id\", \"n_pos\", \n    \"start_home_score\", \"start_away_score\", \"start_minutes\",\n    \"end_home_score\", \"end_away_score\", \"end_minutes\",\n    \"home_points\", \"away_points\", \"minutes\",\n    \"pts_diff\", \"margin\")\n\n\nX_full &lt;-\n2  as.matrix(\n    rapm_data |&gt;\n3      dplyr::select(- tidyr::all_of(context_vars)))\n\n\n1\n\nVector of contextual variables contained in rapm_data\n\n2\n\nConverts from a tibble to a matrix\n\n3\n\nDrops the contextual variables\n\n\n\n\nIn R, we can compute the transpose of a matrix using the function t(). But, it turns out that when we want to compute a product like \\(\\boldsymbol{\\mathbf{X}}^{\\top}\\boldsymbol{\\Delta},\\) it is much more efficient to use the function crossprod().\nThe following code creates a data table pm with one column containing player id’s another column containing plus/minus values. It then uses an inner_join to append player names. The code also computes the number of possessions played by each player. To this end, let \\(\\textrm{pos}_{i}\\) be the number of possessions played during stint \\(i\\). Notice that \\(\\lvert x_{ij} \\rvert\\) is equal to 1 whenever player \\(j\\) is on the court and is equal to 0 otherwise. So, the number of possessions played by player \\(j\\) is simply \\(\\sum_{i = 1}^{n}{\\lvert x_{ij} \\rvert \\textrm{pos}_{i}}.\\)\n\npm &lt;-\n  data.frame( \n1    id = colnames(X_full),\n2    pm = crossprod(x = X_full, y = delta),\n3    n_pos = crossprod(abs(X_full), y = rapm_data |&gt; dplyr::pull(n_pos)),\n4    minutes = crossprod(abs(X_full), y = rapm_data |&gt; dplyr::pull(minutes))) |&gt;\n5  dplyr::inner_join(y = player_table |&gt; dplyr::select(id, Name), by = \"id\") |&gt;\n6  dplyr::select(id, Name, pm, n_pos, minutes) |&gt;\n  dplyr::arrange(dplyr::desc(pm))\n\n\n1\n\nCreates a column containing player ID’s\n\n2\n\nComputes each player’s +/-.\n\n3\n\nComputes the number of possessions played by each player. The function abs() computes absolute value. We used dplyr::pull to extract the number of possessions in each stint.\n\n4\n\nCompute the number of minutes played by each player.\n\n5\n\nAppend only the names of players from player_table whose ID appears as a column name of X_full\n\n6\n\nRe-arrange the columns\n\n\n\n\nLooking at the +/- values for a handful of selected players, we notice that SGA had a much higher +/- than many other prominent players in the league.\n\nselected_players &lt;- \n  c(\"Shai Gilgeous-Alexander\", \n    \"Jayson Tatum\",\n    \"Nikola Jokic\",\n    \"Giannis Antetokounmpo\",\n    \"Luka Doncic\",\n    \"Anthony Davis\", \n    \"LeBron James\")\npm |&gt; dplyr::filter(Name %in% selected_players) |&gt; dplyr::select(Name, pm)\n\n                     Name  pm\n1 Shai Gilgeous-Alexander 888\n2            Jayson Tatum 474\n3            Nikola Jokic 452\n4   Giannis Antetokounmpo 331\n5             Luka Doncic 276\n6           Anthony Davis -78\n7            LeBron James -88\n\n\nThe following code helps visualize just how much larger SGA’s +/- was compared to the rest of these selected players and the rest of the league.\n\n1oi_colors &lt;- palette.colors(palette = \"Okabe-Ito\")\nn &lt;- nrow(X_full)\n2p &lt;- ncol(X_full)\n3y_lim &lt;- max(abs(pm$pm)) * c(-1.01, 1.01)\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\n4plot(1, type = \"n\",\n5     xlim = c(0, p+1), ylim = y_lim,\n6     xlab = \"\", xaxt = \"n\",\n7     ylab = \"+/-\", main = \"Plus-Minus\")\n\nfor(i in 1:p){\n8  lines(x = c(p+1-i,p+1-i), y = c(0, pm$pm[i]),\n9        col = oi_colors[9], lwd = 0.25)\n  if(pm$Name[i] %in% selected_players){\n    points(x = p+1-i, y = pm$pm[i], pch = 16, cex = 0.7, col = oi_colors[3])\n  } else{\n    points(x = p+1-i, y = pm$pm[i], pch = 16, cex = 0.25, col = oi_colors[9])\n  }\n}\nabline(h = 0, col = oi_colors[8])\n\n\n1\n\nLoad a color-blind friendly palette\n\n2\n\nHandy to keep the number of stints and players in our environment\n\n3\n\nTo set symmetric vertical limits in our plot, we adjust the largest absolute +/- value by 1% in each direction.\n\n4\n\nTells R to set up the plot area but not to plot anything in it\n\n5\n\nSet the horizontal and vertical limits of the plot.\n\n6\n\nSuppresses the horizontal axis and its labels\n\n7\n\nLabel the vertical axis and add a title to the plot\n\n8\n\nThe data table pm is sorted with +/- in decreasing order. To visualize these values in increasing order, we plot the \\(i\\)-th largest +/- value at the horizontal coordinate \\(p+1-i\\).\n\n9\n\nThe argument lwd controls the thickness of the lines. Setting it to 0.25 prevents over-plotting in this case\n\n\n\n\n\n\n\n\n\n\n\n\n\nIssues with +/-\nThe gap SGA’s +/- and the +/- values for the rest of the league is striking. Is this gap really attributable to difference in skill alone? Or is the gap an artifact of some systematic factors? When we plot each player’s +/- against the number of possessions played, we find much more variation in the +/- values of players who played more possessions than those who played less.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(pm$n_pos, pm$pm, \n     pch = 16, cex = 0.5, col = oi_colors[9],\n     xlab = \"Possessions played\", ylab = \"+/-\",\n     main = \"Plus-Minus vs Possessions\")\nabline(h = 0, col = oi_colors[8])\n\n\n\n\n\n\n\nFigure 1: Relationship between Plus/Minus and number of possessions played\n\n\n\n\n\nBecause +/- is an aggregated statistic — that is, it is formed by adding up contributions over the course of a season — this is not wholly surprising. We see that SGA, in particular, played many more possessions than the other players with the ten highest +/- values. Consequently, if player A has a higher +/- than player B, we cannot immediately conclude that A is better or more valuable than B due to potential disparities in opportunities.\n\npm |&gt; dplyr::slice_head(n=10)\n\n        id                    Name  pm n_pos minutes\n1  1628983 Shai Gilgeous-Alexander 888  8159 2837.31\n2  1629652           Luguentz Dort 561  5584 1955.17\n3  1630198              Isaiah Joe 552  4896 1715.11\n4  1628401           Derrick White 509  6129 2327.72\n5  1630596             Evan Mobley 508  5944 2049.43\n6  1630598           Aaron Wiggins 507  4868 1683.08\n7  1628378        Donovan Mitchell 491  6101 2106.53\n8  1628369            Jayson Tatum 474  7498 2840.95\n9  1628386           Jarrett Allen 459  6163 2142.82\n10  203999            Nikola Jokic 452  7907 2707.97\n\n\nLooking at the top-10 plus/minus values reveals another potential issue: three of SGA’s teammates (Dort, Joe, Wiggins) appear in the top-10. Do we really believe that Lugentz Dort, who is known primarily for his defensive abilites, is the second-best player in the league? The player with the third-highest +/- is Isaiah Joe, who primarily came off the bench. Does this mean that the Oklahoma City Thunder under-utilitzed potentially the third-best player in the league?\nRecall that +/- is computed by adding up the point differential for every player when they are on the court. So, if a player is fortunate enough to play alongside extremely talented teammates, they might post a high +/- rating, regardless of their actual contribution. Moreover, players who consistently find themselves playing against the opponents’ best players may have a lower +/- rating than players who more often play against their opponents’ second units. Thus, even if player A and player B played a similar number of possessions, A having a higher +/- than B still does not necessarily mean that A is better than B due to potential differences in their teammates and opponents.",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-apm-intro",
    "href": "lectures/lecture04.html#sec-apm-intro",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Adjusted Plus/Minus",
    "text": "Adjusted Plus/Minus\nAn inherent weakness, then, of +/- is that a player’s +/- necessarily depends on the performances of his teammates and opponents, which can vary dramatically across players. Adjusted plus/minus, which was first introduced by Rosenbaum in 2004, attempts to take into account the quality of a player’s teammates and opponents. Paraphrasing Rosenbaum, adjusted plus/minus does not reward players for simply being fortunate enough to play with teammates better than their opponents.\nAs noted above, comparing totals runs the risk of favoring players with many more possessions. So, to facilitate fairer comparisons, APM first converts the point differential into rate, namely point differential per 100 possessions In our rapm_data data table, this quantity is saved in the column margin4. We will denote the margin for stint \\(i\\) by \\(Y_{i}.\\)\nFormally, APM associates a number \\(\\alpha_{j}\\) to every player \\(j = 1, \\ldots, p\\) and models \\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\] where \\(h_{1}(i), \\ldots, h_{5}(i)\\) and \\(a_{1}(i), \\ldots, a_{5}(i)\\) are, respectively, the indices of the homes and away team players on the court during stint \\(i\\); \\(\\alpha_{0}\\) captures the average home court advantage in terms of point-differential per 100 possessions across all teams; and the \\(\\epsilon_{i}\\)’s are independent random errors drawn from a distribution with mean zero. In other words, APM expresses the actually observed point differential per 100 possessions as a random deviation away from the expected point differential per 100 possessions when players \\(h_{1}(i), \\ldots, h_{5}(i)\\) play against \\(a_{1}(i), \\ldots, a_{5}(i).\\)\nTo better understand the model, consider the first stint from the December 23, 2024 game between the Dallas Mavericks (away) and the Golden State Warriors (home). The players on the court were:\n\nMavericks (away): Luka Doncic, Dereck Lively II, Kyrie Irving, P.J. Washington, and Klay Thompson\nWarriors (home): Stephen Curry, Buddy Hield, Andrew Wiggins, Jonathan Kuminga, and Kevon Looney.\n\nBecause the Mavericks were playing on the road, they would be expected to outscore the Warriors by \\[\n\\begin{align}\n&-1 \\times (\\alpha_{0} + \\alpha_{SC} + \\alpha_{BH} + \\alpha_{AW} + \\alpha_{JK} + \\alpha_{KL}) \\\\\n&~~~~~+(\\alpha_{LD} + \\alpha_{KI} + \\alpha_{DL} + \\alpha_{PW} + \\alpha_{KT}).\n\\end{align}\n\\] points per 100 possessions during this stint. Now imagine that Luka Doncic were replaced by Anthony Davis5 but the other 9 players remain on the court. Then, the Mavericks’ expected points differential per 100 possessions would be \\[\n\\begin{align}\n&-1 \\times (\\alpha_{0} + \\alpha_{SC} + \\alpha_{BH} + \\alpha_{AW} + \\alpha_{JK} + \\alpha_{KL}) \\\\\n&~~~~~+(\\alpha_{AD} + \\alpha_{KI} + \\alpha_{DL} + \\alpha_{PW} + \\alpha_{KT}).\n\\end{align}\n\\] Taking the difference between these two expectations, under the APM model, the Mavericks are expected to outscore the Warriors by \\(\\alpha_{\\textrm{AD}} - \\alpha_{\\textrm{LD}}\\) points per 100 possessions when they replace Dončić with Davis. More generally, the quantity \\(\\alpha_{j} - \\alpha_{j'}\\) represents how many more points per 100 possessions a team expects to score when player \\(j\\) is on the court than when he is replaced by player \\(j'\\).\n\nAPM As A Linear Model\nOf course, we don’t know the exact \\(\\alpha_{j}\\) values and must use our data to estimate them. To this end, let \\(\\boldsymbol{\\mathbf{Z}}\\) be the \\(n \\times (p+1)\\) matrix formed by appending a column of 1’s to the matrix \\(\\boldsymbol{\\mathbf{X}}\\) and let \\(\\boldsymbol{\\alpha} = (\\alpha_{0}, \\alpha_{1}, \\ldots, \\alpha_{p})^{\\top}\\) be the vector of length \\(p+1\\) containing the home court advantage \\(\\alpha_{0}\\) and all the player-specific parameters \\(\\alpha_{j}.\\) Then, letting \\(\\boldsymbol{\\mathbf{z}}_{i}\\) be the \\(i\\)-th row of \\(\\boldsymbol{\\mathbf{Z}},\\) the APM model asserts that \\[\nY_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}.\n\\] That is, the APM model is really just a multiple linear regression model6 where the predictors include the intercept and the signed on-court indicators for all players. Because it is just a linear model, it is tempting to estimate the unknown parameter vector \\(\\boldsymbol{\\alpha}\\) using ordinary least squares.\nThat is, we want to find \\(\\hat{\\boldsymbol{\\alpha}}\\) that minimizes the quantity \\[\n\\sum_{i = 1}^{n}{\\left( Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} \\right)^{2}} .\n\\]\nUnfortunately, this problem does not have a unique solution because the matrix \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) does not have a unique inverse. To see this, recall that each row of \\(\\boldsymbol{\\mathbf{Z}}}\\) contains exactly 11 non-zero entries: * The first element in each row is equal to 1, corresponding to the intercept in the APM model * 5 entries equal to 1, corresponding to the five home players on the court during the stint * 5 entries equal to -1, correspond to the five away players on the court during the stint So, each row of \\(\\boldsymbol{\\mathbf{Z}}\\) sums to zero, meaning that the matrix is not of full-rank. This in turn implies that \\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) does not have a unique inverse and that the optimization problem does not have a unique solution.\n\n\nValue Relative to Baseline\nIn short, there are two main difficulties with our initial APM model. The first is practical: we simply cannot obtain player-specific estimates using the method of least squares. And even if we could, we face a more subtle problem: the individual parameters \\(\\alpha_{j}\\) are meaningless on their own and represent an absurd counter-factual situation.\nTo elaborate, earlier we considered a hypothetical scenario when Luka Dončić was replaced on the court by Anthony Davis. From that exercise, we saw that \\(\\alpha_{j} - \\alpha_{j'}\\) quantifies how much a team’s point differential per 100 possessions changes if you replace player \\(j'\\) by player \\(j\\) and leave everything else unchanged. Using the exact same logic, we can conclude that \\(\\alpha_{j}\\) represents the change in point differential per 100 possession when you remove player \\(j\\) from the court and don’t replace him with anybody else. Since teams never play 4-on-5, the absolute \\(\\alpha_{j}\\) values are meaningless.\nTo overcome these challenges, most analysts create a group of “baseline-level” players and, without losing any generality, they re-order the players \\(j = 1, \\ldots, p\\) so that the first \\(p'\\) are non-baseline and the last \\(p-p'\\) are baseline-level. Then, they assume that the \\(\\alpha_{j}\\)’s for all baseline-level players are identical and equal to some common value \\(\\mu.\\) For each non-baseline player \\(j = 1, \\ldots, \\tilde{p},\\) they introduce the parameter \\(\\beta_{j} = \\alpha_{j} - \\mu.\\) Unlike the individual \\(\\alpha_{j}\\) values, the \\(\\beta_{j}\\) parameters for non-baseline players have a sensible interpretation: a team should expect to score \\(\\beta_{j}\\) more points per 100 possessions if they replace a baseline-level player with non-baseline player \\(j\\), keeping all other players on the court the same7.\nThe \\(\\beta_{j}\\)’s can also be estimated using the method of least squares. Formally, let \\(\\beta_{0} = \\alpha_{0};\\) \\(\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p'})^{\\top}\\) be the vector of unknown model parameters; and let \\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\) be the \\(n \\times (p'+1)\\) sub-matrix of \\(\\boldsymbol{\\mathbf{Z}}\\) formed by removing the columns corresponding to baseline players. It turns out that[^prove2] \\[\n\\tilde{\\boldsymbol{\\mathbf{Z}}}\\boldsymbol{\\beta} = \\boldsymbol{\\mathbf{Z}}\\boldsymbol{\\alpha}\n\\] [^prove2]: Again, try to prove this yourself. If you run into any issues, ask on Piazza or stop by office hours.\nHowever, unlike the matrix \\(\\boldsymbol{\\mathbf{Z}},\\) the matrix \\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\) is full-rank, which means that the minimization problem \\[\n\\textrm{argmin}\\sum_{i = 1}^{n}{\\left(Y_{i} - \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta}\\right)^{2}}\n\\] has a unique solution: \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\tilde{\\boldsymbol{\\mathbf{Z}}}\\right)^{-1}\\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\boldsymbol{\\mathbf{Y}}.\n\\]\n\n\nA Re-parametrized Model\nIn practice, we rarely fit linear models like our re-parametrized APM model with manual matrix calculations. Instead, we rely on R’s built-in lm() function to do compute \\(\\hat{\\boldsymbol{\\beta}}.\\) To this end, it suffices to create a data table where each row corresponds to a stint and there are columns containing the margin for the stint and the signed on-court indicators for all non-baseline players.\nBefore proceeding, we need to define the set of baseline-level players. For our analysis, we will use a 250 minute threshold, classifying any player who played fewer than 250 minutes as baseline-level.\n\nnonbaseline_id &lt;-\n  pm |&gt;\n  dplyr::filter(minutes &gt;= 250) |&gt;\n  dplyr::pull(id)\n\nNext, we create a data table containing columns for the signed on-court indicators of all non-baseline players and the point differential per 100 possessions. Then, we fit the linear model and extract the estimated parameters.\n\napm_df &lt;-\n  rapm_data |&gt;\n  dplyr::select(tidyr::all_of(c(\"margin\", nonbaseline_id)))\n\n1apm_fit &lt;- lm(margin ~ ., data = apm_df)\n\n\n2beta0 &lt;- coefficients(apm_fit)[1]\n3beta &lt;- coefficients(apm_fit)[-1]\n\n\n1\n\nFit the linear model\n\n2\n\nExtract the intercept term, which captures the home-court advantage\n\n3\n\nExtract the estimates of \\(\\beta_{j}\\) for all non-baseline players\n\n\n\n\nAfter inspecting the first few elements, we see that the elements of beta are named and that there is an additional character back-tick in the element names.\n\nbeta[1:5]\n\n `1628983`  `1629652`  `1630198`  `1628401`  `1630596` \n10.2568750  1.8500194  4.0660224 -0.8287388  7.9423890 \n\n\nWe will build a data table similar to pm containing the id and estimated adjusted plus/minus value of every non-baseline player. To do this, we need to remove the back-tick from the names of beta.\n\n1names(beta) &lt;- stringr::str_remove_all(string = names(beta), pattern = \"`\")\napm &lt;-\n  data.frame(id = names(beta), apm = beta) |&gt;\n  dplyr::inner_join(y = player_table, by = \"id\")\n2rownames(apm) &lt;- NULL\n\n\n1\n\nRemoves all instances of the back-tick from the names of beta\n\n2\n\nBecause beta is named, the data table apm is created with rownames, which are unnecessary. The top-10 players based on adjusted plus/minus looks quite a bit different than the raw plus/minus!\n\n\n\n\n\napm |&gt;\n  dplyr::arrange(dplyr::desc(apm)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::select(Name, apm)\n\n                    Name      apm\n1          Tobias Harris 17.01331\n2         Mouhamed Gueye 16.79320\n3           Devin Carter 16.60148\n4             Trae Young 15.13287\n5  Giannis Antetokounmpo 14.61082\n6            Isaiah Wong 14.50091\n7           Nikola Jokic 14.40837\n8         Alperen Sengun 13.55544\n9        Quenton Jackson 13.13962\n10    Karl-Anthony Towns 13.13714\n\n\n\n\nWeighted adjusted plus/minus\nAlthough APM takes into account the quality of the teammates and opponents with whom each player plays, it fails to account fully for the context in which players play. Quoting Deshpande and Jensen (2016), as a result, metrics like APM &gt; “can artificially can artificially inflate the importance of performance in low-leverage situations, when the outcome of the game is essentially decided, while simultaneously deflating the importance of high-leverage performance, when the final outcome is still in question. For instance, point diﬀerential-based metrics model the home team’s lead dropping from 5 points to 0 points in the last minute of the first half in exactly the same way that they model the home team’s lead dropping from 30 points to 25 points in the last minute of the second half”.\nTo overcome this limitation, we might try to down-weight low-leverage stints and up-weight high-leverage stints. For instance, we might assign a weight \\(w_{i}\\) to stint \\(i\\) where * \\(w_{i} = 1\\) if, at the start of stint \\(i,\\) the teams are within 10 points of each other * \\(w_{i} = 0\\) if, at the start of the stint \\(i,\\) the difference in scores exceed 30 points * \\(w_{i} = 1 - (\\textrm{StartDiff} - 10)/20\\): if, at the start of stint \\(i,\\) the difference in scores is between 10 and 30 points. This weight smoothly interpolates between 0 (when the difference is 30) and 1 (when the difference is 10).\n\nwapm_df &lt;-\n  rapm_data |&gt;\n  dplyr::mutate(\n1    start_diff = abs(start_home_score - start_away_score),\n    w = dplyr::case_when(\n2      start_diff &lt; 10 ~ 1,\n3      start_diff &gt; 30 ~ 0,\n4      .default = 1 - (start_diff-10)/20)) |&gt;\n  dplyr::select(tidyr::all_of(c(\"margin\", \"w\", nonbaseline_id)))\n\n\n1\n\nCreate a variable recording the lead at the start of the stint\n\n2\n\nSet weight = 1 if the starting lead is less than 10 points\n\n3\n\nSet weight = 0 if starting lead exceeds 30 points\n\n4\n\nAssign a weight between 0 and 1 if the starting lead is between 10 and 30 points\n\n\n\n\nThen the weighted adjusted plus/minus coefficients \\(\\hat{\\boldsymbol{\\beta}}_{w}\\) is the unique minimizer of \\[\n\\sum_{i = 1}^{n}{w_{i}\\left(Y_{i} - \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta}\\right)^{2}}\n\\] and can be computed in closed form as \\[\n\\hat{\\boldsymbol{\\beta}}_{w} = \\left( \\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\tilde{\\boldsymbol{\\mathbf{Z}}}\\right)^{-1}\\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{Y}},\n\\] where \\(\\boldsymbol{\\mathbf{W}}\\) is a \\(n \\times n\\) diagonal matrix with the weights \\(w_{i}\\) along the diagonal.\nAs with regular APM, in practice, we can fit a weighted APM model using lm(). The main difference is that we must (i) include a vector of weights in the data frame that we pass; (ii) specify the weights using the weights argument; and (iii) exclude the weight variable from the linear predictor.\n\nwapm_fit &lt;- \n1  lm(formula = margin ~ . - w,\n2     weights = w,\n     data = wapm_df) \n\nwbeta0 &lt;- coefficients(wapm_fit)[1]\nwbeta &lt;- coefficients(wapm_fit)[-1]\n\nnames(wbeta) &lt;- stringr::str_remove_all(string = names(wbeta), pattern = \"`\")\nwapm &lt;-\n  data.frame(id = names(wbeta), wapm = wbeta) |&gt;\n  dplyr::inner_join(y = player_table, by = \"id\")\nrownames(wapm) &lt;- NULL \n\n\n1\n\nThe -w in the formula argument signals to R that it should not include the variable w as a linear predictor. See this cheatseet for more details about the formula interface\n\n2\n\nTells lm that it should weight observations by whatever is in the column w in wapm_df\n\n\n\n\nBecause we have now weight stints differently, we see that the top-10 players according to our weighted adjusted plus/minus metric is somewhat different than the original adjutsed plus/minus metric.\n\nwapm |&gt;\n  dplyr::arrange(dplyr::desc(wapm)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::select(Name, wapm)\n\n                Name     wapm\n1       Devin Carter 19.48318\n2      Tobias Harris 17.11044\n3    Lauri Markkanen 15.20344\n4     Mouhamed Gueye 14.76474\n5         Trae Young 14.52393\n6       Nikola Jokic 14.14573\n7     Alperen Sengun 14.03023\n8         OG Anunoby 13.96219\n9  Jordan McLaughlin 13.73760\n10      Jericho Sims 13.27850",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#sec-looking-ahead",
    "href": "lectures/lecture04.html#sec-looking-ahead",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nTo fit our adjusted plus/minus model using the method of least squares, we specified a set of baseline-level players and assumed that all the baseline players had the exact same partial effect on their team’s average point differential per 100 possessions. This is a very strong and, frankly, unrealistic assumption! The use of baseline players and the assumed equality of their impact was motivated by a numerical challenge, viz. the inability to solve the least squares minimiziation problem when our design matrix \\(\\boldsymbol{\\mathbf{Z}}\\) had constant row sums. In Lecture 5, we will introduce an alternative approach to fitting adjusted plus/minus models that avoids the need to specify baseline players by solving a regularized version of the least squares problem. We save our player look-up table, the matrix X_full, and the vector of point differentials per 100 possessions so that we can load them next time.\n\nY &lt;- apm_df$margin\nsave(X_full, Y, player_table, file = \"lecture04_05_data.RData\")",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#exercises",
    "href": "lectures/lecture04.html#exercises",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Exercises",
    "text": "Exercises\n\nExplore the sensitivity of APM and our weighted APM to different choices of the minutes cut-off used to define the baseline player. How much do the top- and bottom-10 player rankings change?\nWe weighted stints using only on the lead at the start of the stint. One can reasonably argue that our weights should also account for the time left in the game. Propose your own weighting scheme and see how the player rankings change.\nWe estimated APM using data from a single season. Many analysts prefer to use data from multiple season when fitting APM models. Scrape data from the 2022-23 and 2023-24 regular seasons and fit APM models using (i) data from 2023-24 and 2024-25 and (iii) data from 2022-23, 2023-34, and 2024-25.\nOne can credibly argue that data from seasons further in the past should be weighted less than data from more recent seasons. Propose a weighting scheme that down-weights historical data and fit weighted versions of the models in Exercise 4.3. How do the relative player rankings change?\nEstimate the out-of-sample predictive mean square error of our basic APM model using 100 training/testing splits. That is, for each training and testing split, fit the APM model using the training data and then compute the average of the squared difference between the actual \\(Y\\)’s and the model predictions in the testing split. Is APM a good predictive model? Why or why not?",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#footnotes",
    "href": "lectures/lecture04.html#footnotes",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is the game in which Luka Dončić embarrassed Rudy Gobert hit a step-back 3 pointer over Rudy Gobert to win the game (link).↩︎\nSee this Wikipedia entry for background on on index notation for two-dimensional arrays.↩︎\nSee the Wikipedia entry on matrix-vector multiplication for the formula.↩︎\nAs best I can tell, the name margin refers back to the original article.↩︎\nFire Nico.↩︎\nMultiple linear regression is a focus of STAT 333. If you have taken that class, I encourage you to go back through your notes from it. And if you have not taken that class, I highly recommend reviewing Section 1.6 of the book Beyond Multiple Linear Regression and Sections 3.1 and 3.2 of An Introduction to Statistical Learning.↩︎\nTry proving this for yourself! If you find yourself getting stuck with this, ask your classmates on Piazza, or come to the instructor’s Friday office hours.↩︎",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture06.html",
    "href": "lectures/lecture06.html",
    "title": "Lecture 6: Run Expectancy",
    "section": "",
    "text": "The first game of the 2024 MLB Regular Season, between the Los Angeles Dodgers and the San Diego Padres, took place on March 20, 20241 During the game, Shohei Ohtani recorded two hits in his five plate appearances:\n\nIn the 3rd inning, with 2 outs and no runners on base, Ohtani singled into right field.\nIn the 8th inning, with 1 out and runners on first and second base, Ohtani singled into left center field, driving in one run.\n\nWhich of these singles was more valuable to Dodgers? And how much of that value should be specifically credited to Ohtani?\nBecause the second single directly resulted in a run scoring, it is tempting to conclude that the second single was more valuable than the first. However, although the Dodgers didn’t score as a result of the first single, Ohtani reached first base, putting his team in a position to potential score more runs than if he had not reached first2. In terms of allocating credit, it seems natural to give Ohtani all the credit for whatever value was created on his first single. But on his second single, at least some of the value created is attributable to a baserunner scoring from second base. How should we divide credit between Ohtani and that baserunner?\nOver the next several lectures, we will work with pitch-level tracking data from Major League Baseball to answer these questions. In this lecture, we begin by computing the number of runs that a team can expect to score in the remainder of a half-inning (Section 4). Then, we introduce run values (Section 5), a metric that combines changes in the number of a team expects to score and the number of runs actually scored in an at-bat. We conclude by determining which batters created the most run value for their teams (Section 6).\nIn Lecture 7, we will distribute the run value created in every at-bat between the batter and base runners involved in the at-bat. We will then begin Lecture 8 by distributing the negative of the run value created by th batting team between the pitcher and fielders involved in each at-bat. Finally, we will aggregate the total run value over average created by each player through their hitting, fielding, base running, and pitching and convert that aggregate into a measure of wins above replacement. Our development largely follows (Baumer, Jensen, and Matthews 2015), who created a transparent, open-source version of wins above replacement, a cornerstone of baseball analytics.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-motivation",
    "href": "lectures/lecture06.html#sec-motivation",
    "title": "Lecture 6: Run Expectancy",
    "section": "",
    "text": "The first game of the 2024 MLB Regular Season, between the Los Angeles Dodgers and the San Diego Padres, took place on March 20, 20241 During the game, Shohei Ohtani recorded two hits in his five plate appearances:\n\nIn the 3rd inning, with 2 outs and no runners on base, Ohtani singled into right field.\nIn the 8th inning, with 1 out and runners on first and second base, Ohtani singled into left center field, driving in one run.\n\nWhich of these singles was more valuable to Dodgers? And how much of that value should be specifically credited to Ohtani?\nBecause the second single directly resulted in a run scoring, it is tempting to conclude that the second single was more valuable than the first. However, although the Dodgers didn’t score as a result of the first single, Ohtani reached first base, putting his team in a position to potential score more runs than if he had not reached first2. In terms of allocating credit, it seems natural to give Ohtani all the credit for whatever value was created on his first single. But on his second single, at least some of the value created is attributable to a baserunner scoring from second base. How should we divide credit between Ohtani and that baserunner?\nOver the next several lectures, we will work with pitch-level tracking data from Major League Baseball to answer these questions. In this lecture, we begin by computing the number of runs that a team can expect to score in the remainder of a half-inning (Section 4). Then, we introduce run values (Section 5), a metric that combines changes in the number of a team expects to score and the number of runs actually scored in an at-bat. We conclude by determining which batters created the most run value for their teams (Section 6).\nIn Lecture 7, we will distribute the run value created in every at-bat between the batter and base runners involved in the at-bat. We will then begin Lecture 8 by distributing the negative of the run value created by th batting team between the pitcher and fielders involved in each at-bat. Finally, we will aggregate the total run value over average created by each player through their hitting, fielding, base running, and pitching and convert that aggregate into a measure of wins above replacement. Our development largely follows (Baumer, Jensen, and Matthews 2015), who created a transparent, open-source version of wins above replacement, a cornerstone of baseball analytics.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-statcast",
    "href": "lectures/lecture06.html#sec-statcast",
    "title": "Lecture 6: Run Expectancy",
    "section": "Tracking Data in Baseball",
    "text": "Tracking Data in Baseball\n\nA Brief History\nOn October 4, 2006, during an American League Division Series game between the Oakland Athletics and Minnesota Twins, the sports media company Sportvision3 debuted PitchF/X4, a system of cameras for tracking the position of every pitch as it traveled from the pitcher’s hand to the batter. After installing the system in every MLB ballpark, Sportvision began providing the data in real-time data to the MLB. These data, combined with additional information recorded by an MLB Advanced Media employee, powered the popular GameDay application and were publicly available through the GameDay application programming interface (API) for many years.\nIn 2017, PITCHf/x system was phased out replaced by the radar-based Trackman system, which was originally developed to track the trajectories of golf balls. Trackman is part of the larger Statcast system, which additionally tracks the movement of all players on the field. See this New York Times article for more about the history of Statcast.\n\n\nAccessing Statcast Data in R\nMajor League Baseball hosts a public-facing web interface for accessing Statcast data. Using that interface, users can pull up data for individual players or about all pitches of a certain type. Powering this website is an API, which allows software applications to connect to the underlying Statcast database. It is through this API that the baseballr package acquires data. You can install the package with the code:\n\ndevtools::install_github(repo = \"BillPetti/baseballr\")\n\nThe function baseballr::statcast_search() allows users query all Statcast data by date, player, or player type. One of the original baseballr authors, Bill Petti, wrote a wrapper function that uses baseballr::statcast_search() to pull down an entire season’s worth of pitch-by-pitch data; see this blog post for the wrapper function code and this earlier post for details about its design. Since he published his original function, Statcast has added some new fields, necessitating a few changes to the original script. The code below defines a new scraper, which we will use in the course. An R script containing this code is available at this link. At a high level, the scraping function pulls data from Statcast on a week-by-week basis.\n\n\nShow the code for Statcast scraper\n## Code modified from Bill Petti's original annual Statcast scraper:\n## Main change is in the column names of the fielders\n\nannual_statcast_query &lt;- function(season) {\n  \n  data_base_column_types &lt;- \n    readr::read_csv(\"https://app.box.com/shared/static/q326nuker938n2nduy81au67s2pf9a3j.csv\")\n  \n  dates &lt;- \n    seq.Date(as.Date(paste0(season, '-03-01')),\n             as.Date(paste0(season, '-12-01')), \n             by = '4 days')\n  \n  date_grid &lt;- \n    tibble::tibble(start_date = dates, \n                   end_date = dates + 3)\n  \n  safe_savant &lt;- \n    purrr::safely(baseballr::scrape_statcast_savant)\n  \n  payload &lt;- \n    purrr::map(.x = seq_along(date_grid$start_date),\n               ~{message(paste0('\\nScraping week of ', date_grid$start_date[.x], '...\\n'))\n                 payload &lt;- \n                   safe_savant(start_date = date_grid$start_date[.x], \n                               end_date = date_grid$end_date[.x], \n                               type = 'pitcher')\n                 return(payload)\n               })\n  \n  payload_df &lt;- purrr::map(payload, 'result')\n  \n  number_rows &lt;- \n    purrr::map_df(.x = seq_along(payload_df),\n                  ~{number_rows &lt;- \n                    tibble::tibble(week = .x, \n                                   number_rows = length(payload_df[[.x]]$game_date))\n                  }) %&gt;%\n    dplyr::filter(number_rows &gt; 0) %&gt;%\n    dplyr::pull(week)\n  \n  payload_df_reduced &lt;- payload_df[number_rows]\n  \n  payload_df_reduced_formatted &lt;- \n    purrr::map(.x = seq_along(payload_df_reduced), \n               ~{cols_to_transform &lt;- \n                 c(\"pitcher\", \"fielder_2\", \"fielder_3\",\n                   \"fielder_4\", \"fielder_5\", \"fielder_6\", \"fielder_7\",\n                   \"fielder_8\", \"fielder_9\")\n               df &lt;- \n                 purrr::pluck(payload_df_reduced, .x) %&gt;%\n                 dplyr::mutate_at(.vars = cols_to_transform, as.numeric) %&gt;%\n                 dplyr::mutate_at(.vars = cols_to_transform, function(x) {ifelse(is.na(x), 999999999, x)})\n               character_columns &lt;- \n                 data_base_column_types %&gt;%\n                 dplyr::filter(class == \"character\") %&gt;%\n                 dplyr::pull(variable)\n               numeric_columns &lt;- \n                 data_base_column_types %&gt;%\n                 dplyr::filter(class == \"numeric\") %&gt;%\n                 dplyr::pull(variable)\n               integer_columns &lt;- \n                 data_base_column_types %&gt;%\n                 dplyr::filter(class == \"integer\") %&gt;%\n                 dplyr::pull(variable)\n               df &lt;- \n                 df %&gt;%\n                 dplyr::mutate_if(names(df) %in% character_columns, as.character) %&gt;%\n                 dplyr::mutate_if(names(df) %in% numeric_columns, as.numeric) %&gt;%\n                 dplyr::mutate_if(names(df) %in% integer_columns, as.integer)\n               return(df)\n               })\n  \n  combined &lt;- payload_df_reduced_formatted %&gt;%\n    dplyr::bind_rows()\n  \n  return(combined)\n}\n\n\nTo use this function, it is enough to run something like.\n\nraw_statcast2024 &lt;- annual_statcast_query(2024)\n\n\n\n\n\n\n\nWarning\n\n\n\nScraping a single season of Statcast data can take between 30 and 45 minutes. I highly recommend scraping the data for any season only once and saving the resulting data table in an .RData file that can be loaded into future R sessions.\n\nlibrary(tidyverse)\nraw_statcast2024 &lt;- annual_statcast_query(2024)\nsave(raw_statcast2024, file = \"raw_statcast2024.RData\")\n\nThese .RData files take between 75MB and 150MB of space. So, if you want to work with data from all available seasons (2008 to the present), you will need about 2.5GB of storage space on your computer.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-statcast-basics",
    "href": "lectures/lecture06.html#sec-statcast-basics",
    "title": "Lecture 6: Run Expectancy",
    "section": "Statcast Basics",
    "text": "Statcast Basics\nThe function annual_statcast_query actually scrapes data not only from the regular season but also from the pre-season and the play-offs. The column game_type records the type of game in which each pitch was thrown. Looking at the Statcast documentation, we see that regular season pitches have game_type==\"R\".\n\ntable(raw_statcast2024$game_type, useNA = 'always')\n\n\n     D      F      L      R      S      W   &lt;NA&gt; \n  5182   2488   3540 695136  77056   1576      0 \n\n\nFor our analyses, we will work only with data from regular season games. Below, we filter to those pitches with game_type==\"R\" and remove those with non-sensical values like 4 balls or 3 strikes.\n\nstatcast2024 &lt;-\n  raw_statcast2024 |&gt; \n  dplyr::filter(game_type == \"R\") |&gt;\n  dplyr::filter(\n    strikes &gt;= 0 & strikes &lt; 3 & \n      balls &gt;= 0 & balls &lt; 4 & \n      outs_when_up &gt;= 0 & outs_when_up &lt; 3) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number)\n\nWe’re now left with 695,135 regular season pitches.\n\nPitch- and At-Bat-level Descriptions\nStatcast records a ton of information about each pitch including the type of pitch (pitch_type), the velocity of the pitch when it was released (vx0, vy0, vz0), the acceleration of the pitch at about the half-way point between the pitcher and batter (ax, ay, and az), and horizontal and vertical coordinates of the pitch as it crosses the front edge of home plate (plate_x) and (plate_y). The column type also records whether the pitch resulted ball (type=B), a strike (type=S), or whether the ball was put in play (type=X). For balls in play, Statcast also records things like the launch speed and angle (launch_speed, launch_angle), the coordinates on the field where the ball landed (hc_x) and (hc_y), and the position of the first fielder to touch the ball (hit_location). In Lecture 8, we will work with hc_x and hc_y to build a model that predicts the probability that each fielder makes an out on balls hit to a specific part of the field. As a bit of a preview, here is a plot of all the hc_x and hc_y values.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x, statcast2024$hc_y, \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, col = rgb(0,0,0, 0.1))\n\n\n\n\n\n\n\n\n\n\nAdding Baserunner Information\nThe columns on_1b, on_2b, and on_3b record who is on first, second, or third base at the beginning of each pitch (NA values indicate that nobody is on base). For convenience, we will add a new column to the data table that records the baserunner configuration using a string of 3 binary digits. If there is a runner on first base, the first digit will be a 1 and if there is not a runner on first base, the first digit will be a 0. Similarly, the second and third digits respectively indicate whether there are runners on second and third base. So, the string \"101\" indicates that there are runners on first and third base but not on second. To create the 3-digit binary string encoding baserunner configuration, notice that 1*(!is.na(on_1b)) will return a 1 if there is someone on first base and 0 otherwise. So by pasting together the results of 1*(!is.na(on_1b)), 1*(!is.na(on_2b)), and 1*(!is.na(on_3b)), we can form the 3-digit binary string described above. In following code, we also rename the column outs_when_up to Outs.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::mutate(\n    BaseRunner = \n      paste0(1*(!is.na(on_1b)),1*(!is.na(on_2b)),1*(!is.na(on_3b)))) |&gt;\n  dplyr::rename(Outs = outs_when_up)\n\n\n\nGetting Player Names\nFor every pitch, the Statcast dataset records the identities of the batter (batter), pitcher (pitcher), and the other fielders (fielders_2, …, fielders_9). However, it does not identify them by name, instead using an ID number, which is assigned by MLB Advanced Media. We can look up the corresponding player names using a database maintained by the Chadwick Register. The function baseballr::chadwick_player_lu() downloads the Chadwick database and stores it as a data table in R. This database contains the names and MLB Advanced Media identifiers for players across all seasons, far more than we need for our purposes. So, in the code below, we first download the Chadwick player database and then extract only those players who appeared in the 2024 regular season. Like with the raw pitch-by-pitch data, I recommend that you download this player identity database once and save the table as an .RData object for future use.\n\n1player2024_id &lt;-\n  unique(\n    c(statcast2024$batter, statcast2024$pitcher,\n      statcast2024$on_1b, statcast2024$on_2b, statcast2024$on_3b,\n      statcast2024$fielder_2, statcast2024$fielder_3,\n      statcast2024$fielder_3, statcast2024$fielder_4,\n      statcast2024$fielder_5, statcast2024$fielder_6,\n      statcast2024$fielder_7, statcast2024$fielder_8,\n      statcast2024$fielder_9))\n\nchadwick_players &lt;- baseballr::chadwick_player_lu()\n2save(chadwick_players, file = \"chadwick_players.RData\")\n\nplayer2024_lookup &lt;-\n  chadwick_players |&gt;\n  dplyr::filter(!is.na(key_mlbam) & key_mlbam %in% player2024_id) |&gt;\n  dplyr::mutate(\n3    FullName = paste(name_first, name_last),\n4    Name = stringi::stri_trans_general(FullName, \"Latin-ASCII\"))\nsave(player2024_lookup, file = \"player2024_lookup.RData\")\n\n\n1\n\nThis vector contains the MLB Advanced Media ID for all players who appeared in 2024 as a batter, pitcher, fielder, or baserunner.\n\n2\n\nSave a local copy of the full Chadwick database.\n\n3\n\nCreates a column containing the player’s first and last name.\n\n4\n\nRemoves any accents or special characters, which we will need in Lecture 7.\n\n\n\n\n\n\nGetting Player Positions\nLater in Lecture 7, we will compare each batter’s hitting performance to the average performance of other batters who play the same position in the field. Unfortunately, the Chadwick database does not record the position of each player. Luckily, position information can be obtained using baseballr::mlb_batting_orders(), which retrieves the batting order for each MLB game. In this functions output, the column id contains the MLB Advanced Media id number for each player (i.e., key_mlbam in the data table player2024_lookup that we created above) and the column abbreviation contains the fielding position. For instance, Ohtani was listed as the Designated Hitter in that March 20, 2024 game between the Dodgers and the Padres.\n\nbaseballr::mlb_batting_orders(game_pk = 745444)\n\n── MLB Game Starting Batting Order data from MLB.com ─── baseballr 1.6.0.9002 ──\n\n\nℹ Data updated: 2025-07-30 20:58:12 CDT\n\n\n# A tibble: 18 × 8\n       id fullName         abbreviation batting_order batting_position_num team \n    &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;                &lt;chr&gt;\n 1 605141 Mookie Betts     SS           1             0                    away \n 2 660271 Shohei Ohtani    DH           2             0                    away \n 3 518692 Freddie Freeman  1B           3             0                    away \n 4 669257 Will Smith       C            4             0                    away \n 5 571970 Max Muncy        3B           5             0                    away \n 6 606192 Teoscar Hernánd… RF           6             0                    away \n 7 681546 James Outman     CF           7             0                    away \n 8 518792 Jason Heyward    RF           8             0                    away \n 9 666158 Gavin Lux        2B           9             0                    away \n10 593428 Xander Bogaerts  2B           1             0                    home \n11 665487 Fernando Tatis … RF           2             0                    home \n12 630105 Jake Cronenworth 1B           3             0                    home \n13 592518 Manny Machado    DH           4             0                    home \n14 673490 Ha-Seong Kim     SS           5             0                    home \n15 595777 Jurickson Profar LF           6             0                    home \n16 669134 Luis Campusano   C            7             0                    home \n17 642180 Tyler Wade       3B           8             0                    home \n18 701538 Jackson Merrill  CF           9             0                    home \n# ℹ 2 more variables: teamName &lt;chr&gt;, teamID &lt;int&gt;\n\n\nBecause baseballr::mlb_batting_orders() can be run using only one game identifier (i.e., game_pk) at a time, we need to loop over all of the unique game_pk values in statcast2024 to get all batting orders. In the code block below, we first create a wrapper function, which we call get_lineup around baseballr::mlb_batting_orders() that only retains the columns id and abbreviation and renames those columns.\n\nget_lineup &lt;- function(game_pk){\n  lineup &lt;- baseballr::mlb_batting_orders(game_pk = game_pk)\n  lineup &lt;-\n    lineup |&gt;\n    dplyr::mutate(game_pk = game_pk) |&gt;\n    dplyr::rename(key_mlbam = id, position = abbreviation) |&gt;\n    dplyr::select(game_pk, key_mlbam, position)\n  return(lineup)\n}\n\nNow, to get the batting orders for all regular season games from 2024, we could try writing a for loop that iterates over all unique game_pk values and writes the table to a list.\n\nall_lineups &lt;- list()\nunik_game_pk &lt;- unique(statcast2024$game_pk)\nfor(i in 1:length(unik_game_pk)){\n  all_lineups[[i]] &lt;- get_lineup(game_pk = unik_game_pk[i])\n}\n\nIf get_lineup() throws an error (e.g., because the batting orders for a particular game are not available), then the whole loop gets terminated. To avoid having to manually remove problematic games and re-start the loop, we will use purrr:possibly() to create a version of get_lineup() that returns NULL when it hits an error. We will also use purrr:map instead of writing an explict for loop.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code takes about an hour to run.\n\n\n\nposs_get_lineup &lt;- purrr::possibly(.f = get_lineup, otherwise = NULL) \nunik_game_pk &lt;- unique(statcast2024$game_pk)\n\n1block_starts &lt;- seq(1, length(unik_game_pk), by = 500)\nblock_ends &lt;- c(block_starts[-1], length(unik_game_pk))\n\nall_lineups &lt;- list()\nfor(b in 1:5){\n  tmp &lt;-\n    purrr::map(.x = unik_game_pk[block_starts[b]:block_ends[b]], \n               .f = poss_get_lineup, \n               .progress = TRUE)\n  all_lineups &lt;- c(all_lineups, tmp)\n}\n\nlineups2024 &lt;- \n2  dplyr::bind_rows(all_lineups) |&gt;\n  unique()\n3save(lineups2024, file = \"lineups2024.RData\")\n\n\n1\n\nI was not able to loop over the whole set of unique game_pk values at once. I found it useful to break the loop into blocks of about 500 games each.\n\n2\n\nEach element of all_lineup is a data table. This allows us to stack them on top of one another.\n\n3\n\nIt’s useful to save the table of batting orders just in case we need it again in the future\n\n\n\n\nThe table lineups2024 contains the starters and their position for all the games in our dataset. The following code determines the most commonly listed position for each player.\n\npositions2024 &lt;-\n  lineups2024 |&gt;\n  dplyr::group_by(key_mlbam, position) |&gt;\n1  dplyr::summarise(n = dplyr::n()) |&gt;\n2  dplyr::slice_max(order_by = n, with_ties = FALSE) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::select(key_mlbam, position)\nsave(positions2024, file = \"positions2024.RData\")\n\n\n1\n\nCounts the number of occurrences of each player-position combination\n\n2\n\nAfter we call dplyr::summarise(), the resulting data table is still grouped by batter. So, when we call dplyr::slice_max() it looks at all the rows corresponding to each player and extracts the one with highest count. This is how we determine the most commonly listed position.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-expected-runs",
    "href": "lectures/lecture06.html#sec-expected-runs",
    "title": "Lecture 6: Run Expectancy",
    "section": "Expected Runs",
    "text": "Expected Runs\n\n\n\n\n\n\nDefinition: Expected Runs\n\n\n\nFor each combination of the number of outs (\\(\\textrm{o} \\in \\{0,1,2\\}\\)) and base runner configurations (\\(\\textrm{br} \\in \\{\"000\", \"100\", \"010\", \"001\", \"110\", \"101\", \"011\", \"111\"\\}\\)), let \\(\\rho(\\textrm{o}, \\textrm{br})\\) be the average number of runs that a team scores in the remainder of the half-inning following a pitch thrown with \\(\\textrm{o}\\) outs and base runner configuration \\(\\textrm{br}.\\)\n\n\nComputing \\(\\rho(\\textrm{o}, \\textrm{br})\\) is conceptually straightforward: we need to divide our observed at-bats into 24 bins, one for each combination of \\((\\textrm{o}, \\textrm{br})\\) and then compute the average value of \\(R\\) within each bin. This is exactly the same “binning-and-averaging” procedure we used to fit our initial XG models in Lecture 2. We will do this using pitches taken in the first 8 innings of played in the 2024 regular season. We focus only on the first 8 innings because the 9th and extra innings are fundamentally different than the others. Specifically, the bottom half of the 9th (or later) innings is only played if the game is tied or the home team is trailing after the top of the 9th inning concludes. In those half-innings, if played, the game stops as soon as a winning run is scored. For instance, say that the home team is trailing by 1 runs in the bottom of the 9th and that there are runners on first and second base. If the batter hits a home run, the at-bat is recorded as resulting in only two runs (the tying run from second and the winning run from first). But the exact same scenario would result in 3 runs in an earlier inning.\n\nComputing Runs Scored in the Half-Inning\nSuppose that in a given at-bat \\(a\\) that there are \\(n_{a}\\) pitches. Within at-bat \\(a,\\) for each \\(i = 1, \\ldots, n_{a},\\) let \\(R_{i,a}\\) be the number of runs scored in the half-inning after that pitch (including any runs scored as a result of pitch \\(i\\)). So \\(R_{1,a}\\) is the number of runs scored in the half-inning after the first pitch, \\(R_{2,a}\\) is the number of runs scored subsequent to the second pitch, etc. Our first step towards building the necessary at-bat-level data set will be to append a column of \\(R_{i,a}\\) values to each season’s Statcast data.\nWe start by illustrating the computation using a single half-inning from the Dodgers-Padres game introduced earlier. The code below pulls out all pitches thrown in the top of the 8th inning of the game. During this inning, the Dodgers scored 4 runs.\n\ndodgers_inning &lt;-\n  statcast2024 |&gt;\n  dplyr::filter(game_pk == 745444 & inning == 8 & inning_topbot == \"Top\") |&gt;\n  dplyr::select(\n    at_bat_number, pitch_number, Outs, BaseRunner,\n    bat_score, post_bat_score, events, description, des,\n    type, on_1b, on_2b, on_3b, hc_x, hc_y, hit_location) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number)\n\nThe column bat_score records the batting team’s score before each pitch is thrown. The column post_bat_score records the batting team’s score after the the outcome of the pitch. For most of the 25 pitches, we find that bat_score is equal to post_bat_score; this is because only a few pitches result in scoring events.\n\nrbind(bat_score = dodgers_inning$bat_score, post_bat_score = dodgers_inning$post_bat_score)\n\n               [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nbat_score         1    1    1    1    1    1    1    1    1     1     1     1\npost_bat_score    1    1    1    1    1    1    1    1    1     1     1     1\n               [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nbat_score          1     1     2     3     3     3     4     5     5     5\npost_bat_score     1     2     3     3     3     4     5     5     5     5\n               [,23] [,24] [,25]\nbat_score          5     5     5\npost_bat_score     5     5     5\n\n\nCross-referencing the table above with the play-by-play data, we see that the Dodgers score their second run after the 14th pitch of the half-inning (on a Enrique Hernández sacrifice fly); their third run on the very next pitch (Gavin Lux grounding into a fielder’s choice); and their fourth and fifth runs on consecutive pitches (on singles by Mookie Betts and Shohei Ohtani).\n\n\n\n\n\n\nFigure 1: Play-by-Play from the March 20, 2024 Dodgers-Padres game\n\n\n\nWe can verify this by looking at the variable des, which stores a narrative description about what happened during the at-bat.\n\ndodgers_inning$des[c(14,15, 18, 19)]\n\n[1] \"Enrique Hernández out on a sacrifice fly to left fielder José Azocar. Max Muncy scores.\"                                                                                             \n[2] \"Gavin Lux reaches on a fielder's choice, fielded by first baseman Jake Cronenworth. Teoscar Hernández scores. James Outman to 2nd. Fielding error by first baseman Jake Cronenworth.\"\n[3] \"Mookie Betts singles on a ground ball to left fielder José Azocar. James Outman scores. Gavin Lux to 2nd.\"                                                                           \n[4] \"Shohei Ohtani singles on a line drive to left fielder José Azocar. Gavin Lux scores. Mookie Betts to 2nd.\"                                                                           \n\n\nNotice that, because we’ve sorted the pitches in ascending order by at-bat and pitch number, the very last row of the table corresponds to the last pitch of the inning. Accordingly, the last value in the post_bat_score column is the batting team’s score at the end of the inning. Thus, to compute \\(R_{i,a}\\) for all pitches in this inning, it is enough to subtract the bat_score in each row from the last post_bat_score in the table. To access this last value, we use the function last().\n\ndplyr::last(dodgers_inning$post_bat_score) - dodgers_inning$bat_score\n\n [1] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 2 2 2 1 0 0 0 0 0 0\n\n\nWe now append a column with these values to our data table dodgers_inning\n\ndodgers_inning &lt;-\n  dodgers_inning |&gt;\n  dplyr::mutate(RunsRemaining = dplyr::last(post_bat_score) - bat_score)\n\n\n\nPutting It All Together\nWe’re now ready to extend these calculation to every half-inning of every game in the season. To do this, we will take advantage of the dplyr::group_by() function to apply the same calculation to subsets defined by game and half-inning.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n2  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n3  dplyr::mutate(RunsRemaining = dplyr::last(post_bat_score) - bat_score) |&gt;\n  dplyr::ungroup()\n\n\n1\n\nDivide the data based on the combination of game and half-inning\n\n2\n\nArrange pitches in the appropriate temporal order\n\n3\n\nAdd column for how many runs were scored after each pitch\n\n\n\n\nNow we have the number of runs scored in the half-inning after each pitch. But to compute run expectancy, we need this quantity at the at-bat level and not at the pitch-level. Using our notation from before, note that \\(R_{1,a}\\) is the number of runs scored after the first pitch of at-bat \\(a.\\) So, to compute run expectancy, it is enough to pull out the first pitch from each at-bat (i.e., those pitches with pitch_number == 1) using the filter() function.\n\nexpected_runs &lt;-\n  statcast2024 |&gt;\n1  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(Outs, BaseRunner, RunsRemaining) |&gt;\n2  dplyr::group_by(Outs, BaseRunner) |&gt;\n3  dplyr::summarize(rho = mean(RunsRemaining), .groups = \"drop\")\n\n\n1\n\nGet the first pitch in every at-bat\n\n2\n\nSub-divide the data based on the 24 combinations of out and baserunner\n\n3\n\nCompute the mean of RunRemaining for each game state combination\n\n\n\n\nThe table expected_runs contains one row for every combination of outs and base-runner configuration. Traditionally, expected runs is reported using an \\(8\\times 3\\) matrix, with rows corresponding to base-runner configurations and columns corresponding to outs. We can re-format expected_runs to this matrix format using the tidyr::pivot_wider() function\n\nexpected_runs |&gt; \n  tidyr::pivot_wider(\n    names_from = Outs,\n    values_from = rho,\n    names_prefix=\"Outs: \")\n\n# A tibble: 8 × 4\n  BaseRunner `Outs: 0` `Outs: 1` `Outs: 2`\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 000            0.488     0.262    0.0980\n2 001            1.43      0.972    0.352 \n3 010            1.07      0.672    0.347 \n4 011            2.03      1.44     0.612 \n5 100            0.897     0.529    0.228 \n6 101            1.90      1.22     0.502 \n7 110            1.49      0.926    0.449 \n8 111            2.31      1.58     0.815 \n\n\nIn the March 20, 2024 game against the Padres, Ohtani recorded his first hit in the 3rd inning on a pitch with 2 outs and no runners on base. Based on the game state at the start of the at-bat (i.e., Outs=2 and BaseRunner='000'), his team can expect to score 0.1 runs in the remainder of the half-inning. As a result of Ohtani’s single, he changed the game state to Outs = 2 and BaseRunner=100, a state from which his team can expect to score 0.22 runs, on average. So, although his at-bat did not directly result in a run scoring, Ohtani increased his team’s run expectancy by about 0.12 runs.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-run-value",
    "href": "lectures/lecture06.html#sec-run-value",
    "title": "Lecture 6: Run Expectancy",
    "section": "Run Value",
    "text": "Run Value\nWhile expected runs is a really useful metric, for the purposes of allocating credit, we need to account not only for what we expect to happen but also what actually happened as a result of each at-bat.\n\n\n\n\n\n\nDefinition: Run Value\n\n\n\nThe run value of an at-bat is defined as the the number of runs scored in the at-bat plus the difference in expected runs from the starting to ending state. That is, denoting the number of runs scored in the at-bat as \\(\\textrm{RunsScored}\\) and the starting and ending states as \\((\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\) and \\((\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}),\\) then \\[\n\\textrm{RunValue} = \\textrm{RunsScored} + \\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}) - \\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\n\\]\n\n\nIn a sense, run value rewards batters and base runners for two things, actually scoring runs and putting their team in positions from which they could potentially score more runs.\nTocompute the run value of each at-bat in the 2024 season, we must compute\n\nThe number of runs scored during each at-bat\nThe game state (i.e., the number of outs and the base-runner configuration) at the start and end of each at-bat\nThe change in expected runs during the at-bat (i.e., \\(\\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}) - \\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\)).\n\nWe will first develop the necessary code using the data from Dodger’s 8th inning from their game against the Padres. Then, we will deploy that code to the whole statcast2024 table by grouping by game_pk and at_bat_number.\n\nCalculating Runs Score\nStatcast numbers every at-bat within the game and every pitch within each at-bat. To compute the number of runs scored within each at-bat, we will:\n\nSort the pitches by at-bat number and then by pitch number in ascending order\nTake the difference between the last value of post_bat_score and first value of bat_score within each at-bat.\n\nLet’s try to verify this by looking at pitches from the third, fourth, and fifth at-bats of Dodgers’ 8th inning against the Padres5\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 61:63) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, bat_score, description, post_bat_score)\n\n# A tibble: 7 × 5\n  at_bat_number pitch_number bat_score description   post_bat_score\n          &lt;int&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n1            61            1         1 ball                       1\n2            61            2         1 ball                       1\n3            61            3         1 ball                       1\n4            61            4         1 ball                       1\n5            62            1         1 foul                       1\n6            62            2         1 hit_into_play              2\n7            63            1         2 hit_into_play              3\n\n\nBased on the description column, we see that the first pitch of at-bat 62 was a foul ball and the second pitch was hit into play. When we look at the corresponding row (row 6) of the table, we see that that Dodgers’ pre-pitch score was 1 (bat_score = 1) and that they scored 1 run as a result of the hit (post_bat_score = 2). Reassuringly, the difference between the value of post_bat_score in row 6 (the last row for at-bat 62) and the value of bat_score in row 5 (the first row for at-bat 62) is 1. We can similarly verify our procedure works in at-bat 61: the fourth value of post_bat_score and the first value of bat_score are equal and the Dodgers did not score in this at-bat.\nWe can apply our procedure to the entirety of the Dodgers’ half-inning\n\ndodgers_inning &lt;-\n  dodgers_inning |&gt;\n  dplyr::group_by(at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(RunsScored = dplyr::last(post_bat_score) - dplyr::first(bat_score)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::arrange(at_bat_number, pitch_number)\ndodgers_inning |&gt; dplyr::select(at_bat_number, pitch_number, bat_score, description, post_bat_score, RunsScored)\n\n# A tibble: 25 × 6\n   at_bat_number pitch_number bat_score description   post_bat_score RunsScored\n           &lt;int&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n 1            59            1         1 called_strike              1          0\n 2            59            2         1 ball                       1          0\n 3            59            3         1 ball                       1          0\n 4            59            4         1 foul                       1          0\n 5            59            6         1 blocked_ball               1          0\n 6            60            1         1 ball                       1          0\n 7            60            2         1 foul_tip                   1          0\n 8            60            3         1 hit_into_play              1          0\n 9            61            1         1 ball                       1          0\n10            61            2         1 ball                       1          0\n# ℹ 15 more rows\n\n\nWe can now apply this formula to all pitches in statcast2024 by grouping by game_pk and at_bat_number. We will also save a copy of the data table statcast2024 so that we can load it into future R sessions.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, at_bat_number) |&gt;\n2  dplyr::arrange(pitch_number) |&gt;\n3  dplyr::mutate(RunsScored = dplyr::last(post_bat_score) - dplyr::first(bat_score)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number)\n\nsave(statcast2024, file = \"statcast2024.RData\")\n\n\n1\n\nSub-divide the data based on individual at-bats\n\n2\n\nPlace pitches within each at-bat in sequential order\n\n3\n\nAdd column recording the number of runs scored in the at-bat\n\n\n\n\n\n\nComputing Starting & Ending States\nExcept for the very last pitch in a team’s innings, the ending state of that pitch is, by definition, the starting state of the next pitch. In order to compute \\(\\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}),\\) and \\(\\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\) for each at-bat, we will first create a columns in statcast2024 that encode the game state at the beginning and end of the at-bat.\nTo build up our code, let’s continue with our running example of the Dodgers’ 8th inning, focusing on the at the second through fourth at-bats of the inning.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner)\n\n# A tibble: 9 × 4\n  at_bat_number pitch_number  Outs BaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;     \n1            60            1     0 100       \n2            60            2     0 100       \n3            60            3     0 100       \n4            61            1     0 110       \n5            61            2     0 110       \n6            61            3     0 110       \n7            61            4     0 110       \n8            62            1     0 111       \n9            62            2     0 111       \n\n\nWe start by creating new columns recording the Outs and BaseRunner values of the next pitch using dplyr::lead() function. 6.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner) |&gt;\n  dplyr::mutate(\n    next_Outs = dplyr::lead(Outs), next_BaseRunner = dplyr::lead(BaseRunner))\n\n# A tibble: 9 × 6\n  at_bat_number pitch_number  Outs BaseRunner next_Outs next_BaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;          \n1            60            1     0 100                0 100            \n2            60            2     0 100                0 100            \n3            60            3     0 100                0 110            \n4            61            1     0 110                0 110            \n5            61            2     0 110                0 110            \n6            61            3     0 110                0 110            \n7            61            4     0 110                0 111            \n8            62            1     0 111                0 111            \n9            62            2     0 111               NA &lt;NA&gt;           \n\n\nNow, within each at-bat, we can look at the last values of next_Outs and next_BaseRunner to figure out the ending state of the at-bat.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner) |&gt;\n  dplyr::mutate(\n    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner)) |&gt;\n  dplyr::group_by(at_bat_number) |&gt;\n  dplyr::mutate(\n    endOuts = dplyr::last(next_Outs),\n    endBaseRunner = dplyr::last(next_BaseRunner)) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner, endOuts, endBaseRunner) |&gt;\n  dplyr::ungroup()\n\n# A tibble: 9 × 6\n  at_bat_number pitch_number  Outs BaseRunner endOuts endBaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        \n1            60            1     0 100              0 110          \n2            60            2     0 100              0 110          \n3            60            3     0 100              0 110          \n4            61            1     0 110              0 111          \n5            61            2     0 110              0 111          \n6            61            3     0 110              0 111          \n7            61            4     0 110              0 111          \n8            62            1     0 111             NA &lt;NA&gt;         \n9            62            2     0 111             NA &lt;NA&gt;         \n\n\nWe can repeat this code for all at-bats.\n\nrunValue2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n2    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n3    end_Outs = dplyr::last(next_Outs),\n    end_BaseRunner = dplyr::last(next_BaseRunner)) |&gt; \n  dplyr::ungroup() |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n4  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(\n    game_pk, at_bat_number, \n    inning, inning_topbot, \n    Outs, BaseRunner, \n    RunsScored, RunsRemaining, \n    end_Outs, end_BaseRunner)\n\n\n1\n\nSub-divide into half-innings\n\n2\n\nGet values of Outs and BaseRunner for next pitch in the inning\n\n3\n\nGoes to last pitch of each at-bat and gets the next value of Outs and BaseRunner, which are the starting values of these variables in the next at-bat.\n\n4\n\nGets the first pitch from each at-bat\n\n\n\n\n\n\nComputing Run Values\nNow that we have a table runValue containing information about the starting and ending states of each at-bat, we are ready to compute run-values. In particular, we can use a join to add in the values of the starting and ending expected runs.\nBefore doing that, though, we need to deal with the NA’s introduced by lead(). Looking at the at-bats from the Dodger’s 8th inning from our running example, we see that those NA’s correspond to the very last at-bat of the half-inning.\n\nrunValue2024 |&gt;\n  dplyr::filter(game_pk == 745444 & inning == 8 & inning_topbot == \"Top\") |&gt;\n  dplyr::select(at_bat_number, Outs, BaseRunner, end_Outs, end_BaseRunner)\n\n# A tibble: 8 × 5\n  at_bat_number  Outs BaseRunner end_Outs end_BaseRunner\n          &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;         \n1            59     0 000               0 100           \n2            60     0 100               0 110           \n3            61     0 110               0 111           \n4            62     0 111               1 110           \n5            63     1 110               1 110           \n6            64     1 110               1 110           \n7            65     1 110               1 110           \n8            66     1 110              NA &lt;NA&gt;          \n\n\nBecause the “end of the inning” state is not one of the 24 combinations of outs and baserunner configurations in the expected_runs table, we’re going to row to that table with Outs=3, BaseRunners='000', and rho = 0 (since the team cannot score any more runs in the inning once it is over!).\n\nexpected_runs &lt;-\n  expected_runs |&gt;\n  tibble::add_row(Outs=3, BaseRunner=\"000\", rho = 0)\n\nrunValue2024 &lt;-\n  runValue2024 |&gt;\n  dplyr::mutate(\n    end_Outs = ifelse(is.na(end_Outs), 3, end_Outs),\n    end_BaseRunner = ifelse(is.na(end_BaseRunner), '000', end_BaseRunner))\n\nWe’re now ready to use a join to append the starting and ending expected runs.\n\nend_expected_runs &lt;- \n  expected_runs |&gt;\n  dplyr::rename(\n    end_Outs = Outs,\n    end_BaseRunner = BaseRunner,\n    end_rho = rho)\n\nrunValue2024 &lt;-\n  runValue2024 |&gt;\n  dplyr::left_join(y = expected_runs, by = c(\"Outs\", \"BaseRunner\")) |&gt;\n  dplyr::left_join(y = end_expected_runs, by = c(\"end_Outs\", \"end_BaseRunner\")) |&gt;\n  dplyr::mutate(RunValue = RunsScored + end_rho - rho) |&gt;\n  dplyr::select(game_pk, at_bat_number, RunValue)\n\nrm(end_expected_runs)\nsave(runValue2024, file = \"runValue2024.RData\")",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#sec-batter-prod",
    "href": "lectures/lecture06.html#sec-batter-prod",
    "title": "Lecture 6: Run Expectancy",
    "section": "Assessing Batter Production",
    "text": "Assessing Batter Production\nEach row in runValue2024 corresponds to a single at-bat in the 2024 regular season and is uniquely determined by the game identifier (game_pk) and at-bat number (at_bat_number). We’re now in a position to quantify how run value Ohtani created during the Dodgers-Padres game introduced above?\nTo do this, we first look up Ohtani’s MLB Advanced Media ID number using our table player2024_lookup. Then, we can extract the rows of statcast2024 corresponding to the first pitch in each at-bat from the Dodgers-Padres game statcast2024. Then we can filter this subset to only those at-bats where Ohtani was the batter. Finally, we can join the associated run values.\n\nload(\"player2024_lookup.RData\")\n\n\nohtani_id &lt;- \n  player2024_lookup |&gt;\n  dplyr::filter(FullName == \"Shohei Ohtani\") |&gt;\n  dplyr::pull(key_mlbam)\n\nohtani_ab &lt;-\n  statcast2024 |&gt;\n  dplyr::filter(game_pk == 745444) |&gt;\n  dplyr::filter(pitch_number == 1 & batter == ohtani_id) |&gt;\n  dplyr::select(game_pk, at_bat_number, inning, des) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\")) |&gt;\n  dplyr::select(at_bat_number, RunValue, des)\nohtani_ab\n\n# A tibble: 5 × 3\n  at_bat_number RunValue des                                                    \n          &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                                                  \n1             2   -0.367 Shohei Ohtani grounds into a force out, shortstop Ha-S…\n2            18    0.130 Shohei Ohtani singles on a sharp line drive to right f…\n3            37   -0.367 Shohei Ohtani grounds into a force out, third baseman …\n4            52   -0.164 Shohei Ohtani grounds out softly, pitcher Wandy Peralt…\n5            65    1     Shohei Ohtani singles on a line drive to left fielder …\n\n\nOver the course of the entire game, Ohtani created a total of 0.2310438 in run value. The negative run value created in his first, third, and fourth at-bats (i.e., when he got out) is offset by the positive run value he created by singling in the top of the 3rd inning and driving in a run in the top of the 8th inning. Note the run value of his last at-at is exactly 1 because a single run scored but the base-runner configuration did not change as a result of the at-bat.\nBy repeating this calculation over the course of the entire 2024 regular season, we can identify those batters who created the most run value for their teams.\n\ntmp_lookup &lt;-\n  player2024_lookup |&gt;\n  dplyr::select(key_mlbam, Name) |&gt;\n  dplyr::rename(batter = key_mlbam)\n\nre24 &lt;-\n  statcast2024 |&gt; \n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(game_pk, at_bat_number, batter) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\")) |&gt;\n  dplyr::group_by(batter) |&gt;\n  dplyr::summarise(RE24 = sum(RunValue),N = dplyr::n()) |&gt;\n  dplyr::inner_join(y = tmp_lookup, by = \"batter\") |&gt;\n  dplyr::select(Name, RE24, N) |&gt;\n  dplyr::arrange(dplyr::desc(RE24))\nre24\n\n# A tibble: 649 × 3\n   Name               RE24     N\n   &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n 1 Aaron Judge        89.0   675\n 2 Juan Soto          74.4   693\n 3 Shohei Ohtani      73.1   708\n 4 Bobby Witt         65.9   694\n 5 Brent Rooker       47.9   599\n 6 Vladimir Guerrero  45.0   671\n 7 Ketel Marte        41.2   562\n 8 Kyle Schwarber     40.9   672\n 9 Joc Pederson       39.2   433\n10 Jose Ramirez       39.1   657\n# ℹ 639 more rows\n\n\nWhen we compare our top-10 to FanGraph’s leaderboard for RE24, we see a lot of overlap. But there are some differences, especially with regards to the number of plate appearances and actual RE24 values. For the latter, FanGraph likely used a different expected run matrix. And the Statcast data is not complete; for instance, it is missing 3 games in which Judge played.",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#exercises",
    "href": "lectures/lecture06.html#exercises",
    "title": "Lecture 6: Run Expectancy",
    "section": "Exercises",
    "text": "Exercises\n\nScrape data from the 2022 and 2023 regular seasons and determine which batters created the most run value.\nCompute a new version of expected runs that conditions on the number of outs, baserunner configuration, and ballpark. Using this new version of expected runs, determine which batters create the most run value for their team. Does ballpark appear to make much difference?",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#footnotes",
    "href": "lectures/lecture06.html#footnotes",
    "title": "Lecture 6: Run Expectancy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis was also the first MLB game ever played in South Korea!↩︎\nIn fact, Ohtani went on to steal second base during the next at-bat, further increasing his team’s chances of scoring.↩︎\nSportvision is perhaps most famous for the yellow first down line that appears in American football broadcasts. They were eventually acquired by SMT.↩︎\nSee this article by Mike Fast for more background↩︎\nStatcast assigns each at-bat in a game a unique number. The third, fourth, and fifth at-bats during the Dodger’s 8th inning were the 61st, 62nd, and 63rd at-bats of the game.↩︎\nThe next value of a variable is undefined in the last row of a column, resulting in some NA’s. We’ll deal with those later on.↩︎",
    "crumbs": [
      "Lecture 6: Run Expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture08.html",
    "href": "lectures/lecture08.html",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "",
    "text": "In Lecture 7, we distributed the run value created in each at-bat between the batter and base runners. Aggregating over all at-bats, we computed each player’s \\(\\textrm{RAA}^{\\textrm{br}},\\) which quantifies how much more run value a player created through his base running than would otherwise be expected based on the starting game states and ending events of the at-bats in which he was involved. We also computed \\(\\textrm{RAA}^{\\textrm{b}},\\) which quantifies how much more run value a player created through his hitting than would be expected based on his position.\nAccording to the conservation of runs framework introduced by Baumer, Jensen, and Matthews (2015), whenever the batting team creates \\(\\delta\\) units of run value in at-bat, the fielding team (necessarily) gives up \\(\\delta\\) units of run value. Equivalently, the fielding team creates \\(-\\delta\\) units of run value. In this lecture, we discuss how to divide this \\(-\\delta\\) run value between the pitchers (Section 6) and fielders (Section 5).\nFor at-bats \\(i\\) that end without the ball being put in play (i.e., a strikeout, walk, or a homerun), we will assign the entirety of the \\(-\\delta_{i}\\) run value to the pitcher. But for at-bats that result with balls put in play, we will divide \\(-\\delta_{i}\\) into two parts \\(\\delta^{(f)}_{i} = -\\delta \\times \\hat{p}_{i}\\) and \\(\\delta^{(p)} = -\\delta_{i} \\times (1 - \\hat{p}_{i})\\) where \\(\\hat{p}_{i}\\) is an estimate of the probability that play results in an out given the batted ball’s location (Section 3). We will specifically estimate these out probabilities using Generalized Additive Models (Section 4). Finally, we compute how more run value each player creates through their batting, baserunning, pitching, and fielding than a replacement level player (Section 7).",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-overview",
    "href": "lectures/lecture08.html#sec-overview",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "",
    "text": "In Lecture 7, we distributed the run value created in each at-bat between the batter and base runners. Aggregating over all at-bats, we computed each player’s \\(\\textrm{RAA}^{\\textrm{br}},\\) which quantifies how much more run value a player created through his base running than would otherwise be expected based on the starting game states and ending events of the at-bats in which he was involved. We also computed \\(\\textrm{RAA}^{\\textrm{b}},\\) which quantifies how much more run value a player created through his hitting than would be expected based on his position.\nAccording to the conservation of runs framework introduced by Baumer, Jensen, and Matthews (2015), whenever the batting team creates \\(\\delta\\) units of run value in at-bat, the fielding team (necessarily) gives up \\(\\delta\\) units of run value. Equivalently, the fielding team creates \\(-\\delta\\) units of run value. In this lecture, we discuss how to divide this \\(-\\delta\\) run value between the pitchers (Section 6) and fielders (Section 5).\nFor at-bats \\(i\\) that end without the ball being put in play (i.e., a strikeout, walk, or a homerun), we will assign the entirety of the \\(-\\delta_{i}\\) run value to the pitcher. But for at-bats that result with balls put in play, we will divide \\(-\\delta_{i}\\) into two parts \\(\\delta^{(f)}_{i} = -\\delta \\times \\hat{p}_{i}\\) and \\(\\delta^{(p)} = -\\delta_{i} \\times (1 - \\hat{p}_{i})\\) where \\(\\hat{p}_{i}\\) is an estimate of the probability that play results in an out given the batted ball’s location (Section 3). We will specifically estimate these out probabilities using Generalized Additive Models (Section 4). Finally, we compute how more run value each player creates through their batting, baserunning, pitching, and fielding than a replacement level player (Section 7).",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-hit-location-data",
    "href": "lectures/lecture08.html#sec-hit-location-data",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Hit Location Data",
    "text": "Hit Location Data\nWe begin by loading several of the data tables created in Lecture 6 and Lecture 7 including statcast2024, which contains pitch-level data for all regular season pitches in 2024, runValue2024, which contains the \\(\\delta_{i}\\)’s for each regular season at-bat, and player2024_lookup, which contains the names and MLB Advanced Media identifiers for each player.\n\nload(\"statcast2024.RData\")\nload(\"runValue2024.RData\")\nload(\"player2024_lookup.RData\")\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n\nRecall from Lecture 6 that the Statcast variables hc_x and hc_y record the coordinates where each batted ball is first fielded. When we plotted the available hc_x and hc_y values, we saw the outlines of the park, with home plate located around hc_x = 125 and hc_y = 200. We can also roughly make out the first and third base lines and the infield.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x, statcast2024$hc_y, \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\n\n\n\n\n\n\n\nFigure 1: Locations where all batted balls are first fielded\n\n\n\n\n\nIt is not immediately apparent, however, whether the first base line is on the left or right. Luckily, StatCast also contains the official fielding position of the player who first fields the ball1. When we plot just the pitches first fielded by first basemen (hit_location=3), we see that in the original (hc_x, hc_y) coordinate system, first base is on the right hand side of the plot.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x[statcast2024$hit_location!=3], \n     statcast2024$hc_y[statcast2024$hit_location != 3], \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\npoints(statcast2024$hc_x[statcast2024$hit_location==3], \n     statcast2024$hc_y[statcast2024$hit_location==3],\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[2], alpha.f = 0.1))\n\n\n\n\n\n\n\nFigure 2: Location of all batted balls initially fielded by the first baseman\n\n\n\n\n\nAlthough MLB Stadiums are equipped with lots of very cool camera and ball tracking technology, the variables hc_x and hc_y are not derived from those technologies. Instead a stringer manually marks the location of the batted ball on a tablet so the coordinates hc_x and hc_y are expressed in units of pixels on that tablet. Jim Albert, who is one of the founding fathers of Statistical research in baseball, suggested the following transformation from the original (hc_x, hc_y) coordinate system to one that places home plate at the bottom of the plot (with coordinates (0,0)) and measures distances in feet. \\[\n\\begin{align}\nx &= 2.5 \\times (\\texttt{hc\\_x} - 125.42) & y &= 2.5 \\times (198.27 - \\texttt{hc\\_y})\n\\end{align}\n\\]\nThe following code implements this transformation, storing the new coordinates as x and y and then plots the batted ball locations. It also overlays the first and third base lines, the base paths and places points at the base locations.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::mutate(\n    x = 2.5 * (hc_x - 125.42),\n    y = 2.5 * (198.27 - hc_y))\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$x, \n     statcast2024$y, \n     xlab = \"x\", ylab = \"y\",\n     pch = 16, cex = 0.2, \n     xlim = c(-300, 300),\n     ylim = c(-100, 500),\n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\n\n1abline(a = 0, b = 1, col = oi_colors[5], lwd = 1)\nabline(a = 0, b = -1, col = oi_colors[5], lwd = 1)\n\n2lines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 2)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nThe function abline(a,b) plots the line \\(y = ax + b\\). In our new coordinate system the first and third base lines correspond to the lines \\(y = x\\) and \\(y = -x\\).\n\n2\n\nIn our new coordinate system, home plate is at (0,0). First base and third base are 90 feet away from home plate along the 45 degree lines, meaning that their coordinates are (90/sqrt(2), 90/sqrt(2)) and (-90/sqrt(2), 90/sqrt(2))`.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Locations where batted balls are first fielded in our transformed coordinate system\n\n\n\n\n\nIn Lecture 7, we created a table atbat2024 that contained the beginning and ending states of each at-bat, run value, ending event, and identities of the batter and baserunners. Today, we will build a similar data frame that contains the ending event, identities of the pitcher and fielders, and, if the at-bat resulted in a ball being put into play, the batted ball locations. Like the code we used to build atbat2024, we need to identify the last entry in the column events in each at bat as well as the last entry of the columns x, y, and hit_location. We will also keep the run value.\n\ndef_atbat2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n    end_events = dplyr::last(events),\n    end_x = dplyr::last(x),\n    end_y = dplyr::last(y),\n    end_type = dplyr::last(type),\n    end_hit_location = dplyr::last(hit_location)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::select(\n    game_date, game_pk, at_bat_number,\n    end_events, end_type, des,\n    batter,\n    pitcher, fielder_2, fielder_3,\n    fielder_4, fielder_5, fielder_6,\n    fielder_7, fielder_8, fielder_9,\n    end_x, end_y, end_hit_location) |&gt;\n  dplyr::rename(x = end_x, y = end_y) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\"))",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-out-probs",
    "href": "lectures/lecture08.html#sec-out-probs",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Estimating Out Probabilities",
    "text": "Estimating Out Probabilities\nRecall that we need to estimate the probability that a batted ball results in an out given its coordinate. To do so, we first need to extract those at-bats that resulted in a ball being put in play. The variable end_type in def_atbat2024 records whether the last pitch of each at-bat resulted in a strike (end_type = S), a ball (end_type = B), or a ball being hit into play (end_type = X). As a sanity check, let’s take a look at the ending event for all at-bats with end_type = X. Reassuringly, none of them can occur without a ball being hit.\n\ntable(def_atbat2024$end_events[def_atbat2024$end_type == \"X\"], useNA = 'always')\n\n\n                   double               double_play               field_error \n                     7608                       336                      1093 \n                field_out           fielders_choice       fielders_choice_out \n                    72233                       373                       306 \n                force_out grounded_into_double_play                  home_run \n                     3408                      3152                      5326 \n                 sac_bunt                   sac_fly       sac_fly_double_play \n                      446                      1222                        13 \n                   single                    triple               triple_play \n                    25363                       685                         1 \n                     &lt;NA&gt; \n                        0 \n\n\nAs another sanity check, when we tabulate the end_events for all at-bats with end_type != X, we do not see any of the hitting events from above.\n\ntable(def_atbat2024$end_events[def_atbat2024$end_type != \"X\"], useNA = 'always')\n\n\n                             catcher_interf          hit_by_pitch \n                  308                    97                  1977 \n            strikeout strikeout_double_play          truncated_pa \n                40145                   107                   304 \n                 walk                  &lt;NA&gt; \n                14029                     0 \n\n\nSo, we can isolate those at-bats with a ball put in play by filtering on end_type.\n\nbip &lt;-\n  def_atbat2024 |&gt;\n  dplyr::filter(end_type == \"X\")\n\nThe columns end_events includes values fielders_choice and fielders_choice_out. Presumably, balls recorded as fielders_choice did not result in an out while those recorded as fielders_choice_out did. To check whether this is the case, we will search from the string “out” in the description of the at-bat (des) for all balls with event = fielders_choice\n\ntable(grepl(\"out\", bip$des[bip$end_events == \"fielders_choice\"]))\n\n\nFALSE  TRUE \n  371     2 \n\n\nCuriously, there are two instances where the play is recorded as a fielder’s choice but the string “out” appears. Investigating further, in one instance the string “out” appears as part of a player’s name while in the other, the play actually did result in an out.\n\nwhich(grepl(\"out\", bip$des) & bip$end_events == \"fielders_choice\")\n\n[1] 10235 65138\n\nbip$des[which(grepl(\"out\", bip$des) & bip$end_events == \"fielders_choice\")]\n\n[1] \"Mike Trout reaches on a fielder's choice, fielded by shortstop David Hamilton. Anthony Rendon to 3rd. Nolan Schanuel to 2nd. Throwing error by shortstop David Hamilton.\"        \n[2] \"Ryan Jeffers reaches on a fielder's choice, fielded by second baseman Colt Keith. Byron Buxton scores. Ryan Jeffers out at 2nd, catcher Jake Rogers to first baseman Mark Canha.\"\n\n\nWe will manually change the value of end_events in row 65138 to “fielders_choice_out”. For fitting our out probability model, we will focus only on those at-bats that did not end with a home run. We will also drop at-bats for which the batted ball locations are missing.\n\nbip$end_events[65138] &lt;- \"fielders_choice_out\"\nout_events &lt;- \n  c(\"double_play\", \"field_out\", \"fielders_choice_out\",\n    \"force_out\", \"grounded_into_double_play\", \n    \"sac_bunt\", \"sac_fly\", \"sac_fly_double_play\",\n    \"triple_play\")\nbip &lt;-\n  bip |&gt;\n  dplyr::filter(end_events != \"home_run\" & !is.na(x) & !is.na(y)) |&gt;\n  dplyr::mutate(Out = ifelse(end_events %in% out_events, 1, 0)) \n\n\nBinning and Averaging\nOne potential approach works by dividing the field into small rectangular regions and computing the proportion of balls landing within each region that result in an out. To this end, we will first create a grid of 3ft x 3ft squares that covers the range of our batted ball locations.\n\nrange(bip$x)\n\n[1] -287.725  290.575\n\nrange(bip$y)\n\n[1] -91.85 459.35\n\n\n\n1grid_sep &lt;- 3\n2x_grid &lt;- seq(from = -300, to = 300, by = grid_sep)\ny_grid &lt;- seq(from = -100, to = 500, by = grid_sep)\n3raw_grid &lt;- expand.grid(x = x_grid, y = y_grid)\n\n\n1\n\nSeparation between grid points in each dimension.\n\n2\n\nCreates a sequence of evenly-spaced points\n\n3\n\nCreates a table with every combination of values in x_grid and y_grid\n\n\n\n\nThe table grid contains many combinations of x and y that do not represent plausible batted ball locations. In the figure below, we plot all batted ball locations and overlay all the grid points.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(bip$x, bip$y, \n     xlim = c(-300, 300), ylim = c(-100, 500),\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.2))\npoints(raw_grid$x, raw_grid$y, pch = 16, cex = 0.2, \n       col = adjustcolor(oi_colors[3], alpha.f = 0.3))\n\n\n\n\n\n\n\nFigure 4: The initial grid of points contains many locations at which we would not expect balls to be fielded.\n\n\n\n\n\nWe will remove those (x,y) pairs in grid for which either\n\nx + y &lt; -100: these are locations far below the third base line\ny - x &lt; -100: these are locations far below the first base line\nsqrt(x^2 +y^2 &lt; 580): these are locations very far from home plate\n\n\ngrid &lt;-\n  raw_grid |&gt;\n  dplyr::filter(y + x &gt; -100 & y - x &gt; -100 & sqrt(x^2 + y^2) &lt; 580)\nplot(bip$x, bip$y, \n     xlim = c(-300, 300), ylim = c(-100, 500),\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.2))\npoints(grid$x, grid$y, pch = 16, cex = 0.2, \n       col = adjustcolor(oi_colors[3], alpha.f = 0.3))\n\n\n\n\n\n\n\nFigure 5: A restricted grid of spatial locations.\n\n\n\n\n\nWe can use the cut() function to divide the x and y values in bip into small bins. Here is a quick example of how cut works: we first create a vector with a few numbers. The breaks argument of cut tells it the end points of each bin.\n\nx &lt;- c(0.11, 0.23, 0.45, 0.67, 0.99, 0.02) \ncut(x, breaks = seq(0, 1, by = 0.1))  \n\n[1] (0.1,0.2] (0.2,0.3] (0.4,0.5] (0.6,0.7] (0.9,1]   (0,0.1]  \n10 Levels: (0,0.1] (0.1,0.2] (0.2,0.3] (0.3,0.4] (0.4,0.5] ... (0.9,1]\n\n\nBelow, we first cut the values of x and y that appear in bip. Then, using a grouped summary, we can compute the number of balls hit to each bin and the proportion of those balls that result in an out.\n\nbin_probs &lt;-\n  bip |&gt;\n  dplyr::select(x, y, Out) |&gt;\n  dplyr::mutate(\n1    x_bin = cut(x, breaks = seq(-300-grid_sep/2, 300+grid_sep/2, by = grid_sep)),\n    y_bin = cut(y, breaks = seq(-100 - grid_sep/2, 500+grid_sep/2, by = grid_sep))) |&gt;\n  dplyr::group_by(x_bin, y_bin) |&gt;\n  dplyr::summarise(\n    out_prob = mean(Out), \n    n_balls = dplyr::n(),\n    .groups = \"drop\")\n\n\n1\n\nOffsetting by grid_sep/2 ensures that the x and y values in grid are the centers of the squares in our grid (and not one of the corners)\n\n\n\n\nWe can now bin the x and y values contained in grid and, using a left_join(), append a column that contains the estimated probability of an out hit to each 3ft x 3ft square in our grid. Note that if there is a combination of x_bin and y_bin in grid that is not contained in bin_prob, the estimated probability will be NA. These are grid squares in which no balls were hit.\n\ngrid_probs_bin &lt;-\n  grid |&gt;\n  dplyr::mutate(\n    x_bin = cut(x, breaks = seq(-300-grid_sep/2, 300+grid_sep/2, by = 3)),\n    y_bin = cut(y, breaks = seq(-100-grid_sep/2, 500+grid_sep/2, by = 3))) |&gt;\n  dplyr::left_join(y = bin_probs, by = c(\"x_bin\", \"y_bin\"))\n\nWe can now make a heatmap of our estimated probabilities by looping over the rows in grid_prob_bins, drawing a small rectangle with corners (x-1.5, y-1.5), (x + 1.5, y-1.5), (x+1.5, y+1.5), and (x-1.5, y+1.5), and shading the rectangle based on the corresponding out probability.\n\n\n1col_list &lt;- colorBlindness::Blue2DarkRed18Steps\n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\n2plot(1, type = \"n\",\n3     xlim = c(-300, 300), ylim = c(-100, 500),\n4     xaxt = \"n\", yaxt = \"n\", bty = \"n\",\n5     main = \"Naive out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n6  rect(xleft = grid_probs_bin$x[i] - grid_sep/2,\n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n       col = ifelse(\n         is.na(grid_probs_bin$out_prob[i]), \n7         adjustcolor(oi_colors[1], alpha.f = 0.2),\n8         adjustcolor(rgb(colorRamp(col_list,bias=1)(grid_probs_bin$out_prob[i])/255),\n                     alpha.f = 0.5)),\n9       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nThis is a popular diverging color palette, which ranges from blue to red, that is color-blind friendly. See here for more details.\n\n2\n\nSpecifying type = \"n\" tells R to make an empty plot\n\n3\n\nManually set the horizontal and vertical limits of the plotting area\n\n4\n\nxaxt = \"n\" and yaxt = \"n\" suppress the axis lines and bty = \"n\" suppresses the bounding box\n\n5\n\nmain controls the title of plot while setting xlab = \"\" and ylab = \"\" suppresses horizontal and vertical axis labels\n\n6\n\nTo use rect(), we must pass the coordinates of the bottom left and top right corners of the rectangle\n\n7\n\nIf there were no balls hit to a particular rectangle, we will color it gray. Otherwise, we will color it according to the fitted probability\n\n8\n\nThe expression rgb(colorRamp(col_list, bias=1)(grid_probs_bin$out_prob)/255) maps the fitted probability to a color in col_list using linear interpolation. Here 0% maps to dark blue and 100% maps to dark red.\n\n9\n\nSuppresses the border of the rectangle\n\n\n\n\n\n\n\n\n\n\nFigure 6: Empirical out probabilities based on binning balls in play.\n\n\n\n\n\n\n\nA Logistic Regression Model for Outs\nOur naively estimated out probabilities leave much to be desired. For one thing, there are lots of “gaps” in the fitted surface where our initial model can’t make a prediction. These are the cells in our grid into which no balls were hit. Of much greater concern are the sharp discontinuities evident in the figure. In fact, there are many regions where the fitted out probability jumps from 0% to 100% to 0% in the span of about 6 feet. Such discontinuities are artifacts of the small sample sizes within some of the bins. Indeed, we find that about 85% of the grid cells contain 10 or fewer observations\n\nmean(bin_probs$n_balls &lt;= 10)\n\n[1] 0.8541364\n\n\nLike we did to predict XG using shot distance, we can overcome this challenge by building a statistical model. In particular, we would like to model the log-odds of an out as a function of the batted balls location. That is, \\[\n\\log\\left(\\frac{\\mathbb{P}(\\textrm{out})}{1 - \\mathbb{P}(\\textrm{out})} \\right) = s(x,y),\n\\] where \\(s\\) is a smooth function of the batted balls location. What form should \\(s\\) take?\nOne possibility is to assume that there are parameters \\(\\beta_{0}, \\beta_{x},\\) and \\(\\beta_{y}\\) such that \\[\n\\log\\left(\\frac{\\mathbb{P}(\\textrm{out})}{1 - \\mathbb{P}(\\textrm{out})} \\right) = \\beta_{0} + \\beta_{x}x + \\beta_{y}y\n\\] We can estimate these parameters using glm() and then compute the fitted probability for every cell in our grid.\n\nlogit_fit &lt;-\n  glm(Out ~ x + y,\n      family = binomial(link = \"logit\"), data = bip)\ngrid_logit_preds &lt;-\n  predict(object = logit_fit, \n1          newdata = grid, type = \"response\")\ngrid_logit_cols &lt;- \n2  rgb(colorRamp(col_list,bias=1)(grid_logit_preds)/255)\n\n\n1\n\nTo get fitted values on the probability scale, we need to specify type = \"response\"\n\n2\n\nCreates a vector mapping the fitted probability at each grid location to a color between the two extremes\n\n\n\n\nThe fitted logistic regression model predicts that the probability of an out decreases as one moves vertically away from home plate. The model further predicts that balls hit into the outfield tend not to result in outs, which is at odds with what we see in the data. Indeed, our earlier plot revealed pockets of high out probability in the outfield, roughly corresponding to the usual positioning of the left, right, and center fielders.\n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", \n     xlim = c(-300, 300), ylim = c(-100, 500), \n     xaxt = \"n\", yaxt = \"n\", bty = \"n\", \n     main = \"Logistic regression out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n  rect(xleft = grid_probs_bin$x[i] - grid_sep/2, \n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n1       col = adjustcolor(grid_logit_cols[i], alpha.f = 0.5),\n       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nUnlike with binning-and-averaging, our parametric model is able to make predictions at grid cells not present in the training data. So, we don’t need to check whether the predicted probability is NA\n\n\n\n\n\n\n\n\n\n\nFigure 7: Logistic regression forecasts of out probabilities as a function of location.",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-gam",
    "href": "lectures/lecture08.html#sec-gam",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\nThe root cause of the conflict between our data and the logistic regression model forecasts in Figure 7 is the model’s strong — and totally unrealistic — assumption that the log-odds of an out is linear in both \\(x\\) and \\(y.\\) A more accurate model would allow the log-odds to vary non-linearly.\nGeneralized Additive Models (or GAMs) are an elegant way to introduce non-linearity and create spatially smooth maps. In our context, a GAM model expresses \\(s(x,y)\\) as a linear combination of a large number of two-dimensional splines functions. Each spline function is a piecewise polynomial that is localized to a small region of \\((x,y)\\) space, meaning that it is non-zero inside the region and zero outside the region. Such spline functions can be linearly combined to approximate really complicated functions arbitrarily well2\nWe can fit GAMs in R using the mgcv package, which can be installed using\n\ninstall.packages(\"mgcv\")\n\nThe package provides two fitting functions, gam() and bam(), which is recommended for very large data sets like bip. Both functions are similar lm() and glm() in that they require users to specify a formula and, in the case of non-continuous outcomes, a link function. The main difference is the term s(), which is used to signal to gam() or bam() that we want to smooth over whatever variables appear inside of the brackets of s(). In our case, because we wish to smooth over both x and y, we will include the terms s(x,y)\n\n\n\n\n\n\nWarning\n\n\n\nFitting our out probability model takes a few minutes\n\n\n\nlibrary(mgcv)\ngam_fit &lt;-\n  bam(formula = Out ~ s(x,y), \n      family = binomial(link=\"logit\"), data = bip)\n\nAfter fitting our GAM, we can visualize the estimated out probabilities:\n\ngrid_gam_preds &lt;- \n  predict(object = gam_fit,\n          newdata = grid, type = \"response\")\n\ngrid_gam_cols &lt;-   \n  rgb(colorRamp(col_list,bias=1)(grid_gam_preds)/255) \n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", \n     xlim = c(-300, 300), ylim = c(-100, 500), \n     xaxt = \"n\", yaxt = \"n\", bty = \"n\", \n     main = \"GAM out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n  rect(xleft = grid_probs_bin$x[i] - grid_sep/2, \n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n       col = adjustcolor(grid_gam_cols[i], alpha.f = 0.5),\n       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nThe out probabilities fitted by the GAM appear much more reasonable: balls hit within the in-field and close to the outfielders tend to be result in outs while balls hit in the gap between the infield and outfield tend not to result in outs.\n\nDividing Defensive Run Value\nRecall that if at-bat \\(i\\) results in a ball put in play, we will distribute \\(\\delta_{i}^{(f)} = -\\delta_{i} \\times \\hat{p}_{i}\\) to the fielders and assign \\(\\delta_{i}^{(p)} = -\\delta \\times (1 - \\hat{p}_{i})\\) to the pitcher. The following code adds a column to def_atbat2024 containing the predicted out probabilities from our fitted GAM. Note that this column contains NA values for those at-bats that did not result in a ball in play (e.g., they ended with a walk or a strikeout or a home run) For the vast majority of these at-bats, we will manually set \\(\\hat{p}_{i} = 0\\) so that the pitcher gets all the credit (i.e., \\(\\delta^{(p)}_{i} = -\\delta_{i}\\)).\n\nall_preds &lt;-\n  predict(object = gam_fit, \n1          newdata = def_atbat2024,\n          type = \"response\")\n2def_atbat2024$p_out &lt;- all_preds\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    p_out = dplyr::case_when(\n3      is.na(p_out) & end_events == \"home_run\" ~ 0,\n4      is.na(p_out) & end_type != \"X\" ~ 0,\n      .default = p_out))\n\n\n1\n\nWhen x and y are NA (i.e., whenever the at-bat doesn’t end with a ball in play), predict will return NA.\n\n2\n\nAdds fitted out probabilities to the data table\n\n3\n\nFor home runs, manually set p_out = 0 (so pitcher gets all credit/blame)\n\n4\n\nFor at-bats that end with a ball or strike, set p_out = 0 (so pitcher gets all credit/blame)\n\n\n\n\nEven after manually setting \\(\\hat{p}_{i} = 0\\) on the at-bats that didn’t end with the ball in play, there are still some NA values in the column p_out. On further inspection, it looks like these were at-bats in which the ball was put in play but are missing hit locations.\n\ntable(def_atbat2024$end_events[is.na(def_atbat2024$p_out)])\n\n\n                   double                 field_out grounded_into_double_play \n                        2                         5                         1 \n                   single                    triple \n                        4                         1 \n\nsum(is.na(def_atbat2024$x[is.na(def_atbat2024$p_out)]))\n\n[1] 13\n\n\nWe will remove these rows from our calculations\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::filter(!is.na(p_out))\n\nWe can now finally compute \\(\\delta^{(f)}_{i}\\) and \\(\\delta^{(p)}_{i}\\) for each at-bat.\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    delta_p = -1 * (1 - p_out) * RunValue,\n    delta_f = -1 * p_out * RunValue)",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-fielding",
    "href": "lectures/lecture08.html#sec-fielding",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Fielding Run Values",
    "text": "Fielding Run Values\nIf a ball is hit towards deep left field and results in the fielding team creating a large \\(-\\delta\\), how much blame should the third baseman, who is on the opposite side of the park receive? Following Baumer, Jensen, and Matthews (2015), we will apportion \\(\\delta_{i}^{(f)},\\) the portion of the run value \\(-\\delta_{i}\\) attributable to the fielding in at-bat \\(i\\), based on each fielder’s responsibility for making an out on the batted ball. Specifically, we will assign \\(w_{i,\\ell} \\times \\delta_{i}^{(f)}\\) to the fielder playing position \\(\\ell \\in \\{1, 2, \\ldots, 9\\}\\) during the at-bat where \\[\nw_{i,\\ell} = \\frac{\\hat{p}_{i,\\ell}}{\\hat{p}_{i,1} + \\cdots + \\hat{p}_{i,9}}\n\\] and \\(\\hat{p}_{i,\\ell}\\) is the probability that fielder \\(\\ell\\) makes the out given the location of the batted ball.\nTo estimate the \\(\\hat{p}_{i,\\ell}\\)’s, we will fit 9 separate GAMs, one for each fielding position. For fielding position \\(\\ell,\\) we will create a new variable that is equal to 1 if the ball in play resulted in an out was initially fielded by the player at position \\(\\ell.\\) Unlike our overall out probability model, which we fit using data from all batted balls, each position-specific GAM will be fitted using data from the subset of batted balls that are reasonably close to the typical position location. For instance, when fitting the model for the first basemen, we won’t include data from balls hit into deep left field. We compute the coordinates of the typical position by taking the average value of the x and y coordinates of all batted balls fielded by players at position \\(\\ell.\\) We fit each position-specific GAM using batted balls hit within 150 ft of this typical location3\n\n\n\n\n\n\nWarning\n\n\n\nThis code takes between several minutes to run\n\n\n\n1mid_x &lt;- rep(NA, times = 9)\nmid_y &lt;- rep(NA, times = 9)\n2fit_list &lt;- list()\nfit_time &lt;- rep(NA, times = 9)\nfor(l in 1:9){\n  print(paste0(\"Starting l = \", Sys.time()))\n  \n3  mid_x[l] &lt;- mean(bip$x[bip$end_hit_location == l], na.rm = TRUE)\n  mid_y[l] &lt;- mean(bip$y[bip$end_hit_location==l], na.rm = TRUE)\n  \n  train_df &lt;-\n    bip |&gt;\n4    dplyr::mutate(newOut = ifelse(Out == 1 & end_hit_location == l, 1, 0)) |&gt;\n    dplyr::filter(!is.na(x) & !is.na(y)) |&gt;\n5    dplyr::filter( sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150) |&gt;\n    dplyr::select(x,y,newOut) \n\n6  fit_time[l] &lt;-\n    system.time(\n7      fit_list[[l]] &lt;-\n        bam(formula = newOut ~ s(x, y),\n8            family = binomial(link=\"logit\"),data = train_df))[\"elapsed\"]\n  tmp_df &lt;-\n    def_atbat2024 |&gt; \n    dplyr::mutate(\n9      x = ifelse(sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150, x, NA),\n      y = ifelse(sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150, y, NA)\n    )\n    \n10  preds &lt;- predict(object = fit_list[[l]], newdata = tmp_df, type = \"response\")\n11  def_atbat2024[[paste0(\"p_out_\", l)]] &lt;- preds\n}\n12save(fit_list, mid_x, mid_y, file = \"position_gam_fits.RData\")\n\n\n1\n\nVectors containing coordinates for the typical fielder location\n\n2\n\nHolds all fitted objects, so that we can use them later to visualize position-specific out probabilities\n\n3\n\nCompute typical coordinates for position l\n\n4\n\nCreate position-specific outcome variable\n\n5\n\nSubsets to batted balls within 150 ft of the typical location\n\n6\n\nIt’s often helpful to track how long it takes to fit a model. system.time() can return, among other things, the elapsed time. We save the time for fitting each position’s model in the vector fit_time\n\n7\n\nInstead of creating a different object for each position’s fitted model, we will store all the fitted models in a single list, which can be saved.\n\n8\n\nsystem.time() returns three different time measures. The most pertinent one is the actual elapsed time, stored in the slot “elapsed”.\n\n9\n\nWe only want model predictions for balls hit within 150ft of the typical location. Passing NA x and y values to predict() is a hack to suppress model predictions for balls hit outside this radius. We will later over-write the resulting NA’s with 0’s.\n\n10\n\nfit_list[[l]] is the fitted object return by bam for position l.\n\n11\n\nCreates a column in def_atbat2024 holding the\n\n12\n\nSave’s the list containing all position-specific fits\n\n\n\n\n[1] \"Starting l = 2025-09-13 14:18:24.457871\"\n[1] \"Starting l = 2025-09-13 14:19:40.253699\"\n[1] \"Starting l = 2025-09-13 14:20:48.918107\"\n[1] \"Starting l = 2025-09-13 14:22:15.198928\"\n[1] \"Starting l = 2025-09-13 14:23:42.273536\"\n[1] \"Starting l = 2025-09-13 14:25:04.266525\"\n[1] \"Starting l = 2025-09-13 14:26:25.132992\"\n[1] \"Starting l = 2025-09-13 14:27:06.43358\"\n[1] \"Starting l = 2025-09-13 14:27:46.948843\"\n\n\nThe columns p_out_1, …, p_out_9 in def_atbat2024 contain several NA values. Some of these correspond to at-bats that did not result in ball being put in play (i.e., those with end_type = S or end_type = B) while others correspond to at-bats in which the ball was hit far away from the typical location of the fielder. For these latter at-bats, we will manually set the position-specific out probabilities as follows: 1. For home runs and balls hit outside the 150ft radius for each position, we set p_out_* = 0 2. For at-bats ending with a strike or ball, we keep p_out_* = NA. 3. For balls in play with missing x and y coordinates, we set p_out_* = NA\n\n\nCode\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    p_out_1 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[1])^2 + (y - mid_y[1])^2) &gt;= 150 ~ 0,\n      .default = p_out_1),\n    p_out_2 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[2])^2 + (y - mid_y[2])^2) &gt;= 150 ~ 0,\n      .default = p_out_2),\n    p_out_3 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[3])^2 + (y - mid_y[3])^2) &gt;= 150 ~ 0,\n      .default = p_out_3),\n    p_out_4 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[4])^2 + (y - mid_y[4])^2) &gt;= 150 ~ 0,\n      .default = p_out_4),\n    p_out_5 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[5])^2 + (y - mid_y[5])^2) &gt;= 150 ~ 0,\n      .default = p_out_5),\n    p_out_6 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[6])^2 + (y - mid_y[6])^2) &gt;= 150 ~ 0,\n      .default = p_out_6),\n    p_out_7 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[7])^2 + (y - mid_y[7])^2) &gt;= 150 ~ 0,\n      .default = p_out_7),\n    p_out_8 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[8])^2 + (y - mid_y[8])^2) &gt;= 150 ~ 0,\n      .default = p_out_8),\n    p_out_9 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[9])^2 + (y - mid_y[9])^2) &gt;= 150 ~ 0,\n      .default = p_out_9))\n\n\nWe now normalize the \\(\\hat{p}_{i,\\ell}\\) values to get the weights \\(w_{i,\\ell}\\) measuring the responsibility of each fielder for making an out in the at-bat.\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    total_weight = p_out_1 + p_out_2 + p_out_3 +\n      p_out_4 + p_out_5 + p_out_6 +\n      p_out_7 + p_out_8 + p_out_9,\n    w1 = p_out_1/total_weight,\n    w2 = p_out_2/total_weight,\n    w3 = p_out_3/total_weight,\n    w4 = p_out_4/total_weight,\n    w5 = p_out_5/total_weight,\n    w6 = p_out_6/total_weight,\n    w7 = p_out_7/total_weight,\n    w8 = p_out_8/total_weight,\n    w9 = p_out_9/total_weight)\n\nNow, for each fielding position, we can compute \\(w_{i,\\ell} \\times \\delta^{(f)}_{i}\\) and aggregate these values across players at that position to compute \\(\\textrm{RAA}_{\\ell}^{(f)},\\) which quantifies the total run value created by the player through their fielding at position \\(\\ell.\\) Negative values of \\(\\textrm{RAA}_{\\ell}^{(f)}\\) suggest that the player’s fielding at position \\(\\ell\\) netted positive run value for the batting team, thereby negatively impacthing his own team.\nHere is the calculation for first basemen (fielding position number \\(\\ell = 3\\)). Taking a looking at the largest \\(\\textrm{RAA}_{\\ell}^{f}\\) values, we see several recent Golden Glove winners who are known for their fielding prowess (Santana, Walker, Goldschmidt, Olson, Guerrero).\n\nraa_f3 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f3 = delta_f * w3) |&gt;\n  dplyr::group_by(fielder_3) |&gt;\n  dplyr::summarize(RAA_f3 = sum(RAA_f3, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_3)\n\nraa_f3 |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, RAA_f3) |&gt;\n  dplyr::arrange(dplyr::desc(RAA_f3)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   Name              RAA_f3\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 Carlos Santana      55.7\n 2 Christian Walker    54.6\n 3 Paul Goldschmidt    52.4\n 4 Bryce Harper        51.1\n 5 Matt Olson          51.0\n 6 Ryan Mountcastle    48.7\n 7 Vladimir Guerrero   47.1\n 8 Michael Toglia      46.5\n 9 Freddie Freeman     46.2\n10 Josh Naylor         46.2\n\n\nThe following code repeats this calculation for all fielding positions.\n\n\nShow the code computes the fielding RAA for all positions.\nraa_f1 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f1 = delta_f * w1) |&gt;\n  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarize(RAA_f1 = sum(RAA_f1, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = pitcher) |&gt;\n  dplyr::select(key_mlbam, RAA_f1)\n\nraa_f2 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f2 = delta_f * w2) |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarize(RAA_f2 = sum(RAA_f2, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_2) |&gt;\n  dplyr::select(key_mlbam, RAA_f2)\n\nraa_f3 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f3 = delta_f * w3) |&gt;\n  dplyr::group_by(fielder_3) |&gt;\n  dplyr::summarize(RAA_f3 = sum(RAA_f3, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_3) |&gt;\n  dplyr::select(key_mlbam, RAA_f3)\n\nraa_f4 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f4 = delta_f * w4) |&gt;\n  dplyr::group_by(fielder_4) |&gt;\n  dplyr::summarize(RAA_f4 = sum(RAA_f4, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_4) |&gt;\n  dplyr::select(key_mlbam, RAA_f4)\n\nraa_f5 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f5 = delta_f * w5) |&gt;\n  dplyr::group_by(fielder_5) |&gt;\n  dplyr::summarize(RAA_f5 = sum(RAA_f5, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_5) |&gt;\n  dplyr::select(key_mlbam, RAA_f5)\n\nraa_f6 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f6 = delta_f * w6) |&gt;\n  dplyr::group_by(fielder_6) |&gt;\n  dplyr::summarize(RAA_f6 = sum(RAA_f6, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_6) |&gt;\n  dplyr::select(key_mlbam, RAA_f6)\n\nraa_f7 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f7 = delta_f * w7) |&gt;\n  dplyr::group_by(fielder_7) |&gt;\n  dplyr::summarize(RAA_f7 = sum(RAA_f7, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_7) |&gt;\n  dplyr::select(key_mlbam, RAA_f7)\n\nraa_f8 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f8 = delta_f * w8) |&gt;\n  dplyr::group_by(fielder_8) |&gt;\n  dplyr::summarize(RAA_f8 = sum(RAA_f8, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_8) |&gt;\n  dplyr::select(key_mlbam, RAA_f8)\n\nraa_f9 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f9 = delta_f * w9) |&gt;\n  dplyr::group_by(fielder_9) |&gt;\n  dplyr::summarize(RAA_f9 = sum(RAA_f9, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_9) |&gt;\n  dplyr::select(key_mlbam, RAA_f9)\n\n\nLike we did when computing \\(\\textrm{RAA}^{(br)}\\) in Lecture 7, we will concatenate raa_f1, …, raa_f9 and sum every players total contributions across all fielding positions.\n\nraa_f &lt;-\n  raa_f1 |&gt;\n  dplyr::full_join(y = raa_f2, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f3, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f4, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f5, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f6, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f7, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f8, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f9, by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(\n    list(RAA_f1=0, RAA_f2=0, RAA_f3=0, RAA_f4=0, \n         RAA_f5=0, RAA_f6 = 0, RAA_f7=0, RAA_f8 = 0, RAA_f9=0)) |&gt;\n  dplyr::mutate(\n    RAA_f = RAA_f1 + RAA_f2 + RAA_f3 + RAA_f4 + \n      RAA_f5 + RAA_f6 + RAA_f7 + RAA_f8 + RAA_f9) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_f, RAA_f1, RAA_f2, RAA_f3, RAA_f4, RAA_f5,\n                RAA_f6, RAA_f7, RAA_f8, RAA_f9)",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-pitching",
    "href": "lectures/lecture08.html#sec-pitching",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Pitching Run Values",
    "text": "Pitching Run Values\nRecall that \\(\\delta_{i}^{(p)}\\) is the amount of run value created by the pitcher in at-bat \\(i.\\) Summing this value across all at-bats involving each pitcher, we obtain each pitchers \\(\\textrm{RAA}^{(p)}.\\)\n\nraa_p &lt;-\n  def_atbat2024 |&gt;\n  dplyr::select(pitcher, delta_p) |&gt;\n  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarise(RAA_p = sum(delta_p, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = pitcher) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_p)\n\nWe see that the two 2024 Cy Young Award winners4, Tarik Skubal and Chris Sale, have the highest \\(\\textrm{RAA}^{(p)}\\) values.\n\nraa_p |&gt;\n  dplyr::arrange(dplyr::desc(RAA_p)) |&gt;\n  dplyr::slice_head(n=10)\n\n# A tibble: 10 × 3\n   Name            key_mlbam RAA_p\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt;\n 1 Tarik Skubal       669373 14.7 \n 2 Chris Sale         519242 14.6 \n 3 Ryan Walker        676254 13.5 \n 4 Cade Smith         671922 13.3 \n 5 Paul Skenes        694973 12.3 \n 6 Emmanuel Clase     661403 11.2 \n 7 Kirby Yates        489446  9.99\n 8 Griffin Jax        643377  9.53\n 9 Edwin Uceta        670955  9.23\n10 Garrett Crochet    676979  9.13\n\n\nWe will save both raa_f and raa_p for later use\n\nsave(raa_f, raa_p, file = \"raa_defensive2024.RData\")",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#sec-war",
    "href": "lectures/lecture08.html#sec-war",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Replacement Level",
    "text": "Replacement Level\nWe now combine the fielding and pitching RAA values with the baserunning and batting RAA values we computed in Lecture 7 into a single table.\n\nload(\"raa_offensive2024.RData\")\n\nraa &lt;-\n  raa_b |&gt;\n  dplyr::select(-Name) |&gt;\n  dplyr::full_join(y = raa_br |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_p |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(list(RAA_b = 0, RAA_br = 0, RAA_f = 0, RAA_p = 0)) |&gt;\n  dplyr::mutate(RAA = RAA_b + RAA_br + RAA_f + RAA_p) |&gt;\n  dplyr::left_join(y = player2024_lookup |&gt; dplyr::select(key_mlbam, Name), by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, RAA_b, RAA_br, RAA_f, RAA_p)\n\n\\(RAA\\) is a comprehensive measure of a player’s performance that accounts not only for the actual runs scored (or given up) due to his contributions but also the changes in the expected runs that can be attributed to his play. We constructed \\(RAA\\) so that larger numbers indicate better performance. While the absolute \\(RAA\\) values are useful on their own, they become even more useful — and easier to interpret — when calibrated to measure performance relative to a baseline player. As argued by Baumer, Jensen, and Matthews (2015), although comparing an individual players \\(RAA\\) to the league-average value is intuitive, average players are themselves quite valuable. Further, it is not reasonable to expect a team to be able to replace any player with a league average one. Thus, it is more useful to compare each player’s performance relative to a replacement-level player.\nAs discussed in class (and also in (Baumer, Jensen, and Matthews 2015, sec. 3.8)), existing definitions of replacement level are fairly arbitrary. For instance, back in 2010, FanGraphs asserted that a team of players making the minimum MLB salary would win 29.7% of its games (so between 48 and 49 games). Similar definitions have been adopted by other sites like Baseball Propsectus and Baseball Reference. Unfortunately, there is little empirical justification for this number.\nWe will instead use Baumer, Jensen, and Matthews (2015)’s roster-based definition of replacement level, which is motivated and computed as follows:\n\nEach of the 30 major league teams typically carries 25 players, 13 of whom are position players and 12 of whom are pitchers\nOn any given day, there are generally \\(12 \\times 30 = 360\\) pitchers and \\(13 \\times 30 = 390\\) active players.\nSo, we will treat the 390 position players with the most plate appearances and the 360 pitchers who faced the most batters as non-replacement level and everyone else as replacement-level.\n\nRemember that our table def_atbat2024 contains information from all available at-bats in the 2024 regular season. We can use the data in this table to count the number of plate appearances for each non-pitcher and number of batters faced by each pitcher. To this end, we first create lists of the MLB Advanced Media IDs of all pitchers and all non-pitchers who appear in our dataset.\n\n1all_players &lt;- unique(\n  c(def_atbat2024$batter, def_atbat2024$pitcher, def_atbat2024$fielder_2,\n    def_atbat2024$fielder_3, def_atbat2024$fielder_4, def_atbat2024$fielder_5,\n    def_atbat2024$fielder_6, def_atbat2024$fielder_7, def_atbat2024$fielder_8, def_atbat2024$fielder_9))\n2pitchers &lt;- unique(def_atbat2024$pitcher)\n\n3position_players &lt;- all_players[!all_players %in% pitchers]\n\n\n1\n\nGet the ID for all players\n\n2\n\nGet the ID for all pitchers\n\n3\n\nPull out the ID for all non-pitchers\n\n\n\n\nUsing grouped summaries, we can count the number of at-bats faced by each position player as a batter and by each pitcher. Since the overwhelming majority of at-bats involved only one batter, these counts effectively tell us the number of batters faced by each pitcher.\n\nposition_pa &lt;-\n  def_atbat2024 |&gt;\n1  dplyr::filter(batter %in% position_players) |&gt;\n  dplyr::group_by(batter) |&gt;\n2  dplyr::summarise(n = dplyr::n()) |&gt;\n3  dplyr::arrange(dplyr::desc(n)) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\npitcher_pa &lt;-\n  def_atbat2024 |&gt;\n4  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarise(n = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(n)) |&gt;\n5  dplyr::rename(key_mlbam = pitcher)\n\n\nrepl_position_players &lt;- position_pa$key_mlbam[-(1:390)]\nrepl_pitchers &lt;- pitcher_pa$key_mlbam[-(1:360)]\n\ncat(\"Cut-off for position players:\", position_pa$n[390], \"\\n\")\ncat(\"Cut-off for pitchers:\", pitcher_pa$n[360], \"\\n\")\n\n\n1\n\nThis removes at-bats in which a pitcher is hitting\n\n2\n\nCounts the number of at-bats in which each position player is batting\n\n3\n\nArranges the counts in decreasing order so that we can determine replacement-level cut-offs\n\n4\n\nSince the vector pitchers is just the unique values of def_atbat2024$pitcher, there is no need to filter\n\n5\n\nGet the IDs of the position players and pitchers outside, respectively, the top 390 and 360 numbers of plate appearances\n\n\n\n\nCut-off for position players: 131 \nCut-off for pitchers: 204 \n\n\nUltimately, we wish to compare each player’s \\(RAA\\) to the \\(RAA\\) that would have been created had the player been replaced by a replacement-level player. To estimate this counter-factual \\(RAA\\), we will divide the total \\(RAA\\) produced by all replacement-level players by the total number of plate appearances.\n\nrepl_position_raa &lt;-\n  raa |&gt;\n  dplyr::filter(key_mlbam %in% repl_position_players) |&gt;\n1  dplyr::inner_join(y = position_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n)\n\nrepl_pitch_raa &lt;-\n  raa |&gt;\n  dplyr::filter(key_mlbam %in% repl_pitchers) |&gt;\n  dplyr::inner_join(y = pitcher_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n) \n\nrepl_avg_pos &lt;- sum(repl_position_raa$RAA)/sum(repl_position_raa$n)\nrepl_avg_pitch &lt;- sum(repl_pitch_raa$RAA)/sum(repl_pitch_raa$n)\n\n\n1\n\nUsing an inner join ensures that we only append the n values for replacement-level players\n\n\n\n\nWe see that the replacement-level RAA per-at-bat is 0 for position players and -0.02 for pitchers. Multiplying the the replacement-level per-at-bat RAA values by the number of plate appearances faced by each non-replacement-level player gives us an estimate of how each player’s replacement-level “shadow” would perform. Finally, using the heuristic of 10 runs per win, dividing the difference between the actual RAA and the RAA for the replacement-level shadow yields our wins above replacement for position players.\n\nposition_war &lt;-\n  raa |&gt;\n  dplyr::filter(!key_mlbam %in% repl_position_players) |&gt;\n  dplyr::inner_join(y = position_pa, by = \"key_mlbam\") |&gt; \n  dplyr::select(Name, key_mlbam, RAA, n) |&gt;\n  dplyr::mutate(shadowRAA = n * repl_avg_pos) |&gt;\n  dplyr::mutate(WAR = (RAA - shadowRAA)/10)\n\nposition_war |&gt;\n  dplyr::arrange(dplyr::desc(WAR)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 6\n   Name              key_mlbam   RAA     n shadowRAA   WAR\n   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Bobby Witt           677951 165.    694     -1.36 16.6 \n 2 Gunnar Henderson     683002 117.    702     -1.37 11.8 \n 3 Elly De La Cruz      682829 114.    679     -1.33 11.5 \n 4 Jose Ramirez         608070 113.    657     -1.28 11.5 \n 5 Zach Neto            687263 110.    590     -1.15 11.1 \n 6 Marcus Semien        543760 108.    701     -1.37 10.9 \n 7 Ketel Marte          606466 101.    562     -1.10 10.2 \n 8 Vladimir Guerrero    665489  99.0   671     -1.31 10.0 \n 9 Francisco Lindor     596019  98.5   689     -1.35  9.98\n10 Jose Altuve          514888  98.2   661     -1.29  9.95\n\n\nWe can run a similar calculation for pitchers.\n\npitch_war &lt;-\n  raa |&gt;\n  dplyr::filter(!key_mlbam %in% repl_pitchers) |&gt;\n  dplyr::inner_join(y = pitcher_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n) |&gt;\n  dplyr::mutate(shadowRAA = n * repl_avg_pitch) |&gt;\n  dplyr::mutate(WAR = (RAA - shadowRAA)/10)\n\npitch_war |&gt;\n  dplyr::arrange(dplyr::desc(WAR)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 6\n   Name            key_mlbam   RAA     n shadowRAA   WAR\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Tarik Skubal       669373 17.2    730    -17.7   3.49\n 2 Chris Sale         519242 17.2    702    -17.0   3.42\n 3 Zack Wheeler       554430 10.1    759    -18.4   2.86\n 4 Paul Skenes        694973 14.6    495    -12.0   2.66\n 5 Garrett Crochet    676979  9.91   596    -14.5   2.44\n 6 Bryce Miller       682243  7.66   681    -16.5   2.42\n 7 Seth Lugo          607625  2.47   813    -19.7   2.22\n 8 Ryan Walker        676254 14.7    302     -7.33  2.21\n 9 Reynaldo Lopez     625643  8.82   542    -13.2   2.20\n10 Cade Smith         671922 14.2    289     -7.02  2.12",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#exercises",
    "href": "lectures/lecture08.html#exercises",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Exercises",
    "text": "Exercises\n\nWe modeled the probability of a batted ball resulting in out using just the batted ball location. Try adjusting for additional factors about the pitch and swing in our GAM. How do the out probabilities and our downstream analysis change when you account for these factors?",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#footnotes",
    "href": "lectures/lecture08.html#footnotes",
    "title": "Lecture 8: Defensive Credit Allocation in Baseball",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe official position numbers are: 1 (pitcher), 2 (catcher), 3 (first baseman), 4 (second baseman), 5 (third baseman), 6 (shortstop), 7 (left fielder), 8 (center fielder), and 9 (right fielder).↩︎\nIf you’re keen to learn more about splines, consider taking STAT 351 (Introduction to Nonparametric Statistics). A lot of the mathematical theory underpinning smoothing splines was developed by Prof. Grace Wahba, who was a long-serving member of the faculty here.↩︎\nThis choice is decidedly ad hoc. You should experiment with different cut-offs to see how the downstream results change!↩︎\nThe Cy Young Award is given to the best pitchers in the National League and American League.↩︎",
    "crumbs": [
      "Lecture 8: Defensive Credit Allocation in Baseball"
    ]
  },
  {
    "objectID": "lectures/lecture10.html",
    "href": "lectures/lecture10.html",
    "title": "Lecture 10: NFl WAR",
    "section": "",
    "text": "In Lecture 9, we looked at two touchdowns from the 2024-25 NFL season:\n\nA 86-yard touchdown by Ladd McConckey on a pass from Justin Herbert (video link)\nA 64-yard touchdown by Kavontae Turpin on a pass from Cooper Rush (video link)\n\nThe McConckey touchdown was thrown on a 3rd and 26 from the Charger’s 14 yard line. From that position, teams are expected to score about -1.54 points; that is, starting from that positions, teams, on average, give up points rather than score them. Over the course of the play, the Chargers offense generated around 8.54 points of EP. The Turpin touchdown, on the other hand, was thrown on a 3rd and 10 from the Cowboys’ 36 yard line, a position from which teams score about 0.77 on average. The Cowboys offense generated about 6.23 points of EPA during that play. How much of the created EPA on these plays should be credited to the quarterbacks and how much should be credited to the receivers? Which players\nIn this lecture, we will use multilevel modeling to answer these questions. We separately model EPA on passing and running plays. For passing plays, we will decompose the EPA into two parts, the expected points gained while the pass is in the air and the expected points gained on the ground after the catch (if the pass was caught). Each of our multilevel models will include random intercepts for players nested by their position (e.g., passer and receiver in the passing EPA models) and will also adjust for several important fixed effects. From these models, we will determine how many more expected points per play each player adds relative to the league average. Finally, like we did in Lecture 8, we will translate these expected points added to the win scale and conclude with a version of wins above replacement for offensive players in football. Our development closely follows the nflWAR model introduced by Yurko, Ventura, and Horowitz (2019) but makes some simplifications.",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#overview",
    "href": "lectures/lecture10.html#overview",
    "title": "Lecture 10: NFl WAR",
    "section": "",
    "text": "In Lecture 9, we looked at two touchdowns from the 2024-25 NFL season:\n\nA 86-yard touchdown by Ladd McConckey on a pass from Justin Herbert (video link)\nA 64-yard touchdown by Kavontae Turpin on a pass from Cooper Rush (video link)\n\nThe McConckey touchdown was thrown on a 3rd and 26 from the Charger’s 14 yard line. From that position, teams are expected to score about -1.54 points; that is, starting from that positions, teams, on average, give up points rather than score them. Over the course of the play, the Chargers offense generated around 8.54 points of EP. The Turpin touchdown, on the other hand, was thrown on a 3rd and 10 from the Cowboys’ 36 yard line, a position from which teams score about 0.77 on average. The Cowboys offense generated about 6.23 points of EPA during that play. How much of the created EPA on these plays should be credited to the quarterbacks and how much should be credited to the receivers? Which players\nIn this lecture, we will use multilevel modeling to answer these questions. We separately model EPA on passing and running plays. For passing plays, we will decompose the EPA into two parts, the expected points gained while the pass is in the air and the expected points gained on the ground after the catch (if the pass was caught). Each of our multilevel models will include random intercepts for players nested by their position (e.g., passer and receiver in the passing EPA models) and will also adjust for several important fixed effects. From these models, we will determine how many more expected points per play each player adds relative to the league average. Finally, like we did in Lecture 8, we will translate these expected points added to the win scale and conclude with a version of wins above replacement for offensive players in football. Our development closely follows the nflWAR model introduced by Yurko, Ventura, and Horowitz (2019) but makes some simplifications.",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-data-prep",
    "href": "lectures/lecture10.html#sec-data-prep",
    "title": "Lecture 10: NFl WAR",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe will restrict our analysis to plays from the 2024-25 regular season and exclude all post-season/play-off games. Later, in Section 5.1, we will define replacement-level players on a position-by-position basis. So that we can look up player positions, we will re-load the roster information using nflfastR::fast_scraper_roster().\n\npbp2024 &lt;-\n  nflfastR::load_pbp(season = 2024) |&gt;\n  dplyr::filter(season_type == \"REG\") \nroster2024 &lt;-\n  nflfastR::fast_scraper_roster(seasons = 2024) |&gt;\n  dplyr::filter(!is.na(gsis_id))\n\n\nPassing Plays Data\nWe will extract information about all passing plays from pbp2024 into a data table called pass2024.\n\npass2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(\n    play_type == \"pass\" &\n    !is.na(posteam) & \n1    !grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc) &\n2    !grepl(\"sack\", desc)  ) |&gt;\n3  dplyr::filter(passer_player_id != receiver_player_id) |&gt;\n  dplyr::select(\n    passer_player_id,\n    receiver_player_id,\n    posteam, defteam,\n    air_yards, shotgun, \n    qb_hit, no_huddle,\n    posteam_type, \n    pass_location,\n    yards_after_catch,\n    epa, air_epa, yac_epa, complete_pass, desc) |&gt;\n4  dplyr::mutate(gsis_id = receiver_player_id) |&gt;\n5  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, position), by = \"gsis_id\") |&gt;\n6  dplyr::rename(receiver_position = position) |&gt;\n7  dplyr::select(-gsis_id)\n\n\n1\n\nRemove pass attempts on two-point conversions\n\n2\n\nnflfastR classifies sacks as passing plays. For the purposes of building a version of WAR for the NFL, we will treat sacks as running plays, similar to Yurko, Ventura, and Horowitz (2019).\n\n3\n\nRemove plays like this where the passer catches their own pass (usually off a deflection).\n\n4\n\nCopy receiver ID into a temporary column, which will be used append receiver position with a join.\n\n5\n\nAppend receiver position.\n\n6\n\nRename the column containing receiver position.\n\n7\n\nRemove the temporary column holding the receiver ID\n\n\n\n\nIn addition to the identity of the passer (passer_player_id) and intended receiver (receiver_player_id), the data table contains the following columns: * posteam: the identity of the team in possession of the ball * defteam: the identity of the team on defense * air_yards: the distance traveled by the pass through the air[^airyard] * shotgun: whether the pass was thrown out of the shotgun formation * qb_hit: whether the quarterback was hit while throwing * no_huddle: whether the play was run without a huddle * posteam_type: whether the home team had possession (i.e., was on offense) * pass_location: whether the pass was thrown to the left, right, or center of the field * yards_after_catch: for completions, how many yards did the receiver gain after catching the pass * epa: the expected points added on the play * air_epa and yac_epa: the expected points added through the air and running after the catch (for completed passes) * complete_pass: whether the pass was completed * receiver_position: the position of the actual receiver on completed passes and the intended receiver on incomplete passes\nFor every passing play, nflfastR computes two different EPA values. The first, air_epa is the expected points added through the air and yac_epa is the expected points added after the catch. For completed passes, computing air_epa and yac_epa is relatively straightforward: as soon as the pass is caught, imagine stopping the play and computing the expected points based on the resulting game state. Then, the difference between this intermediate EP and the starting EP of the original play is air_epa and the difference between the final EP and this intermediate EP is yac_epa. Computing air_epa and yac_epa for incomplete passes is trickier and involves computing the difference in expected points between the final state, initial state, and an intermediate state that would result had the pass been caught.\n\n\nRushing Plays Data\nSuccess in the run game depends not only on the skill of the individual runner to run through the space created by his teammates and evade defenders but also on the ability of his teammates (usually linemen) to create space near the line and to make blocks down the field. Unfortunately, publicly available NFL play-by-play does not record the identities of all 22 players on the field nor does it include information about play (i.e., whether the play involved a pulling guard). It does, however, contain two variables, run_location, and run_gap that provide some context about the play.\nWe start by building a data table containing all rushing plays. As noted above, we will treat sacks as rushing plays.\n\nrush2024 &lt;-\n  pbp2024 |&gt;\n  dplyr::filter(\n    !grepl(\"TWO-POINT CONVERSION ATTEMPT\", desc)) |&gt;\n  dplyr::filter(\n1    !grepl(\"No play\", desc, ignore.case = TRUE)) |&gt;\n  dplyr::filter(\n    play_type == \"run\" |\n    grepl(\"sack\", desc, ignore.case = TRUE)) |&gt;\n  dplyr::select(\n    game_id, play_id,\n    posteam, defteam, \n2    passer_player_id,\n    rusher_player_id,\n    posteam_type,\n    shotgun, no_huddle,\n    run_location, run_gap, \n    epa, desc) \n\n\n1\n\nRemove the plays that are whistled dead or involve penalties like roughing the passer or are otherwise classified as “No Play”\n\n2\n\nOn sacks, the quarterback is sometimes not classified as a runner but as passer\n\n\n\n\nThe data table rush2024 contains several of the same variables pass2024 including the identities of the offensive and defensive teams, the EPA, and the indicators shotgun, no_huddle, and posteam_type. It also includes the ID for the runner and their listed position and two variables, run_location and run_gap, that contain some contextual information about the run. Unfortunately, there are a lot of missing values of rusher_player_id, all of which correspond to sacks.\n\nsum(is.na(rush2024$rusher_player_id))\n\n[1] 1327\n\nsum(grepl(\"sack\", rush2024$desc, ignore.case = TRUE) & is.na(rush2024$rusher_player_id))\n\n[1] 1327\n\n\n\nHandling Sacks\nnflfastR treats sacks as passing plays and not running plays. However, because the ball is not thrown during a sack, there is no well-defined air_epa or yac_epa. We instead follow the example of Yurko, Ventura, and Horowitz (2019) and treat sacks as running plays. Unfortunately, for the vast majority of sacks, the rusher_player_id variable is not available.\n\nsacks &lt;-\n  rush2024 |&gt;\n  dplyr::filter(grepl(\"sack\", desc, ignore.case = TRUE))\nmean(is.na(sacks$rusher_player_id))\n\n[1] 0.999247\n\n\nInstead, the id of the player who was sacked — whom we wish to treat as the rusher in our models — is saved as passer_player_id. We first verify that all sacks with a missing rusher_player_id do not also have a missing passer_player_id\n\n1sum(is.na(sacks$rusher_player_id))\n2sum(!is.na(sacks$passer_player_id) & is.na(sacks$rusher_player_id))\n\n\n1\n\nNumber of sacks for which we are missing the rusher_player_id\n\n2\n\nNumber of sacks for which are missing the rusher_player_id but are not missing the passer_player_id\n\n\n\n\n[1] 1327\n[1] 1327\n\n\nThe following code overwrites the missing rusher_player_id values on sacks and then appends the rusher’s position.\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::mutate(\n    rusher_player_id = \n      dplyr::case_when(\n        is.na(rusher_player_id) & grepl(\"sack\", desc, ignore.case = TRUE) ~ passer_player_id,\n        .default = rusher_player_id)) |&gt;\n  dplyr::mutate(gsis_id = rusher_player_id) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, position), by = \"gsis_id\") |&gt;\n  dplyr::rename(rusher_position = position) |&gt;\n  dplyr::select(-gsis_id)\n\n\n\nRun Context\nThe data table rush2024 contains two variables that describe the run play. The first, run_location, records whether the run was towards the left, right or middle of the offensive line. To describe run_gap, we need to introduce a little bit of additional notation. Every offensive play in American football begins with 5 offensive players (the “linemen”) on the line of scrimmage. The center is responsible for snapping the ball to the quarterback — thereby starting the play — and is positioned in the middle of the line. Next to the center are the two guards and next to the guards are the two tackles. The spaces between the linemen are known as “gaps” and are conventionally labelled with letters: “A” gap between the center and guard; “B” gap between guard and tackle, and “C” gap outside the tackle. Sometimes, but not always, a tight end lines up next to one or both of the tackles. When that happens, the space between the tackle and end is labelled “C” gap and the space outside the tight end is known as “D” gap.\nAlthough the naming convention is very prevalent within football (i.e, amongst coaches and players), the NFL play-by-play data does not use this convention. Instead, the variable run_gap takes the value guard for runs through the “B” gap, tackle for runs through the “C” gap, and end for runs through the “D” gap. For runs through the “B”, “C”, or “D” gap, the variable run_location takes on the value “left” or “right”, to indicate which side of the line But for runs through the “A” gap, run_gap is NA and run_location is equal to “middle”.\n\ntable(rush2024[,c(\"run_gap\", \"run_location\")], useNA = 'always')\n\n        run_location\nrun_gap  left middle right &lt;NA&gt;\n  end    1958      0  1805    0\n  guard  1822      0  1739    0\n  tackle 1645      0  1628    0\n  &lt;NA&gt;      0   3565     0 1447\n\n\nWe will combine these two variables into a composite variable run_context. For runs where both run_gap and run_location are not NA, we will concatenate the three variables (e.g., \"left end\" or \"right tackle\"). For runs through the “A” gap (when run_location == \"middle\" and run_gap is NA), we will set the contextual variable to be \"middle\". It turns out that the remaining 1447 running plays with missing run_location and run_gap involve a sack or fumble (and sometimes both!)\n\n1tmp &lt;-\n  rush2024 |&gt;\n  dplyr::filter(is.na(run_gap) & is.na(run_location))\n\n2all(\n  grepl(\"sack\", tmp$desc, ignore.case = TRUE) | #&lt;3\n3    grepl(\"fumble\", tmp$desc, ignore.case = TRUE) )\n\n\n1\n\nPull out all the plays missing both run_gap and run_location.\n\n2\n\nall returns TRUE if all elements in a logical vector are true\n\n3\n\nCheck whether the strings “sack” or “fumble” appears in the play description.\n\n\n\n\n[1] TRUE\n\n\nFor these plays, we will set run_context == \"other\". We further combine run_context with the identity of the offensive team (i.e., posteam) to create what Yurko, Ventura, and Horowitz (2019) describe as a “proxy for the offensive linemen or blockers involved in a rushing attempt.”\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::mutate(\n    run_context = dplyr::case_when(\n      run_location == \"middle\" ~ paste(posteam, \"middle\", sep = \"_\"),\n      is.na(run_location) & is.na(run_gap) ~ paste(posteam, \"other\", sep = \"_\"),\n      .default = paste(posteam, run_gap, run_location, sep = \"_\")))\n\n\n\n\nAccounting for Team Strengths\nRecall one of the main drawbacks of plus/minus was its inability to account for the quality of the player’s teammates. When trying to construct a version of WAR for offensive players in the NFL, we need to make sure that we do not overly reward players who happen to play for good teams or overly penalize players who play on bad offenses. In our multilevel models, we will quantify each offense’ running and passing strength using the average EPA on rushing and passing plays.\n\npass_strength &lt;-\n  pass2024 |&gt;\n  dplyr::group_by(posteam) |&gt;\n  dplyr::summarise(pass_strength = mean(epa,na.rm = TRUE))\nrush_strength &lt;-\n  rush2024 |&gt;\n  dplyr::group_by(posteam) |&gt;\n  dplyr::summarise(rush_strength = mean(epa, na.rm = TRUE))\n\n\npass2024 &lt;-\n  pass2024 |&gt;\n  dplyr::inner_join(pass_strength, by = \"posteam\") |&gt;\n  dplyr::inner_join(rush_strength, by = \"posteam\")\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::inner_join(pass_strength, by = \"posteam\") |&gt;\n  dplyr::inner_join(rush_strength, by = \"posteam\")",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-passing-models",
    "href": "lectures/lecture10.html#sec-passing-models",
    "title": "Lecture 10: NFl WAR",
    "section": "Passing Models",
    "text": "Passing Models\n\nConstructing Composite Outcomes\nWe’re now ready to determine how much individual players contribute to their team’s overall EPA on passing plays. While it is tempting to fit separate multilevel models to predict the values in the air_epa and yac_epa, we need to be a bit more careful in how we deal with incomplete passes. Specifically, on an incomplete pass, there is necessarily no run after the catch and any change in EP is driven entirely by what happens while the ball is in the air. To account for this, we will define two composite outcomes that are based on air_epa, yac_epa, and epa.\nFormally, say that there are \\(n\\) passing plays and for passing play \\(i,\\) let \\(\\Delta_{i}\\) be the observed EPA on the play. Let \\(\\delta_{i,\\textrm{air}}\\) and \\(\\delta_{i,\\textrm{yac}}\\) be the values of air_epa and yac_epa provided by nflfastR. We will decompose \\(\\Delta_{i} = \\Delta_{i,\\textrm{air}} + \\Delta_{i,\\textrm{yac}}\\) where \\[\n\\Delta_{i, \\textrm{air}} =\n\\begin{cases}\n\\delta_{i, \\textrm{air}} & \\textrm{if the pass is caught} \\\\\n\\Delta_{i} & \\textrm{if the pass is incomplete,}\n\\end{cases}\n\\] and \\[\n\\Delta_{i, \\textrm{yac}} =\n\\begin{cases}\n\\delta_{i, \\textrm{yac}} & \\textrm{if the pass is caught} \\\\\n0 & \\textrm{if the pass is incomplete.}\n\\end{cases}\n\\] The following code creates columns for these composite outcomes and also converts categorical predictors like defensive team and receiver position into factor variables.\n\npass2024 &lt;-\n  pass2024 |&gt;\n  dplyr::mutate(\n    Delta_air = ifelse(complete_pass == 1, air_epa, epa),\n    Delta_yac = ifelse(complete_pass == 1, yac_epa, 0),\n    passer_player_id = factor(passer_player_id),\n    receiver_player_id = factor(receiver_player_id),\n    posteam = factor(posteam),\n    defteam = factor(defteam),\n    pass_location = factor(pass_location),\n    receiver_position = factor(receiver_position))\n\n\n\nModeling \\(\\Delta_{\\textrm{air}}\\)\nOur first multilevel model, fitted to data from all passing plays, includes separate random intercepts for the passers, receivers, and defenses. It also accounts for the fixed effects of * air_yards: the distance traveled by the pass through the air * shotgun: whether the pass was thrown out of the shotgun formation * qb_hit: whether the quarterback was hit while throwing * no_huddle: whether the play was run without a huddle * posteam_type: whether the home team had possession (i.e., was on offense) * pass_location: whether the pass was thrown to the left, right, or center of the field * receiver_position: the position of the intended receiver * rush_strength: the average EPA per rush attempt for the offense\nBefore writing down our model, we introduce some more notation. Formally, suppose that there are \\(n_{q}\\) passers1, \\(n_{c}\\) receivers2, and \\(n_{D}\\) defenses. For passing play \\(i,\\) let \\(q[i], c[i],\\) and \\(d[i]\\) denote the identity of the passer, receiver, and defense involved in the play and let \\(\\boldsymbol{\\mathbf{x}}_{i}\\) be the vector of fixed effect covariates (i.e., the ones listed above).\nWe model \\[\n\\begin{align}\n\\Delta_{i, \\textrm{air}} &= Q_{q[i]} + C_{c[i]} + D_{d[i]} + \\boldsymbol{\\mathbf{x}}_{i}^{\\top}\\boldsymbol{\\beta} + \\epsilon_{i} \\quad \\text{for all passes}\\ i = 1, \\ldots, n \\\\\nQ_{q} &\\sim N(\\mu_{Q}, \\sigma^{2}_{Q}) \\quad \\textrm{for all passers}\\ q = 1, \\ldots, n_{Q} \\\\\nC_{c} &\\sim N(\\mu_{C}, \\sigma^{2}_{C}) \\quad \\textrm{for all pass catchers}\\ c = 1, \\ldots, n_{C},  \\\\\nD_{d} &\\sim N(\\mu_{D}, \\sigma^{2}_{D}) \\quad \\textrm{for all defenseive teams}\\ d = 1, \\ldots n_{D}.\n\\end{align}\n\\]\nWe can fit this model with lme4::lmer() using similar notation as was used in Lecture 9. Just like in that lecture, we can read off lots of important information and parameter estimates from the model summary. For instance, there is much more variability in the receiver effects than the passer or defense effects.\n\nlibrary(lme4)\n\n\n1\n\nWe want random intercepts for the passer, receiver, and defensive team\n\n2\n\nSpecify the fixed effects\n\n\n\n\nLoading required package: Matrix\n\nair_model &lt;-\n1  lmer(Delta_air ~ 1 + (1|passer_player_id) + (1 | receiver_player_id) + (1 | defteam )\n2       + air_yards + shotgun + qb_hit + no_huddle + posteam_type + pass_location + receiver_position + rush_strength,\n       data = pass2024)\nsummary(air_model)\n\n\nCorrelation matrix not shown by default, as p = 16 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Delta_air ~ 1 + (1 | passer_player_id) + (1 | receiver_player_id) +  \n    (1 | defteam) + air_yards + shotgun + qb_hit + no_huddle +  \n    posteam_type + pass_location + receiver_position + rush_strength\n   Data: pass2024\n\nREML criterion at convergence: 56640.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-9.6663 -0.4475 -0.0140  0.4521  4.2381 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n receiver_player_id (Intercept) 0.011544 0.10745 \n passer_player_id   (Intercept) 0.002242 0.04735 \n defteam            (Intercept) 0.001354 0.03680 \n Residual                       1.622752 1.27387 \nNumber of obs: 17000, groups:  \nreceiver_player_id, 492; passer_player_id, 99; defteam, 32\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)         -2.642055   0.573232  -4.609\nair_yards            0.035027   0.001077  32.537\nshotgun             -0.081497   0.028132  -2.897\nqb_hit              -0.276667   0.036182  -7.647\nno_huddle            0.134666   0.029805   4.518\nposteam_typehome    -0.008663   0.019728  -0.439\npass_locationmiddle  0.172170   0.026424   6.516\npass_locationright  -0.034797   0.022336  -1.558\nreceiver_positionDL  4.627420   1.401649   3.301\nreceiver_positionLB  3.750986   1.070748   3.503\nreceiver_positionOL  2.527147   0.729736   3.463\nreceiver_positionQB  3.661843   0.934966   3.917\nreceiver_positionRB  1.922465   0.573394   3.353\nreceiver_positionTE  2.279044   0.573261   3.976\nreceiver_positionWR  2.254500   0.573040   3.934\nrush_strength        0.307995   0.118268   2.604\n\n\nFor each passer \\(q,\\) \\(Q_{q}\\) is a latent3 quantity measuring how many expected points passer \\(q\\) adds through the air, after you account for the fixed effects and also the quality of the intended receiver and the defense. The quantity \\(\\mu_{Q}\\) represents the global average of these latent quantities. Similarly, \\(C_{c}\\) is the latent measurement of how many expected points receiver \\(c\\) adds while the pass is in the air4 after accounting for the fixed effects and the quality of the passer and of the opposing defense. And \\(D_{d}\\) is the latent measurement of how many expected points are added for the offensive team by defensive team \\(d\\) while the pass is in the air. Since EPA is signed so that positive values are considered good for the offense, large negative values of \\(D_{d}\\) are treated as good for the defense. In our model, the random intercepts \\(Q_{q}, C_{c},\\) and \\(D_{d}\\) are all modeled as noisy deviations away from league-wide averages \\(\\mu_{Q}, \\mu_{C},\\) and \\(\\mu_{D}.\\)\nUnfortunately, we cannot estimate \\(\\mu_{Q}, \\mu_{C},\\) and \\(\\mu_{D}\\) because these parameters are not identified. Without going into too many technical details5, there are infinitely many settings of the values of these three parameters that yield exactly the same fit to the data. Because we cannot estimate \\(\\mu_{Q}, \\mu_{C},\\) and \\(\\mu_{D},\\) we are unable to obtain estimates for the random intercepts \\(Q_{q}, C_{c},\\) and \\(D_{d}\\) like we did in Lecture 9. We are, however, able to estimate the deviations \\(Q_{q} - \\mu_{Q},\\) \\(C_{c} - \\mu_{C},\\) and \\(D_{d} - \\mu_{D}.\\) For each passer \\(q\\), the quantity \\(Q_{q} - \\mu_{Q}\\) measures many more expected points per play passer \\(q\\) adds through the air than the league average. Following Yurko, Ventura, and Horowitz (2019), we will refer to this number as passer \\(q\\)’s individual points added over average or \\(\\textrm{IPA}^{(Q)}_{\\textrm{air},q}\\)6. We can similarly define \\(\\textrm{IPA}^{(C)}_{\\textrm{air}, c}\\) for receivers. For each defense \\(d,\\) let \\(D_{d} - \\mu_{D}\\) is the number of points per passing play over league average given up by the defensive team while the bar is in the air. We will denote this quantity by \\(\\textrm{TPA}^{(D)}_{\\textrm{air},d},\\) with negative \\(\\textrm{TPA}\\) values corresponding to better defenses.\nWe can extract the IPA and TPA values using the function ranef().\n\n1tmp_air &lt;- ranef(air_model)\n\nair_passer_effects &lt;-\n  data.frame(\n2    gsis_id = rownames(tmp_air[[\"passer_player_id\"]]),\n    ipa_air_pass = tmp_air[[\"passer_player_id\"]][,1])\nair_receiver_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_air[[\"receiver_player_id\"]]),\n    ipa_air_rec = tmp_air[[\"receiver_player_id\"]][,1])\nair_defense_effects &lt;-\n  data.frame(\n    Team = rownames(tmp_air[[\"defteam\"]]),\n    tpa_air_def = tmp_air[[\"defteam\"]][,1])\n\n\n1\n\nExtract the passer, receiver, and defensive random intercepts in a large list\n\n2\n\nBecause we will eventually append player names by inner_join()’ing with roster2024, we’ll record the passer IDs in a column called gsis_id.\n\n\n\n\nTaking a quick look at the top-5 (resp. bottom-5) IPA passer values, we see a number of highly regarded (resp. heavily criticized) quarterbacks.\n\nair_passer_effects |&gt;\n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::select(full_name, ipa_air_pass) |&gt;\n  dplyr::arrange(dplyr::desc(ipa_air_pass)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n                  full_name ipa_air_pass\n1                Joe Burrow   0.06736333\n2               Sam Darnold   0.04033663\n3             Lamar Jackson   0.03292716\n4            Tua Tagovailoa   0.03281935\n5               Brock Purdy   0.03000549\n6                 Drew Lock  -0.02737783\n7                    Bo Nix  -0.02808707\n8  Dorian Thompson-Robinson  -0.03832239\n9           Spencer Rattler  -0.05005097\n10       Anthony Richardson  -0.07074920\n\n\n\n\nModeling \\(\\Delta_{\\textrm{yac}}\\)\nBy fitting a similar multilevel model to predict the \\(\\Delta_{i, \\textrm{yac}}\\)’s, we can estimate how much each passer, receiver, and defense contributes to EPA during the part of a passing play following a catch. Because these contributions can, by definition, only be made following a completed pass, we fit this model using only the data from completed pass7.\n\ncompletions &lt;-\n  pass2024 |&gt;\n  dplyr::filter(complete_pass==1)\n\nyac_model &lt;-\n  lmer(Delta_yac ~ 1 + (1|passer_player_id) + (1 | receiver_player_id) + (1 | defteam ) \n       + air_yards + shotgun + qb_hit + no_huddle + posteam_type + pass_location + receiver_position + rush_strength, \n       data = completions)\n\nboundary (singular) fit: see help('isSingular')\n\n\nWhile fitting the model, lmer printed a message saying that the fit was singular. The manual page for isSingular, which the message instructs us to visit, contains lots of information about singular models. Briefly, when fitting complicated multilevel models with lots of random effects, it can sometimes be the case that some of the “between” group variances are estimated to be exactly zero. Quoting from the manual\n\nWhile singular models are statistically well-defined … there are real concerns that (1) singular fits correspond to overfitted models that may have poor power; (2) chances of numerical problems or mis-convergence are higher for singular models … ; (3) standard inferential procedures such as Wald statistics and likelihood ratio tests may be inappropriate.\n\nTaking a look at the model summary, we see that the passer-to-passer variation has been set to zero.\n\nsummary(yac_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Delta_yac ~ 1 + (1 | passer_player_id) + (1 | receiver_player_id) +  \n    (1 | defteam) + air_yards + shotgun + qb_hit + no_huddle +  \n    posteam_type + pass_location + receiver_position + rush_strength\n   Data: completions\n\nREML criterion at convergence: 32367.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.9533 -0.5437 -0.2533  0.2882  7.4637 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev.\n receiver_player_id (Intercept) 0.0190108 0.1379  \n passer_player_id   (Intercept) 0.0000000 0.0000  \n defteam            (Intercept) 0.0006814 0.0261  \n Residual                       0.9296857 0.9642  \nNumber of obs: 11627, groups:  \nreceiver_player_id, 468; passer_player_id, 90; defteam, 32\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)          1.710173   0.689922   2.479\nair_yards           -0.022666   0.001214 -18.675\nshotgun             -0.046899   0.025546  -1.836\nqb_hit               0.037119   0.039172   0.948\nno_huddle           -0.174386   0.027107  -6.433\nposteam_typehome    -0.007157   0.018083  -0.396\npass_locationmiddle -0.057884   0.024339  -2.378\npass_locationright   0.011441   0.020532   0.557\nreceiver_positionDL -1.567487   1.193518  -1.313\nreceiver_positionLB -0.728621   0.974493  -0.748\nreceiver_positionOL -1.046652   0.781368  -1.340\nreceiver_positionQB -0.728794   0.974704  -0.748\nreceiver_positionRB -0.515251   0.689574  -0.747\nreceiver_positionTE -0.837277   0.689576  -1.214\nreceiver_positionWR -0.847828   0.689446  -1.230\nrush_strength        0.382583   0.105680   3.620\n\n\n\nCorrelation matrix not shown by default, as p = 16 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nWe can further confirm that this is the case by checking whether all the estimated deviation between each passer’s random intercept and the global mean across all passers (i.e., all \\(\\textrm{IPA}^{(Q)}_{\\textrm{yac},q})\\)’s) are equal to zero.\n\n1all(ranef(yac_model)[[\"passer_player_id\"]][,1] == 0)\n\n\n1\n\nChecks whether all of the passer IPA values are equal to 0.\n\n\n\n\n[1] TRUE\n\n\nThis phenomenon makes some intuitive sense: once the catch is made, there is very little for the passer, who is typically several yards away, to contribute. The manual entry for isSingular() goes on to say that “There is not yet consensus about how to deal with singularity, or more generally to choose which random-effects specifications (from a range of choices of varying complexity to use)”8 and suggests that one option is to “avoid fitting overly complex models in the first place.” So, at this point we have a choice: either we move forward with our model or we re-fit it without the passer random intercepts.\nRecall that we ultimately plan to aggregate the IPA from air yards, YAC, and rushing for all offensive players. If we move forward with our existing model, the YAC IPA values for passers are already set to zero. But if we re-fit the model, we will have to manually set these values to zero. To avoid this extra step, we’ll continue to use our current model9.\n\ntmp_yac &lt;- ranef(yac_model)\n\nyac_passer_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_yac[[\"passer_player_id\"]]), \n    ipa_yac_pass = tmp_yac[[\"passer_player_id\"]][,1])\nyac_receiver_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_yac[[\"receiver_player_id\"]]),\n    ipa_yac_rec = tmp_yac[[\"receiver_player_id\"]][,1])\nyac_defense_effects &lt;-\n  data.frame(\n    Team = rownames(tmp_yac[[\"defteam\"]]),\n    tpa_yac_def = tmp_yac[[\"defteam\"]][,1])\n\nAmong the receivers with the largest \\(\\textrm{IPA}_{\\textrm{YAC}, c}^{(C)}\\) values, we see players like Ja’Marr Chase, Khalil Shakir, and Brock Bowers who led the league in yards after the catch during the 2024 regular season.\n\nyac_receiver_effects |&gt;\n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::arrange(dplyr::desc(ipa_yac_rec)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::select(full_name, ipa_yac_rec)\n\n         full_name ipa_yac_rec\n1  Marvin Mims Jr.   0.4038113\n2    Khalil Shakir   0.2763021\n3  KaVontae Turpin   0.2225070\n4    Xavier Worthy   0.2179003\n5     Tucker Kraft   0.2141390\n6     Brock Bowers   0.1885809\n7    Austin Ekeler   0.1830305\n8   Antonio Gibson   0.1818104\n9    Ja'Marr Chase   0.1778265\n10  Raheem Mostert   0.1729994",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-rush-model",
    "href": "lectures/lecture10.html#sec-rush-model",
    "title": "Lecture 10: NFl WAR",
    "section": "Rushing Model",
    "text": "Rushing Model\nAs Yurko, Ventura, and Horowitz (2019) notes, we cannot distinguish between designed quarterback runs from scrambles on broken plays using publicly available play-by-play data. So, we will fit two separate models of EPA for running plays, one for all quarterback runs (i.e., rusher_position==\"QB\") and one for all non-quarterback runs (i.e., rusher_position != \"QB\"). Before creating separate data tables for the two types of running plays, we convert several categorical covariates to factors.\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::mutate(\n    rusher_player_id = factor(rusher_player_id),\n    posteam = factor(posteam),\n    defteam = factor(defteam),\n    posteam_type = factor(posteam_type),\n    rusher_position == factor(rusher_position),\n    run_context = factor(run_context))\n\nqb_runs &lt;-\n  rush2024 |&gt;\n  dplyr::filter(rusher_position == \"QB\")\n\nnonqb_runs &lt;-\n  rush2024 |&gt;\n  dplyr::filter(rusher_position != \"QB\")\n\nWe now fit multilevel models to predict the EPA on running plays for both QB-runs and non-QB-runs. Like with our other models, we extract the number of expected points per play each player adds over the league average via their running.\n\nqbrun_fit &lt;-\n  lmer(epa ~ 1 + (1 | rusher_player_id) + (1 | defteam) +\n1         shotgun + no_huddle + posteam_type + pass_strength,\n       data = qb_runs)\nnonqb_run_fit &lt;-\n  lmer(epa ~ 1 + (1 | rusher_player_id) + (1 | defteam) +  \n2         shotgun + no_huddle + posteam_type + rusher_position + run_context + pass_strength,\n       data = nonqb_runs)\n\n\n1\n\nFor the quarterback run model, there is no need to include rusher_position since every rusher in the dataset plays the same position. We also exclude run_context since many of the observations in this data table are not designed runs.\n\n2\n\nFor the non-quarterback run model, we adjust for the runner’s position and the run context\n\n\n\n\nfixed-effect model matrix is rank deficient so dropping 2 columns / coefficients\n\ntmp_qbrun &lt;- ranef(qbrun_fit)\nqbrun_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_qbrun[[\"rusher_player_id\"]]), \n    ipa_qbrun = tmp_qbrun[[\"rusher_player_id\"]][,1])\n\ntmp_run &lt;- ranef(nonqb_run_fit) \nrun_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_run[[\"rusher_player_id\"]]),\n    ipa_run = tmp_run[[\"rusher_player_id\"]][,1])\n\nTaking a look at the top non-QB IPA values, we see several highly-regarded running backs\n\nrun_effects |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::select(full_name, ipa_run) |&gt;\n  dplyr::arrange(dplyr::desc(ipa_run)) |&gt;\n  dplyr::slice_head(n = 10)\n\n         full_name    ipa_run\n1    De'Von Achane 0.13626662\n2   Saquon Barkley 0.10131412\n3     Jahmyr Gibbs 0.09383069\n4      Jerome Ford 0.08473154\n5    Chuba Hubbard 0.08423388\n6     J.K. Dobbins 0.07766485\n7  Emari Demercado 0.07656891\n8    Dameon Pierce 0.06755530\n9      Rico Dowdle 0.06517368\n10     Taysom Hill 0.06219509",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-ipaa",
    "href": "lectures/lecture10.html#sec-ipaa",
    "title": "Lecture 10: NFl WAR",
    "section": "Total Individual Points Added Above Average",
    "text": "Total Individual Points Added Above Average\nFor each offensive player, we have estimates for the number of expected points they add per play through the air, after the catch, and through their running. By multiplying each player’s IPA by the number of times they attempted the corresponding play, we can compute each player’s individual points added above average or IPAA for every type of play.\nFor instance, let’s start with quarterbacks and how they generate EPA through the air on passing plays. Our modeling suggests that Joe Burrow created 42 more points of EP for his team through the air than a league-average quarterback would have. Burrow’s IPAA of 42 points through the air is substantially larger than the next best quarterbacks (Darnold and Jackson). Recall from Section 3.3 that every passer has an IPA of zero based on their contributions after the catch is made.\n\npasser_ipaa &lt;-\n  pass2024 |&gt;\n1  dplyr::group_by(passer_player_id) |&gt;\n  dplyr::summarise(n_pass = dplyr::n()) |&gt;\n2  dplyr::rename(gsis_id = passer_player_id) |&gt;\n3  dplyr::left_join(air_passer_effects, by = \"gsis_id\") |&gt;\n4  dplyr::left_join(yac_passer_effects, by = \"gsis_id\") |&gt;\n  dplyr::mutate(\n    ipaa_air_pass = ipa_air_pass * n_pass,\n    ipaa_yac_pass = ipa_yac_pass * n_pass) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\npasser_ipaa |&gt;\n  dplyr::select(full_name, n_pass, ipaa_air_pass, ipaa_yac_pass) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_air_pass)) |&gt;\n  dplyr::slice_head(n = 10)\n\n\n1\n\nCompute the number of passing attempts for each passer\n\n2\n\nRename the column with passer ID so that we can append their IPA values\n\n3\n\nAppend the IPA values for all passers\n\n4\n\nCompute the IPAA by multiplying number of attempts by IPA\n\n\n\n\n# A tibble: 10 × 4\n   full_name      n_pass ipaa_air_pass ipaa_yac_pass\n   &lt;chr&gt;           &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Joe Burrow        628         42.3              0\n 2 Sam Darnold       512         20.7              0\n 3 Lamar Jackson     454         14.9              0\n 4 Brock Purdy       440         13.2              0\n 5 Tua Tagovailoa    388         12.7              0\n 6 Justin Herbert    484         12.7              0\n 7 Baker Mayfield    562         11.1              0\n 8 C.J. Stroud       509          8.45             0\n 9 Russell Wilson    306          7.07             0\n10 Geno Smith        554          6.07             0\n\n\nWe can repeat these calculations for receivers on passing plays\n\nreceiver_ipaa &lt;-\n  pass2024 |&gt;\n  dplyr::group_by(receiver_player_id) |&gt;\n  dplyr::summarise(n_rec = dplyr::n()) |&gt; \n  dplyr::rename(gsis_id = receiver_player_id) |&gt; \n  dplyr::left_join(air_receiver_effects, by = \"gsis_id\") |&gt; \n  dplyr::left_join(yac_receiver_effects, by = \"gsis_id\") |&gt;\n  dplyr::mutate(\n   ipaa_air_rec = ipa_air_rec * n_rec,\n   ipaa_yac_rec = ipa_yac_rec * n_rec) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\nHere are the top 10 IPAA values for receivers based on their contributions while the ball is in the air:\n\nreceiver_ipaa |&gt;\n  dplyr::select(full_name, n_rec, ipaa_air_rec, ipaa_yac_rec) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_air_rec)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 4\n   full_name         n_rec ipaa_air_rec ipaa_yac_rec\n   &lt;chr&gt;             &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Tee Higgins         109         19.9       -8.07 \n 2 Amon-Ra St. Brown   141         19.6        7.93 \n 3 Ladd McConkey       112         16.5       -1.38 \n 4 Terry McLaurin      117         13.7       -7.16 \n 5 DeVonta Smith        89         12.7       -3.41 \n 6 Jauan Jennings      113         12.6      -13.3  \n 7 Jakobi Meyers       129         12.0        0.373\n 8 Justin Jefferson    153         11.4        8.66 \n 9 Courtland Sutton    135         11.0      -21.1  \n10 Jonnu Smith         111         10.1       18.4  \n\n\nAnd here are the top 10 IPAA values for receivers based on their contributions following a catch:\n\nreceiver_ipaa |&gt;\n  dplyr::select(full_name, n_rec, ipaa_air_rec, ipaa_yac_rec) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_yac_rec)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 4\n   full_name        n_rec ipaa_air_rec ipaa_yac_rec\n   &lt;chr&gt;            &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Ja'Marr Chase      175         4.84         31.1\n 2 Brock Bowers       153        -4.53         28.9\n 3 Khalil Shakir      100       -10.3          27.6\n 4 DJ Moore           140       -14.4          22.5\n 5 Brian Thomas Jr.   133        -2.92         22.4\n 6 Xavier Worthy       98       -10.7          21.4\n 7 Marvin Mims Jr.     52        -7.46         21.0\n 8 Jonnu Smith        111        10.1          18.4\n 9 Puka Nacua         106         4.33         16.4\n10 Josh Downs         107        -1.95         15.2\n\n\nIt is particularly interesting to note that there are several receivers whose contributions while the ball in the air are worse than league average but who are better than league average once they catch the ball.\nWe repeat this calculation for the IPA values arising from yards after the catch and for both types of running play.\n\nqbrun_ipaa &lt;-\n  qb_runs |&gt;\n  dplyr::group_by(rusher_player_id) |&gt;\n  dplyr::summarise(n_qbrun = dplyr::n()) |&gt;\n  dplyr::rename(gsis_id = rusher_player_id) |&gt;\n  dplyr::left_join(y = qbrun_effects, by = \"gsis_id\") |&gt;\n  dplyr::mutate(ipaa_qbrun = n_qbrun * ipa_qbrun) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\nrun_ipaa &lt;-\n  nonqb_runs |&gt;\n  dplyr::group_by(rusher_player_id) |&gt;\n  dplyr::summarise(n_run = dplyr::n()) |&gt;\n  dplyr::rename(gsis_id = rusher_player_id) |&gt;\n  dplyr::left_join(y = run_effects, by = \"gsis_id\") |&gt;\n  dplyr::mutate(ipaa_run = n_run * ipa_run) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\n\nrun_ipaa |&gt;\n  dplyr::select(full_name, ipaa_run) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_run)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   full_name      ipaa_run\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Saquon Barkley     35.1\n 2 De'Von Achane      27.7\n 3 Jahmyr Gibbs       23.5\n 4 Chuba Hubbard      21.1\n 5 Derrick Henry      16.9\n 6 Rico Dowdle        15.3\n 7 J.K. Dobbins       15.1\n 8 Bijan Robinson     14.5\n 9 Najee Harris       12.7\n10 Chase Brown        11.6\n\n\n\nReplacement Level for Skill Positions\nAlthough the IPAA values are interesting and informative, they represent a comparison between players and a nebulously defined average. Recall that IPAA is just a scaled version of IPA, which we defined to be the difference between the player-specific random intercept and a global mean parameter in our multilevel model. That mean does not represent the average of all observed players in the league. Instead, it captures the average intercept across a theoretical, infinite super-population of players from which we assume the set of observed players is sampled. So, it is not strictly correct to say that IPA measures a player’s expected contribution over and above a league-average player’s contribution on a per-play basis.\nTo facilitate comparisons that are easier to interpret, we now translate the IPAA values into measures of a player’s contribution relative to a replacement-level player. As discussed in the context of baseball, there are myriad ways to define what replacement level. Like we did in Lecture 8, we take a roster-based approach and define different replacement levels for different combinations of position and our EPA models. Noting that most NFL teams carry 3 running backs (RBs), 4 wide receivers (WRs), and 2 tight ends (TE), we define replacement levels based o on the following thresholds:\n\nWRs on passing plays: The top \\(32 \\times 4 = 128\\) WRs sorted by receiving attempts (i.e., n_rec in pass_ipaa) are non-replacement-level; everyone else is replacement-level\nTEs on passing plays: The top \\(32 \\times 2 = 64\\) TEs sorted by receiving attempts are non-replacement-level; everyone else is replacement-level\nRBs on passing plays: The top \\(32 \\times 3 = 96\\) RBs sorted by receiving attempts are non-replacement-level; everyone else is replacement-level\nWR/TEs on running plays: The top \\(32 \\times 1 = 32\\) WR and TEs sorted by rushing attempts are non-replacement-level; everyone else is replacement-level\nRBs on running plays: The top \\(32 \\times 3 = 96\\) RBs sorted by rusing attempts are non-replacement-level; everyone else is replacement-level.\n\nThese definitions allow for the possibility that a RB may be non-replacement-level based on his running but is at or below replacement-level when it comes to receiving. Like we did in Lecture 8, for every player we can compare his IPAA to the IPAA that a replacement level player would achieve if given the same number of attempts. To compute this “shadow” IPAA, we will multiply the number of attempts by the average IPA value across all corresponding replacement-level players.\nThe following code computes the receiving attempt thresholds for each position on passing plays. It then computes the replacement-level IPA\n\nwr_pass_threshold &lt;- \n  receiver_ipaa |&gt;\n  dplyr::filter(position == \"WR\") |&gt;\n  dplyr::arrange(desc(n_rec)) |&gt;\n1  dplyr::slice(128) |&gt;\n  dplyr::pull(n_rec)\n\nte_pass_threshold &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(position == \"TE\") |&gt;\n  dplyr::arrange(desc(n_rec)) |&gt;\n  dplyr::slice(64) |&gt;\n  dplyr::pull(n_rec)\n\nrb_pass_threshold &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(position == \"RB\") |&gt;\n  dplyr::arrange(desc(n_rec)) |&gt;\n  dplyr::slice(32) |&gt;\n  dplyr::pull(n_rec)\n\n\n1\n\nPlayers with fewer passing plays these value will be considered replacement-level at their position\n\n\n\n\nWe add columns to receiver_ipaa indicating whether each player is considered to be replacement-level at his position or not.\n\nreceiver_ipaa &lt;-\n  receiver_ipaa |&gt;\n  dplyr::mutate(\n    repl_wr = dplyr::case_when(\n      position == \"WR\" & n_rec &lt; wr_pass_threshold ~ 1,\n      position == \"WR\" & n_rec &gt;= wr_pass_threshold ~ 0,\n      position != \"WR\" ~ NA),\n    repl_te = dplyr::case_when(\n      position == \"TE\" & n_rec &lt; te_pass_threshold ~ 1,\n      position == \"TE\" & n_rec &gt;= te_pass_threshold ~ 0,\n      position != \"TE\" ~ NA),\n    repl_rb = dplyr::case_when(\n      position == \"RB\" & n_rec &lt; rb_pass_threshold ~ 1,\n      position == \"RB\" & n_rec &gt;= rb_pass_threshold ~ 0,\n      position != \"RB\" ~ NA))\n\nWe now average the IPA across all replacement-level players at each position in both phases of a passing play\n\nrepl_wr_ipa_air &lt;- \n  receiver_ipaa |&gt;\n  dplyr::filter(repl_wr == 1) |&gt;\n  dplyr::pull(ipa_air_rec) |&gt;\n  mean(na.rm = TRUE)\nrepl_wr_ipa_yac &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(repl_wr == 1) |&gt;\n  dplyr::pull(ipa_yac_rec) |&gt;\n  mean(na.rm = TRUE)\n\nrepl_te_ipa_air &lt;- \n  receiver_ipaa |&gt;\n  dplyr::filter(repl_te == 1) |&gt;\n  dplyr::pull(ipa_air_rec) |&gt;\n  mean(na.rm=TRUE)\nrepl_te_ipa_yac &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(repl_te == 1) |&gt;\n  dplyr::pull(ipa_yac_rec) |&gt;\n  mean(na.rm = TRUE)\n\nrepl_rb_ipa_air &lt;- \n  receiver_ipaa |&gt;\n  dplyr::filter(repl_rb == 1) |&gt;\n  dplyr::pull(ipa_air_rec) |&gt;\n  mean(na.rm = TRUE)\nrepl_rb_ipa_yac &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(repl_rb == 1) |&gt;\n  dplyr::pull(ipa_yac_rec) |&gt;\n  mean(na.rm = TRUE)\n\nFinally, we compute the difference between every player’s actual IPAA and their replacement-level shadow IPAA value, to obtain their individual points added over replacement (or IPAR):\n\nreceiver_ipar &lt;-\n  receiver_ipaa |&gt;\n  dplyr::filter(position %in% c(\"WR\",\"RB\", \"TE\")) |&gt;\n  dplyr::mutate(\n    shadow_air_rec = dplyr::case_when(\n      position == \"WR\" ~ n_rec * repl_wr_ipa_air,\n      position == \"TE\" ~ n_rec * repl_te_ipa_air,\n      position == \"RB\" ~ n_rec * repl_rb_ipa_air),\n    shadow_yac_rec = dplyr::case_when(\n      position == \"WR\" ~ n_rec * repl_wr_ipa_yac,\n      position == \"TE\" ~ n_rec * repl_te_ipa_yac,\n      position == \"RB\" ~ n_rec * repl_rb_ipa_yac),\n    ipar_air_rec = ipaa_air_rec - shadow_air_rec,\n    ipar_yac_rec = ipaa_yac_rec - shadow_yac_rec)\n\nWe perform similar calculations to compute the IPAR value from running plays for each non-QB player:\n\n\nCode\nwrte_run_threshold &lt;-\n  run_ipaa |&gt;\n  dplyr::filter(position %in% c(\"WR\", \"TE\")) |&gt;\n  dplyr::arrange(desc(n_run)) |&gt;\n  dplyr::slice(32) |&gt;\n  dplyr::pull(n_run)\n\nrb_run_threshold &lt;-\n  run_ipaa |&gt;\n  dplyr::filter(position == \"RB\") |&gt;\n  dplyr::arrange(desc(n_run)) |&gt;\n  dplyr::slice(96) |&gt;\n  dplyr::pull(n_run)\n\n\nrun_ipaa &lt;-\n  run_ipaa |&gt;\n  dplyr::mutate(\n    repl_wrte = dplyr::case_when(\n      position %in% c(\"WR\", \"TE\") & n_run &lt; wrte_run_threshold ~ 1,\n      position %in% c(\"WR\", \"TE\") & n_run &gt;= wrte_run_threshold ~ 0,\n      !position %in% c(\"WR\", \"TE\") ~ NA),\n    repl_rb = dplyr::case_when(\n      position == \"RB\" & n_run &lt; rb_run_threshold ~ 1,\n      position == \"RB\" & n_run &gt;= rb_run_threshold ~ 0,\n      position != \"RB\" ~ NA))\n\n\n\nrepl_wrte_ipa_run &lt;- \n  run_ipaa |&gt;\n  dplyr::filter(repl_wrte == 1) |&gt;\n  dplyr::pull(ipa_run) |&gt;\n  mean(na.rm = TRUE)\nrepl_rb_ipa_run &lt;-\n  run_ipaa |&gt;\n  dplyr::filter(repl_rb == 1) |&gt;\n  dplyr::pull(ipa_run) |&gt;\n  mean(na.rm = TRUE)\n\n\nrun_ipar &lt;-\n  run_ipaa |&gt;\n  dplyr::filter(position %in% c(\"WR\", \"RB\", \"TE\")) |&gt;\n   dplyr::mutate(\n    shadow_run = dplyr::case_when(\n      position %in% c(\"WR\", \"TE\") ~ n_run * repl_wrte_ipa_run,\n      position == \"RB\" ~ n_run * repl_rb_ipa_run),\n    ipar_run = ipaa_run - shadow_run)\n\n\n\n\nComputing IPAR for QB’s\nWe defined replacement-levels for running backs, wide receivers, and tight ends based on typical patterns of roster construction. Due to the unique nature of the quarterback position — virtually every offensive play involves runs through the quarterback — we need to define “replacement” level somewhat differently. One option would be to create a group of 32 non-replacement quarterbacks by identifying the quarterback one each team who were involved in the most passing and rushing attempts. While simple to implement, this implicitly assumes that every NFL team has at least one non-replacement quarterback. Instead, we sort quarterbacks by the total number of passing and rushing plays in which they were involved and designate the top-32 to be non-replacement level.\n\nqb_ipaa &lt;-\n  passer_ipaa |&gt;\n  dplyr::filter(position == \"QB\") |&gt;\n  dplyr::select(gsis_id, full_name, n_pass, ipa_air_pass, ipaa_air_pass) |&gt;\n  dplyr::left_join(y = qbrun_ipaa |&gt; dplyr::select(gsis_id, n_qbrun, ipa_qbrun, ipaa_qbrun), by = \"gsis_id\") |&gt;\n  dplyr::mutate(n_plays = n_pass + n_qbrun)\n\nqb_threshold &lt;-\n  qb_ipaa |&gt;\n  dplyr::arrange(dplyr::desc(n_plays)) |&gt;\n  dplyr::slice(32) |&gt;\n  dplyr::pull(n_plays)\n\nqb_ipaa &lt;-\n  qb_ipaa |&gt;\n  dplyr::mutate(repl_qb = ifelse(n_plays &lt; qb_threshold, 1, 0))\n\nrepl_qb_ipa_air &lt;-\n  qb_ipaa |&gt;\n  dplyr::filter(repl_qb == 1) |&gt;\n  dplyr::pull(ipa_air_pass) |&gt;\n  mean(na.rm = TRUE)\n\nrepl_qb_ipa_qbrun &lt;-\n  qb_ipaa |&gt;\n  dplyr::filter(repl_qb == 1) |&gt;\n  dplyr::pull(ipa_qbrun) |&gt;\n  mean(na.rm = TRUE)\n\nqb_ipar &lt;-\n  qb_ipaa |&gt;\n  dplyr::mutate(\n    shadow_air = n_pass * repl_qb_ipa_air,\n    shadow_qbrun = n_qbrun * repl_qb_ipa_qbrun,\n    ipar_air_pass = ipaa_air_pass - shadow_air,\n    ipar_qbrun = ipaa_qbrun - shadow_qbrun)",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#sec-pts-win",
    "href": "lectures/lecture10.html#sec-pts-win",
    "title": "Lecture 10: NFl WAR",
    "section": "From Points to Wins",
    "text": "From Points to Wins\nThe IPAR values are on the scale of point differential. Figure shows point differential plotted against wins. If we fit a line to these data — that is, if we find some \\(\\alpha\\) and \\(\\beta\\) such that \\(\\textrm{Wins} \\approx \\alpha + \\beta \\times \\textrm{PtsDiff}\\) — then we can multiply the IPAR values by the estimated slope to convert from the point differential scale to the win scale.\nTo get this, we load the schedules for the last 10 seasons using the function nflreader::load_schedules() from the nflreadr package, which can be installed using the code\n\ninstall.packages(\"nflreader\")\n\nThe following code loads schedules and computes point differentials and records.\n\nschedules &lt;-\n  nflreadr::load_schedules(seasons = 2015:2024) |&gt;\n  dplyr::filter(game_type == \"REG\") |&gt;\n  dplyr::select(season, away_team, home_team, home_score, away_score, result) |&gt;\n  dplyr::mutate(winning_team = ifelse(result &gt; 0, home_team, away_team),\n                losing_team = ifelse(result &lt; 0, home_team, away_team),\n                winning_ptsdiff = ifelse(winning_team == home_team, result, -1*result),\n                losing_ptsdiff = ifelse(losing_team == home_team, result, -1*result))\n\nwin_diff &lt;-\n  schedules |&gt;\n  dplyr::group_by(season, winning_team) |&gt;\n  dplyr::summarise(wins = dplyr::n(), win_diff = sum(winning_ptsdiff), .groups = 'drop') |&gt;\n  dplyr::rename(team = winning_team)\n\nloss_diff &lt;-\n  schedules |&gt;\n  dplyr::group_by(season, losing_team) |&gt;\n  dplyr::summarise(loss = dplyr::n(), loss_diff = sum(losing_ptsdiff), .groups = 'drop') |&gt;\n  dplyr::rename(team = losing_team)\n\nrecords &lt;-\n  win_diff |&gt;\n  dplyr::full_join(y = loss_diff, by = c(\"season\", \"team\")) |&gt;\n  dplyr::mutate(scoring_differential = win_diff + loss_diff)\n\nPlotting wins against total scoring differential, we see an obvious increasing trend\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(records$scoring_differential,\n     records$wins,\n     xlab = \"Total scoring margin\",\n     ylab = \"Wins\",\n     main = \"Wins vs Score Differential\",\n     pch = 16, cex = 0.5)\n\n\n\n\n\n\n\nFigure 1: Scoring differential is positively associated with wins\n\n\n\n\n\nWe fit a linear to these data\n\nwin_score_fit &lt;-\n  lm(wins~scoring_differential, data = records)\nbeta &lt;- coefficients(win_score_fit)[2]\nbeta\n\nscoring_differential \n          0.02807309 \n\n\nWe see that every additional point scored is associated with about an increase of about 0.02 wins, on average.\nWe can now multiply each IPAR value by this factor to determine how wins each player contributes above replacement in different phases of the game.\n\nall_skill_players &lt;-\n  unique(c(run_ipar$gsis_id, receiver_ipar$gsis_id))\n\nskill_war &lt;-\n  data.frame(gsis_id = all_skill_players) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position, team), by = \"gsis_id\") |&gt;\n  dplyr::left_join(y = run_ipar |&gt; dplyr::select(gsis_id, n_run, ipar_run), by = \"gsis_id\") |&gt;\n  dplyr::left_join(y = receiver_ipar |&gt; dplyr::select(gsis_id, n_rec, ipar_air_rec, ipar_yac_rec), by = \"gsis_id\") |&gt;\n  tidyr::replace_na(list(ipar_run=0, ipar_air_rec=0, ipar_yac_rec=0)) |&gt;\n  dplyr::mutate(\n    war_air_rec = ipar_air_rec * beta,\n    war_yac_rec = ipar_yac_rec * beta,\n    war_run = ipar_run * beta,\n    war = war_air_rec + war_yac_rec + war_run)\n\nqb_war &lt;-\n  qb_ipar |&gt;\n  dplyr::select(gsis_id, full_name, n_pass, n_qbrun, ipar_air_pass, ipar_qbrun) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, team, position), by = \"gsis_id\") |&gt;\n  tidyr::replace_na(list(ipar_air_pass=0, ipar_qbrun=0)) |&gt;\n  dplyr::mutate(\n    war_air_pass = ipar_air_pass * beta,\n    war_qbrun = ipar_qbrun * beta,\n    war = war_air_pass + war_qbrun)\n\nHere are the top-10 skill-position players based on our WAR calculations are\n\nskill_war |&gt;\n  dplyr::arrange(dplyr::desc(war)) |&gt;\n  dplyr::select(full_name, war) |&gt;\n  dplyr::slice_head(n=10)\n\n           full_name       war\n1      De'Von Achane 1.2408245\n2     Saquon Barkley 1.1551989\n3      Ja'Marr Chase 1.0545348\n4       Jahmyr Gibbs 0.8666242\n5        Jonnu Smith 0.8446721\n6  Amon-Ra St. Brown 0.8086111\n7       Brock Bowers 0.7438843\n8      Derrick Henry 0.6381019\n9         Puka Nacua 0.6084343\n10  Justin Jefferson 0.6012925\n\n\nAnd here are the top-10 quarterbacks based on our WAR calculations\n\nqb_war |&gt;\n  dplyr::arrange(dplyr::desc(war)) |&gt;\n  dplyr::select(full_name, war) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   full_name            war\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 Jayden Daniels     2.32 \n 2 Josh Allen         2.31 \n 3 Lamar Jackson      1.93 \n 4 Jalen Hurts        1.72 \n 5 Anthony Richardson 1.36 \n 6 Brock Purdy        1.31 \n 7 Bo Nix             1.08 \n 8 Kyler Murray       1.05 \n 9 Daniel Jones       0.912\n10 Patrick Mahomes    0.903",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#exercises",
    "href": "lectures/lecture10.html#exercises",
    "title": "Lecture 10: NFl WAR",
    "section": "Exercises",
    "text": "Exercises\n\nRepeat the analysis using win probability added instead of expected points added. Note,you do not need to convert from the points to win scale in this re-analysis. Which players appear to contribute the most to their team’s overall win probability?",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#footnotes",
    "href": "lectures/lecture10.html#footnotes",
    "title": "Lecture 10: NFl WAR",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBecause the vast majority of passes are thrown by quarterbacks, we’ll index passers with “q”.↩︎\nBecause “r” could refer to either receiver or runner, we’ll index receivers with “c” for pass catchers.↩︎\nThat is, unobservable.↩︎\nE.g., via his route running.↩︎\nIf you have taken STAT 310 or 312, the issue is that the likelihood function depends on these parameters only through their sum \\(\\mu_{Q} + \\mu_{C} + \\mu_{D}.\\) So, the data alone cannot distinguish configurations of these parameters that yield the same sum. For instance, setting \\(\\mu_{Q} = \\mu_{C} = \\mu_{D} = 0\\) yields exactly the same fit to the data \\(\\mu_{Q} = 5, \\mu_{C} = -5, \\mu_{D} = 0.\\) One way to get around this inject prior information and fit a hierarchical Bayesian model, but that is beyond the scope of this course.↩︎\nThe superscript \\(^{(Q)}\\) is there to remind us that this is for passers (i.e., quarterbacks) and the subscript \\(\\textrm{air}\\) reminds us that the IPA values are derived from our model for EPA through the air.↩︎\nIn the original nflWAR paper (Yurko, Ventura, and Horowitz 2019), they set \\(\\Delta_{\\textrm{air},i} =\\Delta_{\\textrm{yac},i} = \\Delta_{i}\\) on incomplete passes and fit the YAC EPA model using data from both complete and incomplete passes. This effectively “double counts” contributions of offensive players.↩︎\nPersonally, I disagree with the statement. A very natural way around this issue is to use a Bayesian hierarchical model. In fact, I’d go further and say that multilevel models of the type fitted by lmer() are just impoverished versions of Bayesian hierarchical models. But Bayesian models are regrettably outside the scope of the course and non-Bayesian multilevel models are very widely used so you have to learn a little about them.↩︎\nBut you should check whether our downstream results change if we estimate the YAC IPA values for receivers using a reduced model that excludes the passer random intercept!↩︎",
    "crumbs": [
      "Lecture 10: NFl WAR"
    ]
  },
  {
    "objectID": "lectures/lecture12.html",
    "href": "lectures/lecture12.html",
    "title": "Lecture 12: Bradley-Terry Models",
    "section": "",
    "text": "The Wisconsin Badgers defeated the Ohio State Buckeyes women in the championship game of the 2025 NCAA Women’s Ice Hockey Tournament1. During the 2024-25 regular season, the Badgers won 38 games, lost 1 game, and tied 2 games. The Buckeyes won 35 games, lost 4 games, and did not tie any games. In this lecture, we’ll consider the broad question “Did the better team win?” and introduce a framework for measuring latent team strength and predicting the outcome of individual matches.\nTo motivate our developments, let’s consider two slightly different — but certainly related — questions, which are somewhat more nuanced than “did the better team win?”:\n\nIf we could repeatedly play the championship game, how often would the Badgers win?\nIf the NCAA Championship were awarded to the winner in a “best-of-5” series2 at a neutral site, what is the likelihood that the Badgers would win the championship?\n\nTo answer these questions, we introduce a model for estimating latent team strengths from pairwise comparisons (i.e., match results; Section 2). We build a data set containing the outcomes of every Division 1 NCAA Women’s Ice Hockey game from the 2024-25 season (Section 3). We then estimate the model parameters (Section 4) before using the model estimates to simulate a best-of-5 series (Section 4.3) between the Badgers and Buckeyes.",
    "crumbs": [
      "Lecture 12: Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture12.html#motivation-the-2025-ncaa-womens-ice-hockey-tournament",
    "href": "lectures/lecture12.html#motivation-the-2025-ncaa-womens-ice-hockey-tournament",
    "title": "Lecture 12: Bradley-Terry Models",
    "section": "",
    "text": "The Wisconsin Badgers defeated the Ohio State Buckeyes women in the championship game of the 2025 NCAA Women’s Ice Hockey Tournament1. During the 2024-25 regular season, the Badgers won 38 games, lost 1 game, and tied 2 games. The Buckeyes won 35 games, lost 4 games, and did not tie any games. In this lecture, we’ll consider the broad question “Did the better team win?” and introduce a framework for measuring latent team strength and predicting the outcome of individual matches.\nTo motivate our developments, let’s consider two slightly different — but certainly related — questions, which are somewhat more nuanced than “did the better team win?”:\n\nIf we could repeatedly play the championship game, how often would the Badgers win?\nIf the NCAA Championship were awarded to the winner in a “best-of-5” series2 at a neutral site, what is the likelihood that the Badgers would win the championship?\n\nTo answer these questions, we introduce a model for estimating latent team strengths from pairwise comparisons (i.e., match results; Section 2). We build a data set containing the outcomes of every Division 1 NCAA Women’s Ice Hockey game from the 2024-25 season (Section 3). We then estimate the model parameters (Section 4) before using the model estimates to simulate a best-of-5 series (Section 4.3) between the Badgers and Buckeyes.",
    "crumbs": [
      "Lecture 12: Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture12.html#sec-bt-def",
    "href": "lectures/lecture12.html#sec-bt-def",
    "title": "Lecture 12: Bradley-Terry Models",
    "section": "The Bradley-Terry Model",
    "text": "The Bradley-Terry Model\nSuppose we observe the outcome of \\(n\\) games played between \\(p\\) teams, which we label \\(1, 2, \\ldots, p.\\) The basic Bradley-Terry model associates each team \\(j = 1, \\ldots, p,\\) with a latent strength \\(\\lambda_{j}\\) and asserts that \\[\n\\mathbb{P}(\\textrm{team i beats team j}) = \\frac{e^{\\lambda_{i}}}{e^{\\lambda_{i}} + e^{\\lambda_{j}}} = \\frac{1}{1 + e^{-1 \\times (\\lambda_{i} - \\lambda_{j})}}\n\\] In other words, under the basic Bradley-Terry model, the log-odds of team \\(i\\) beating team \\(j\\) is \\(\\lambda_{i} - \\lambda_{j}.\\)\n\nExample: Predicted Probabilities\nConsider a round-robin tournament between 4 teams (numbered 1, 2, 3, and 4) in which every teams plays every other team exactly once. Further suppose that the latent team strengths are \\(\\lambda_{1} = 1, \\lambda_{2} = 0.5, \\lambda_{3} = 0.3\\) and \\(\\lambda_{4} = 0.\\) According to the Bradley-Terry model, if team 1 plays team 2, it is expected to win with probability \\[\n\\mathbb{P}(\\textrm{team 1 beats team 2}) = \\frac{e^{1}}{e^{1} + e^{0.5}} \\approx 62\\%\n\\]\n\nlambda &lt;- c(1, 0.5, 0.3, 0)\n11/(1 + exp(-1 * (lambda[1] - lambda[2])))\n\n\n1\n\nIt is more numerically stable to compute the difference in strengths on the log-odds scale and apply the inverse logistic transform than to exponentiate the \\(\\lambda\\)’s in the numerator and denominator.\n\n\n\n\n[1] 0.6224593\n\n\nManually computing the probability that each team beats every other team is not too difficulty3. To compute all these probabilities in one go, we can make use of the outer() function to assemble a matrix containing all pairwise differences of the \\(\\lambda_{i}'\\).\n\n1outer(X = lambda, Y = lambda, FUN = \"-\")\n\n\n1\n\nouter() applies the function specified with the FUN argument to every combination of its arguments X and Y.\n\n\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.0  0.5  0.7  1.0\n[2,] -0.5  0.0  0.2  0.5\n[3,] -0.7 -0.2  0.0  0.3\n[4,] -1.0 -0.5 -0.3  0.0\n\n\nOnce we have these differences, which are on the log-odds scale, we can apply the inverse logistic function to transform them to the probability scale. The result is a matrix whose \\((i,j)\\) element is \\(\\mathbb{P}(\\textrm{team i beats team j})\\).\n\nprobs &lt;- 1/(1 + exp(-1 * outer(X = lambda, Y = lambda, FUN = \"-\")))\n1diag(probs) &lt;- NA\nround(probs, digits = 3)\n\n\n1\n\nThe probability of a team beating itself is not well-defined so we will manually set these values to NA\n\n\n\n\n      [,1]  [,2]  [,3]  [,4]\n[1,]    NA 0.622 0.668 0.731\n[2,] 0.378    NA 0.550 0.622\n[3,] 0.332 0.450    NA 0.574\n[4,] 0.269 0.378 0.426    NA\n\n\nArmed with the probabilities of each team beating every other team, we can start to compute the probabilities of more elaborate events. Assuming the match outcomes are independent, the probability that team 1 wins its matches against Teams 2 and 3 but loses to Team 4 is \\[\n0.622 \\times 0.668 \\times (1 - 0.731) \\approx 11.1\\%\n\\]\n\nround(probs[1,2] * probs[1,3] * probs[1,4], digits = 3)\n\n[1] 0.304\n\n\nWe can also compute the probability that Team 2 wins exactly 2 of its 3 games \\[\n\\begin{align}\n\\mathbb{P}(\\textrm{team 2 wins 2 games}) &= \\mathbb{P}(\\textrm{team 2 beats 1 \\& 3 and loses to 4}) \\\\\n&~+ \\mathbb{P}(\\textrm{team 2 beats 1 \\& 4 and loses to 3}) \\\\\n&~+ \\mathbb{P}(\\textrm{team 2 betas 3 \\& 4 and loses to 1})\n\\end{align}\n\\]\n\nprobs[2,1] * probs[2,3] * probs[4,2] + \n  probs[2,1] * probs[2,4] * probs[3,2] + \n  probs[1,2] * probs[2,3] * probs[2,4]\n\n[1] 0.3971986\n\n\n\n\nExample: Tournament Simulation\nIn our 4-team round-robin tournament, what is the probability that Team 3 finishes in the top-2 in terms of wins4? To compute this probability by hand, we would need to enumerate all the possible ways in which Team 3 can finish in the top-2 positions after all 6 matches. While not impossible to do in our 4-team example, such manual computation becomes challenging as we increase the number of teams.\nA much more elegant solution is to estimate this probability via simulation. Specifically, we can use the estimated match outcome probabilities to simulate each of the 6 games in our tournament many, many times. We can then compute the proportion of simulations in which Team 3 finishes in the top-2 in terms of wins.\nTo perform this simulation, we begin by enumerating each pair of teams using the combn() function.\n\ncombn(x = 1:4, m = 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    1    1    2    2    3\n[2,]    2    3    4    3    4    4\n\n\nThe function combn() returns an array with two rows (one for each team) and columns for every game in the tournament. It will be more useful for us to convert this into a data table with rows for each game. In this table, we will label the teams player1 and player2 and also append a column containing the probability that player1 beats player2\n\ndesign &lt;- \n  combn(x = 1:4, m = 2) |&gt; \n1  t() |&gt;\n2  as.data.frame() |&gt;\n3  dplyr::rename(player1 = V1, player2 = V2) |&gt;\n  dplyr::rowwise() |&gt;\n4  dplyr::mutate(prob = probs[player1, player2]) |&gt;\n  dplyr::ungroup()\ndesign\n\n\n1\n\nSince combn() returns a matrix whose columns index games, we need to take its transpose\n\n2\n\nFor the dplyr verbs to work, we need to convert our table from a matrix to a data.frame\n\n3\n\nBy default as.data.frame will use the columns names V1, V2, etc. So, we change them here to something more informative\n\n4\n\nWithout the rowwise(), this line would create columns for all pairwise probabilities and not the desired one.\n\n\n\n\n# A tibble: 6 × 3\n  player1 player2  prob\n    &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;\n1       1       2 0.622\n2       1       3 0.668\n3       1       4 0.731\n4       2       3 0.550\n5       2       4 0.622\n6       3       4 0.574\n\n\nSince match outcomes are binary, simulating the outcomes of the 6 matches is equivalent to flipping 6 independent coins, each with its own probability of landing heads. We interpret the event that a particular coin lands heads with the event that player1 wins the match and the event of the coin landing tails as the event that player2 wins the match. We can simulate these binary events using the rbinom() function\n\nset.seed(479)\noutcomes &lt;- rbinom(n = 6, size = 1, prob = design$prob)\noutcomes\n\n[1] 0 1 1 0 1 1\n\n\nThe first three entries in outcomes correspond to the games in the first three rows of design, which are played between Team 1 and Teams 2, 3, and 4. So, in this simulation Team 1 loses to Team 2 but beats Teams 3 and 4. The fourth and fifth entry in outcomes corresponds to the games between Team 2 and Teams 3 & 4. These entries are 0 and 1, respectively, meaning that in this simulation Team 2 lost to Team 3 but beat Team 4. Finally, the sixth entry corresponds to the game between Team 3 and Team 4. It is equal to 1, meaning that in this simulation Team 3 defeated Team 4.\nTo tabulate each teams’ wins, we create a temporary data frame containing the participants and a column recording the winner (winner). Then, we can count the number of times each team appears in the column winner using a grouped summary. In this simulation, Teams 1, 2, and 3 each won exactly 2 games.\n\ntmp_df &lt;-\n  design |&gt;\n  dplyr::select(player1, player2) |&gt;\n  dplyr::mutate(\n    outcome = outcomes,\n    winner = ifelse(outcome == 1, player1, player2),\n    player1 = factor(player1, levels = 1:4),\n    player2 = factor(player2, levels = 1:4),\n    winner = factor(winner, levels = 1:4))\n\nwins &lt;-\n  tmp_df |&gt;\n  dplyr::group_by(winner) |&gt;\n  dplyr::summarise(Wins = dplyr::n()) |&gt;\n  dplyr::rename(Team = winner) |&gt;\n1  tidyr::complete(Team, fill = list(Wins = 0))\n\n\n1\n\nIf a team wins no games, they will not appear in the column winner. Using complete(), we can add rows for these teams and manually assign them 0 wins.\n\n\n\n\nWe can now repeat this simulation many times, each time recording the number of wins for each team. In the code block below, build an array whose rows correspond to simulation replications and whose columns record the number of wins for each team.\n\nn_sims &lt;- 5000\nsimulated_wins &lt;- matrix(data = NA, nrow = n_sims, ncol = 4)\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  outcomes &lt;- rbinom(n = 6, size = 1, prob = design$prob)\n  wins &lt;-\n    design |&gt;\n    dplyr::select(player1, player2) |&gt;\n    dplyr::mutate(\n      outcome = outcomes,\n      winner = ifelse(outcome == 1, player1, player2),\n      winner = factor(winner, levels = unique(c(design$player1, design$player2)))) |&gt;\n  dplyr::group_by(winner) |&gt;\n  dplyr::summarise(Wins = dplyr::n()) |&gt;\n  dplyr::rename(Team = winner) |&gt;\n  tidyr::complete(Team, fill = list(Wins = 0))\n  \n  simulated_wins[r,] &lt;- wins |&gt; dplyr::pull(Wins)\n}\n\nOut of the 5000 simulations, Team 1 won all 3 of its games in 1534 simulations. Based on this, our estimated probability of Team 1 winning all 3 of its matches is \\(1534/5000 \\approx 30.6\\%,\\) which is quite close to the exact value of 30.4% (which we can obtain by multiplying probs[1,2] * probs[1,3] * probs[1,4]).\n\ntable(simulated_wins[,1])\n\n\n   0    1    2    3 \n 163 1105 2198 1534 \n\n\nTo estimate the probability that Team 3 finishes in the top-2, we will first rank each team based on the number of games they won in each simulation. Like in Lecture 5, we will call rank on the negative value of wins so that teams with more wins get lower numerical ranks. In the case of ties, we will assign the minimum possible rank (using the argument ties.method=\"min\"). As an illustration, say that Teams 1 and 2 both won 2 games and Teams 3 and 4 both won 1 game each. This convention would assign Teams 1 and 2 a rank of 1 and Teams 3 and 4 a rank of 3 (there would be no second place).\n\nrank(-1*c(2,2,1,1), ties.method = \"min\")\n\n[1] 1 1 3 3\n\n\nWe compute the team rankings for each simulation replication using the apply() function.\n\nsimulated_ranks &lt;-\n1  t(\n2    apply(-1*simulated_wins, MARGIN = 1,\n3        FUN = rank, ties.method = \"min\"))\n\n\n1\n\nIn this case, apply() will return a matrix with 4 rows and 5000 columns, so we need to transpose it\n\n2\n\nWe specify MARGIN = 1 because we want to rank values within each row (i.e., simulation replication)\n\n3\n\napply() allows us to pass additional arguments needed for the function we passed via the argument FUN.\n\n\n\n\nWe find that in 1565 of the 5000 simulations, Team 3 had the most wins (rank of 1) and in 1092 of the 5000 simulations, Team 3 had the second-most wins (rank of 2). So, the estimated probability of Team 3 finished in the top-2 in terms of wins is about 0.531.\n\ntable(simulated_ranks[,3])\n\n\n   1    2    3    4 \n1565 1092 1547  796 \n\n\n\n\nIdentifability\nThe basic Bradley-Terry model assigns each team \\(j\\) a latent strength \\(\\lambda_{j}\\) and models the log-odds of team \\(i\\) beating team \\(j\\) as the difference in strengths \\(\\lambda_{i} - \\lambda_{j}.\\) Notice that these log-odds (and hence, the probability of team \\(i\\) beating team \\(j\\)) does not change if we add the same constant (e.g., 10) to each \\(\\lambda_{j}.\\) This means that we can only estimates the underlying latent strengths up to an additive constant. In practice, we generally fix one of the \\(\\lambda_{j}\\)’s to be zero; usually, the team names are represented are factor variables and we simply set the latent strength for the reference level to be zero.",
    "crumbs": [
      "Lecture 12: Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture12.html#sec-scraping",
    "href": "lectures/lecture12.html#sec-scraping",
    "title": "Lecture 12: Bradley-Terry Models",
    "section": "Scraping D1 Women’s Hockey Results",
    "text": "Scraping D1 Women’s Hockey Results\nWe will fit a Bradley-Terry model to estimate the latent team strengths of all D1 Women’s Ice Hockey Teams. To do this, we need to build a data table that contains, at a minimum, the identities of the home and away teams and an indicator of whether the home team won for each regular-season game. Unfortunately, there is not (yet) an existing R package that contains such a table or even provides the tools needed to scrape the data. So, we will need to acquire the data ourselves.\nUS College Hockey Online is an independent media organization focused on college hockey. In addition to posting articles, weekly polls & rankings, and team & individual statistics, USCHO publishes box score and play-by-play data. For instance, here is the play-by-play from the 2025 championship game. They also publish match results for each season in a tabular form5\nThis information is readily scraped using tools like rvest.\n\n\n\n\n\n\nDownload pre-scraped data\n\n\n\nUnfortunately, USCHO changed their website back-end and the following code no longer works (as of October 5, 2025). The data for this lecture is available as a CSV file here.\n\n\nThe codeblock below defines a function that scrapes a single season’s worth of D1 Women’s Ice Hockey results.\n\n\nFunction for scraping\nscrape_uscho &lt;- function(season){\n  \n  col_names &lt;-\n    c(\"Day\", \"\", \"Date\", \"Time\", \n      \"Opponent\", \"Opp Score\", \"\", \"Home\", \"Score\", \"OT\",\n      \"Notes\", \"Type\", \"Summary\", \"TV\")\n  \n  target_url &lt;- \n    paste0(\"https://www.uscho.com/scoreboard/division-i-women/\",\n           season, \"-\", season+1, \n           \"/composite-schedule/\")\n  raw_data &lt;-\n    rvest::read_html(target_url)\n  raw_elements &lt;-\n    raw_data |&gt;\n    rvest::html_elements(xpath = '//*[(@id = \"ez-outstream-ez-stuck-close\")] | //td') |&gt; \n    rvest::html_text2()\n  \n  assign(paste0(\"raw_wd1hockey_\", season, \"_\", season+1), raw_data)\n  save(list = paste0(\"raw_wd1hockey_\", season, \"_\", season+1),\n       file = paste0(\"raw_wd1hockey_\", season, \"_\", season+1, \".RData\"))\n  \n  if(!identical(col_names, raw_elements[1:14])){\n    message(paste0(\"Season \", season,\"-\",season+1, \": did not find expected column names in html elements!\"))\n    results &lt;- NULL\n  } \n  \n  # 2022-23 does not have TV abbreviations\n  \n  altitude_ix &lt;- which(grepl(\"Altitude\", raw_elements))[1]\n  \n  if(altitude_ix %% 14 != 1){\n    message(paste0(\"Season \", season, \"-\", season+1, \": did not find expected number of elements in table!\"))\n    results &lt;- NULL\n  } else{\n    # First 14 elements \n    n_games &lt;- (altitude_ix - 1)/14 - 1 \n    games &lt;- \n      data.frame(\n        Day = rep(NA, times = n_games),\n        Date = rep(NA, times = n_games),\n        Time = rep(NA, times = n_games),\n        Opponent = rep(NA, times = n_games),\n        OppScore = rep(NA, times = n_games),\n        Home = rep(NA, times = n_games),\n        HomeScore = rep(NA, times = n_games),\n        OT = rep(NA, times = n_games),\n        Notes = rep(NA, times = n_games),\n        Type = rep(NA, times = n_games))\n    \n    games[[\"Day\"]] &lt;- \n      raw_elements[seq(15, (n_games+1)*14, by = 14)] \n    games[[\"Date\"]] &lt;-\n      raw_elements[seq(17, (n_games+1)*14, by = 14)]\n    games[[\"Time\"]] &lt;-\n      raw_elements[seq(18, (n_games+1)*14, by = 14)]\n    games[[\"Opponent\"]] &lt;-\n      raw_elements[seq(19, (n_games+1)*14, by = 14)]\n    games[[\"OppScore\"]] &lt;-\n      raw_elements[seq(20, (n_games+1)*14, by = 14)]\n    games[[\"Home\"]] &lt;-\n      raw_elements[seq(22, (n_games+1)*14, by = 14)]\n    games[[\"HomeScore\"]] &lt;-\n      raw_elements[seq(23, (n_games+1)*14, by = 14)]\n    games[[\"OT\"]] &lt;-\n      raw_elements[seq(24, (n_games+1)*14, by = 14)]\n    games[[\"Notes\"]] &lt;-\n      raw_elements[seq(25, (n_games+1)*14, by = 14)]\n    games[[\"Type\"]] &lt;-\n      raw_elements[seq(26, (n_games+1)*14, by = 14)]\n    results &lt;- \n      tidyr::as_tibble(games) |&gt;\n      dplyr::mutate(\n        OT = ifelse(OT == \"\", NA_character_, OT),\n        Notes = ifelse(Notes == \"\", NA_character_, Notes),\n        HomeScore = as.integer(HomeScore),\n        OppScore = as.integer(OppScore))\n  }\n  return(results)\n}\n\n\nWe can use this function to obtain game-by-game results from the 2024-25 regular season\n\nwd1hockey_2024_2025 &lt;- scrape_uscho(season = 2024)\n1save(wd1hockey_2024_2025, file = \"wd1hockey_2024_2025.RData\")\n\n\n1\n\nSave a copy of the data table so that we can load it later without have to re-scrape it\n\n\n\n\nIn addition to the recording the date and time of each game, our data table wd1hockey_2024_2025 records the home and away team names (Home and Opponent); the home and away team scores (HomeScore and OppScore); whether the game went into overtime (OT = \"OT\") or not (OT = NA).\n\n\n# A tibble: 10 × 10\n   Day   Date       Time     Opponent OppScore Home  HomeScore OT    Notes Type \n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 Sat.  1/4/2025   1:00 CT  Minneso… 4        Lind… 1         &lt;NA&gt;  &lt;NA&gt;  NC   \n 2 Fri.  12/6/2024  6:00 ET  Connect… 2        New … 1         &lt;NA&gt;  &lt;NA&gt;  HE   \n 3 Fri.  12/6/2024  6:00 ET  Dartmou… 3        Clar… 5         &lt;NA&gt;  &lt;NA&gt;  EC   \n 4 Fri.  11/29/2024 6:00 ET  Colgate  3        Syra… 1         &lt;NA&gt;  &lt;NA&gt;  NC   \n 5 Sat.  1/18/2025  3:00 ET  Renssel… 0        Dart… 0         OT    DAR … EC   \n 6 Sat.  3/1/2025   4:30 ET  Maine    3        Bost… 4         &lt;NA&gt;  HEA … NC   \n 7 Fri.  1/3/2025   7:00 ET  Yale     8        Robe… 1         &lt;NA&gt;  &lt;NA&gt;  NC   \n 8 Fri.  11/1/2024  3:00 CT  Minneso… 2        Bemi… 1         &lt;NA&gt;  &lt;NA&gt;  WC   \n 9 Fri.  1/10/2025  6:00 ET  St. Law… 4        Union 2         &lt;NA&gt;  &lt;NA&gt;  EC   \n10 Sat.  11/16/2024 12:00 ET Vermont  0        Prov… 0         OT    VER … HE   \n\n\nIn the following subsections, we will investigate what the columns Type and Notes record.\n\nDetermining the Winner\nIn order to fit a Bradley-Terry model using these data, we still need to determine whether the home team won the game or not. Intuitively, we can do this by adding a column to wd1hockey_2024_2025 that compares HomeScore to OppScore. Unfortunately, this simple strategy is not quite adequate as there are a number of games in which HomeScore = OppScore. It turns out that many of these games ended in a shootout[^shootout] and the ultimate winner is recorded in the Notes column.\n\nwd1hockey_2024_2025 |&gt;\n  dplyr::filter(HomeScore == OppScore) |&gt;\n  dplyr::select(Opponent, OppScore, Home, HomeScore, Notes) |&gt;\n  dplyr::slice_head(n=5)\n\n# A tibble: 5 × 5\n  Opponent     OppScore Home          HomeScore Notes          \n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;          \n1 Maine        2        New Hampshire 2         UNH wins SO 2-1\n2 Assumption   1        New Hampshire 1         AC wins SO 2-1 \n3 Minnesota    1        Ohio State    1         OSU wins SO 2-1\n4 Rensselaer   3        RIT           3         RIT wins SO 1-0\n5 Northeastern 4        Holy Cross    4         NOE wins SO 2-1\n\n\nNotes uses a three-character abbreviation instead of the team’s full name. We can look up the abbreviations by inspecting the box score for each game. For instance, by looking under the “Team” column here, we see that Wisconsin and Ohio States’ abbreviations are, respectively, \"WIS\" and \"OSU\". A table listing every team and its USCHO’s abbreviation is available here. After downloading this table and saving it in our working directory, we can load it into our R session\n\nteam_abbr &lt;- readr::read_csv(file = \"wd1hockey_teams.csv\")\n\nTo see whether the home team won a match, we first check whether HomeScore and OppScore are identical. If not, then we can check whether HomeScore &gt; OppScore. But if they are the same, we can determine the winner by parsing the text in Notes. The following code block implement a function that executes this procedure.\n\n\nFunction that determines if the home team won the game\nget_home_winner &lt;- \n  function(Home_abbr, HomeScore, Opp_abbr, OppScore, Notes)\n{\n  home_winner &lt;- NA\n  if(HomeScore &gt; OppScore) home_winner &lt;- 1\n  else if(HomeScore &lt; OppScore) home_winner &lt;- 0\n  else{\n1    if(!is.na(Notes) & grepl(\"SO\", Notes)){\n      winner &lt;- \n2        stringr::str_split(string = Notes, pattern = \" wins\")[[1]][1]\n      if(winner == Home_abbr) home_winner &lt;- 1\n      else if(winner == Opp_abbr) home_winner &lt;- 0\n    }\n  }\n  return(home_winner)\n}\n\n\n\n1\n\nHere is where we check if Notes contains the string “SO”\n\n2\n\nPulls out the three-character abbreviation for the winning team\n\n\n\n\nWe can now apply this function to every row in our data table and append a columns Home_Winner and Opp_Winner containing an indicator of whether the home team won the game.\n\nwd1hockey_2024_2025 &lt;-\n  wd1hockey_2024_2025 |&gt;\n  dplyr::mutate(\n    HomeScore = as.integer(HomeScore),\n    OppScore = as.integer(OppScore)) |&gt;\n1  dplyr::left_join(y = team_abbr |&gt; dplyr::rename(Home_abbr = abbr, Home = team),\n                   by = \"Home\") |&gt;\n  dplyr::left_join(y = team_abbr |&gt; dplyr::rename(Opp_abbr = abbr, Opponent = team),\n                   by = \"Opponent\") |&gt;\n2  dplyr::rowwise() |&gt;\n  dplyr::mutate(\n    Home_Winner = \n      get_home_winner(Home_abbr, HomeScore, Opp_abbr, OppScore, Notes)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::mutate(Opp_Winner = 1-Home_Winner)\n\n\n1\n\nAdd columns containing the home and away team abbreviations\n\n2\n\nrowwise() is like group_by() in that divides the data table by row and applies the same operation to each row.\n\n\n\n\n\n\nIdentifying Regular Season Games\nThe column Type records whether the game was an exhibition game (Type=\"EX\"), non-conference game (Type=\"NC\"), or a conference game. In the case of a conference game, Type records an abbreviation of the conference.\n\ntable(wd1hockey_2024_2025$Type)\n\n\n AH  EC  EX  HE  NC NEW  WC \n 60 132  14 135 246 112 112 \n\n\nWe will fit our Bradley-Terry model using data only from the regular season, which includes conference tournaments and mid-season tournaments6 but not the NCAA Tournament, which is used to determine the national championship. The column Notes, in addition to recording the result of shootouts, also records whether the game was part of a tournament.\n\nreg_season &lt;-\n  wd1hockey_2024_2025 |&gt;\n  dplyr::filter(Type != \"EX\" & !grepl(\"NCAA W Tournament\", Notes)) \n\nOf the 787 games in reg_season, there are 18 that ended in ties. We will remove these from our analysis7\n\nno_ties &lt;-\n  reg_season |&gt; dplyr::filter(!is.na(Home_Winner))",
    "crumbs": [
      "Lecture 12: Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture12.html#sec-bt-est",
    "href": "lectures/lecture12.html#sec-bt-est",
    "title": "Lecture 12: Bradley-Terry Models",
    "section": "Fitting Bradley-Terry Models",
    "text": "Fitting Bradley-Terry Models\nWe will use the package BradleyTerry2 to fit Bradley-Terry models in R. You can install the package using\n\ndevtools::install_github(\"hturner/BradleyTerry2\")\n\n\nRe-formatting Our Data\nThe package BradleyTerry2 uses a somewhat idiosyncratic syntax. Instead of passing a table with one row per game, it instead wants each row to correspond to a home team-away team pair. It further encodes the match results using a pair of numbers, the number of home team wins and the number of away team wins. In the following code block, we first divide the data into subgroups corresponding to unique pairs of home and away teams. Then, within each of those subgroups, we count up the number of home team and away team wins.\n\nunik_teams &lt;- sort(unique(c(no_ties$Home, no_ties$Opponent)))\n\n\nresults &lt;-\n  no_ties |&gt;\n  dplyr::rename(home.team = Home, away.team = Opponent) |&gt;\n1  dplyr::group_by(home.team, away.team) |&gt;\n  dplyr::summarise(\n2    home.win = sum(Home_Winner),\n    away.win = sum(Opp_Winner), .groups = 'drop') |&gt;\n  dplyr::mutate(\n3    home.team = factor(home.team, levels = unik_teams),\n    away.team = factor(away.team,levels = unik_teams))\n\n\n1\n\nSubdivide data table of all games by home team - away team pairs\n\n2\n\nCount the number of home and away team wins in each subgroup\n\n3\n\nConvert home and away team identities into factor variables.\n\n\n\n\nTo better understand the structure of results, we can take a look at a few rows corresponding to Wisconsin’s home games.\n\nset.seed(123)\nresults |&gt; dplyr::filter(home.team == \"Wisconsin\") |&gt; dplyr::slice_sample(n=5)\n\n# A tibble: 5 × 4\n  home.team away.team       home.win away.win\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt;\n1 Wisconsin St. Cloud State        2        0\n2 Wisconsin St. Thomas             2        0\n3 Wisconsin Minnesota              3        0\n4 Wisconsin Ohio State             1        1\n5 Wisconsin Lindenwood             2        0\n\n\nThe data table records that Wisconsin played Ohio State at home twice, winning one game and losing one game. Similarly, Wisconsin played Minnesota three times, winning all three games.\n\n\nEstimating & Extracting Team Strengths\nWe’re now ready to estimate the latent strength parameters using the function BTm from the BradleyTerry2 package.\n\nfit &lt;-\n  BradleyTerry2::BTm(\n1    outcome = cbind(home.win, away.win),\n2    player1 = home.team, player2 = away.team,\n3    refcat = \"Assumption\",\n4    data = results)\n\n\n1\n\nThe outcome argument needs to be a two-column matrix giving the number of home & away team wins for pair of home & away teams.\n\n2\n\nWe use player1 and player2 to indicate the home and way teams\n\n3\n\nWe manually specify the reference team for whom we set \\(\\lambda = 0.\\)\n\n4\n\nBTm looks for the variables referenced in the outcome, player1, and player2 arguments within the data table passed in the data argument.\n\n\n\n\nThe function BTabilities() returns a matrix with columns containing the estimated value of \\(\\lambda\\) for each team and the associated standard error. Taking a look at the first few rows, we can verify that BTm set Assumption College’s latent strength to be 0. We further see that the model estimates the next several teams as having very large \\(\\lambda\\)’s, suggesting that they are very likely to beat Assumption.\n\nlambda_hat &lt;- BradleyTerry2::BTabilities(fit)\nlambda_hat[1:5,]\n\n                   ability     s.e.\nAssumption        0.000000 0.000000\nBemidji State     3.861526 1.282782\nBoston College    5.537068 1.230659\nBoston University 5.786624 1.226730\nBrown             4.798288 1.217776\n\n\nWisconsin, Ohio State, Cornell, and Minnesota all have even larger positive estimated strengths. This is not especially surprising, as these are four of the better teams in the country[^frozenfour].\n\nlambda_hat[c(\"Wisconsin\", \"Ohio State\", \"Cornell\", \"Minnesota\"),]\n\n            ability     s.e.\nWisconsin  9.341843 1.430356\nOhio State 7.847175 1.294598\nCornell    7.242125 1.258623\nMinnesota  7.346209 1.268487\n\n\nFigure 1 visualizes the estimated latent strength of each team relative to Assumption. We see the vast majority of the estimates are positive and many of the 95% confidence intervals do not cross 0, indicating that most teams are favored against Assumption.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\noi_colors &lt;- palette.colors(palette = \"Okabe-Ito\")\n\n1n_teams &lt;- nrow(lambda_hat)\ny_limit &lt;- range(c(lambda_hat[,1] - 2*lambda_hat[,2], lambda_hat[,1] + 2*lambda_hat[,2]))\n\n2ix &lt;- order(lambda_hat[,1])\nplot(1, type = \"n\", \n     main = \"Latent strength (relative to Assumption)\",\n     xlim = c(0, n_teams), ylim = y_limit,\n     xaxt = \"n\", xlab = \"\",\n     ylab = \"Estimated lambda\")\nabline(h = 0, col = oi_colors[2], lwd = 0.5, lty = 2)\nfor(i in 1:n_teams){\n  lines(x = c(i,i), \n3        y = lambda_hat[ix[i],\"ability\"] + c(-2,2) * lambda_hat[ix[i], \"s.e.\"],\n        col = oi_colors[9], lwd = 0.5)\n  points(x = i, y = lambda_hat[ix[i],\"ability\"], pch = 16, cex = 0.5, col = oi_colors[1])\n    team_name &lt;- rownames(lambda_hat)[ix[i]]\n  abbr &lt;- \n    team_abbr |&gt;\n    dplyr::filter(team == team_name) |&gt; dplyr::pull(abbr)\n  if(i %% 2 == 0){\n      text(x = i, \n       y = 0.25 + lambda_hat[ix[i], \"ability\"]  + 2 * lambda_hat[ix[i], \"s.e.\"],\n       labels = abbr, cex = 0.7)\n  } else{\n      text(x = i, \n       y = -0.25 + lambda_hat[ix[i], \"ability\"]  - 2 * lambda_hat[ix[i], \"s.e.\"],\n       labels = abbr, cex = 0.7)\n  }\n}\n\n\n1\n\nGet the number of teams and range of \\(\\hat{\\lambda}\\)’s\n\n2\n\nSort the \\(\\hat{\\lambda}\\)’s in increasing order and get the indices\n\n3\n\nConfidence interval for \\(\\lambda_{j}\\) formed by taking the point estimate \\(\\pm 2\\) standard errors\n\n\n\n\n\n\n\n\n\n\nFigure 1: Most teams are favored against the reference team (Assumption College)\n\n\n\n\n\nInstead of using Assumption College as the reference, we can re-center our estimated \\(\\hat{\\lambda}_{j}\\)’s using the function update()\n\nfit_nh &lt;- update(fit, refcat = \"New Hampshire\")\nlambda_hat_nh &lt;- BradleyTerry2::BTabilities(fit_nh)\nlambda_hat_nh[c(\"Assumption\", \"New Hampshire\", \"Wisconsin\", \"Ohio State\"),]\n\n                ability      s.e.\nAssumption    -4.662547 1.2191462\nNew Hampshire  0.000000 0.0000000\nWisconsin      4.679297 0.9932217\nOhio State     3.184629 0.7887300\n\n\nVisualizing these shifted \\(\\hat{\\lambda}_{j}\\)’ we find that some teams are significantly stronger than New Hampshire and others are substantially weaker.\n\n\n\n\n\n\n\n\nFigure 2: Some teams are significantly stronger and some teams are significantly weaker than New Hampshire\n\n\n\n\n\n\n\nSimulating a Best-of-5 Series\nAccording to our model estimates, the probability that Wisconsin beats Ohio State in a single game is about 81%\n\n1/(1 + exp(-1 * (lambda_hat[\"Wisconsin\", \"ability\"] - lambda_hat[\"Ohio State\", \"ability\"])))\n\n[1] 0.8167779\n\n\nWe can use a simulation to estimate the probability that Wisconsin would defeat Ohio State in a best-of-5 series. We first develop code to simulate the series, recording the eventual winner and the number of games needed to be played. For this simulation, we use a while() loop in which each iteration involves simulating a single game. While running this loop, we keep a running tally of each teams’ wins (wi_wins and osu_wins) as well as the number of games played game_counter. We want to iterate through the loop until either (i) wi_wins == 3 & osu_wins &lt; 3 (Wisconsin wins the series) or (ii) osu_wins == 3 & wi_wins &lt; 3 (Ohio State wins the series).\n\n1wi_wins &lt;- 0\nosu_wins &lt;- 0\n\nwi_prob &lt;- \n2  1/(1 + exp(-1 * (lambda_hat[\"Wisconsin\", \"ability\"] - lambda_hat[\"Ohio State\", \"ability\"])))\n\n3game_counter &lt;- 0\noutcomes &lt;- rep(NA, times= 5)\nset.seed(481)\n4while( wi_wins  &lt; 3 & osu_wins &lt; 3 & game_counter &lt; 5){\n  game_counter &lt;- game_counter + 1\n5  outcomes[game_counter] &lt;- rbinom(n = 1, size = 1, prob = wi_prob)\n  \n6  if(outcomes[game_counter] == 1) wi_wins &lt;- wi_wins + 1\n  else if(outcomes[game_counter] == 0) osu_wins &lt;- osu_wins + 1\n\n}\nif(wi_wins == 3){ \n  winner &lt;- \"Wisconsin\"\n} else if(osu_wins == 3){\n  winner &lt;- \"Ohio State\"\n} else{\n  winner &lt;- NA\n}\n \ncat(\"Series ended after\", game_counter, \" games. Winner = \", winner, \"\\n\")\ncat(\"Wisconsin: \", wi_wins, \" Ohio State: \", osu_wins, \"\\n\")\ncat(\"Game results:\", outcomes, \"\\n\")\n\n\n1\n\nInitialize the count of each teams’ wins to be 0\n\n2\n\nP(Wisconsin beats Ohio State)\n\n3\n\nInitialize the number of games played to 0. This counter is incremented in every loop iteration\n\n4\n\nWe simulate a game only if the series is not yet decided\n\n5\n\nSimulate outcome of the match: outcome = 1 means Wisconsin won and outcome = 0 means Ohio State won\n\n6\n\nIncrement running tallies of each teams’ wins based on outcome\n\n\n\n\nSeries ended after 4  games. Winner =  Wisconsin \nWisconsin:  3  Ohio State:  1 \nGame results: 1 1 0 1 NA \n\n\nIn this single simulation, Wisconsin loses the first game, wins the second and third game, loses the fourh game, and wins the fifth game. To estimate the probability that Wisconsin wins a 5 game series, we can repeat this simulation many, many times. In the following code block, we create a data frame series_results whose rows corresponding to individual simulation replications and which contains two columns, one recording the winner and the other recording the number of games played to determine the winner.\n\nn_sims &lt;- 5000\nseries_results &lt;- data.frame(Winner = rep(NA, times = n_sims), Games = rep(NA, times = n_sims))\n\n\nwi_prob &lt;- \n1  1/(1 + exp(-1 * (lambda_hat[\"Wisconsin\", \"ability\"] - lambda_hat[\"Ohio State\", \"ability\"])))\n\nfor(r in 1:n_sims){\n2  wi_wins &lt;- 0\n  osu_wins &lt;- 0\n  game_counter &lt;- 0\n  winner &lt;- NA\n  \n  set.seed(479+r)\n  while(wi_wins &lt; 3 & osu_wins &lt; 3 & game_counter &lt; 5){\n    game_counter &lt;- game_counter + 1\n    outcome &lt;- rbinom(n = 1, size = 1, prob = wi_prob)\n    \n3    if(outcome == 1) wi_wins &lt;- wi_wins+1\n    else osu_wins &lt;- osu_wins &lt;- osu_wins + 1\n  }\n  \n  series_results[r, \"Games\"] &lt;- game_counter\n  if(wi_wins == 3) series_results[r, \"Winner\"] &lt;- \"Wisconsin\"\n  else if(osu_wins == 3) series_results[r, \"Winner\"] &lt;- \"Ohio State\"\n}\n\n\n1\n\nSince \\(\\mathbb{P}(\\textrm{Wisconsin beats Ohio State})\\) does not change across simulation iterations, we define this outside the main loop\n\n2\n\nAt the beginning of each simulation, we need to re-initialize the running tallies of each team’s wins and the number of games played\n\n3\n\nSince we’re only interested in the ultimate winner and number of games played, there is no need to save the individual match outcomes. But if you want to estimate the probability that Wisconsin wins the series but loses the first game, you’ll need to save individual match outcomes.\n\n\n\n\nAcross the 5000 simulated series, Wisconsin won about 95% (4768/5000). We further estimate that the series ends in 3 games with probability 55% (2767/5000), 4 games with probability 32% (1585/5000), and 5 games with probability 13% (648/5000).\n\ntable(series_results$Winner)\n\n\nOhio State  Wisconsin \n       232       4768 \n\ntable(series_results$Games)\n\n\n   3    4    5 \n2767 1585  648",
    "crumbs": [
      "Lecture 12: Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture12.html#looking-ahead",
    "href": "lectures/lecture12.html#looking-ahead",
    "title": "Lecture 12: Bradley-Terry Models",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nSome of the games in our dataset were played at team’s home venue (e.g., La Bahn arean here in Madison) while other games were played a neutral venue (e.g., the championship round of the Beanpot, which is typically played at T.D. Garden in Boston). Our initial Bradley-Terry model does not account for any potential home-ice advantage. In Lecture 13, we will add an term to our model that is 1 if the a game was played at one of the teams’ home and is 0 if it was played a netural site.\nWe will then simulate a more complex, single-elimination tournament and use the simulation to estimated probabilities of events like “Wisconsin reaches the semi-finals”. To facilitate this, we will save the data table no_ties for future use\n\nsave(no_ties, file = \"wd1hockey_regseason_2024_2025.RData\")",
    "crumbs": [
      "Lecture 12: Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture12.html#footnotes",
    "href": "lectures/lecture12.html#footnotes",
    "title": "Lecture 12: Bradley-Terry Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe full game is available to view here.↩︎\nIn a “best of” series, teams play each other until one has won a majority of the contests. So, in a “best-of-5” series, the first team to reach 3 wins is declared the series winner. For more, check out the Wikipedia entry.↩︎\nThat is, manually computing 1/(1 + exp(-1 * (lambda[i] - lambda[j]))) for every pair of (i,j).↩︎\nMany competitions features a round-robin “group stage” with the top-2 finishers for each group advancing to a single-elimination tournament to decide the champion.↩︎\nSee here for the 2024-25 results↩︎\nThe Beanpot is a good example: it is a tournament played by teams from schools in the Greater Boston area.↩︎\nThere are extensions of the Bradley-Terry model that include ties. But the software we’ll be using in this class does not yet support fitting such extensions. So, we will exclude these games from our analysis↩︎",
    "crumbs": [
      "Lecture 12: Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture14.html",
    "href": "lectures/lecture14.html",
    "title": "Lecture 14: Markov Chains I",
    "section": "",
    "text": "In Lecture 6, we introduced a state-based representation of baseball: each half-inning consists of several at-bats, each of which can be characterized by (i) the number of outs and (ii) configuration of the base-runners. We then estimated the expected number of runs that team can expect to score following at-bats that begin in each of the 24 combinations of outs and base-runner configurations. Today, we will focus less on evaluating what happens in each at-bat (e.g., using run expectancy or run value) and more on how the game transitions from state-to-state.\nTo motivate our work, let’s reconsider our running example from Lecture 6, the March 20, 2024 game between the Dodgers and Padres. Specifically, let’s load the table atbat2024 created in Lecture 7 and look at the sequence of game states that the Dodgers visited during the 8th inning.\n\nload(\"atbat2024.RData\")\ndodgers_inning &lt;- \n  atbat2024 |&gt;\n  dplyr::filter(game_pk == 745444 & inning == 8 & inning_topbot == \"Top\")\ndodgers_inning |&gt; dplyr::select(at_bat_number, Outs, BaseRunner, end_events, end_Outs, end_BaseRunner)\n\n# A tibble: 8 × 6\n  at_bat_number  Outs BaseRunner end_events      end_Outs end_BaseRunner\n          &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;         \n1            59     0 000        walk                   0 100           \n2            60     0 100        single                 0 110           \n3            61     0 110        walk                   0 111           \n4            62     0 111        sac_fly                1 110           \n5            63     1 110        fielders_choice        1 110           \n6            64     1 110        single                 1 110           \n7            65     1 110        single                 1 110           \n8            66     1 110        double_play            3 &lt;NA&gt;          \n\n\nWe see that the inning consisted of 8 at-bats. During the first at-bat, in which Max Muncy was walked, the game transitioned from the state 0.000 (i.e., no outs and no runners on base) to the state 0.100 (i.e., no outs and a runner on 1st). During the 6th at-bat, when Mookie Betts drove in a run off a single, the game state did not change: at the beginning and end of the at-bat there was 1 out and runners on 1st and 2nd base.\nIf we were to replay this half-inning over and over again, how often would the half-inning end after 8 at-bats? How often would the the first at-bat result in a batter reaching first base? How unusual is it to reach the state 0.111 from the state 0.110?\nWe will introduce a simple probabilistic model of these state transitions — a Markov chain — that allows us to answer such questions. We briefly introduce the relevant mathematical details about Markov chains in Section 2 and work through some simple examples of Markov chains with two (Section 2.1) and three (Section 2.3) states. Then in Section 3, we build a Markov chain model to simulate an entire half-inning of baseball. Using this simulation, we study the distributions of the number of at-bats in a half-inning (Section 3.3) and the number of runs scored in a half-inning (Section 3.4).\n\n\nThe first at-bat in any half-inning of baseball has a deterministic game state: it always in the state 0.000, with no outs and no runners on base. The starting state of the next at-bat is, in contrast, non-deterministic. Sometimes the first batter gets out out, in which the game transitions to the state 1.000. But other times, the first batter gets base on with single (0.1000), double (0.010), or triple (0.001). More rarely, the batter scores a home run and the game state remains at 0.000. Put another way, although the first game state visited in a half-inning is deterministic, the second game state visited is random.\nFrom each possible second state, the game could move into multiple possible third states. Continuing this logic until the end of the half-inning, when the game state has Outs = 3 and BaseRunner is undefined, the whole sequence of states visited is stochastic. If a team were to play the half-inning over and over again, the exact number of states visited (i.e., at-bats) and the precise sequence might vary from repetition to repetition.\nIn our data table atbat2024, the variable GameState records the value of \\(G_{t}\\) and was formed by concatenating the variables Outs and BaseRunner. In the code below, we create the variable end_GameState recording the ending state of each at-bat. We also create a table of all 25 unique states.\n\natbat2024 &lt;-\n  atbat2024 |&gt;\n  dplyr::mutate(\n1    end_BaseRunner = ifelse(end_Outs == 3, \"000\", end_BaseRunner),\n    end_GameState = paste(end_Outs, end_BaseRunner, sep = \".\"))\n\n\n1\n\nFor simplicity, we will set end_BaseRunner = \"000\" when end_Outs == 3 (i.e., the inning is over).\n\n\n\n\nIt will later be convenient for us to keep a table listing the single game states as well as the corresponding number of outs and baserunner configurations.\n\nouts &lt;- c(0,1,2, 3)\nbr &lt;- c(\"000\", \"100\", \"010\", \"001\", \"110\", \"101\", \"011\", \"111\")\nunik_states &lt;-\n  expand.grid(Outs = outs, BaseRunner = br) |&gt;\n  as.data.frame() |&gt;\n1  dplyr::filter(Outs &lt;= 2 | Outs == 3 & BaseRunner == \"000\") |&gt;\n  dplyr::arrange(Outs) |&gt;\n  dplyr::mutate(\n    GameState = paste(Outs, BaseRunner, sep = \".\")) |&gt; \n  dplyr::select(Outs, BaseRunner, GameState)\n\n\n1\n\nRemove impossible combinations like 3 outs and runners on 1st and 2nd base.",
    "crumbs": [
      "Lecture 14: Markov Chains I"
    ]
  },
  {
    "objectID": "lectures/lecture14.html#sec-overview",
    "href": "lectures/lecture14.html#sec-overview",
    "title": "Lecture 14: Markov Chains I",
    "section": "",
    "text": "In Lecture 6, we introduced a state-based representation of baseball: each half-inning consists of several at-bats, each of which can be characterized by (i) the number of outs and (ii) configuration of the base-runners. We then estimated the expected number of runs that team can expect to score following at-bats that begin in each of the 24 combinations of outs and base-runner configurations. Today, we will focus less on evaluating what happens in each at-bat (e.g., using run expectancy or run value) and more on how the game transitions from state-to-state.\nTo motivate our work, let’s reconsider our running example from Lecture 6, the March 20, 2024 game between the Dodgers and Padres. Specifically, let’s load the table atbat2024 created in Lecture 7 and look at the sequence of game states that the Dodgers visited during the 8th inning.\n\nload(\"atbat2024.RData\")\ndodgers_inning &lt;- \n  atbat2024 |&gt;\n  dplyr::filter(game_pk == 745444 & inning == 8 & inning_topbot == \"Top\")\ndodgers_inning |&gt; dplyr::select(at_bat_number, Outs, BaseRunner, end_events, end_Outs, end_BaseRunner)\n\n# A tibble: 8 × 6\n  at_bat_number  Outs BaseRunner end_events      end_Outs end_BaseRunner\n          &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;         \n1            59     0 000        walk                   0 100           \n2            60     0 100        single                 0 110           \n3            61     0 110        walk                   0 111           \n4            62     0 111        sac_fly                1 110           \n5            63     1 110        fielders_choice        1 110           \n6            64     1 110        single                 1 110           \n7            65     1 110        single                 1 110           \n8            66     1 110        double_play            3 &lt;NA&gt;          \n\n\nWe see that the inning consisted of 8 at-bats. During the first at-bat, in which Max Muncy was walked, the game transitioned from the state 0.000 (i.e., no outs and no runners on base) to the state 0.100 (i.e., no outs and a runner on 1st). During the 6th at-bat, when Mookie Betts drove in a run off a single, the game state did not change: at the beginning and end of the at-bat there was 1 out and runners on 1st and 2nd base.\nIf we were to replay this half-inning over and over again, how often would the half-inning end after 8 at-bats? How often would the the first at-bat result in a batter reaching first base? How unusual is it to reach the state 0.111 from the state 0.110?\nWe will introduce a simple probabilistic model of these state transitions — a Markov chain — that allows us to answer such questions. We briefly introduce the relevant mathematical details about Markov chains in Section 2 and work through some simple examples of Markov chains with two (Section 2.1) and three (Section 2.3) states. Then in Section 3, we build a Markov chain model to simulate an entire half-inning of baseball. Using this simulation, we study the distributions of the number of at-bats in a half-inning (Section 3.3) and the number of runs scored in a half-inning (Section 3.4).\n\n\nThe first at-bat in any half-inning of baseball has a deterministic game state: it always in the state 0.000, with no outs and no runners on base. The starting state of the next at-bat is, in contrast, non-deterministic. Sometimes the first batter gets out out, in which the game transitions to the state 1.000. But other times, the first batter gets base on with single (0.1000), double (0.010), or triple (0.001). More rarely, the batter scores a home run and the game state remains at 0.000. Put another way, although the first game state visited in a half-inning is deterministic, the second game state visited is random.\nFrom each possible second state, the game could move into multiple possible third states. Continuing this logic until the end of the half-inning, when the game state has Outs = 3 and BaseRunner is undefined, the whole sequence of states visited is stochastic. If a team were to play the half-inning over and over again, the exact number of states visited (i.e., at-bats) and the precise sequence might vary from repetition to repetition.\nIn our data table atbat2024, the variable GameState records the value of \\(G_{t}\\) and was formed by concatenating the variables Outs and BaseRunner. In the code below, we create the variable end_GameState recording the ending state of each at-bat. We also create a table of all 25 unique states.\n\natbat2024 &lt;-\n  atbat2024 |&gt;\n  dplyr::mutate(\n1    end_BaseRunner = ifelse(end_Outs == 3, \"000\", end_BaseRunner),\n    end_GameState = paste(end_Outs, end_BaseRunner, sep = \".\"))\n\n\n1\n\nFor simplicity, we will set end_BaseRunner = \"000\" when end_Outs == 3 (i.e., the inning is over).\n\n\n\n\nIt will later be convenient for us to keep a table listing the single game states as well as the corresponding number of outs and baserunner configurations.\n\nouts &lt;- c(0,1,2, 3)\nbr &lt;- c(\"000\", \"100\", \"010\", \"001\", \"110\", \"101\", \"011\", \"111\")\nunik_states &lt;-\n  expand.grid(Outs = outs, BaseRunner = br) |&gt;\n  as.data.frame() |&gt;\n1  dplyr::filter(Outs &lt;= 2 | Outs == 3 & BaseRunner == \"000\") |&gt;\n  dplyr::arrange(Outs) |&gt;\n  dplyr::mutate(\n    GameState = paste(Outs, BaseRunner, sep = \".\")) |&gt; \n  dplyr::select(Outs, BaseRunner, GameState)\n\n\n1\n\nRemove impossible combinations like 3 outs and runners on 1st and 2nd base.",
    "crumbs": [
      "Lecture 14: Markov Chains I"
    ]
  },
  {
    "objectID": "lectures/lecture14.html#sec-markov-chain",
    "href": "lectures/lecture14.html#sec-markov-chain",
    "title": "Lecture 14: Markov Chains I",
    "section": "Markov Chains",
    "text": "Markov Chains\nFormally, let us denote the game state at the start of at-bat \\(t\\) of a half-inning as \\(G_{t}.\\) There are 25 possible values for each \\(G_{t}\\): the 24 different combinations of Outs (0,1, and 2) and BaseRunner (“000”, “100”, “010”, “001”, “110”, “101”, “011”, and “111”) and the one state corresponding to the end of the half-inning. As discussed in Section 1, although \\(G_{1}\\) is always equal to 0.000, the remaining \\(G_{t}\\)’s are random and the length of the sequence of states visited in the half-inning is also random.\nFormally, this involves coming up with a joint probability distribution for the (possibly infinite) sequence \\(G_{1}, G_{2}, \\ldots.\\) That is, for every possible sequence of states visited, we would need to assign a probability. Once we do this, we could compute things like the probability of a half-inning lasting at least 5 at-bats by finding all sequences of length 5 or more and adding up the corresponding probabilities[^equiv]. Similarly, if we wanted to know the probability that the half-inning began by visiting the states 0.000, 0.100, 0.110, and 0.111, we would find all sequences with that starting sub-sequence and add up the probabilities.\nOf course, calculating the probability for every possible sequence of every possible length is practically impossible. Luckily, it turns out that the probability distribution of a sequence of random variables \\(\\{X_{t}\\}\\) is fully determined by the sequence of conditional distributions of the form \\(X_{t} \\vert X_{t-1}, \\ldots, X_{1}.\\) In other words, it suffices to specify a probability distribution over the next state visited based on its past history.\nA Markov chain is a probabilistic model in which the probability of the next state visited depends only on the current state and not all the previous states.\n\n\n\n\n\n\nDefinition: Markov Chain\n\n\n\nAn sequence of random variables \\(\\left\\{X_{t}\\right\\}_{n = 1}^{\\infty}\\) is called a Markov chain if for all \\(t \\geq 1\\), all sets \\(A,\\) and trajectories \\(x_{1}, \\ldots, x_{t-1}\\) \\[\n\\mathbb{P}(X_{t} \\in A \\vert X_{t-1} = x_{t-1}, \\ldots, X_{1} = x_{1}) = \\mathbb{P}(X_{t} \\in A \\vert X_{t-1} = x_{t-1}).\n\\]\n\n\nWhen the set of states is discrete (e.g., out-baserunner combinations), Markov chains are fully characterized by a transition probability matrix. If there are \\(S\\) different states, labelled without loss of generality as \\(s = 1, \\ldots, S,\\) then the \\((s,s')\\) entry of the transition probability matrix is exactly \\(\\mathbb{P}(X_{t} = s' \\vert X_{t-1} = s)\\), the probability of transitioning from state \\(s\\) to state \\(s'.\\)\n\nExample: A 2-state Markov Chain\nSuppose a system exists in one of two states 1 and 2. Further suppose that    * When the system is in state1, it can remain there with probability 0.9 and it can move to state2with probability 0.1.   * When the system is in state2, it remains there with probability 0.2 and it moves to state1` with probability 0.8.\nIf we let \\(X_{t}\\) be the state of the system at time \\(t,\\) the transition matrix for the Markov chain \\(\\{X_{t}\\}\\) is \\[\n\\begin{pmatrix}\n0.9 & 0.1 \\\\\n0.8 & 0.2\n\\end{pmatrix}\n\\] The following code simulate 10 time steps from this Markov chain starting from the state 2\n\nset.seed(129) \n1states &lt;- c(1,2)\n2transition_matrix &lt;- matrix(c(0.9, 0.1, 0.8, 0.2), nrow = 2, ncol = 2, byrow = TRUE)\n\nn_steps &lt;- 10\ninit_state &lt;- 2\nstates_visited &lt;- rep(NA, times = n_steps)\n\nstates_visited[1] &lt;- init_state\n\nfor(t in 2:n_steps){\n3  prev_state &lt;- states_visited[t-1]\n  probs &lt;- transition_matrix[prev_state,]\n4  next_state &lt;- sample(states, size = 1, prob = probs)\n  states_visited[t] &lt;- next_state\n}\n\nstates_visited\n\n\n1\n\nCollection of all possible states\n\n2\n\nSetting byrow = TRUE forces R to build the matrix in row-major order instead of column-major (see this Wikipedia entry)\n\n3\n\nLook up the current state and probabilities of moving to every other state from it. Note that this is a row of the transition matrix.\n\n4\n\nSample the next state based on the relevant row of the transition matrix\n\n\n\n\n [1] 2 1 1 1 1 1 1 1 1 2\n\n\nIn this example, we see that the chain immediately transitions from state 2 to state 1; remains in state 2 for 8 steps; and then return to state 2.\n\n\nAbsorbing States\nIn our two-state example, the chain is able to transition from state 1 to state 2 and from state 2 to state 1. We consequently say that states 1 and 2 communicate with one another.\nSuppose that \\(\\{X_{t}\\}\\) is a Markov chain defined over the states \\(\\{1, 2, \\ldots, S\\}\\) and let \\(\\boldsymbol{\\mathbf{P}}\\) be its transition matrix.\n\n\n\n\n\n\nDefinition: Absorbing State\n\n\n\nA state \\(s\\) is called an absorbing state if for all \\(s' \\neq s,\\) \\(\\mathbb{P}(X_{t} = s' \\vert X_{t-1} = s) = 0\\) and \\(\\mathbb{P}(X_{t} = s \\vert X_{t-1} = s) = 1.\\)\n\n\nThat is, an absorbing state is one from which there are no out-going transitions: once the chain reaches an absorbing state, it cannot leave. In our baseball analysis, the state 3.000 corresponding to the end of the half-inning is an absorbing.\nAs an illustration, here is the transition matrix for 3-state Markov chain model with one absorbing state (3) \\[\n\\begin{pmatrix}\n0.5 & 0.25 & 0.25 \\\\\n0.25 & 0.25 & 0.5 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\] If the chain is currently in state 1, it (i) remains in state 1 50% of the time and (ii) moves to states 2 25% of the time; and (iii) moves to state 3 25% of the time. If the chain is currently in state 2, it (i) moves to state 1 25% of the time; (ii) remains in state 2 25% of the time; and moves to state 3 50% of the time.\nThe following code simulates 10 steps of the Markov chain beginning in state 1.\n\nset.seed(129) \nstates &lt;- c(1,2, 3)\ntransition_matrix &lt;- \n  matrix(c(0.25, 0.5, 0.25,\n           0.25, 0.25, 0.5,\n           0, 0, 1), nrow = 3, ncol = 3, byrow = TRUE)\n\nn_steps &lt;- 10\ninit_state &lt;- 1\nstates_visited &lt;- rep(NA, times = n_steps)\n\nstates_visited[1] &lt;- init_state\n\nfor(t in 2:n_steps){\n  prev_state &lt;- states_visited[t-1]\n  probs &lt;- transition_matrix[prev_state,]\n  next_state &lt;- sample(states, size = 1, prob = probs)\n  states_visited[t] &lt;- next_state\n}\n\nstates_visited\n\n [1] 1 2 3 3 3 3 3 3 3 3\n\n\nIn this particular simulation, the chain begins in state 1, moves to state 2, and then moves to the absorbing state 3. The following code block repeats the simulation 10 times, each time starting at state 1 and simulating 10 steps of the Markov chain. Sometimes, the chain immediately moves from 1 to the absorbing state 3. But other times, the chain hops between states 1 and 2 before eventually being absorbed.\n\nn_sims &lt;- 10\nn_steps &lt;- 10\ninit_state &lt;- 1\nstates_visited &lt;- rep(NA, times = n_steps)\n\nfor(r in 1:n_sims){\n1  set.seed(479+r)\n  states_visited &lt;- rep(NA, times = n_steps)\n  states_visited[1] &lt;- init_state\n  for(t in 2:n_steps){\n2    states_visited[t] &lt;-\n      sample(states, size = 1,\n              prob = transition_matrix[states_visited[t-1],])\n  }\n3  cat(\"Simulation \", r, \":\", states_visited, \"\\n\")\n}\n\n\n1\n\nSetting seed ensures reproducibility. Allowing the seed to change systematically with replication number ensures we don’t get the exact same results across replications.\n\n2\n\nThere is actually no need to introduce the intermediate variables prev_state and next_state\n\n3\n\nPrints out the sequence of states visited\n\n\n\n\nSimulation  1 : 1 3 3 3 3 3 3 3 3 3 \nSimulation  2 : 1 2 3 3 3 3 3 3 3 3 \nSimulation  3 : 1 2 2 3 3 3 3 3 3 3 \nSimulation  4 : 1 2 3 3 3 3 3 3 3 3 \nSimulation  5 : 1 2 2 1 1 2 3 3 3 3 \nSimulation  6 : 1 2 3 3 3 3 3 3 3 3 \nSimulation  7 : 1 2 3 3 3 3 3 3 3 3 \nSimulation  8 : 1 3 3 3 3 3 3 3 3 3 \nSimulation  9 : 1 2 1 1 1 1 2 3 3 3 \nSimulation  10 : 1 1 3 3 3 3 3 3 3 3 \n\n\n\n\nExample: Simulating Time to Absorption\nIn our 3-state example, let \\(T^{A}_{1}\\) be the number of steps the chain takes until it hits state 3 starting from state 1: \\[\nT^{A}_{1} = \\min\\left\\{t \\geq 1 : X_{t} = 3\\right\\}.\n\\] We can use simulation to study the distribution of \\(T^{A}_{1}\\) and to estimate quantities like \\(\\mathbb{P}(T^{A}_{1} &gt; 3)\\) or \\(\\mathbb{P}(T^{A}_{1} = 5).\\)\nThe following code block uses a while() loop to simulate the Markov chain for several iterations until either (i) the absorbing state 3 is reached or (ii) some maximum number of iterations is hit1. Unlike our earlier simulation code, we will not save the full trajectory of states visited. Instead, we will only keep track of the number of iterations needed until the chain reaches state 3.\n\n1n_sims &lt;- 1e4\n2max_iterations &lt;- 1e3\nstates &lt;- c(1,2,3)\ntransition_matrix &lt;- \n  matrix(c(0.25, 0.5, 0.25,\n           0.25, 0.25, 0.5,\n           0, 0, 1), nrow = 3, ncol = 3, byrow = TRUE)\nabsorption_time &lt;- rep(NA, times = n_sims)\nfor(r in 1:n_sims){\n3  if(r %% 1000 == 0) print(paste(\"Simulation\", r, \"at\", Sys.time()))\n  \n4  iteration_counter &lt;- 1\n  current_state &lt;- 1\n5  while(current_state != 3 & iteration_counter &lt; max_iterations){\n    \n    current_state &lt;-\n      sample(states, size = 1,\n6             prob = transition_matrix[current_state,])\n    \n7    iteration_counter &lt;- iteration_counter + 1\n  }\n8  if(iteration_counter &lt; max_iterations & current_state == 3){\n    absorption_time[r] &lt;- iteration_counter\n  }\n}\n\n\n1\n\nNumber of simulation runs\n\n2\n\nMaximum number of Markov chain iterations/steps per simulation\n\n3\n\nWhen running large simulations, it’s helpful to print out your progress after a fixed number of iterations (in this case 100). Sys.time() prints the system time.\n\n4\n\nIn each simulation (i.e., iteration of the outer for loop), we need to re-set the counter tracking the number of steps for which we simulate the Markov chain (i.e., iteration counter in the inner while() loop) and re-set the current_state variable to the starting state (in this case, 1).\n\n5\n\nBefore starting each iteration, the while() loop checks that the chain isn’t in state 3 and we haven’t yet hit the maximum number of iterations.\n\n6\n\nR evaluates the whole expression on the right-hand side of the &lt;- before assigning it to whatever is on the left-hand side. So, there is no danger here of simultaneously accessing and over-writing the variable current_state, which appears on both sides.\n\n7\n\nIn a while() loop, it’s imperative to increment the iterator!\n\n8\n\nThe condition fails only when the chain has not reached the absorbing state within the maximum number of allowed iterations (max_iterations).\n\n\n\n\n[1] \"Simulation 1000 at 2025-10-12 10:32:59.9493\"\n[1] \"Simulation 2000 at 2025-10-12 10:32:59.966283\"\n[1] \"Simulation 3000 at 2025-10-12 10:32:59.973903\"\n[1] \"Simulation 4000 at 2025-10-12 10:32:59.987753\"\n[1] \"Simulation 5000 at 2025-10-12 10:32:59.995445\"\n[1] \"Simulation 6000 at 2025-10-12 10:33:00.008812\"\n[1] \"Simulation 7000 at 2025-10-12 10:33:00.016478\"\n[1] \"Simulation 8000 at 2025-10-12 10:33:00.030004\"\n[1] \"Simulation 9000 at 2025-10-12 10:33:00.03737\"\n[1] \"Simulation 10000 at 2025-10-12 10:33:00.044903\"\n\n\nBecause there are only 3 states and the probabilities of transitioning to state 3 from states 1 and 2 are relatively high, simulating the Markov chain until it hits state 3 takes very little time. Tabulating the different values in absorption_time, we see that the chain 1. Immediately transitioned from state 1 to state 3 in one step (so that \\(T^{A}_{1} = 2\\)) in 2440 of the 10,000 simulations 2. Reached state 3 in exactly two steps (so \\(T^{A}_{1} = 2\\)) in 3214 of the 10,000 simulations 3. Reached state 3 after 20 or more steps in just 2 of the 10,000 simulation 4. Always reached state 3 within the maximum number of iterations (as indicated by the fact that there are no NA values in absorption_time)\n\ntable(absorption_time, useNA = 'always')\n\nabsorption_time\n   2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17 \n2511 3209 1701  995  586  395  226  159   83   52   32   20   12    8    6    1 \n  18   19   24 &lt;NA&gt; \n   2    1    1    0",
    "crumbs": [
      "Lecture 14: Markov Chains I"
    ]
  },
  {
    "objectID": "lectures/lecture14.html#sec-half-inning",
    "href": "lectures/lecture14.html#sec-half-inning",
    "title": "Lecture 14: Markov Chains I",
    "section": "A Markov Chain Model for Half-Innings",
    "text": "A Markov Chain Model for Half-Innings\nOur goal is to build a Markov chain model for a half-inning of baseball. To do this, we need to estimate the transition probabilities between every pair of the 24 non-absorbing states and the absorbing state corresponding to the end of the half-inning. Because the 9th inning (and any extra innings) can end before a team has amassed three outs2, we exclude all data from the 9th inning and beyond.\n\natbat2024 &lt;-\n  atbat2024 |&gt;\n  dplyr::filter(inning &lt; 9)\n\n\nEstimating Transition Probabilities\nA natural estimate of the the probability for transitioning from state \\(s\\) to the state $s’ $ is divide the number of at-bats that start in state \\(s\\) and end in state \\(s'\\) by the number of at-bats that start in state \\(s.\\)\nWe first build a table that counts the number of at-bats starting in every state\n\nstart_counts &lt;-\n  atbat2024 |&gt;\n  dplyr::group_by(GameState) |&gt;\n  dplyr::summarise(n_start = dplyr::n(), .groups = \"drop\")\n\nNext, by grouping by each of GameState and end_GameState, we can count the number of times an at-bat started in some state \\(s\\) and ended in state \\(s'.\\)\n\nend_counts &lt;-\n  atbat2024 |&gt;\n  dplyr::group_by(GameState, end_GameState) |&gt;\n  dplyr::summarise(n_start_end = dplyr::n(), .groups = \"drop\")\n\nTo compute our transition probability estimates, we start by enumerating all possible combinations of starting and ending state. Then, we will append columns containing the starting state counts n_start and the counts of each pair of state transitions (i.e., n_start_end). For certain pairs of transitions that do not appear in our data (e.g., 0.000 to 3.000), we will manually set n_start_end = 0. Finally, we will divide n_start_end by n_start\n\ntransitions &lt;-\n  expand.grid(GameState = unik_states$GameState,\n1              end_GameState = unik_states$GameState) |&gt;\n  as.data.frame() |&gt;\n2  dplyr::left_join(y = start_counts, by = \"GameState\") |&gt;\n  dplyr::left_join(y = end_counts, by = c(\"GameState\", \"end_GameState\")) |&gt;\n3  tidyr::replace_na(replace = list(n_start_end=0)) |&gt;\n4  dplyr::mutate(prob = n_start_end/n_start)\n\n\n1\n\nEnumerate all 625 possible combinations of starting and ending states of an at-bat\n\n2\n\nAppend columns with the number of at-bats (i) starting in each state and (ii) starting & ending in each combination of states\n\n3\n\nManually set n_start_end = 0 for transitions that are not observed in the data\n\n4\n\nEstimate the transition probability\n\n\n\n\nIt is perhaps unsurprising to see that some of the highest transition probabilities are from 2-out states (e.g., 2.000 or 2.100) to the absorbing 3-out state.\n\ntransitions |&gt;\n  dplyr::arrange(dplyr::desc(prob)) |&gt;\n  dplyr::slice_head(n = 10) |&gt;\n  dplyr::mutate(prob = round(prob, digits = 3))\n\n   GameState end_GameState n_start n_start_end  prob\n1      2.110         3.000    4797        3350 0.698\n2      2.000         3.000   23149       16118 0.696\n3      2.001         3.000    1975        1369 0.693\n4      1.000         2.000   29062       20103 0.692\n5      2.101         3.000    2431        1673 0.688\n6      0.000         1.000   39733       27316 0.687\n7      2.100         3.000   11962        8203 0.686\n8      2.111         3.000    1631        1112 0.682\n9      2.010         3.000    4840        3258 0.673\n10     2.011         3.000    1164         776 0.667\n\n\nTo form a transition matrix, we will first “widen” the table transitions using the function tiyr::pivot_wider(). We see that each row of the resulting, temporary table, which we call tmp, corresponds to a starting game state value and there are columns for each of the ending game states.\n\ntmp &lt;-\n  transitions |&gt;\n  dplyr::select(GameState, end_GameState, prob) |&gt;\n  tidyr::pivot_wider(names_from = \"end_GameState\",\n                      values_from = \"prob\")\ntmp |&gt; \n  dplyr::select(GameState, `0.000`, `0.100`, `1.000`, `2.000`) |&gt; \n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 5\n   GameState `0.000`  `0.100`  `1.000` `2.000`\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 0.000      0.0316 0.231    0.687    0      \n 2 0.100      0.0312 0.00277  0.00170  0.120  \n 3 0.010      0.0207 0.0573   0.00282  0.00987\n 4 0.001      0.0380 0.179    0.213    0.0114 \n 5 0.110      0.0304 0.000440 0.000440 0.00132\n 6 0.101      0.0358 0.00133  0.00265  0.0491 \n 7 0.011      0.0370 0.0545   0.00654  0      \n 8 0.111      0.0417 0        0.00348  0      \n 9 1.000      0      0        0.0295   0.692  \n10 1.100      0      0        0.0339   0.00300\n\n\nLooking at these selected entries, we find that the estimated transition probability from 0.000 to 2.000 is zero. This makes some intuitive sense as the batting team cannot, with no outs and no runners on base, lost two outs in a single at-bat. We also see that the transition probabilities from 1 out states (e.g., 1.000 and 1.1000) to the no-out states (e.g., 0.000, 0.110, etc.) are also estimated to be zero. Transitions like 0.100 to 0.000 correspond to situations in which the at-bat begins with a runner on first base and ends with the batter hitting a homerun and driving in the runner.\nWe can now turn this table into a matrix row and column names are the possible starting and ending game states, we will drop the column GameState from tmp, convert it into a matrix, and then assign the rownames.\n\nstate_names &lt;- tmp |&gt; dplyr::pull(GameState)\ntransition_matrix &lt;-\n  tmp |&gt;\n  dplyr::select(-GameState) |&gt;\n  as.matrix()\nrownames(transition_matrix) &lt;- state_names\n\nround(transition_matrix[1:5, 1:5], digits = 3)\n\n      0.000 0.100 0.010 0.001 0.110\n0.000 0.032 0.231 0.045 0.005 0.000\n0.100 0.031 0.003 0.014 0.004 0.198\n0.010 0.021 0.057 0.048 0.003 0.093\n0.001 0.038 0.179 0.046 0.008 0.000\n0.110 0.030 0.000 0.011 0.005 0.039\n\n\n\n\nSimulating a Half-Inning\nNow that we have estimated the transition probabilities between all pairs of game states, we are in a position to simulate a single half-inning of baseball. In the following code, we set the initial state to 0.000, since an innings begins with no outs and no runners on base. Then, we use a while() loop to simulate a Markov chain that randomly walks between the states (according to the transition matrix) until it hits the absorbing state 3.000. We set the maximum iterations to be 30, which is an admittedly gross over-estimate on the number of at-bats that can take place in a single half-inning.\n\nmax_iterations &lt;- 30\nstates_visited &lt;- rep(NA, times = max_iterations)\n\niteration_counter &lt;- 1\n1states_visited[1] &lt;- \"0.000\"\n2current_state &lt;- \"0.000\"\nset.seed(479)\nwhile(current_state != \"3.000\" & iteration_counter &lt; max_iterations){\n  \n  current_state &lt;-\n    sample(unik_states$GameState, size = 1,\n           prob = transition_matrix[current_state,])\n3  iteration_counter &lt;- iteration_counter + 1\n4  states_visited[iteration_counter] &lt;- current_state\n}\n5states_visited[1:(iteration_counter)]\n\n\n1\n\nThe first state is always 0.000\n\n2\n\nVariable that keeps track of the current state of the Markov chain.\n\n3\n\nWe increment iteration_counter as soon as we draw the next state\n\n4\n\nSave the next state in states_visited\n\n5\n\nOnly print out the states visited until the inning ends.\n\n\n\n\n[1] \"0.000\" \"0.010\" \"1.010\" \"2.010\" \"2.000\" \"3.000\"\n\n\nThis simulated half-inning consisted of 5 at-bats. The first at-bat started in state 0.000 and ended in 0.010, which means that the first batter in the inning hit a double. The second at-bat started in state 0.010 but ended in 1.010, which means that the second batter got out. Similarly, the third at-bat moved the game from 1.010 to 2.010, which means that the third batter also got out. During the fourth simulated at-bat, however, the game state transitions from 2.010 to 2.000. The only way for this to happen is for the batter to score and drive in the runner who was initially on second.\n\n\nLength of a Half-Inning\nLike we did in Section 2.3, we can use simulation to estimate the distribution of the number of at-bats in a half-inning. Unlike that simulation, however, we will keep track of the whole sequence of states visited instead of the absorption time.\n\nn_sim &lt;- 5e4\nmax_iterations &lt;- 30\nstates_visited &lt;- matrix(NA, nrow = n_sim, ncol = max_iterations)\n\nfor(r in 1:n_sim){\n  set.seed(479+r)\n  states_visited[r,1] &lt;- \"0.000\"\n  current_state &lt;- \"0.000\"\n  iteration_counter &lt;- 1\n  while(current_state != \"3.000\" & iteration_counter &lt; max_iterations){\n    \n    current_state &lt;-\n      sample(x = unik_states$GameState,\n             size = 1, replace = 1,\n             prob = transition_matrix[current_state,])\n    iteration_counter &lt;- iteration_counter+1\n    states_visited[r,iteration_counter] &lt;- current_state\n  }\n}\n\nHere are the first several rows of states_visited, which record the sequence of game states reached in some of our simulations\n\nstates_visited[1:10, 1:8]\n\n      [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]\n [1,] \"0.000\" \"1.000\" \"2.000\" \"3.000\" NA      NA      NA      NA  \n [2,] \"0.000\" \"1.000\" \"2.000\" \"2.100\" \"3.000\" NA      NA      NA  \n [3,] \"0.000\" \"1.000\" \"1.100\" \"2.100\" \"2.011\" \"3.000\" NA      NA  \n [4,] \"0.000\" \"1.000\" \"2.000\" \"3.000\" NA      NA      NA      NA  \n [5,] \"0.000\" \"1.000\" \"1.100\" \"3.000\" NA      NA      NA      NA  \n [6,] \"0.000\" \"1.000\" \"2.000\" \"3.000\" NA      NA      NA      NA  \n [7,] \"0.000\" \"1.000\" \"2.000\" \"3.000\" NA      NA      NA      NA  \n [8,] \"0.000\" \"1.000\" \"2.000\" \"2.100\" \"3.000\" NA      NA      NA  \n [9,] \"0.000\" \"1.000\" \"1.100\" \"2.010\" \"2.100\" \"2.110\" \"3.000\" NA  \n[10,] \"0.000\" \"0.010\" \"1.001\" \"1.100\" \"2.100\" \"3.000\" NA      NA  \n\n\nWe find, for instance, that in simulations 1, 4, 6, and 7, the pitching team retired the batting side in 3 at-bats. In Simulation 5, the last at-bat began in the state 1.100 and ended in 3.000. From this we infer that the last at-bat involved an inning-ending double play.\nTo determine how many at-bats there were in each simulated half-inning, we will first determine which entry in every row is equal to 3.000. In the case of simulation 1, for instance, 3.000 is the 4th entry of row 1. Since the inning ends as soon as the game enters the state 3.000, we need to subtract one from this number to compute the number of at-bats in the half-inning.\n\ninning_length &lt;-\n  apply(states_visited, \n        MARGIN = 1,\n        FUN = function(x){ return(which(x == \"3.000\") - 1)} )\ntable(inning_length)\n\ninning_length\n    3     4     5     6     7     8     9    10    11    12    13    14    15 \n19138 14368  8685  4222  2014   921   401   131    72    31    11     4     2 \n\n\nIn 19,138 of our 50,000 simulations, the inning ended after exactly 3 at-bats. And in 46413 the inning ended with 6 or fewer at-bats. Interestingly, there are two simulations in which the half-inning included 15 at-bats. Here is the sequence of states visited in that first simulation\n\nstates_visited[which(inning_length == 15)[1], 1:15]\n\n [1] \"0.000\" \"0.100\" \"0.110\" \"1.110\" \"1.000\" \"1.100\" \"1.000\" \"1.100\" \"1.000\"\n[10] \"1.000\" \"1.100\" \"1.110\" \"1.101\" \"2.100\" \"2.000\"\n\n\nIn this simulated half-inning, the batting team drove in runs in the 4th, 6th, 7th, 8th, 9th, 13th, and 14th at-bats.\n\n\nRuns Scored in a Half-Inning\nTo count how runs were scored in each of our 50,000 simulated half-innings, we need to determine how many runs were scored during each simulated at-bat. If we let \\(O_{t}\\) and \\(B_{t}\\) be the number of outs and runners at the start of at-bat \\(t\\) and \\(O^{\\star}_{t}\\) and \\(B^{\\star}_{t}\\) be the numbers of outs and runners at the end of at-bat \\(t\\), it is not difficult to verify that the number of runs scored during at-bat \\(t\\) is \\[\n(O_{t} + B_{t} + 1) - (O_{t}^{\\star} + B_{t}^{\\star}).\n\\] As an example, suppose at an at-bat starts in state 1.110 and ends in state 1.000. The only way for such a transition to occur is for the batter to hit a homerun and drive in runs from 1st and 2nd base. That is, three runs are scored during such an at-bat. We verify that \\(O_{t} = 1, B_{t} = 2, O_{t}^{\\star} = 1\\) and \\(B_{t}^{\\star} = 0.\\) So, as expected \\((O_{t} + B_{t} + 1) - (O_{t}^{\\star} + B_{t}^{\\star}) = 3.\\)\nWe can elaborate our Markov chain simulation to count the number of runs scored in each at-bat. To do this, we will need to look up how many outs and baserunners there are based on each game-state. Rather than determining this programmatically, we can manually update the table unik_states.\n\nunik_states &lt;-\n  unik_states |&gt;\n  dplyr::mutate(\n    n_runners = \n      dplyr::case_when(\n        BaseRunner == \"000\" ~ 0,\n        BaseRunner %in% c(\"100\", \"010\", \"001\") ~ 1,\n        BaseRunner %in% c(\"110\", \"101\", \"011\") ~ 2,\n        BaseRunner == \"111\" ~ 3))\n\nWe now elaborate our Markov chain simulation code to (i) get the number of outs and baserunners at the beginning and end of each simulated at-bat and (ii) add the number of runs scored in the at-bat to a running tally of runs scored in the half-inning.\n\n\n\n\n\n\nWarning\n\n\n\nThis code takes several minutes. The lines looking up the number of outs and base-runners corresponding to each game state introduce some computational redundancies.\n\n\n\nn_sim &lt;- 5e4\nmax_iterations &lt;- 30\nstates_visited &lt;- matrix(NA, nrow = n_sim, ncol = max_iterations)\n\n1runs_scored &lt;- rep(NA, times = n_sim)\n\nfor(r in 1:n_sim){\n  set.seed(479+r)\n  if(r %% 5000 == 0) print(paste(\"Simulation\", r, \"at\", Sys.time()))\n  states_visited[r,1] &lt;- \"0.000\"\n  current_state &lt;- \"0.000\"\n  iteration_counter &lt;- 1\n2  runs &lt;- 0\n  \n3  n_outs_start &lt;- 0\n  n_runners_start &lt;- 0\n  n_outs_end &lt;- 0\n  n_runners_end &lt;- 0\n  \n  while(current_state != \"3.000\" & iteration_counter &lt; max_iterations){\n    \n4    n_outs_start &lt;-\n      unik_states |&gt; dplyr::filter(GameState == current_state) |&gt; dplyr::pull(Outs)\n    n_runners_start &lt;-\n      unik_states |&gt; dplyr::filter(GameState == current_state) |&gt; dplyr::pull(n_runners)\n    \n    current_state &lt;-\n      sample(x = unik_states$GameState,\n             size = 1, replace = 1,\n             prob = transition_matrix[current_state,])\n    iteration_counter &lt;- iteration_counter+1\n    states_visited[r,iteration_counter] &lt;- current_state\n    \n    n_outs_end &lt;-\n      unik_states |&gt; dplyr::filter(GameState == current_state) |&gt; dplyr::pull(Outs)\n    n_runners_end &lt;-\n      unik_states |&gt; dplyr::filter(GameState == current_state) |&gt; dplyr::pull(n_runners)\n\n5    runs &lt;-\n      runs +\n      (n_outs_start + n_runners_start + 1) -\n      (n_outs_end + n_runners_end)\n  }\n6  runs_scored[r] &lt;- runs\n}\n\n\n1\n\nContainer to save the number of runs scored in each simulated half-inning\n\n2\n\nVariable that will store the running tally of runs scored\n\n3\n\nTemporary variables storing the numbers of outs and baserunners at the start and end of an at-bat\n\n4\n\nAt the start of a simulated at-bat, get the numbers of outs and runners\n\n5\n\nAt the end of a simulated at-bat, get the numbers of outs and runners\n\n6\n\nAdd the number of runs scored in the current at-bat to the running tally\n\n\n\n\n[1] \"Simulation 5000 at 2025-10-12 10:33:33.501278\"\n[1] \"Simulation 10000 at 2025-10-12 10:34:04.4989\"\n[1] \"Simulation 15000 at 2025-10-12 10:34:35.688561\"\n[1] \"Simulation 20000 at 2025-10-12 10:35:07.392822\"\n[1] \"Simulation 25000 at 2025-10-12 10:35:38.505577\"\n[1] \"Simulation 30000 at 2025-10-12 10:36:09.795408\"\n[1] \"Simulation 35000 at 2025-10-12 10:36:41.31038\"\n[1] \"Simulation 40000 at 2025-10-12 10:37:12.930233\"\n[1] \"Simulation 45000 at 2025-10-12 10:37:45.754027\"\n[1] \"Simulation 50000 at 2025-10-12 10:38:17.400918\"",
    "crumbs": [
      "Lecture 14: Markov Chains I"
    ]
  },
  {
    "objectID": "lectures/lecture14.html#sec-looking-ahead",
    "href": "lectures/lecture14.html#sec-looking-ahead",
    "title": "Lecture 14: Markov Chains I",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nIn Lecture 15, we will lean on some mathematical properties of Markov chains to understand how many more at-bats a team can expect to have in a half-inning given the current game state. We will then estimate team-specific transition probability matrices, which will enable somewhat more granular Markov chain simulations. So that we don’t have to re-compute it, we will save the transition matrix and the set of unique states\n\nsave(transition_matrix, unik_states, file = \"atbat2024_markov_chain.RData\")",
    "crumbs": [
      "Lecture 14: Markov Chains I"
    ]
  },
  {
    "objectID": "lectures/lecture14.html#exercises",
    "href": "lectures/lecture14.html#exercises",
    "title": "Lecture 14: Markov Chains I",
    "section": "Exercises",
    "text": "Exercises\n\nWhen simulating the number of runs scored in the half-inning, we initialized our Markov chains from 0.000, which is the state at the start of every half-inning. By initializing at some other state — say 1.000 — we can study the distribution of runs scored after an at-bat beginning in that state. Use a Markov chain simulation to estimate the run expectancy for every state and compare it to the matrix we estimated in Lecture 6.",
    "crumbs": [
      "Lecture 14: Markov Chains I"
    ]
  },
  {
    "objectID": "lectures/lecture14.html#footnotes",
    "href": "lectures/lecture14.html#footnotes",
    "title": "Lecture 14: Markov Chains I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is possible to construct Markov chains where some absorbing states are never reached from certain initial states. Without a stopping criterion based on the number of iterations, the while() could continue indefinitely.↩︎\nFor instance, if the home team begins the bottom of the 9th inning tied and their first batter hits a homerun, then the game ends immediately.↩︎",
    "crumbs": [
      "Lecture 14: Markov Chains I"
    ]
  },
  {
    "objectID": "slides/lecture01.html",
    "href": "slides/lecture01.html",
    "title": "Lecture 1",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/lecture03.html",
    "href": "slides/lecture03.html",
    "title": "Lecture 3: Estimating XG",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "slides/lecture05.html",
    "href": "slides/lecture05.html",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "slides/lecture07.html",
    "href": "slides/lecture07.html",
    "title": "Lecture 7: Offensive Credit",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 7: Offensive Credit"
    ]
  },
  {
    "objectID": "slides/lecture09.html",
    "href": "slides/lecture09.html",
    "title": "Lecture 9: Multilevel Models",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 9: Multilevel Models"
    ]
  },
  {
    "objectID": "slides/lecture11.html",
    "href": "slides/lecture11.html",
    "title": "Lecture 11: Pitch Framing",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "slides/lecture13.html",
    "href": "slides/lecture13.html",
    "title": "Lecture 13: Bradley-Terry Models II",
    "section": "",
    "text": "You can view the presentation in a full screen window here. Code for the lecture is available at this link",
    "crumbs": [
      "Lecture 13: Bradley-Terry Models II"
    ]
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#motivation-beth-mead-in-euro-2022",
    "href": "slides/raw_slides/slides_02.html#motivation-beth-mead-in-euro-2022",
    "title": "STAT 479 Lecture 2",
    "section": "Motivation: Beth Mead in EURO 2022",
    "text": "Motivation: Beth Mead in EURO 2022\n\nBeth Mead scored 6 goals in EURO 2022. Which was more impressive?\n\nGoal in 15th minute against Austria link\nGoal in 37th minute against Norway link\nGoal in 33rd minute against Sweden link\n\n\n\n\nWe can argue endlessly about qualitative differences\n\nOne-on-one vs in-traffic; left or right foot;\nType of shot; time; score; …\n\n\n\n\n\nGoal: quantitative comparison"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#a-thought-experiment",
    "href": "slides/raw_slides/slides_02.html#a-thought-experiment",
    "title": "STAT 479 Lecture 2",
    "section": "A Thought Experiment",
    "text": "A Thought Experiment\n\nWhat if we could replay each shot over and over again?\nHow often would she score?\n\n\n\n\n\n\n\nThis long-run frequency is Expected Goals (XG)"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#statsbomb-hudl",
    "href": "slides/raw_slides/slides_02.html#statsbomb-hudl",
    "title": "STAT 479 Lecture 2",
    "section": "StatsBomb & Hudl",
    "text": "StatsBomb & Hudl\n\nPlayer location data for all events\n\nComputer vision + input from 5 human annotators\nHumans log events (shots, tackles, passes, etc.)\nComputer vision to extra location information\n\nLocations mapped to fixed coordinate system\nStatsBombR package: available via GitHub\n\n\ndevtools::install_github(\"statsbomb/StatsBombR\")"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#statsbomb-coordinates",
    "href": "slides/raw_slides/slides_02.html#statsbomb-coordinates",
    "title": "STAT 479 Lecture 2",
    "section": "StatsBomb Coordinates",
    "text": "StatsBomb Coordinates\n\nOffensive action moves left-to-right\nVertical coordinate increases as you move top-to-bottom\n\n\n\n\nStatsBomb Coordinates"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#free-competitions",
    "href": "slides/raw_slides/slides_02.html#free-competitions",
    "title": "STAT 479 Lecture 2",
    "section": "Free Competitions",
    "text": "Free Competitions\n\nFreeCompetitions() returns table of available competitions\n\n\n\n[1] \"Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please credit StatsBomb as your data source when using the data and visit https://statsbomb.com/media-pack/ to obtain our logos for public use.\"\n\n\n   competition_id season_id      competition_name season_name\n1              16       276      Champions League   1970/1971\n2              16        76      Champions League   1999/2000\n3               2        44        Premier League   2003/2004\n4             116        68 North American League        1977\n5              11         1               La Liga   2017/2018\n6              43        51        FIFA World Cup        1974\n7              53       106     UEFA Women's Euro        2022\n8               7       235               Ligue 1   2022/2023\n9              11        37               La Liga   2004/2005\n10             35        75    UEFA Europa League   1988/1989"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#free-matches",
    "href": "slides/raw_slides/slides_02.html#free-matches",
    "title": "STAT 479 Lecture 2",
    "section": "Free Matches",
    "text": "Free Matches\n\nStatsBombR::FreeCompetitions() |&gt;\n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt;\n  dplyr::select(home_team.home_team_name, away_team.away_team_name, home_score, away_score) |&gt;\n  dplyr::slice_sample(n=5)\n\n\n\n\n# A tibble: 5 × 4\n  home_team.home_team_name away_team.away_team_name home_score away_score\n  &lt;chr&gt;                    &lt;chr&gt;                         &lt;int&gt;      &lt;int&gt;\n1 England Women's          Norway Women's                    8          0\n2 England Women's          Austria Women's                   1          0\n3 Denmark Women's          WNT Finland                       1          0\n4 Germany Women's          Austria Women's                   2          0\n5 Austria Women's          Norway Women's                    1          0"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#event-data",
    "href": "slides/raw_slides/slides_02.html#event-data",
    "title": "STAT 479 Lecture 2",
    "section": "Event Data",
    "text": "Event Data\n\nGet EURO2022 EventsExtract All Shots\n\n\n\neuro2022_events &lt;-\n  StatsBombR::FreeCompetitions() |&gt; \n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt; \n  StatsBombR::free_allevents() |&gt; \n  StatsBombR::allclean() |&gt; \n  StatsBombR::get.opposingteam()\n\n\n\n\n\neuro2022_shots &lt;-\n  euro2022_events |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\n\n\n\n [1] \"minute\"              \"player.name\"         \"shot.technique.name\"\n [4] \"shot.body_part.name\" \"shot.type.name\"      \"location.x\"         \n [7] \"location.y\"          \"DistToGoal\"          \"DistToKeeper\"       \n[10] \"AngleToGoal\"         \"AngleToKeeper\"       \"OpposingTeam\""
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#meads-shots",
    "href": "slides/raw_slides/slides_02.html#meads-shots",
    "title": "STAT 479 Lecture 2",
    "section": "Mead’s Shots",
    "text": "Mead’s Shots\n\nmead_shots &lt;-\n  euro2022_shots |&gt;\n  dplyr::filter(player.name == \"Bethany Mead\")\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y)\n\n# A tibble: 15 × 5\n   OpposingTeam     minute shot.body_part.name shot.technique.name     Y\n   &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;\n 1 Austria Women's      15 Right Foot          Lob                     1\n 2 Norway Women's       29 Right Foot          Normal                  0\n 3 Norway Women's       33 Head                Normal                  1\n 4 Norway Women's       37 Left Foot           Normal                  1\n 5 Norway Women's       52 Right Foot          Volley                  0\n 6 Norway Women's       80 Left Foot           Volley                  1\n 7 Northern Ireland      5 Head                Normal                  0\n 8 Northern Ireland     15 Right Foot          Half Volley             0\n 9 Northern Ireland     43 Left Foot           Normal                  1\n10 Northern Ireland     56 Right Foot          Normal                  0\n11 Northern Ireland     83 Right Foot          Normal                  0\n12 Sweden Women's        4 Head                Normal                  0\n13 Sweden Women's       19 Left Foot           Normal                  0\n14 Sweden Women's       33 Right Foot          Half Volley             1\n15 Sweden Women's       46 Left Foot           Normal                  0"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#thought-experiment-repeating-shots",
    "href": "slides/raw_slides/slides_02.html#thought-experiment-repeating-shots",
    "title": "STAT 479 Lecture 2",
    "section": "Thought Experiment: Repeating Shots",
    "text": "Thought Experiment: Repeating Shots\n\nWhat proportion of repetitions result in goals?\nAcross all repetitions, conditions kept the same"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#setup-notation",
    "href": "slides/raw_slides/slides_02.html#setup-notation",
    "title": "STAT 479 Lecture 2",
    "section": "Setup & Notation",
    "text": "Setup & Notation\n\nSuppose our data set contains \\(n\\) shots\nFor shot \\(i = 1, \\ldots, n,\\) we observe\n\n\\(Y_{i}\\): indicator shot resutled in goal (\\(Y = 1\\)) or not (\\(Y = 0\\))\n\\(\\boldsymbol{\\mathbf{X}}_{i}\\): vector of \\(p\\) features about the shot\n\nFeatures could include things like\n\nBody part used & shot technique\nDist. to nearest defenders"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#conditional-probability",
    "href": "slides/raw_slides/slides_02.html#conditional-probability",
    "title": "STAT 479 Lecture 2",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nAssumption: data are a representative sample from an infinite super-population of shots\nEach shot in super-population characterized by pair \\((\\boldsymbol{\\mathbf{X}}, Y)\\)\nRepeating shot with features \\(\\boldsymbol{\\mathbf{x}}\\) equivalent to sampling from slice of super-population with \\(\\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}.\\)\n\n\n\\[\n\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}] = \\mathbb{P}(Y = 1 \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}})\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#defining-the-super-population",
    "href": "slides/raw_slides/slides_02.html#defining-the-super-population",
    "title": "STAT 479 Lecture 2",
    "section": "Defining The Super-population",
    "text": "Defining The Super-population\n\nUltimate goal is to assess Mead’s performance in EURO 2022\n\nFocus on women’s internationals\nFocus on shots attempted w/ foot or head\n\n\n\nGet EventsExtract Shots\n\n\n\nwi_events &lt;-\n  StatsBombR::FreeCompetitions() |&gt; \n  dplyr::filter(competition_gender == \"female\" & competition_international) |&gt;\n  StatsBombR::FreeMatches() |&gt; \n  StatsBombR::free_allevents() |&gt; \n  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam()\n\n\n\n\nwi_shots &lt;-\n  wi_events |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;  \n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#idealized-calculation",
    "href": "slides/raw_slides/slides_02.html#idealized-calculation",
    "title": "STAT 479 Lecture 2",
    "section": "Idealized Calculation",
    "text": "Idealized Calculation\n\nSuppose our only feature is shot.body_part.name\n\n\n\n\n      Head  Left Foot Right Foot \n       920       1280       2560 \n\n\n\nIf we could access infinite super-population, compute \\(\\textrm{XG}(\\text{right-footed shot})\\) by\n\nForming subgroup containing only right-footed shots\nCalculating proportion of goals score"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#practical-calculation",
    "href": "slides/raw_slides/slides_02.html#practical-calculation",
    "title": "STAT 479 Lecture 2",
    "section": "Practical Calculation",
    "text": "Practical Calculation\n\nSince we cannot access infinite super-population, we must estimate XG from data\nWe do so by mimicking idealized calculation\n\nDivide data into subgroups based on shot.body_part.name\nCompute proportion of goals within subgroups\n\nRely on dplyr’s group_by() functionality to do this"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#estimating-xgbody-part",
    "href": "slides/raw_slides/slides_02.html#estimating-xgbody-part",
    "title": "STAT 479 Lecture 2",
    "section": "Estimating XG(body part)",
    "text": "Estimating XG(body part)\n\nxg_model1 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y), n = dplyr::n())\nxg_model1\n\n\n\n\n# A tibble: 3 × 3\n  shot.body_part.name   XG1     n\n  &lt;chr&gt;               &lt;dbl&gt; &lt;int&gt;\n1 Head                0.112   920\n2 Left Foot           0.114  1280\n3 Right Foot          0.111  2560"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#assessing-beth-meads-performance",
    "href": "slides/raw_slides/slides_02.html#assessing-beth-meads-performance",
    "title": "STAT 479 Lecture 2",
    "section": "Assessing Beth Mead’s Performance",
    "text": "Assessing Beth Mead’s Performance\n\nAppend XG estimates to mead_shots w/ left_join\nBased on bodypart, each goal is about equally impressive\n\n\nmead_shots &lt;- mead_shots |&gt;\n  dplyr::left_join(y = xg_model1 |&gt; dplyr::select(shot.body_part.name, XG1),\n                   by = \"shot.body_part.name\")\n\n\n\n\n# A tibble: 3 × 5\n  OpposingTeam    minute shot.body_part.name     Y   XG1\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Austria Women's     15 Right Foot              1 0.111\n2 Norway Women's      37 Left Foot               1 0.114\n3 Sweden Women's      33 Right Foot              1 0.111"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#accounting-for-shot-technique",
    "href": "slides/raw_slides/slides_02.html#accounting-for-shot-technique",
    "title": "STAT 479 Lecture 2",
    "section": "Accounting For Shot Technique",
    "text": "Accounting For Shot Technique\n\nEach goal was scored off a different type of shot\nWhat if we condition body part & shot technique?\n\n\n\ntable(wi_shots |&gt; dplyr::pull(shot.technique.name))\n\n\n     Backheel Diving Header   Half Volley           Lob        Normal \n           35            10           648            28          3720 \nOverhead Kick        Volley \n           17           302"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#new-xg-estimates",
    "href": "slides/raw_slides/slides_02.html#new-xg-estimates",
    "title": "STAT 479 Lecture 2",
    "section": "New XG Estimates",
    "text": "New XG Estimates\n\nXG EstimatesMead’s Performance\n\n\n\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarize(XG2 = mean(Y), n = dplyr::n(), .groups = \"drop\")\nxg_model2 |&gt; dplyr::arrange(dplyr::desc(XG2))\n\n# A tibble: 14 × 4\n   shot.body_part.name shot.technique.name    XG2     n\n   &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt; &lt;int&gt;\n 1 Right Foot          Lob                 0.208     24\n 2 Left Foot           Volley              0.163     98\n 3 Left Foot           Normal              0.121    947\n 4 Right Foot          Normal              0.121   1863\n 5 Head                Normal              0.113    910\n 6 Right Foot          Backheel            0.103     29\n 7 Right Foot          Half Volley         0.0892   426\n 8 Right Foot          Overhead Kick       0.0714    14\n 9 Left Foot           Half Volley         0.0676   222\n10 Right Foot          Volley              0.0637   204\n11 Head                Diving Header       0         10\n12 Left Foot           Backheel            0          6\n13 Left Foot           Lob                 0          4\n14 Left Foot           Overhead Kick       0          3\n\n\n\n\n\nmead_shots &lt;-\n  mead_shots |&gt;\n  dplyr::inner_join(\n    y = xg_model2 |&gt; dplyr::select(-n), \n    by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y, XG2) |&gt;\n  dplyr::slice(c(1, 4, 14))\n\n# A tibble: 3 × 6\n  OpposingTeam    minute shot.body_part.name shot.technique.name     Y    XG2\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n1 Austria Women's     15 Right Foot          Lob                     1 0.208 \n2 Norway Women's      37 Left Foot           Normal                  1 0.121 \n3 Sweden Women's      33 Right Foot          Half Volley             1 0.0892"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#accounting-for-more-features",
    "href": "slides/raw_slides/slides_02.html#accounting-for-more-features",
    "title": "STAT 479 Lecture 2",
    "section": "Accounting for more features",
    "text": "Accounting for more features\n\nOur 2-feature model is still too coarse\n\nDoesn’t account for distance of shot\nDoesn’t account for defenders and keeper\n\nStatsBomb records many more features about shots\n\n\n\n [1] \"shot.type.name\"      \"shot.technique.name\" \"shot.body_part.name\"\n [4] \"DistToGoal\"          \"DistToKeeper\"        \"AngleToGoal\"        \n [7] \"AngleToKeeper\"       \"AngleDeviation\"      \"avevelocity\"        \n[10] \"density\"             \"density.incone\"      \"distance.ToD1\"      \n[13] \"distance.ToD2\"       \"AttackersBehindBall\" \"DefendersBehindBall\"\n[16] \"DefendersInCone\"     \"InCone.GK\"           \"DefArea\"            \n\n\n\nHow can we adjust for these in our XG model?"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#digression-what-is-the-cone",
    "href": "slides/raw_slides/slides_02.html#digression-what-is-the-cone",
    "title": "STAT 479 Lecture 2",
    "section": "Digression: What is the cone?",
    "text": "Digression: What is the cone?\n\nCone: area between shot location & goalposts \n\n\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#idealized-computation",
    "href": "slides/raw_slides/slides_02.html#idealized-computation",
    "title": "STAT 479 Lecture 2",
    "section": "Idealized Computation",
    "text": "Idealized Computation\n\nIf we could access super-population, easy to condition on more features\n\nSlice population along \\(\\mathbf{\\boldsymbol{x}}\\)\nCompute proportion of goals among the slice with \\(\\mathbf{\\boldsymbol{X}} = \\mathbf{\\boldsymbol{x}}\\)\n\n\n\n\nCan we mimic this with our observed data?"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#practical-challenges",
    "href": "slides/raw_slides/slides_02.html#practical-challenges",
    "title": "STAT 479 Lecture 2",
    "section": "Practical Challenges",
    "text": "Practical Challenges\n\nAdjust for body part, technique, and # defenders in cone\n\n\n\n# A tibble: 10 × 5\n   shot.body_part.name shot.technique.name DefendersInCone    XG     n\n   &lt;chr&gt;               &lt;chr&gt;                         &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 Left Foot           Normal                            7 1         1\n 2 Right Foot          Lob                               2 1         1\n 3 Right Foot          Normal                           10 1         1\n 4 Left Foot           Volley                            0 0.343    35\n 5 Right Foot          Overhead Kick                     0 0.333     3\n 6 Right Foot          Overhead Kick                     4 0         2\n 7 Right Foot          Overhead Kick                     6 0         1\n 8 Right Foot          Volley                            5 0        12\n 9 Right Foot          Volley                            6 0         9\n10 Right Foot          Volley                            7 0         1"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#model-based-xg",
    "href": "slides/raw_slides/slides_02.html#model-based-xg",
    "title": "STAT 479 Lecture 2",
    "section": "Model-based XG",
    "text": "Model-based XG\n\n“Binning-and-averaging” not viable w/ many features due to small sample sizes\nStatistical models overcome these challenges by “borrowing strength”\n\\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) informed by shots with \\(\\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}\\) and \\(\\boldsymbol{\\mathbf{X}} \\approx \\boldsymbol{\\mathbf{x}}\\)\nHow a model “borrows strength” depends on its underlying assumptions\n\n\n\nSeveral challenges:\n\n\\(\\mathbf{\\boldsymbol{X}}\\) may be high-dimensional\n\\(\\textrm{XG}\\) likely depends on many interactions\n\\(\\textrm{XG}\\) likely highly non-linear\n\nStatsBomb has a proprietary model"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#statsbombs-xg-estimates",
    "href": "slides/raw_slides/slides_02.html#statsbombs-xg-estimates",
    "title": "STAT 479 Lecture 2",
    "section": "StatsBomb’s XG Estimates",
    "text": "StatsBomb’s XG Estimates\n\nshot.statsbomb_xg records proprietary XG estimate\nGoal against Sweden had smallest XG and is, therefore, the most impressive\n\n\n\n# A tibble: 3 × 5\n  OpposingTeam    minute   XG1   XG2 shot.statsbomb_xg\n  &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;\n1 Austria Women's     15 0.111 0.208             0.361\n2 Norway Women's      37 0.114 0.121             0.444\n3 Sweden Women's      33 0.111 0.089             0.091"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#did-mead-outperform-expectations",
    "href": "slides/raw_slides/slides_02.html#did-mead-outperform-expectations",
    "title": "STAT 479 Lecture 2",
    "section": "Did Mead Outperform Expectations?",
    "text": "Did Mead Outperform Expectations?\n\nHow to interpret the difference \\(Y_{i} - \\textrm{XG}_{i}\\) when it is\n\nLarge & positive\nLarge & negative\nClose to 0\n\n\n\n\nBeth Mead scored 2.89 more goals than expected\n\n\nsum(mead_shots$Y - mead_shots$shot.statsbomb_xg)\n\n[1] 2.896323"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#assessing-all-euro2022-players",
    "href": "slides/raw_slides/slides_02.html#assessing-all-euro2022-players",
    "title": "STAT 479 Lecture 2",
    "section": "Assessing All EURO2022 Players",
    "text": "Assessing All EURO2022 Players\n\ngoe &lt;- \n  euro2022_shots |&gt;\n  dplyr::mutate(diff = Y - shot.statsbomb_xg) |&gt;\n  dplyr::group_by(player.name) |&gt;\n  dplyr::summarise(GOE = sum(diff), Goals = sum(Y), n_shots = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(GOE)) \ngoe |&gt; dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n# A tibble: 10 × 4\n   player.name                GOE Goals n_shots\n   &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Alexandra Popp           3.34      6      16\n 2 Bethany Mead             2.90      6      15\n 3 Alessia Russo            1.79      4      12\n 4 Francesca Kirby          1.79      2       5\n 5 Lina Magull              1.70      3      14\n 6 Ada Stolsmo Hegerberg   -0.829     0       8\n 7 Nadia Nadim             -0.886     0       6\n 8 Lauren Hemp             -1.26      1      11\n 9 Emma Stina Blackstenius -2.15      1      17\n10 Wendie Renard           -2.39      0      17"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#looking-ahead",
    "href": "slides/raw_slides/slides_02.html#looking-ahead",
    "title": "STAT 479 Lecture 2",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nI strongly recommend you work through code yourself\n\nFull code available here\nTry more binning-and-averaging XG estimates w/ other features\nTry to replicate goals over expected with EURO 2025 data\n\nWill post code for plotting all players during a shot\n\nshot.freeze_frame contains data frame with other players & their positions"
  },
  {
    "objectID": "slides/raw_slides/slides_02.html#project-groups",
    "href": "slides/raw_slides/slides_02.html#project-groups",
    "title": "STAT 479 Lecture 2",
    "section": "Project Groups",
    "text": "Project Groups\n\nForm groups by Friday September 12 (sign up on Canvas)\nUse Piazza to help find teammates"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#motivation",
    "href": "slides/raw_slides/slides_04.html#motivation",
    "title": "STAT 479: Lecture 4",
    "section": "Motivation",
    "text": "Motivation\n\nHow do NBA players help their teams win?\nHow do we quantify contributions?\n\n\n\nIdea: good players do things that show up in the box score\n\nPoints, rebounds, assists, steals, blocks, turnovers\nEasy to collect, sort, explain\n\n\n\n\n\nFails to account for roles\n\n10 assists for a guard vs 10 rebounds by a center\n\nProblem: should some stats weigh more heavily than others?\n\n\n\n\nBigger problem: Not everything appears in the box score\n\nSetting screens, rotating on defense, communicating\nGood shot selection, diving for loose balls\nThe “little things”"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#plusminus",
    "href": "slides/raw_slides/slides_04.html#plusminus",
    "title": "STAT 479: Lecture 4",
    "section": "Plus/Minus",
    "text": "Plus/Minus\n\n\n\nDefinition: Plus/Minus\n\n\nA player’s plus/minus is the point differential that a player’s team accrues while they are on the court.\n\n\n\n\nIntuition: if your team outscores opponent while you’re on the court, you must be doing something right\nTo compute, must know who is on the court at all times"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#play-by-play-nba-data",
    "href": "slides/raw_slides/slides_04.html#play-by-play-nba-data",
    "title": "STAT 479: Lecture 4",
    "section": "Play-by-Play NBA Data",
    "text": "Play-by-Play NBA Data\n\n\n\n\n\n\nEntry created when player does something tracked by scorekeeper\nCan use the hoopR package to scrape play-by-play data into R"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#stint-level-data",
    "href": "slides/raw_slides/slides_04.html#stint-level-data",
    "title": "STAT 479: Lecture 4",
    "section": "Stint-Level Data",
    "text": "Stint-Level Data\n\nStint: period of play b/w substitutions where the same 10 players remain on the court.\nCan form a data table from play-by-play log where\n\nRows correspond to stints\nColumns for game context: start & end scores, length in minutes, etc.\nColumn for every player’s signed on-court indicator\n\nSigned on-court indicators:\n\n+1 if on-court and playing at home\n-1 if on-court and playing on the road\n0 if not on court"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#data-snapshot",
    "href": "slides/raw_slides/slides_04.html#data-snapshot",
    "title": "STAT 479: Lecture 4",
    "section": "Data Snapshot",
    "text": "Data Snapshot\n\nContext columns:\n\nGame & Stint ID; num. possessions; duration;\nStart & end scores & times; point differential\n\n569 columns of signed on-court indicators\n\n\nContextOn-Court IndicatorsPlayer IDs\n\n\n\n\n# A tibble: 5 × 7\n  stint_id n_pos start_minutes minutes home_points away_points pts_diff\n     &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1        1    14          0      5.42           18          12        6\n2        2     5          5.42   1.51            2           2        0\n3        3     1          6.93   0.220           0           2       -2\n4        4     4          7.15   2.17            9           1        8\n5        5    13          9.32   4.13           13          11        2\n\n\n\n\n\n\n# A tibble: 5 × 7\n  stint_id `201143` `201950` `1627759` `1628369` `1628436` `1630202`\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1        1        1        1         1         1         0         0\n2        2        1        0         0         1         1         1\n3        3        1        0         0         1         1         1\n4        4        0        0         0         1         1         1\n5        5        0        1         0         1         1         1\n\n\n\n\n\nplayer_table &lt;-\n  hoopR::nba_commonallplayers()[[\"CommonAllPlayers\"]] |&gt;\n  dplyr::select(PERSON_ID, DISPLAY_FIRST_LAST) |&gt;\n  dplyr::rename(id = PERSON_ID, FullName = DISPLAY_FIRST_LAST) |&gt;\n  dplyr::mutate(\n    Name = stringi::stri_trans_general(FullName, \"Latin-ASCII\")) \n\nplayer_table |&gt;\n  dplyr::filter(id %in% c(\"201143\", \"201950\", \"1627759\")) |&gt;\n  dplyr::pull(FullName)\n\n[1] \"Jaylen Brown\" \"Jrue Holiday\" \"Al Horford\""
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-individual---concept",
    "href": "slides/raw_slides/slides_04.html#computing-individual---concept",
    "title": "STAT 479: Lecture 4",
    "section": "Computing Individual +/- (Concept)",
    "text": "Computing Individual +/- (Concept)\n\nConsider Shai Gilgeous-Alexander (2024-25 MVP)\nTo compute SGA’s +/-:\n\nSum the home team point differentials for all stints where SGA was on the court and playing at home.\nSum the negative of the home team point differentials for all shifts where SGA was on the court and playing on the road.\nAdd the two totals from Steps 1 and 2."
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-individual---formula",
    "href": "slides/raw_slides/slides_04.html#computing-individual---formula",
    "title": "STAT 479: Lecture 4",
    "section": "Computing Individual +/- (Formula)",
    "text": "Computing Individual +/- (Formula)\n\n\\(\\Delta_{i}\\): home team point differential in shift \\(i\\)\n\\(x_{i, \\textrm{SGA}}\\): SGA’s signed on-court indicator:\n\n\\(x_{i, \\textrm{SGA}} = 0\\) if SGA off-court in stint \\(i\\)\n\\(x_{i, \\textrm{SGA}} = 1(-1)\\) if SGA on-court at home (away) in stint \\(i\\)\n\nSGA’s +/- is just \\[\n\\sum_{i = 1}^{n}{x_{i,\\textrm{SGA}} \\times \\Delta_{i}}.\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-individual---code",
    "href": "slides/raw_slides/slides_04.html#computing-individual---code",
    "title": "STAT 479: Lecture 4",
    "section": "Computing Individual +/- (Code)",
    "text": "Computing Individual +/- (Code)\n\nWhen SGA was on the floor, Thunder outscored opponents by 888 pts\nWhen Jokic was on the floor, Nuggets outscored opponents by 452\n\n\nSGAJokic\n\n\n\nshai_id &lt;- player_table |&gt;\n  dplyr::filter(Name == \"Shai Gilgeous-Alexander\") |&gt; dplyr::pull(id) \nshai_x &lt;- rapm_data |&gt; dplyr::pull(shai_id) \ndelta &lt;- rapm_data |&gt; dplyr::pull(pts_diff) \nsum(shai_x * delta)\n\n\n\n\n[1] 888\n\n\n\n\n\n\njokic_id &lt;-\n  player_table |&gt;\n  dplyr::filter(Name == \"Nikola Jokic\") |&gt;\n  dplyr::pull(id)\n\njokic_x &lt;- rapm_data |&gt; dplyr::pull(jokic_id) \nsum(jokic_x * delta) \n\n[1] 452"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#notation",
    "href": "slides/raw_slides/slides_04.html#notation",
    "title": "STAT 479: Lecture 4",
    "section": "Notation",
    "text": "Notation\n\n\\(n\\): total number of stints in the season\n\\(p\\): total number of players\nFor each stint \\(i = 1, \\ldots, n\\) and player \\(j = 1, \\ldots, p\\):\n\n\\(x_{ij} = 1\\) if player \\(j\\) on-court at home in stint \\(i\\)\n\\(x_{ij} = -1\\) if player \\(j\\) on-court on road in stint \\(i\\)\n\\(x_{ij} = 0\\)\n\n\\(\\Delta_{i}\\): home-team differential in stint \\(i\\) . . .\nPlayer \\(j\\)’s +/-: \\(\\sum_{i}{x_{ij}\\Delta_{i}}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#stint-design-matrix",
    "href": "slides/raw_slides/slides_04.html#stint-design-matrix",
    "title": "STAT 479: Lecture 4",
    "section": "Stint Design Matrix",
    "text": "Stint Design Matrix\n\nArrange \\(x_{ij}\\)’s into an \\(n \\times p\\) matrix\n\n\\[\n\\boldsymbol{\\mathbf{X}} =\n\\begin{pmatrix}\nx_{1,1} & \\cdots & x_{1,p} \\\\\n\\vdots & & \\vdots \\\\\nx_{n,1} & \\cdots & x_{n,p}\n\\end{pmatrix}\n\\]\n\n\nCollect all \\(n\\) \\(\\Delta_{i}\\)’s into a vector of length \\(n\\) \\[\n\\boldsymbol{\\Delta} = \\begin{pmatrix} \\Delta_{1} \\\\ \\vdots \\\\ \\Delta_{n} \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-all--s",
    "href": "slides/raw_slides/slides_04.html#computing-all--s",
    "title": "STAT 479: Lecture 4",
    "section": "Computing all +/-’s",
    "text": "Computing all +/-’s\n\nCan compute all player’s w/ matrix-vector multiplication \\(\\boldsymbol{\\mathbf{X}}^{\\top}\\boldsymbol{\\Delta}\\) \\[\n\\begin{pmatrix}\nx_{1,1} & \\cdots & x_{n,1} \\\\\n\\vdots & & \\vdots \\\\\nx_{1,p} & \\cdots & x_{n,p}\n\\end{pmatrix}\n\\begin{pmatrix} \\Delta_{1} \\\\ \\vdots \\\\ \\Delta_{n} \\end{pmatrix}\n=\n\\begin{pmatrix}\nx_{1,1}\\Delta_{1} + x_{2,1}\\Delta_{2} + \\cdots + x_{n,1}\\Delta_{n}\\\\\n\\vdots \\\\\nx_{1,p}\\Delta_{1} + x_{2,p}\\Delta_{2} + \\cdots + x_{n,p}\\Delta_{n}\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#computing-plusminus",
    "href": "slides/raw_slides/slides_04.html#computing-plusminus",
    "title": "STAT 479: Lecture 4",
    "section": "Computing Plus/Minus",
    "text": "Computing Plus/Minus\n\nBuilding \\(\\boldsymbol{\\mathbf{X}}\\)Computing Plus/MinusSelected Players\n\n\n\ncontext_vars &lt;-\n  c(\"game_id\", \"stint_id\", \"n_pos\", \n    \"start_home_score\", \"start_away_score\", \"start_minutes\",\n    \"end_home_score\", \"end_away_score\", \"end_minutes\",\n    \"home_points\", \"away_points\", \"minutes\",\n    \"pts_diff\", \"margin\")\n\nX_full &lt;- as.matrix( \n    rapm_data |&gt; dplyr::select(- tidyr::all_of(context_vars))) \n\n\n\n\npm &lt;-\n  data.frame( \n    id = colnames(X_full), \n    pm = crossprod(x = X_full, y = rapm_data |&gt; dplyr::pull(pts_diff)), \n    n_pos = crossprod(abs(X_full), y = rapm_data |&gt; dplyr::pull(n_pos)), \n    minutes = crossprod(abs(X_full), y = rapm_data |&gt; dplyr::pull(minutes))) |&gt; \n  dplyr::inner_join(y = player_table |&gt; dplyr::select(id, Name), by = \"id\") |&gt; \n  dplyr::select(id, Name, pm, n_pos, minutes) |&gt; \n  dplyr::arrange(dplyr::desc(pm))\n\n\n\n\n\n                     Name  pm\n1 Shai Gilgeous-Alexander 888\n2            Jayson Tatum 474\n3            Nikola Jokic 452\n4   Giannis Antetokounmpo 331\n5             Luka Doncic 276\n6           Anthony Davis -78\n7            LeBron James -88"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#visualizing-plusminus",
    "href": "slides/raw_slides/slides_04.html#visualizing-plusminus",
    "title": "STAT 479: Lecture 4",
    "section": "Visualizing Plus/Minus",
    "text": "Visualizing Plus/Minus\n\n\n\n\n\n\n\n\nFigure 1: Large gap b/w SGA and rest of the league"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#plusminus-and-possessions",
    "href": "slides/raw_slides/slides_04.html#plusminus-and-possessions",
    "title": "STAT 479: Lecture 4",
    "section": "Plus/Minus and Possessions",
    "text": "Plus/Minus and Possessions\n\n\n\n\n\n\n\n\nFigure 2: Variability in +/- increases with number of possessions!"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#issues-with-plusminus",
    "href": "slides/raw_slides/slides_04.html#issues-with-plusminus",
    "title": "STAT 479: Lecture 4",
    "section": "Issues with Plus/Minus",
    "text": "Issues with Plus/Minus\n\npm |&gt; dplyr::slice_head(n=10)\n\n        id                    Name  pm n_pos minutes\n1  1628983 Shai Gilgeous-Alexander 888  8159 2837.31\n2  1629652           Luguentz Dort 561  5584 1955.17\n3  1630198              Isaiah Joe 552  4896 1715.11\n4  1628401           Derrick White 509  6129 2327.72\n5  1630596             Evan Mobley 508  5944 2049.43\n6  1630598           Aaron Wiggins 507  4868 1683.08\n7  1628378        Donovan Mitchell 491  6101 2106.53\n8  1628369            Jayson Tatum 474  7498 2840.95\n9  1628386           Jarrett Allen 459  6163 2142.82\n10  203999            Nikola Jokic 452  7907 2707.97\n\n\n\nIs Lou Dort really better than Jayson Tatum???\n\n\n\nDifferences in +/- could be a result of\n\nDifferences in skill\nDifferences in playing time\nDifferences in teammate & opponent quality"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#from-totals-to-rates",
    "href": "slides/raw_slides/slides_04.html#from-totals-to-rates",
    "title": "STAT 479: Lecture 4",
    "section": "From Totals to Rates",
    "text": "From Totals to Rates\n\nComparing totals favors players with more playing time\nAPM works with rates: point differential per 100 possessions\n\n\n\n# A tibble: 10 × 4\n   stint_id pts_diff n_pos margin\n      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1        1        6    14   42.9\n 2        2        0     5    0  \n 3        3       -2     1 -200  \n 4        4        8     4  200  \n 5        5        2    13   15.4\n 6        6        4     8   50  \n 7        8        6     9   66.7\n 8        9        5    29   17.2\n 9       10        0     6    0  \n10       11       -2     5  -40"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#an-initial-apm-model",
    "href": "slides/raw_slides/slides_04.html#an-initial-apm-model",
    "title": "STAT 479: Lecture 4",
    "section": "An Initial APM Model",
    "text": "An Initial APM Model\n\nAssociate each player \\(j\\) with a latent strength \\(\\alpha_{j}\\)\n\\(\\alpha_{j}\\)’s are unknown: they must be estimated from data\n\\(Y_{i}\\): point differential per 100 possessions in stint \\(i\\)\n\\(h_{1}(i), \\ldots, h_{5}(i)\\) & \\(a_{1}(i), \\ldots, a_{5}(i)\\): identities of players on court in stint \\(i\\)\n\n\\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#example",
    "href": "slides/raw_slides/slides_04.html#example",
    "title": "STAT 479: Lecture 4",
    "section": "Example",
    "text": "Example\n\nDec 23, 2024 game Dallas Mavericks (away) at Golden State Warriors (home):\n\nDAL: Luka Doncic, Dereck Lively II, Kyrie Irving, P.J. Washington, and Klay Thompson\nGSW: Stephen Curry, Buddy Hield, Andrew Wiggins, Jonathan Kuminga, and Kevon Looney.\n\nPer 100 possessions, with these lineups DAL expects to outscore GSW by \\[\n-1 \\times (\\alpha_{0} + \\alpha_{SC} + \\alpha_{BH} + \\alpha_{AW} + \\alpha_{JK} + \\alpha_{KL}) +(\\alpha_{LD} + \\alpha_{KI} + \\alpha_{DL} + \\alpha_{PW} + \\alpha_{KT}).\n\\]\n\n\n\nNow imagine that you replaced Doncic w/ Anthony Davis.\nPer 100 possessions, DAL expects to outscore GSW by \\[\n-1 \\times (\\alpha_{0} + \\alpha_{SC} + \\alpha_{BH} + \\alpha_{AW} + \\alpha_{JK} + \\alpha_{KL}) +(\\alpha_{AD} + \\alpha_{KI} + \\alpha_{DL} + \\alpha_{PW} + \\alpha_{KT}).\n\\]\n\n\n\n\nDAL expects to score \\(\\alpha_{\\textrm{AD}} - \\alpha_{\\textrm{LD}}\\) more points per 100 possessions with Davis than Doncic, all else being equal"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#potential-issue-interpretation",
    "href": "slides/raw_slides/slides_04.html#potential-issue-interpretation",
    "title": "STAT 479: Lecture 4",
    "section": "Potential Issue: Interpretation",
    "text": "Potential Issue: Interpretation\n\nIndividual \\(\\alpha_{j}\\)’s are meaningless!\n\\(\\alpha_{j}\\): change in point differential per 100 possessions b/w\n\nPlaying 5-on-5 w/ player \\(j\\) on the court\nPlaying 5-on-4 w/ player \\(j\\) off the court\n\nLuckily, we can interpret differences (or contrasts) like \\(\\alpha_{j}-\\alpha_{j'}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#apm-as-a-linear-model",
    "href": "slides/raw_slides/slides_04.html#apm-as-a-linear-model",
    "title": "STAT 479: Lecture 4",
    "section": "APM As A Linear Model",
    "text": "APM As A Linear Model\n\nAppend a column of 1’s to \\(\\boldsymbol{\\mathbf{X}}\\) to form \\(n \\times (p+1)\\) matrix \\(\\boldsymbol{\\mathbf{Z}}\\)\nLet \\(\\boldsymbol{\\mathbf{z}}_{i}\\) be the \\(i\\)-th row of \\(\\boldsymbol{\\mathbf{Z}}\\)\nAPM asserts: \\(Y_{i} = \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} + \\epsilon_{i}.\\)\n\n\n\nTempting to estimate \\(\\boldsymbol{\\alpha}\\) with least squares \\[\n\\textrm{argmin} \\sum_{i = 1}^{n}{\\left( Y_{i} - \\boldsymbol{\\mathbf{z}}_{i}^{\\top}\\boldsymbol{\\alpha} \\right)^{2}} .\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#non-identifiability",
    "href": "slides/raw_slides/slides_04.html#non-identifiability",
    "title": "STAT 479: Lecture 4",
    "section": "Non-identifiability",
    "text": "Non-identifiability\n\nRecall that the model asserts \\[\n\\begin{align}\nY_{i} &= \\alpha_{0} + \\alpha_{h_{1}(i)} + \\alpha_{h_{2}(i)} + \\alpha_{h_{3}(i)} + \\alpha_{h_{4}(i)} + \\alpha_{h_{5}(i)} \\\\\n~&~~~~~~~~~~- \\alpha_{a_{1}(i)} - \\alpha_{a_{2}(i)} - \\alpha_{a_{3}(i)} - \\alpha_{a_{4}(i)} - \\alpha_{a_{5}(i)} + \\epsilon_{i},\n\\end{align}\n\\]\nImagine we add 5 to every \\(\\alpha_{j}\\): right-hand side remains unchanged\nWhile we can’t hope to learn \\(\\alpha_{j}\\)’s exactly, can still interpret constrasts \\(\\alpha_{j} - \\alpha_{j'}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#singularity",
    "href": "slides/raw_slides/slides_04.html#singularity",
    "title": "STAT 479: Lecture 4",
    "section": "Singularity",
    "text": "Singularity\n\nLeast squares problem does not have a unique solution!\nColumns of \\(\\boldsymbol{\\mathbf{Z}}\\) are linearly dependent\n\nThe first element in each row is equal to 1 (for intercept)\n5 entries equal to 1 (for home players)\n5 entries equal to -1 (for away players)\n\nIf you know all but one column, you can perfectly determine that column\n\\(\\boldsymbol{\\mathbf{Z}}^{\\top}\\boldsymbol{\\mathbf{Z}}\\) not invertible"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#baseline-contrasts",
    "href": "slides/raw_slides/slides_04.html#baseline-contrasts",
    "title": "STAT 479: Lecture 4",
    "section": "Baseline Contrasts",
    "text": "Baseline Contrasts\n\nClassify certain players as “baseline”-level (e.g., \\(&lt; 250\\) minutes)\nRe-number players so first \\(p'\\) are non-baseline\nAssumption: \\(\\alpha_{j} = \\mu\\) for all baseline players \\(j &gt; p'\\)\n\nAll baseline players assumed to have the same underlying skill\n\n\n\n\nFor non-baseline \\(j = 1, \\ldots, p,\\) let \\(\\beta_{j} = \\alpha_{j} - \\mu\\)\n\\(\\beta_{j}\\): effect of replacing player \\(j\\) with a baseline player"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#a-re-parametrized-model",
    "href": "slides/raw_slides/slides_04.html#a-re-parametrized-model",
    "title": "STAT 479: Lecture 4",
    "section": "A Re-parametrized Model",
    "text": "A Re-parametrized Model\n\n\\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\) be the \\(n \\times (p'+1)\\) submatrix of \\(\\boldsymbol{\\mathbf{Z}}\\) s.t.\n\nFirst column is all 1’s\nRemaining columns: signed on-court indicators for non-baseline players\n\nTurns out:\n\n\\(\\tilde{\\boldsymbol{\\mathbf{Z}}}\\boldsymbol{\\beta} = \\boldsymbol{\\mathbf{Z}}\\boldsymbol{\\alpha}\\)\nThere quantity \\(\\sum_{i = 1}^{n}{\\left(Y_{i} - \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta}\\right)^{2}}\\) has a unique minimizer\n\n\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\tilde{\\boldsymbol{\\mathbf{Z}}}\\right)^{-1}\\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\boldsymbol{\\mathbf{Y}}.\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#estimating-boldsymbolbeta",
    "href": "slides/raw_slides/slides_04.html#estimating-boldsymbolbeta",
    "title": "STAT 479: Lecture 4",
    "section": "Estimating \\(\\boldsymbol{\\beta}\\)",
    "text": "Estimating \\(\\boldsymbol{\\beta}\\)\n\nDefining BaselinesEstimating \\(\\boldsymbol{\\beta}\\)Top-10 APM’s\n\n\n\nnonbaseline_id &lt;-\n  pm |&gt;\n  dplyr::filter(minutes &gt;= 250) |&gt;\n  dplyr::pull(id)\n\n\n\n\napm_df &lt;-\n  rapm_data |&gt;\n  dplyr::select(tidyr::all_of(c(\"margin\", nonbaseline_id)))\napm_fit &lt;- lm(margin ~ ., data = apm_df)\nbeta0 &lt;- coefficients(apm_fit)[1]\nbeta &lt;- coefficients(apm_fit)[-1]\n\n\n\n\n\n                    Name      apm\n1          Tobias Harris 17.01331\n2         Mouhamed Gueye 16.79320\n3           Devin Carter 16.60148\n4             Trae Young 15.13287\n5  Giannis Antetokounmpo 14.61082\n6            Isaiah Wong 14.50091\n7           Nikola Jokic 14.40837\n8         Alperen Sengun 13.55544\n9        Quenton Jackson 13.13962\n10    Karl-Anthony Towns 13.13714"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#missing-context",
    "href": "slides/raw_slides/slides_04.html#missing-context",
    "title": "STAT 479: Lecture 4",
    "section": "Missing Context",
    "text": "Missing Context\n\nAPM does not account for context\nCan over-inflate garbage-time performance when outcome essentially determined\n\n\n“can artificially can artificially inflate the importance of performance in low-leverage situations, when the outcome of the game is essentially decided, while simultaneously deflating the importance of high-leverage performance, when the final outcome is still in question. For instance, point diﬀerential-based metrics model the home team’s lead dropping from 5 points to 0 points in the last minute of the first half in exactly the same way that they model the home team’s lead dropping from 30 points to 25 points in the last minute of the second half”."
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#a-weighted-version-of-apm",
    "href": "slides/raw_slides/slides_04.html#a-weighted-version-of-apm",
    "title": "STAT 479: Lecture 4",
    "section": "A Weighted Version of APM",
    "text": "A Weighted Version of APM\n\nIntroduce weight \\(w_{i}\\) for stint \\(i\\)\nFind \\(\\tilde{\\boldsymbol{\\beta}}\\) minimizing \\(\\sum{w_{i}\\left(Y_{i} - \\tilde{\\boldsymbol{\\mathbf{z}}}_{i}^{\\top}\\boldsymbol{\\beta}\\right)^{2}}\\)\nSolution: letting \\(\\boldsymbol{\\mathbf{W}}\\) denote diagonal matrix w/ entries \\(w_{i}\\) \\[\n\\hat{\\boldsymbol{\\beta}}_{w} = \\left( \\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\tilde{\\boldsymbol{\\mathbf{Z}}}\\right)^{-1}\\tilde{\\boldsymbol{\\mathbf{Z}}}^{\\top}\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{Y}}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#estimating-wapm",
    "href": "slides/raw_slides/slides_04.html#estimating-wapm",
    "title": "STAT 479: Lecture 4",
    "section": "Estimating wAPM",
    "text": "Estimating wAPM\n\nExample weights:\n\n\\(w_{i} = 1\\) if lead \\(&lt; 10\\)\n\\(w_{i} = 0\\) if one team leads by \\(&gt; 30\\) at start of \\(i\\)\n\\(w_{i} = 1 - (\\textrm{StartDiff} - 10)/20\\): if \\(10 \\leq \\textrm{lead} \\leq 30\\)\n\n\n\nDefining WeightsFit wAPMTop-10 wAPM\n\n\n\nwapm_df &lt;-\n  rapm_data |&gt;\n  dplyr::mutate(\n    start_diff = abs(start_home_score - start_away_score), \n    w = dplyr::case_when(\n      start_diff &lt; 10 ~ 1, \n      start_diff &gt; 30 ~ 0, \n      .default = 1 - (start_diff-10)/20)) |&gt; \n  dplyr::select(tidyr::all_of(c(\"margin\", \"w\", nonbaseline_id)))\n\n\n\n\nwapm_fit &lt;- \n  lm(formula = margin ~ . - w, \n     weights = w, \n     data = wapm_df) \n\n\n\n\n\n                Name     wapm\n1       Devin Carter 19.48318\n2      Tobias Harris 17.11044\n3    Lauri Markkanen 15.20344\n4     Mouhamed Gueye 14.76474\n5         Trae Young 14.52393\n6       Nikola Jokic 14.14573\n7     Alperen Sengun 14.03023\n8         OG Anunoby 13.96219\n9  Jordan McLaughlin 13.73760\n10      Jericho Sims 13.27850"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#looking-ahead",
    "href": "slides/raw_slides/slides_04.html#looking-ahead",
    "title": "STAT 479: Lecture 4",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n(w)APM is quite sensitive to certain choices:\n\nDefinition of baseline players\nChoice of weights\n\nConstant baseline skill assumption is highly unsatisfactory\nIssues due to inability to use least squares\nNext time: alternative estimation strategy\n\nAvoids having to specify baseline players\nEstimates a latent strength for all players"
  },
  {
    "objectID": "slides/raw_slides/slides_04.html#project-1",
    "href": "slides/raw_slides/slides_04.html#project-1",
    "title": "STAT 479: Lecture 4",
    "section": "Project 1",
    "text": "Project 1\n\nThanks for sorting yourselves into group!\n\nIf you still don’t have a group, let me know!\nOn Canvas, you need to be in a group under “Projects 1&2 Groups”\n\nProject information posted:\n\nRequirements for written report and presentation\nNon-exhaustive list of project topics\nMain requirement: must use play-by-play or event-level data\nI.e., use data more granular than game- or season-level totals\n\nProject Check-in: by Friday 26 September email me a short overview of your project\n\nPrecise problem statement & overview of data and analysis plan\nHappy to help brainstorm & narrow down analysis in Office Hours (or by appt.)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#motivation",
    "href": "slides/raw_slides/slides_06.html#motivation",
    "title": "STAT 479: Lecture 6",
    "section": "Motivation",
    "text": "Motivation\n\nMarch 20, 2024 Dodgers vs Padres game\nShohei Ohtani hit two singles\n\n3rd inning, 2 outs and no runners on base, Ohtani singled into right field.\n8th inning, with 1 out and runners on first and second base, Ohtani singled into left center field, driving in one run\n\nWhich single was more valuable?\n\n\n\nSecond single scored a run\nBut first single put runner in scoring position\n\n\n\n\nThis lecture: a “currency” for evaluating plays that accounts for\n\nActual runs scored\nPotential to score more runs\n\nLectures 7 & 8: apportioning offensive & defensive credit + WAR"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#history-of-tracking-data-in-baseball",
    "href": "slides/raw_slides/slides_06.html#history-of-tracking-data-in-baseball",
    "title": "STAT 479: Lecture 6",
    "section": "History of Tracking Data in Baseball",
    "text": "History of Tracking Data in Baseball\n\n2006: Sportvision debuts camera system for tracking pitch trajectory\n2008: Sportvision releases PITCHf/x data to MLB to power GameDay app\n2008-2012(?): people realize GameDay API is publicly accessible & start scraping\n2017: PITCHf/x phased out in favor of radar-based Trackman\n\nTrackman originally developed for golf\n\nStatcast: ball & player tracking\nMade available through BaseballSavant (also scrapable)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#statcast-data-in-r",
    "href": "slides/raw_slides/slides_06.html#statcast-data-in-r",
    "title": "STAT 479: Lecture 6",
    "section": "Statcast Data in R",
    "text": "Statcast Data in R\n\nBill Petti’s baseballR package: download development version from GitHub\n\n\ndevtools::install_github(repo = \"BillPetti/baseballr\")\n\n\nProvides tools for scraping\n\nStatcast data via baseballsavant.mlb.com\nFanGraphs, BaseballReferencs\n\n\n\n\nSee course website for function annual_statcast_quary()\n\nScrapes whole season’s Statcast data\nTakes 30-45 minutes per season: run once & save\nModifies Petti’s original code to account for API changes (# fields, names of variables, etc.)\n\n\n\nraw_statcast2024 &lt;- annual_statcast_query(2024)\nsave(raw_statcast2024, file = \"raw_statcast2024.RData\")"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#extent-of-data",
    "href": "slides/raw_slides/slides_06.html#extent-of-data",
    "title": "STAT 479: Lecture 6",
    "section": "Extent of Data",
    "text": "Extent of Data\n\nannual_statcast_query() loops over weeks in a year and picks up games from\n\nExhibition & Spring Training\nPost-season: wildcard (F) and Divisional, League championship, & World series\nRegular season\n\n\n\ntable(raw_statcast2024$game_type, useNA = 'always')\n\n\n     D      F      L      R      S      W   &lt;NA&gt; \n  5182   2488   3540 695136  77056   1576      0"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#contextual-variables",
    "href": "slides/raw_slides/slides_06.html#contextual-variables",
    "title": "STAT 479: Lecture 6",
    "section": "Contextual Variables",
    "text": "Contextual Variables\n\nEach row corresponds to a pitch\nNumeric id for games (game_pk), at-bats w/in games (atbat_num), & pitches w/in at-bats (pitch_num)\ninning and inning_top_bot\nballs, strikes, outs_when_up\nbatter, on_1b, on_2b, on_3b: offensive player IDs\npitcher, fielder_2, …, fielder_9: defensive player IDs"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#extracting-regular-season-data",
    "href": "slides/raw_slides/slides_06.html#extracting-regular-season-data",
    "title": "STAT 479: Lecture 6",
    "section": "Extracting Regular Season Data",
    "text": "Extracting Regular Season Data\n\nWe’ll focus on regular season games\nAlso remove pitches w/ obvious mistakes\n\n\nstatcast2024 &lt;-\n  raw_statcast2024 |&gt; \n  dplyr::filter(game_type == \"R\") |&gt;\n  dplyr::filter(\n    strikes &gt;= 0 & strikes &lt; 3 & \n      balls &gt;= 0 & balls &lt; 4 & \n      outs_when_up &gt;= 0 & outs_when_up &lt; 3) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#pitch-outcome-variables",
    "href": "slides/raw_slides/slides_06.html#pitch-outcome-variables",
    "title": "STAT 479: Lecture 6",
    "section": "Pitch Outcome Variables",
    "text": "Pitch Outcome Variables\n\ntype: Ball, Strike, contact (X)\ndescription: pitch-level outcome\n\n\n\n                         type\ndescription                    B      S      X\n  ball                    231032      0      0\n  blocked_ball             14717      0      0\n  bunt_foul_tip                0     15      0\n  called_strike                0 113912      0\n  foul                         0 126012      0\n  foul_bunt                    0   1208      0\n  foul_tip                     0   7218      0\n  hit_by_pitch              1979      0      0\n  hit_into_play                0      7 121744\n  missed_bunt                  0    196      0\n  pitchout                    52      0      0\n  swinging_strike              0  73208      1\n  swinging_strike_blocked      0   3834      0"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#data-snapshot",
    "href": "slides/raw_slides/slides_06.html#data-snapshot",
    "title": "STAT 479: Lecture 6",
    "section": "Data Snapshot",
    "text": "Data Snapshot\n\nPitch OutcomesOffensive Players\n\n\n\n\n# A tibble: 20 × 6\n   at_bat_number pitch_number balls strikes type  des                           \n           &lt;int&gt;        &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                         \n 1             1            1     0       0 B     Mookie Betts walks.           \n 2             1            2     1       0 S     Mookie Betts walks.           \n 3             1            3     1       1 B     Mookie Betts walks.           \n 4             1            4     2       1 B     Mookie Betts walks.           \n 5             2            1     0       0 B     Shohei Ohtani grounds into a …\n 6             2            2     1       0 S     Shohei Ohtani grounds into a …\n 7             2            3     1       1 B     Shohei Ohtani grounds into a …\n 8             2            4     2       1 X     Shohei Ohtani grounds into a …\n 9             3            1     0       0 S     Freddie Freeman called out on…\n10             3            2     0       1 S     Freddie Freeman called out on…\n11             3            3     0       2 B     Freddie Freeman called out on…\n12             3            4     1       2 S     Freddie Freeman called out on…\n13             4            1     0       0 B     Will Smith flies out to left …\n14             4            2     1       0 X     Will Smith flies out to left …\n15             5            1     0       0 B     Xander Bogaerts flies out to …\n16             5            2     1       0 S     Xander Bogaerts flies out to …\n17             5            3     1       1 B     Xander Bogaerts flies out to …\n18             5            4     2       1 S     Xander Bogaerts flies out to …\n19             5            5     2       2 B     Xander Bogaerts flies out to …\n20             5            6     3       2 X     Xander Bogaerts flies out to …\n\n\n\n\n\n\n# A tibble: 20 × 6\n   batter outs_when_up  on_1b on_2b on_3b des                                   \n    &lt;int&gt;        &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                 \n 1 605141            0     NA    NA    NA Mookie Betts walks.                   \n 2 605141            0     NA    NA    NA Mookie Betts walks.                   \n 3 605141            0     NA    NA    NA Mookie Betts walks.                   \n 4 605141            0     NA    NA    NA Mookie Betts walks.                   \n 5 660271            0 605141    NA    NA Shohei Ohtani grounds into a force ou…\n 6 660271            0 605141    NA    NA Shohei Ohtani grounds into a force ou…\n 7 660271            0 605141    NA    NA Shohei Ohtani grounds into a force ou…\n 8 660271            0 605141    NA    NA Shohei Ohtani grounds into a force ou…\n 9 518692            1 660271    NA    NA Freddie Freeman called out on strikes.\n10 518692            1 660271    NA    NA Freddie Freeman called out on strikes.\n11 518692            1 660271    NA    NA Freddie Freeman called out on strikes.\n12 518692            1 660271    NA    NA Freddie Freeman called out on strikes.\n13 669257            2 660271    NA    NA Will Smith flies out to left fielder …\n14 669257            2 660271    NA    NA Will Smith flies out to left fielder …\n15 593428            0     NA    NA    NA Xander Bogaerts flies out to right fi…\n16 593428            0     NA    NA    NA Xander Bogaerts flies out to right fi…\n17 593428            0     NA    NA    NA Xander Bogaerts flies out to right fi…\n18 593428            0     NA    NA    NA Xander Bogaerts flies out to right fi…\n19 593428            0     NA    NA    NA Xander Bogaerts flies out to right fi…\n20 593428            0     NA    NA    NA Xander Bogaerts flies out to right fi…"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#player-ids",
    "href": "slides/raw_slides/slides_06.html#player-ids",
    "title": "STAT 479: Lecture 6",
    "section": "Player IDs",
    "text": "Player IDs\n\nStatcast uses MLB Advanced Media ID number for players\nWill be useful to look up player names using IDs (and vice versa)\nChadwick Register maintains a database\n\n\nDownloading Player DatabaseCreating Lookup TableExample\n\n\n\nchadwick_players &lt;- baseballr::chadwick_player_lu()\nsave(chadwick_players, file = \"chadwick_players.RData\")\n\n\n\n\nplayer2024_id &lt;- \n  unique(\n    c(statcast2024$batter, statcast2024$pitcher,\n      statcast2024$on_1b, statcast2024$on_2b, statcast2024$on_3b,\n      statcast2024$fielder_2, statcast2024$fielder_3,\n      statcast2024$fielder_3, statcast2024$fielder_4,\n      statcast2024$fielder_5, statcast2024$fielder_6,\n      statcast2024$fielder_7, statcast2024$fielder_8,\n      statcast2024$fielder_9))\nplayer2024_lookup &lt;-\n  chadwick_players |&gt;\n  dplyr::filter(!is.na(key_mlbam) & key_mlbam %in% player2024_id) |&gt;\n  dplyr::mutate(\n    FullName = paste(name_first, name_last), \n    Name = stringi::stri_trans_general(FullName, \"Latin-ASCII\")) \nsave(player2024_lookup, file = \"player2024_lookup.RData\")\n\n\n\n\nplayer2024_lookup |&gt; \n  dplyr::filter(Name == \"Shohei Ohtani\") |&gt;\n  dplyr::pull(key_mlbam)\n\n[1] 660271\n\nplayer2024_lookup |&gt;\n  dplyr::filter(key_mlbam == 605141) |&gt;\n  dplyr::pull(Name)\n\n[1] \"Mookie Betts\""
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#batting-order",
    "href": "slides/raw_slides/slides_06.html#batting-order",
    "title": "STAT 479: Lecture 6",
    "section": "Batting Order",
    "text": "Batting Order\n\nbaseballr::mlb_batting_orders(): retrieves batting order for every game\n\n\nbaseballr::mlb_batting_orders(game_pk = 745444)\n\n# A tibble: 18 × 8\n       id fullName         abbreviation batting_order batting_position_num team \n    &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;                &lt;chr&gt;\n 1 605141 Mookie Betts     SS           1             0                    away \n 2 660271 Shohei Ohtani    DH           2             0                    away \n 3 518692 Freddie Freeman  1B           3             0                    away \n 4 669257 Will Smith       C            4             0                    away \n 5 571970 Max Muncy        3B           5             0                    away \n 6 606192 Teoscar Hernánd… RF           6             0                    away \n 7 681546 James Outman     CF           7             0                    away \n 8 518792 Jason Heyward    RF           8             0                    away \n 9 666158 Gavin Lux        2B           9             0                    away \n10 593428 Xander Bogaerts  2B           1             0                    home \n11 665487 Fernando Tatis … RF           2             0                    home \n12 630105 Jake Cronenworth 1B           3             0                    home \n13 592518 Manny Machado    DH           4             0                    home \n14 673490 Ha-Seong Kim     SS           5             0                    home \n15 595777 Jurickson Profar LF           6             0                    home \n16 669134 Luis Campusano   C            7             0                    home \n17 642180 Tyler Wade       3B           8             0                    home \n18 701538 Jackson Merrill  CF           9             0                    home \n# ℹ 2 more variables: teamName &lt;chr&gt;, teamID &lt;int&gt;"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#player-positions",
    "href": "slides/raw_slides/slides_06.html#player-positions",
    "title": "STAT 479: Lecture 6",
    "section": "Player Positions",
    "text": "Player Positions\n\nLecture 7: compare batter’s performance to position-level average\nScrape batting orders & determine each player’s most frequent position\n\n\nLineup FunctionNaive LoopSafer LoopDetermine Position\n\n\n\nget_lineup &lt;- function(game_pk){\n  lineup &lt;- baseballr::mlb_batting_orders(game_pk = game_pk)\n  lineup &lt;-\n    lineup |&gt;\n    dplyr::mutate(game_pk = game_pk) |&gt;\n    dplyr::rename(key_mlbam = id, position = abbreviation) |&gt;\n    dplyr::select(game_pk, key_mlbam, position)\n  return(lineup)\n}\n\n\n\n\nall_lineups &lt;- list()\nunik_game_pk &lt;- unique(statcast2024$game_pk)\nfor(i in 1:length(unik_game_pk)){\n  all_lineups[[i]] &lt;- get_lineup(game_pk = unik_game_pk[i])\n}\n\n\n\n\nposs_get_lineup &lt;- purrr::possibly(.f = get_lineup, otherwise = NULL) \nunik_game_pk &lt;- unique(statcast2024$game_pk)\n\nblock_starts &lt;- seq(1, length(unik_game_pk), by = 500)\nblock_ends &lt;- c(block_starts[-1], length(unik_game_pk))\n\nall_lineups &lt;- list()\nfor(b in 1:5){\n  tmp &lt;-\n    purrr::map(.x = unik_game_pk[block_starts[b]:block_ends[b]], \n               .f = poss_get_lineup, \n               .progress = TRUE)\n  all_lineups &lt;- c(all_lineups, tmp)\n}\n\nlineups2024 &lt;- \n  dplyr::bind_rows(all_lineups) |&gt;\n  unique()\nsave(lineups2024, file = \"lineups2024.RData\")\n\n\n\n\npositions2024 &lt;-\n  lineups2024 |&gt;\n  dplyr::group_by(key_mlbam, position) |&gt;\n  dplyr::summarise(n = dplyr::n()) |&gt;\n  dplyr::slice_max(order_by = n, with_ties = FALSE) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::select(key_mlbam, position)\nsave(positions2024, file = \"positions2024.RData\")"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#baserunner-configuration",
    "href": "slides/raw_slides/slides_06.html#baserunner-configuration",
    "title": "STAT 479: Lecture 6",
    "section": "Baserunner Configuration",
    "text": "Baserunner Configuration\n\non_1b, on_2b, and on_3b: tells us who is on base\nUseful to encode configuration w/ 3 binary digits\n\n1st digit for first base, 2nd for second base, 3rd for third base\n101 for runners on 1st and 3rd\n\nAlso useful to rename outs_when_up\n\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::mutate(\n    BaseRunner = \n      paste0(1*(!is.na(on_1b)),1*(!is.na(on_2b)),1*(!is.na(on_3b)))) |&gt;\n  dplyr::rename(Outs = outs_when_up)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#definition",
    "href": "slides/raw_slides/slides_06.html#definition",
    "title": "STAT 479: Lecture 6",
    "section": "Definition",
    "text": "Definition\n\nFor each pitch let \\(R\\) be numbers of runs scored in remainder of half-inning\n\\(\\textrm{o} \\in \\{0,1,2\\}\\) be number of outs\n\\(\\textbf{br} \\in \\{\"000\", \"100\", \"010\", \"001\", \"110\", \"101\", \"011\", \"111\"\\}\\)\n\\(\\rho(\\textrm{o}, \\textrm{br}) = \\mathbb{E}[R \\vert \\textrm{o}, \\textrm{br}]\\)\nAvg. number of runs team expects to score based on current game-state"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#runs-scored-in-half-inning",
    "href": "slides/raw_slides/slides_06.html#runs-scored-in-half-inning",
    "title": "STAT 479: Lecture 6",
    "section": "Runs Scored in Half-Inning",
    "text": "Runs Scored in Half-Inning\n\nSuppose there are \\(n_{a}\\) pitces in at-bat \\(a\\)\n\\(R_{i,a}\\): number of runs scored in half-inning after pitch \\(i\\) in at-bat \\(a\\)\nStep 1: append a column to statcast2024 containing \\(R_{i,a}\\) values\nWill utilize the following Statcast variables\n\nbat_score: batting team score before the pitch is thrown\npost_bat_score: batting team score after pitch is thrown"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#illustration-dodgers-8th-inning",
    "href": "slides/raw_slides/slides_06.html#illustration-dodgers-8th-inning",
    "title": "STAT 479: Lecture 6",
    "section": "Illustration: Dodger’s 8th Inning",
    "text": "Illustration: Dodger’s 8th Inning\n\nPlay-by-PlayDataRuns Remaining\n\n\n\n\n\nPlay-by-play\n\n\n\n\n\nrbind(bat_score = dodgers_inning$bat_score, post_bat_score = dodgers_inning$post_bat_score)\n\n               [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nbat_score         1    1    1    1    1    1    1    1    1     1     1     1\npost_bat_score    1    1    1    1    1    1    1    1    1     1     1     1\n               [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nbat_score          1     1     2     3     3     3     4     5     5     5\npost_bat_score     1     2     3     3     3     4     5     5     5     5\n               [,23] [,24] [,25]\nbat_score          5     5     5\npost_bat_score     5     5     5\n\ndodgers_inning$des[c(14,15, 18, 19)]\n\n[1] \"Enrique Hernández out on a sacrifice fly to left fielder José Azocar. Max Muncy scores.\"                                                                                             \n[2] \"Gavin Lux reaches on a fielder's choice, fielded by first baseman Jake Cronenworth. Teoscar Hernández scores. James Outman to 2nd. Fielding error by first baseman Jake Cronenworth.\"\n[3] \"Mookie Betts singles on a ground ball to left fielder José Azocar. James Outman scores. Gavin Lux to 2nd.\"                                                                           \n[4] \"Shohei Ohtani singles on a line drive to left fielder José Azocar. Gavin Lux scores. Mookie Betts to 2nd.\"                                                                           \n\n\n\n\n\ndplyr::last(dodgers_inning$post_bat_score) - dodgers_inning$bat_score\n\n [1] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 2 2 2 1 0 0 0 0 0 0"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#computing-all-r_ias",
    "href": "slides/raw_slides/slides_06.html#computing-all-r_ias",
    "title": "STAT 479: Lecture 6",
    "section": "Computing All \\(R_{i,a}\\)’s",
    "text": "Computing All \\(R_{i,a}\\)’s\n\nSplit data table based on half-inning\n\ngroup_by(game_pk, inning_number, inning_topbot)\n\nGet the last value of post_bat_score w/in half-inning\n\\(R_{i,a}\\): dplyr::last(post_bat_score) - bat_score\n\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(game_pk, inning, inning_topbot) |&gt; \n  dplyr::arrange(at_bat_number, pitch_number) |&gt; \n  dplyr::mutate(RunsRemaining = dplyr::last(post_bat_score) - bat_score) |&gt;\n  dplyr::ungroup()"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#computing-expected-runs",
    "href": "slides/raw_slides/slides_06.html#computing-expected-runs",
    "title": "STAT 479: Lecture 6",
    "section": "Computing Expected Runs",
    "text": "Computing Expected Runs\n\nRecall definition: \\(\\rho(\\textrm{o}, \\textrm{br}) = \\mathbb{E}[R \\vert \\textrm{o}, \\textrm{br}]\\)\nGroup pitches by Outs and BaseRunner and average RunsRemaining\n\n\nCodeTerminal StateExpected Runs MatrixOhtani’s Performance\n\n\n\nexpected_runs &lt;-\n  statcast2024 |&gt;\n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(Outs, BaseRunner, RunsRemaining) |&gt;\n  dplyr::group_by(Outs, BaseRunner) |&gt;\n  dplyr::summarize(rho = mean(RunsRemaining), .groups = \"drop\")\n\n\n\n\nUseful to create 25th state for end of inning\n\n\nexpected_runs &lt;-\n  expected_runs |&gt;\n  tibble::add_row(Outs=3, BaseRunner=\"000\", rho = 0)\n\n\n\n\n\n# A tibble: 8 × 5\n  BaseRunner `Outs: 0` `Outs: 1` `Outs: 2` `Outs: 3`\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 000            0.488     0.262    0.0980         0\n2 001            1.43      0.972    0.352         NA\n3 010            1.07      0.672    0.347         NA\n4 011            2.03      1.44     0.612         NA\n5 100            0.897     0.529    0.228         NA\n6 101            1.90      1.22     0.502         NA\n7 110            1.49      0.926    0.449         NA\n8 111            2.31      1.58     0.815         NA\n\n\n\n\n\nFirst single: Ohtani’s team increased run expectance by 0.12\n\nStarting Outs=2, BaseRunner = '000': \\(\\rho \\approx 0.1\\)\nEnding: Outs=2, BaseRunner= '100': \\(\\rho \\approx 0.22\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#run-value",
    "href": "slides/raw_slides/slides_06.html#run-value",
    "title": "STAT 479: Lecture 6",
    "section": "Run Value",
    "text": "Run Value\n\n\\(\\textrm{RunsScored}\\): number of runs scored in each at-bat \\[\n\\textrm{RunValue} = \\textrm{RunsScored} + \\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}) - \\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\n\\]\nRun value tracks actual number of runs scored and change in expectancy\n\n\n\nTo compute \\(\\textrm{RunValue}\\) we must\n\nCompute \\(\\textrm{RunsScored}\\)\nDetermine starting and ending game-states (i.e., Outs and BaseRunner)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#computing-textrmrunsscored",
    "href": "slides/raw_slides/slides_06.html#computing-textrmrunsscored",
    "title": "STAT 479: Lecture 6",
    "section": "Computing \\(\\textrm{RunsScored}\\)",
    "text": "Computing \\(\\textrm{RunsScored}\\)\n\nComputing \\(\\textrm{RunsScored}\\) involves\n\nSort pitches by at-bat number & pitch-number\nSubtract first bat_score from last post_bat_score in each at-bat\n\n\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt; \n  dplyr::arrange(pitch_number) |&gt; \n  dplyr::mutate(RunsScored = dplyr::last(post_bat_score) - dplyr::first(bat_score)) |&gt; \n  dplyr::ungroup() |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#starting-ending-states",
    "href": "slides/raw_slides/slides_06.html#starting-ending-states",
    "title": "STAT 479: Lecture 6",
    "section": "Starting & Ending States",
    "text": "Starting & Ending States\n\nState of Next PitchEnding State of At-BatAt-Bat Level Data\n\n\n\nStarting state of pitch \\(i+1\\) is ending state of pitch \\(i\\)\nUse dplyr::lead() to get next value (next_Outs, next_BaseRuner)\n\n\nrunValue2024 &lt;- \n  statcast2024 |&gt;\n  dplyr::group_by(game_pk, inning, inning_topbot) |&gt; \n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n    next_Outs = dplyr::lead(Outs), \n    next_BaseRunner = dplyr::lead(BaseRunner)) |&gt;\n  dplyr::ungroup() |&gt;\n\n\n\n\nLast value of next_Outs and next_BaseRunner gives at-bat’s ending state\nCompute this w/in every at-bat in every game\n\n\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n    end_Outs = dplyr::last(next_Outs), \n    end_BaseRunner = dplyr::last(next_BaseRunner)) |&gt; \n  dplyr::ungroup() |&gt;\n\n\n\n\nMust convert from pitch- to at-bat-level\nFirst pitch in at-bat gives starting state\n\n\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::filter(pitch_number == 1) |&gt; \n  dplyr::select(\n    game_pk, at_bat_number, \n    inning, inning_topbot, \n    Outs, BaseRunner, \n    RunsScored, RunsRemaining, \n    end_Outs, end_BaseRunner)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#computing-textrmrunvalue",
    "href": "slides/raw_slides/slides_06.html#computing-textrmrunvalue",
    "title": "STAT 479: Lecture 6",
    "section": "Computing \\(\\textrm{RunValue}\\)",
    "text": "Computing \\(\\textrm{RunValue}\\)\n\nTerminal StatesJoining Expected Runs\n\n\n\ndplyr::lead() produces NA’s at the end of half-inning\n\n\nrunValue2024 &lt;-\n  runValue2024 |&gt;\n  dplyr::mutate(\n    end_Outs = ifelse(is.na(end_Outs), 3, end_Outs),\n    end_BaseRunner = ifelse(is.na(end_BaseRunner), '000', end_BaseRunner))\n\n\n\n\nend_expected_runs &lt;- \n  expected_runs |&gt;\n  dplyr::rename(\n    end_Outs = Outs,\n    end_BaseRunner = BaseRunner,\n    end_rho = rho)\n\nrunValue2024 &lt;-\n  runValue2024 |&gt;\n  dplyr::left_join(y = expected_runs, by = c(\"Outs\", \"BaseRunner\")) |&gt;\n  dplyr::left_join(y = end_expected_runs, by = c(\"end_Outs\", \"end_BaseRunner\")) |&gt;\n  dplyr::mutate(RunValue = RunsScored + end_rho - rho) |&gt;\n  dplyr::select(game_pk, at_bat_number, RunValue)"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#ohtanis-single-game-performance",
    "href": "slides/raw_slides/slides_06.html#ohtanis-single-game-performance",
    "title": "STAT 479: Lecture 6",
    "section": "Ohtani’s Single-Game Performance",
    "text": "Ohtani’s Single-Game Performance\n\nExtract every Ohtani at-bat from game against Padres\nAppend run value by joining using game_pk and at_bat_number\n\n\nohtani_id &lt;- \n  player2024_lookup |&gt;\n  dplyr::filter(FullName == \"Shohei Ohtani\") |&gt;\n  dplyr::pull(key_mlbam)\n\nohtani_ab &lt;-\n  statcast2024 |&gt;\n  dplyr::filter(game_pk == 745444) |&gt;\n  dplyr::filter(pitch_number == 1 & batter == ohtani_id) |&gt;\n  dplyr::select(game_pk, at_bat_number, inning, des) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\")) |&gt;\n  dplyr::select(inning, RunValue, des)\nohtani_ab\n\n# A tibble: 5 × 3\n  inning RunValue des                                                           \n   &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                                                         \n1      1   -0.367 Shohei Ohtani grounds into a force out, shortstop Ha-Seong Ki…\n2      3    0.130 Shohei Ohtani singles on a sharp line drive to right fielder …\n3      5   -0.367 Shohei Ohtani grounds into a force out, third baseman Tyler W…\n4      7   -0.164 Shohei Ohtani grounds out softly, pitcher Wandy Peralta to fi…\n5      8    1     Shohei Ohtani singles on a line drive to left fielder José Az…"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#season-leaders",
    "href": "slides/raw_slides/slides_06.html#season-leaders",
    "title": "STAT 479: Lecture 6",
    "section": "Season Leaders",
    "text": "Season Leaders\n\nTemporary LookupComputing RE24RE24 Leaders\n\n\n\nUse batter in statcast2024 to look up player name in player2024_lookup\nProblem: player2024_lookup doesn’t have column batter\nSolution: temporary look-up table renaming key_mlbam to batter\n\n\ntmp_lookup &lt;-\n  player2024_lookup |&gt;\n  dplyr::select(key_mlbam, Name) |&gt;\n  dplyr::rename(batter = key_mlbam)\n\n\n\n\nFor each player, sum RunValue across all at-bats (RE24)\n\n\nre24 &lt;-\n  statcast2024 |&gt; \n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(game_pk, at_bat_number, batter) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\")) |&gt;\n  dplyr::group_by(batter) |&gt;\n  dplyr::summarise(RE24 = sum(RunValue),N = dplyr::n()) |&gt;\n  dplyr::inner_join(y = tmp_lookup, by = \"batter\") |&gt;\n  dplyr::select(Name, RE24, N)\n\n\n\n\nAaron Judge appears to have created the most run value for his team in 2024\n\n\nre24 |&gt; \n  dplyr::arrange(dplyr::desc(RE24)) |&gt;\n  dplyr::slice_head(n=10)\n\n# A tibble: 10 × 3\n   Name               RE24     N\n   &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n 1 Aaron Judge        89.0   675\n 2 Juan Soto          74.4   693\n 3 Shohei Ohtani      73.1   708\n 4 Bobby Witt         65.9   694\n 5 Brent Rooker       47.9   599\n 6 Vladimir Guerrero  45.0   671\n 7 Ketel Marte        41.2   562\n 8 Kyle Schwarber     40.9   672\n 9 Joc Pederson       39.2   433\n10 Jose Ramirez       39.1   657"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#looking-ahead",
    "href": "slides/raw_slides/slides_06.html#looking-ahead",
    "title": "STAT 479: Lecture 6",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nSuppose batter hits a single and baserunner advances from 1st to 3rd\n\nRun Value reflects change in run expectancy. BaseRunner: '100' \\(\\rightarrow\\) '101')\nIs it fair to give batter all the credit for creating run value?\n\nLecture 7: Dividing run value b/w baserunner and batter\nLecture 8: Divide negative run value b/w pitcher and fielder"
  },
  {
    "objectID": "slides/raw_slides/slides_06.html#projects",
    "href": "slides/raw_slides/slides_06.html#projects",
    "title": "STAT 479: Lecture 6",
    "section": "Projects",
    "text": "Projects\n\nProject Check-in: by Friday 26 September email me a short overview of your project\n\nPrecise problem statement & overview of data and analysis plan\nHappy to help brainstorm & narrow down analysis in Office Hours (or by appt.)\nInspiration: exercises in lecture notes & project information page\n\nOffice Hours this week:\n\nTuesday (today): 4pm - 5:45pm (MH 5586)\nWednesday: 3pm-4pm (MH5586)\nFriday: 1pm - 3pm (MH closed after 3pm)"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#offensive-credit-allocation",
    "href": "slides/raw_slides/slides_08.html#offensive-credit-allocation",
    "title": "STAT 479: Lecture 8",
    "section": "Offensive Credit Allocation",
    "text": "Offensive Credit Allocation\n\nDuring at-bat \\(i\\), batting team generates \\(\\delta_{i}\\) run value\nLecture 7: divide \\(\\delta_{i}\\) b/w batter and baserunners\nConservation of run value: if batting team gains \\(\\delta\\), fielding team gains \\(-\\delta\\)\nToday: divide \\(-\\delta_{i}\\) b/w pitcher and fielders"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#two-possibilities",
    "href": "slides/raw_slides/slides_08.html#two-possibilities",
    "title": "STAT 479: Lecture 8",
    "section": "Two Possibilities",
    "text": "Two Possibilities\n\nBall is not put in play during at-bat\n\nAt-bat ends w/ homerun, strikeout, walk\nPitcher deserves all credit (or blame)\n\n\n\n\nBall is put in play (e.g., flyout, groundout, single, double, triple, etc.)\n\n\\(\\hat{p}\\): probability of making an out (based on location)\nGive \\(\\delta^{(p)} = -\\delta \\times (1-\\hat{p})\\) to the pitcher\nDivide \\(\\delta^{(f)} = -\\delta \\times \\hat{p}\\) among other fielders"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#statcast-coordinates",
    "href": "slides/raw_slides/slides_08.html#statcast-coordinates",
    "title": "STAT 479: Lecture 8",
    "section": "Statcast Coordinates",
    "text": "Statcast Coordinates\n\nBackgroundVisualizationFirst Base\n\n\n\nhc_x and hc_y: coordinates where batted ball is first fielded\nhit_location: position of player who first fielding ball\nStatcast coordinate system\n\nHome plate at top of plot near (125, 200)\nFirst baseline on the left\nUnits are not in feet\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Locations where all batted balls are first fielded\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Location of all batted balls initially fielded by the first baseman"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#transformed-coordinates",
    "href": "slides/raw_slides/slides_08.html#transformed-coordinates",
    "title": "STAT 479: Lecture 8",
    "section": "Transformed Coordinates",
    "text": "Transformed Coordinates\n\nDetailsVisualization\n\n\n\nTransform: x = 2.5 * (hc_x - 125.42) & y = 2.5 * (198.27 - hc_y)\nDefine new coordinate system where\n\nHome plate at bottom of plot at (0,0)\nUnits are in feet\nFirst base on the right around (90/sqrt(2), 90/sqrt(2))\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: New coordinate system"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#more-data-preparation",
    "href": "slides/raw_slides/slides_08.html#more-data-preparation",
    "title": "STAT 479: Lecture 8",
    "section": "More Data Preparation",
    "text": "More Data Preparation\n\ndef_atbat2024: at-bat level data table containing\n\nPitcher & fielder identities\nBatted ball locations & fielder position\nEnding values of type and events: i.e. end_type and end_events\n\nUseful for determining at-bat level outcomes\n\nNarrative descriptions: des\n\nSee lecture notes for full code"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#unique-events-i",
    "href": "slides/raw_slides/slides_08.html#unique-events-i",
    "title": "STAT 479: Lecture 8",
    "section": "Unique Events I",
    "text": "Unique Events I\n\nend_type: did last pitch of at-bat end in Strike, Ball, or contact (X)?\nevents: more granular description of at-bat outcome\nAll events when contact is made\n\n\ntable(def_atbat2024$end_events[def_atbat2024$end_type == \"X\"], useNA = 'always')\n\n\n                   double               double_play               field_error \n                     7608                       336                      1093 \n                field_out           fielders_choice       fielders_choice_out \n                    72233                       373                       306 \n                force_out grounded_into_double_play                  home_run \n                     3408                      3152                      5326 \n                 sac_bunt                   sac_fly       sac_fly_double_play \n                      446                      1222                        13 \n                   single                    triple               triple_play \n                    25363                       685                         1 \n                     &lt;NA&gt; \n                        0"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#unique-events-ii",
    "href": "slides/raw_slides/slides_08.html#unique-events-ii",
    "title": "STAT 479: Lecture 8",
    "section": "Unique Events II",
    "text": "Unique Events II\n\nAll events for balls and strikes\n\n\ntable(def_atbat2024$end_events[def_atbat2024$end_type != \"X\"], useNA = 'always')\n\n\n                             catcher_interf          hit_by_pitch \n                  308                    97                  1977 \n            strikeout strikeout_double_play          truncated_pa \n                40145                   107                   304 \n                 walk                  &lt;NA&gt; \n                14029                     0"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#extract-balls-in-play",
    "href": "slides/raw_slides/slides_08.html#extract-balls-in-play",
    "title": "STAT 479: Lecture 8",
    "section": "Extract Balls in Play",
    "text": "Extract Balls in Play\n\nSome manual correction was needed\n\n\nbip &lt;- def_atbat2024 |&gt; dplyr::filter(end_type == \"X\")\nbip$end_events[65138] &lt;- \"fielders_choice_out\"\nout_events &lt;- \n  c(\"double_play\", \"field_out\", \"fielders_choice_out\",\n    \"force_out\", \"grounded_into_double_play\", \n    \"sac_bunt\", \"sac_fly\", \"sac_fly_double_play\",\n    \"triple_play\")\nbip &lt;-\n  bip |&gt;\n  dplyr::filter(end_events != \"home_run\" & !is.na(x) & !is.na(y)) |&gt;\n  dplyr::mutate(Out = ifelse(end_events %in% out_events, 1, 0))"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#binning-averaging",
    "href": "slides/raw_slides/slides_08.html#binning-averaging",
    "title": "STAT 479: Lecture 8",
    "section": "Binning & Averaging",
    "text": "Binning & Averaging\n\nGrid CodeGridEstimation CodeEstimates\n\n\n\nDivide field into grid of 3ft x 3ft bins\nRemove unrealistic grid locations\n\n\ngrid_sep &lt;- 3 \nx_grid &lt;- seq(from = -300, to = 300, by = grid_sep)\ny_grid &lt;- seq(from = -100, to = 500, by = grid_sep)\nraw_grid &lt;- expand.grid(x = x_grid, y = y_grid)\n\ngrid &lt;- raw_grid |&gt;\n  dplyr::filter(y + x &gt; -100 & y - x &gt; -100 & sqrt(x^2 + y^2) &lt; 580)\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Restricted grid of spatial locations\n\n\n\n\n\n\n\n\nbin_probs &lt;-\n  bip |&gt;\n  dplyr::select(x, y, Out) |&gt;\n  dplyr::mutate(\n    x_bin = cut(x, breaks = seq(-300-grid_sep/2, 300+grid_sep/2, by = grid_sep)), \n    y_bin = cut(y, breaks = seq(-100 - grid_sep/2, 500+grid_sep/2, by = grid_sep))) |&gt;\n  dplyr::group_by(x_bin, y_bin) |&gt;\n  dplyr::summarise(\n    out_prob = mean(Out), \n    n_balls = dplyr::n(),\n    .groups = \"drop\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Empirical out probabilities based on binning balls in play."
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#logistic-regression",
    "href": "slides/raw_slides/slides_08.html#logistic-regression",
    "title": "STAT 479: Lecture 8",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nWhat about a logistic regression model? \\[\n\\log\\left(\\frac{\\mathbb{P}(\\textrm{out})}{1 - \\mathbb{P}(\\textrm{out})}\\right) = \\beta_{0} + \\beta_{1}x + \\beta_{2}y\n\\]\n\n\nlogit_fit &lt;-\n  glm(Out ~ x + y,\n      family = binomial(link = \"logit\"), data = bip)"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#logistic-regression-estimates",
    "href": "slides/raw_slides/slides_08.html#logistic-regression-estimates",
    "title": "STAT 479: Lecture 8",
    "section": "Logistic Regression Estimates",
    "text": "Logistic Regression Estimates\n\n\n\n\n\n\n\n\nFigure 6: Logistic regression forecasts of out probabilities as a function of location."
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#generalized-additive-model",
    "href": "slides/raw_slides/slides_08.html#generalized-additive-model",
    "title": "STAT 479: Lecture 8",
    "section": "Generalized Additive Model",
    "text": "Generalized Additive Model\n\\[\n\\log\\left(\\frac{\\mathbb{P}(\\textrm{out})}{1 - \\mathbb{P}(\\textrm{out})} \\right) = s(x,y),\n\\]\n\n\\(s(x,y)\\) is a smooth function in both \\(x\\) and \\(y\\)\n\nTechnically: \\(s(x,y) = \\sum_{d=1}^{D}{\\beta_{d}\\phi_{d}(x,y)}\\)\n\\(\\phi_{1}, \\ldots, \\phi_{D}\\): fixed set of spline functions\n\\(\\phi_{d}\\) piecewise polynomial localized to small spatial region\n\nCan be fit using the mgcv package"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#gam-estimates",
    "href": "slides/raw_slides/slides_08.html#gam-estimates",
    "title": "STAT 479: Lecture 8",
    "section": "GAM Estimates",
    "text": "GAM Estimates\n\nFit GAMVisualizationAppending Out Probs\n\n\n\nUse mgcv::bam for large datasets\n\n\nlibrary(mgcv)\ngam_fit &lt;-\n  bam(formula = Out ~ s(x,y), \n      family = binomial(link=\"logit\"), data = bip)\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: GAM-estimated out probabilities\n\n\n\n\n\n\n\n\nall_preds &lt;-\n  predict(object = gam_fit, \n          newdata = def_atbat2024,\n          type = \"response\")\ndef_atbat2024$p_out &lt;- all_preds\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    p_out = dplyr::case_when(\n      is.na(p_out) & end_events == \"home_run\" ~ 0, \n      is.na(p_out) & end_type != \"X\" ~ 0, \n      .default = p_out)) |&gt;\n  dplyr::filter(!is.na(p_out))"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#defining-deltap-and-deltaf",
    "href": "slides/raw_slides/slides_08.html#defining-deltap-and-deltaf",
    "title": "STAT 479: Lecture 8",
    "section": "Defining \\(\\delta^{(p)}\\) and \\(\\delta^{(f)}\\)",
    "text": "Defining \\(\\delta^{(p)}\\) and \\(\\delta^{(f)}\\)\n\nDivide run value \\(\\delta\\) into\n\n\\(\\delta^{(f)} = -1 \\times \\hat{p} \\times \\delta\\)\n\\(\\delta^{(p)} = -1 \\times (1 - \\hat{p}) \\times \\delta\\)\n\n\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    delta_p = -1 * (1 - p_out) * RunValue,\n    delta_f = -1 * p_out * RunValue)"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#textrmraap-definition",
    "href": "slides/raw_slides/slides_08.html#textrmraap-definition",
    "title": "STAT 479: Lecture 8",
    "section": "\\(\\textrm{RAA}^{(p)}\\) (definition)",
    "text": "\\(\\textrm{RAA}^{(p)}\\) (definition)\n\n\\(\\delta_{i}^{(p)} = -1 \\times (1 - \\hat{p}_{i}) \\times \\delta_{i}\\)\n\nRun value created by pitcher in at-bat \\(i\\)\nPositive values indicate good performance\n\n\\(\\textrm{RAA}^{(p)}\\): sum \\(\\delta_{i}^{(p)}\\)’s across each pitchers at-bats"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#computing-textrmraap",
    "href": "slides/raw_slides/slides_08.html#computing-textrmraap",
    "title": "STAT 479: Lecture 8",
    "section": "Computing \\(\\textrm{RAA}^{(p)}\\)",
    "text": "Computing \\(\\textrm{RAA}^{(p)}\\)\n\nCodeTop-10\n\n\n\nraa_p &lt;-\n  def_atbat2024 |&gt;\n  dplyr::select(pitcher, delta_p) |&gt;\n  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarise(RAA_p = sum(delta_p, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = pitcher) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_p)\n\n\n\n\nraa_p |&gt;\n  dplyr::arrange(dplyr::desc(RAA_p)) |&gt;\n  dplyr::slice_head(n=10)\n\n# A tibble: 10 × 3\n   Name            key_mlbam RAA_p\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt;\n 1 Tarik Skubal       669373 14.7 \n 2 Chris Sale         519242 14.6 \n 3 Ryan Walker        676254 13.5 \n 4 Cade Smith         671922 13.3 \n 5 Paul Skenes        694973 12.3 \n 6 Emmanuel Clase     661403 11.2 \n 7 Kirby Yates        489446  9.99\n 8 Griffin Jax        643377  9.53\n 9 Edwin Uceta        670955  9.23\n10 Garrett Crochet    676979  9.13"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#fielding-responsibility",
    "href": "slides/raw_slides/slides_08.html#fielding-responsibility",
    "title": "STAT 479: Lecture 8",
    "section": "Fielding Responsibility",
    "text": "Fielding Responsibility\n\nSay ball hit towards gap b/w 1st base and right field\n\nBatting team creates large positive \\(\\delta\\)\n\\(\\hat{p} \\approx 0\\)\n\\(\\delta^{(f)}\\) is large and negative\n\nHow much blame should third baseman receive?"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#fielder-out-probabilities",
    "href": "slides/raw_slides/slides_08.html#fielder-out-probabilities",
    "title": "STAT 479: Lecture 8",
    "section": "Fielder Out Probabilities",
    "text": "Fielder Out Probabilities\n\n1B Out Prob3B Out ProbRF Out Prob\n\n\n\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#position-weights",
    "href": "slides/raw_slides/slides_08.html#position-weights",
    "title": "STAT 479: Lecture 8",
    "section": "Position Weights",
    "text": "Position Weights\n\nIdea: assign \\(w_{\\ell} \\delta^{(f)}\\) to fielder at position \\(\\ell\\) \\[\nw_{\\ell} = \\frac{\\hat{p}_{\\ell}}{\\hat{p}_{1} + \\cdots + \\hat{p}_{9}}\n\\]\n\\(\\hat{p}_{\\ell}\\): prob. that fielder position \\(\\ell\\) makes out based on location\nCan estimate \\(\\hat{p}_{\\ell}\\) with fielder-specific GAMs\n\nSee lecture notes for code\nTakes ~10-15 minutes to run\n\nNumerical stability: restrict to balls w/in 150ft of typical fielder location\n\nI.e., balls hit to deep left field likely irrelevant for first baseman model"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#position-weights-examples",
    "href": "slides/raw_slides/slides_08.html#position-weights-examples",
    "title": "STAT 479: Lecture 8",
    "section": "Position Weights Examples",
    "text": "Position Weights Examples\n\n1B Weight3B WeightRF Weight\n\n\n\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#run-value-by-fielding-position",
    "href": "slides/raw_slides/slides_08.html#run-value-by-fielding-position",
    "title": "STAT 479: Lecture 8",
    "section": "Run Value by Fielding Position",
    "text": "Run Value by Fielding Position\n\nDefinitionFirst Base (code)First Base (Top-10)\n\n\n\nFor each at-bat \\(i\\) assign \\(w_{i,\\ell}\\delta_{i}^{(f)}\\) to player at position \\(\\ell\\)\nSums these values for each player-position to get \\(\\textrm{RAA}_{\\ell}^{(f)}\\)\n\nSome players play multiple positions in the field\n\n\\(\\textrm{RAA}_{\\ell}^{(f)}\\): run value created by playing position \\(\\ell\\)\n\n\n\n\nraa_f3 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f3 = delta_f * w3) |&gt;\n  dplyr::group_by(fielder_3) |&gt;\n  dplyr::summarize(RAA_f3 = sum(RAA_f3, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_3)\n\n\n\n\nraa_f3 |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, RAA_f3) |&gt;\n  dplyr::arrange(dplyr::desc(RAA_f3)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   Name              RAA_f3\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 Carlos Santana      55.7\n 2 Christian Walker    54.6\n 3 Paul Goldschmidt    52.4\n 4 Bryce Harper        51.1\n 5 Matt Olson          51.0\n 6 Ryan Mountcastle    48.7\n 7 Vladimir Guerrero   47.1\n 8 Michael Toglia      46.5\n 9 Freddie Freeman     46.2\n10 Josh Naylor         46.2"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#total-fielding-run-value-textrmraaf",
    "href": "slides/raw_slides/slides_08.html#total-fielding-run-value-textrmraaf",
    "title": "STAT 479: Lecture 8",
    "section": "Total Fielding Run Value \\(\\textrm{RAA}^{(f)}\\)",
    "text": "Total Fielding Run Value \\(\\textrm{RAA}^{(f)}\\)\n\nFor each player we have \\(\\textrm{RAA}^{(f)}_{1}, \\ldots, \\textrm{RAA}^{(f)}_{9}\\)\n\n\\(\\textrm{RAA}^{(f)}_{\\ell}\\): total run value created from fielding at position \\(\\ell\\)\n\n\\(\\textrm{RAA}^{(f)} = \\textrm{RAA}^{(f)}_{1} + \\cdots + \\textrm{RAA}^{(f)}_{9}\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#putting-it-all-together",
    "href": "slides/raw_slides/slides_08.html#putting-it-all-together",
    "title": "STAT 479: Lecture 8",
    "section": "Putting It All Together",
    "text": "Putting It All Together\n\n\\(\\textrm{RAA} = \\textrm{RAA}^{(b)} + \\textrm{RAA}^{(br)} + \\textrm{RAA}^{(f)} + \\textrm{RAA}^{(p)}\\)\n\n\nraa &lt;-\n  raa_b |&gt;\n  dplyr::select(-Name) |&gt;\n  dplyr::full_join(y = raa_br |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_p |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(list(RAA_b = 0, RAA_br = 0, RAA_f = 0, RAA_p = 0)) |&gt;\n  dplyr::mutate(RAA = RAA_b + RAA_br + RAA_f + RAA_p) |&gt;\n  dplyr::left_join(y = player2024_lookup |&gt; dplyr::select(key_mlbam, Name), by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, RAA_b, RAA_br, RAA_f, RAA_p)"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#raa-leaderboard",
    "href": "slides/raw_slides/slides_08.html#raa-leaderboard",
    "title": "STAT 479: Lecture 8",
    "section": "RAA Leaderboard",
    "text": "RAA Leaderboard\n\n\n# A tibble: 10 × 7\n   Name              key_mlbam   RAA  RAA_b RAA_br RAA_f RAA_p\n   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bobby Witt           677951 165.  61.1   24.9    78.7     0\n 2 Gunnar Henderson     683002 117.  44.8   -2.82   75.0     0\n 3 Elly De La Cruz      682829 114.  19.4   15.4    79.4     0\n 4 Jose Ramirez         608070 113.  30.9   32.7    49.7     0\n 5 Zach Neto            687263 110.   3.57  15.4    91.2     0\n 6 Marcus Semien        543760 108.   0.501 17.7    89.6     0\n 7 Ketel Marte          606466 101.  35.2   10.5    55.3     0\n 8 Vladimir Guerrero    665489  99.0 47.3   -0.734  52.4     0\n 9 Francisco Lindor     596019  98.5 30.6    0.699  67.2     0\n10 Jose Altuve          514888  98.2 19.3   13.9    65.0     0"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#recap",
    "href": "slides/raw_slides/slides_08.html#recap",
    "title": "STAT 479: Lecture 8",
    "section": "Recap",
    "text": "Recap\n\n\\(\\textrm{RAA} = \\textrm{RAA}^{(b)} + \\textrm{RAA}^{(br)} + \\textrm{RAA}^{(f)} + \\textrm{RAA}^{(p)}\\)\nComprehensive measure of player performance in all parts of the game\nAbsolute \\(\\textrm{RAA}\\) are interesting…\n… but are much more useful when calibrated to some baseline"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#replacement-level-1",
    "href": "slides/raw_slides/slides_08.html#replacement-level-1",
    "title": "STAT 479: Lecture 8",
    "section": "Replacement Level",
    "text": "Replacement Level\n\nQuoting from FanGraphs\n\n\nReplacement level is simply the level of production you could get from a player that would cost you nothing but the league minimum salary to acquire.\n\n\n\nDefinition is fairly arbitrary!\n\n\nwe believe that a team making the MLB minimum would win about 29.7% of its games in a give year, or roughly 47-48 per team"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#roster-based-definition",
    "href": "slides/raw_slides/slides_08.html#roster-based-definition",
    "title": "STAT 479: Lecture 8",
    "section": "Roster-based Definition",
    "text": "Roster-based Definition\n\nMost MLB teams carry 12 pitchers & 13 position players\nOn any given day:\n\n\\(30 \\times 12 = 360\\) available pitchers\n\\(30 \\times 13 = 390\\) available position players\n\nSort position players by number of at-bats in which they batted\n\nTop 390: not-replacement\n\nSort pitchers by numbers of at-bats in which they pitched\n\nTop 360: not-replacement"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#identifying-non-replacement-players",
    "href": "slides/raw_slides/slides_08.html#identifying-non-replacement-players",
    "title": "STAT 479: Lecture 8",
    "section": "Identifying Non-Replacement Players",
    "text": "Identifying Non-Replacement Players\n\nPlayer ListsCount PAsReplacement Thresholds\n\n\n\nall_players &lt;- unique(\n  c(def_atbat2024$batter, def_atbat2024$pitcher, def_atbat2024$fielder_2,\n    def_atbat2024$fielder_3, def_atbat2024$fielder_4, def_atbat2024$fielder_5,\n    def_atbat2024$fielder_6, def_atbat2024$fielder_7, def_atbat2024$fielder_8, def_atbat2024$fielder_9))\npitchers &lt;- unique(def_atbat2024$pitcher)\nposition_players &lt;- all_players[!all_players %in% pitchers]\n\n\n\n\nposition_pa &lt;-\n  def_atbat2024 |&gt;\n  dplyr::filter(batter %in% position_players) |&gt;\n  dplyr::group_by(batter) |&gt;\n  dplyr::summarise(n = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(n)) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\npitcher_pa &lt;-\n  def_atbat2024 |&gt;\n  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarise(n = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(n)) |&gt;\n  dplyr::rename(key_mlbam = pitcher)\n\n\n\n\nrepl_position_players &lt;- position_pa$key_mlbam[-(1:390)]\nrepl_pitchers &lt;- pitcher_pa$key_mlbam[-(1:360)]\n\ncat(\"Cut-off for position players:\", position_pa$n[390], \"\\n\")\ncat(\"Cut-off for pitchers:\", pitcher_pa$n[360], \"\\n\")\n\nCut-off for position players: 131 \nCut-off for pitchers: 204"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#replacement-level-per-at-bat-raa",
    "href": "slides/raw_slides/slides_08.html#replacement-level-per-at-bat-raa",
    "title": "STAT 479: Lecture 8",
    "section": "Replacement-Level Per-At-Bat RAA",
    "text": "Replacement-Level Per-At-Bat RAA\n\nIf we replace Ohtani w/ a replacement-level player, what \\(\\textrm{RAA}\\) values would they achieve?\nIdea: multiply Ohtani’s number of at-bats by average per-at-bat \\(\\textrm{RAA}\\) of replacement-level players\nDivide total \\(\\textrm{RAA}\\) for all replacement-level players by total number of at-bats\n\n\nrepl_position_raa &lt;-\n  raa |&gt;\n  dplyr::filter(key_mlbam %in% repl_position_players) |&gt;\n  dplyr::inner_join(y = position_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n)\n\nrepl_pitch_raa &lt;-\n  raa |&gt;\n  dplyr::filter(key_mlbam %in% repl_pitchers) |&gt;\n  dplyr::inner_join(y = pitcher_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n) \n\nrepl_avg_pos &lt;- sum(repl_position_raa$RAA)/sum(repl_position_raa$n)\nrepl_avg_pitch &lt;- sum(repl_pitch_raa$RAA)/sum(repl_pitch_raa$n)\n\ncat(\"Replacement-level per-at-bat RAA (position players):\", round(repl_avg_pos, digits = 4), \"\\n\")\ncat(\"Replacement-level per-at-bat RAA (pitchers):\", round(repl_avg_pitch, digits = 4),\"\\n\")\n\nReplacement-level per-at-bat RAA (position players): -0.002 \nReplacement-level per-at-bat RAA (pitchers): -0.0243"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#position-player-war",
    "href": "slides/raw_slides/slides_08.html#position-player-war",
    "title": "STAT 479: Lecture 8",
    "section": "Position Player WAR",
    "text": "Position Player WAR\n\nCompare player’s RAA to what replacement-level would achieve in same opportunities\n1 Win = 10 Runs\n\n\nCodeTop-10\n\n\n\nposition_war &lt;-\n  raa |&gt;\n  dplyr::filter(!key_mlbam %in% repl_position_players) |&gt;\n  dplyr::inner_join(y = position_pa, by = \"key_mlbam\") |&gt; \n  dplyr::select(Name, key_mlbam, RAA, n) |&gt;\n  dplyr::mutate(shadowRAA = n * repl_avg_pos) |&gt;\n  dplyr::mutate(WAR = (RAA - shadowRAA)/10)\n\n\n\n\n\n# A tibble: 10 × 6\n   Name              key_mlbam   RAA     n shadowRAA   WAR\n   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Bobby Witt           677951 165.    694     -1.36 16.6 \n 2 Gunnar Henderson     683002 117.    702     -1.37 11.8 \n 3 Elly De La Cruz      682829 114.    679     -1.33 11.5 \n 4 Jose Ramirez         608070 113.    657     -1.28 11.5 \n 5 Zach Neto            687263 110.    590     -1.15 11.1 \n 6 Marcus Semien        543760 108.    701     -1.37 10.9 \n 7 Ketel Marte          606466 101.    562     -1.10 10.2 \n 8 Vladimir Guerrero    665489  99.0   671     -1.31 10.0 \n 9 Francisco Lindor     596019  98.5   689     -1.35  9.98\n10 Jose Altuve          514888  98.2   661     -1.29  9.95"
  },
  {
    "objectID": "slides/raw_slides/slides_08.html#looking-ahead",
    "href": "slides/raw_slides/slides_08.html#looking-ahead",
    "title": "STAT 479: Lecture 8",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nTonight: Guest lecture by Namita Nanadakumar (Seattle Kraken)\n\n6:00pm in Morgridge Hall 1524\nSponsored by Sports Analytics Club\n\nAdjusted Office Hours:\n\nToday: 4pm - 5:30pm in MH 5586\nWednesday: 4pm - 5pm in MH 5586 (not 3pm!)\n\nBefore Lecture 9: please read Chapters 7 & 8 of Beyond Multiple Linear Regression"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#recap-epa-in-football",
    "href": "slides/raw_slides/slides_10.html#recap-epa-in-football",
    "title": "STAT 479 Lecture 10",
    "section": "Recap: EPA in Football",
    "text": "Recap: EPA in Football\n\n86-yard touchdown by Ladd McConckey on a pass from Justin Herbert (video link)\n\nThrown on a 3rd and 26 from Chargers’ 14 yard line. EP = -1.54\nChargers generated 7 - (-1.54) = 8.54 points of EP\n\n64-yard touch down by Kavontae Turpin on a pass from Cooper Rush (video link)\n\nThrown on 3rd and 10 from Cowboys’ 36 yard line. EP = 0.77\nCowboys generated 7 - 0.77 = 6.23 points of EP\n\n\n\n\nHow should we divide credit between QB and receiver?\nWhat about running?"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#plan",
    "href": "slides/raw_slides/slides_10.html#plan",
    "title": "STAT 479 Lecture 10",
    "section": "Plan",
    "text": "Plan\n\nEstimate EPA per-play for each passer, receiver, & rusher\nSeparately model three phases:\n\nPassing I: EPA due to actions while ball is in the air\nPassing II: EPA due to yards after catch\nRushing\n\nMulti-level modeling:\n\nRandom intercepts for passers, receivers, rushers\nMust adjust for important fixed effects\n\n\n\n\nW/ per-play EPA can compare actual production to replacement “shadow”\nThese contrasts will lead to a version of WAR"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#overview",
    "href": "slides/raw_slides/slides_10.html#overview",
    "title": "STAT 479 Lecture 10",
    "section": "Overview",
    "text": "Overview\n\nFocus on 2024-25 regular season\nNeed roster data to define position-specific replacement level"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#passing-data",
    "href": "slides/raw_slides/slides_10.html#passing-data",
    "title": "STAT 479 Lecture 10",
    "section": "Passing Data",
    "text": "Passing Data\n\nExtract all passing plays (play_type == \"pass\")\nFor each pass, record\n\nposteam, defteam, posteamtype\npasser_player_id, receiver_player_id\nshotgun, no_huddle, qb_hit, pass_location\nair_yards, yards_after_catch, completed_pass\n\n\n\ntable(pass2024$pass_location)\n\n\n  left middle  right \n  6373   3810   6817"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#air-vs-yac-epa",
    "href": "slides/raw_slides/slides_10.html#air-vs-yac-epa",
    "title": "STAT 479 Lecture 10",
    "section": "Air vs YAC EPA",
    "text": "Air vs YAC EPA\n\nTeams generate EPA on completed passes while\n\nThe ball is in the air\nOn the ground, after the catch\n\nnflfastR decomposes epa into air_epa and yac_epa\nCompleted passes: imagine pausing game as soon as catch completed\n\nEP at the intermediate state used to compute air_epa and yac_epa\n\nSacks: no good definition for air_epa & yac_epa\n\nWe will treat sacks as rushing plays"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#rushing-data",
    "href": "slides/raw_slides/slides_10.html#rushing-data",
    "title": "STAT 479 Lecture 10",
    "section": "Rushing Data",
    "text": "Rushing Data\n\nInclude columns for\n\npasser_player_id & rusher_player_id\nshotgun & no_huddle\nrun_location & run_gap\n\nMust manually specify rusher_player_id on sacks"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#run-context-i",
    "href": "slides/raw_slides/slides_10.html#run-context-i",
    "title": "STAT 479 Lecture 10",
    "section": "Run Context I",
    "text": "Run Context I\n\nrun_location indicates direction\n\n\n\n\n  left middle  right \n  5425   3565   5172 \n\n\n\nrun_gap:\n\n\"guard\": runs through B gap (b/w guard & tackles)\n\"tackle\": run through C gap (outside tackle or b/w tackle & end)\n\"end\": run through D gap (outside TE)\nNA: run through A gap (b/w center & guard)\n\n\n\n\n        run_location\nrun_gap  left middle right &lt;NA&gt;\n  end    1958      0  1805    0\n  guard  1822      0  1739    0\n  tackle 1645      0  1628    0\n  &lt;NA&gt;      0   3565     0 1447"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#run-context-ii",
    "href": "slides/raw_slides/slides_10.html#run-context-ii",
    "title": "STAT 479 Lecture 10",
    "section": "Run Context II",
    "text": "Run Context II\n\nWe will create a new variable run_context\nConcatenate run_location & run_gap when both not NA\nFor “A” gap runs, run_context = \"middle\"\nFor sacks & fumbles, set run_context = \"other\"\nAdditionally add posteam: proxy for offensive linemen & blockers\n\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::mutate(\n    run_context = dplyr::case_when(\n      run_location == \"middle\" ~ paste(posteam, \"middle\", sep = \"_\"),\n      is.na(run_location) & is.na(run_gap) ~ paste(posteam, \"other\", sep = \"_\"),\n      .default = paste(posteam, run_gap, run_location, sep = \"_\")))"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#team-strengths",
    "href": "slides/raw_slides/slides_10.html#team-strengths",
    "title": "STAT 479 Lecture 10",
    "section": "Team Strengths",
    "text": "Team Strengths\n\nImportant to account for teammates & opponent quality (recall Lecture 4)\n\nDon’t overly reward players on good teams\nDon’t overly penalize players on bad teams\n\nMeasure passing & rushing strength with mean EPA\nAdd columns to pass2024 and rush2024\n\n\npass_strength &lt;-\n  pass2024 |&gt;\n  dplyr::group_by(posteam) |&gt;\n  dplyr::summarise(pass_strength = mean(epa,na.rm = TRUE))\nrush_strength &lt;-\n  rush2024 |&gt;\n  dplyr::group_by(posteam) |&gt;\n  dplyr::summarise(rush_strength = mean(epa, na.rm = TRUE))\n\npass2024 &lt;-\n  pass2024 |&gt;\n  dplyr::inner_join(pass_strength, by = \"posteam\") |&gt;\n  dplyr::inner_join(rush_strength, by = \"posteam\")\n\nrush2024 &lt;-\n  rush2024 |&gt;\n  dplyr::inner_join(pass_strength, by = \"posteam\") |&gt;\n  dplyr::inner_join(rush_strength, by = \"posteam\")"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#composite-outcomes",
    "href": "slides/raw_slides/slides_10.html#composite-outcomes",
    "title": "STAT 479 Lecture 10",
    "section": "Composite Outcomes",
    "text": "Composite Outcomes\n\n\\(\\Delta\\): observed EPA on a passing play\n\\(\\delta_{\\textrm{air}}\\) and \\(\\delta_{\\textrm{yac}}\\): air_epa and yac_epa given by nflfastR\nDecompose \\(\\Delta = \\Delta_{\\textrm{air}} + \\Delta_{\\textrm{yac}}\\) where \\[\n\\Delta_{\\textrm{air}} =\n\\begin{cases}\n\\delta_{\\textrm{air}} & \\textrm{if the pass is caught} \\\\\n\\Delta & \\textrm{if the pass is incomplete,}\n\\end{cases}\n\\] and \\[\n\\Delta_{\\textrm{yac}} =\n\\begin{cases}\n\\delta_{\\textrm{yac}} & \\textrm{if the pass is caught} \\\\\n0 & \\textrm{if the pass is incomplete.}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#modeling-delta_textrmair",
    "href": "slides/raw_slides/slides_10.html#modeling-delta_textrmair",
    "title": "STAT 479 Lecture 10",
    "section": "Modeling \\(\\Delta_{\\textrm{air}}\\)",
    "text": "Modeling \\(\\Delta_{\\textrm{air}}\\)\n\\[\n\\begin{align}\n\\Delta_{i, \\textrm{air}} &= Q_{q[i]} + C_{c[i]} + D_{d[i]} + \\boldsymbol{\\mathbf{x}}_{i}^{\\top}\\boldsymbol{\\beta} + \\epsilon_{i}\\\\\nQ_{q} &\\sim N(\\mu_{Q}, \\sigma^{2}_{Q})\\\\\nC_{c} &\\sim N(\\mu_{C}, \\sigma^{2}_{C})\\\\\nD_{d} &\\sim N(\\mu_{D}, \\sigma^{2}_{D}).\n\\end{align}\n\\]\n\nFixed effects include:\n\nair_yards, shotgun, qb_hit, no_huddle\npostteam_type (i.e. Home/Away), pass_location\nreceiver_position & rush_strength\n\nRandom intercepts for Quarterback, reCeiver, and Defense"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#air-model-results",
    "href": "slides/raw_slides/slides_10.html#air-model-results",
    "title": "STAT 479 Lecture 10",
    "section": "Air Model Results",
    "text": "Air Model Results\n\nlibrary(lme4)\n\nair_model &lt;-\n  lmer(Delta_air ~ 1 + (1|passer_player_id) + (1 | receiver_player_id) + (1 | defteam )\n       + air_yards + shotgun + qb_hit + no_huddle + posteam_type + pass_location + receiver_position + rush_strength,\n       data = pass2024)\nsummary(air_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Delta_air ~ 1 + (1 | passer_player_id) + (1 | receiver_player_id) +  \n    (1 | defteam) + air_yards + shotgun + qb_hit + no_huddle +  \n    posteam_type + pass_location + receiver_position + rush_strength\n   Data: pass2024\n\nREML criterion at convergence: 56640.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-9.6663 -0.4475 -0.0140  0.4521  4.2381 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n receiver_player_id (Intercept) 0.011544 0.10745 \n passer_player_id   (Intercept) 0.002242 0.04735 \n defteam            (Intercept) 0.001354 0.03680 \n Residual                       1.622752 1.27387 \nNumber of obs: 17000, groups:  \nreceiver_player_id, 492; passer_player_id, 99; defteam, 32\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)         -2.642055   0.573232  -4.609\nair_yards            0.035027   0.001077  32.537\nshotgun             -0.081497   0.028132  -2.897\nqb_hit              -0.276667   0.036182  -7.647\nno_huddle            0.134666   0.029805   4.518\nposteam_typehome    -0.008663   0.019728  -0.439\npass_locationmiddle  0.172170   0.026424   6.516\npass_locationright  -0.034797   0.022336  -1.558\nreceiver_positionDL  4.627420   1.401649   3.301\nreceiver_positionLB  3.750986   1.070748   3.503\nreceiver_positionOL  2.527147   0.729736   3.463\nreceiver_positionQB  3.661843   0.934966   3.917\nreceiver_positionRB  1.922465   0.573394   3.353\nreceiver_positionTE  2.279044   0.573261   3.976\nreceiver_positionWR  2.254500   0.573040   3.934\nrush_strength        0.307995   0.118268   2.604"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#model-interpretation",
    "href": "slides/raw_slides/slides_10.html#model-interpretation",
    "title": "STAT 479 Lecture 10",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\n\\(Q_{q}\\) : Per-pass EP added through air by passer \\(q\\) after adjusting for fixed effects\n\\(C_{c}\\): per-pass EP added when ball in air by receiver \\(c\\) after adjusting for fixed effects\n\\(D_{d}\\): per-pass EP added by offensive team when ball in air while facing defense \\(d\\)\n\nNegative \\(D_{d}\\) are good for the defense!\n\nModel: \\(Q_{q}, C_{c}\\) and \\(D_{d}\\) noisy deviations from league-wide averages \\(\\mu_{Q}, \\mu_{C},\\) and \\(\\mu_{D}.\\)\n\n\n\nCannot estimate the individual \\(Q_{q}\\)’s, \\(C_{c}\\)’s, or \\(D_{d}\\)’s\nCannot estimate the global averages \\(\\mu_{Q}, \\mu_{C},\\) or \\(\\mu_{D}\\)\nThese parameters are not identifiable with the data alone\nLuckily: we can estimate deviations (e.g., \\(Q_{q} - \\mu_{Q}\\))"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#individual-points-added-over-average",
    "href": "slides/raw_slides/slides_10.html#individual-points-added-over-average",
    "title": "STAT 479 Lecture 10",
    "section": "Individual Points Added over Average",
    "text": "Individual Points Added over Average\n\nDefinitionCodeTop & Bottom 5 Passers\n\n\n\nFor passer \\(q\\), let \\(\\textrm{IPA}^{(Q)}_{\\textrm{air}, q} = Q_{q} - \\mu_{Q}\\)\n\nHow much more EP does \\(q\\) add per play than league-average\n\nSimilarly \\(\\textrm{IPA}^{(C)}_{\\textrm{air}, c}\\) and \\(\\textrm{IPA}^{(D)}_{\\textrm{air}, d}.\\)\n\n\n\n\ntmp_air &lt;- ranef(air_model) \n\nair_passer_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_air[[\"passer_player_id\"]]), \n    ipa_air_pass = tmp_air[[\"passer_player_id\"]][,1])\nair_receiver_effects &lt;-\n  data.frame(\n    gsis_id = rownames(tmp_air[[\"receiver_player_id\"]]),\n    ipa_air_rec = tmp_air[[\"receiver_player_id\"]][,1])\nair_defense_effects &lt;-\n  data.frame(\n    Team = rownames(tmp_air[[\"defteam\"]]),\n    tpa_air_def = tmp_air[[\"defteam\"]][,1])\n\n\n\n\nair_passer_effects |&gt;\n  dplyr::inner_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name), by = \"gsis_id\") |&gt;\n  dplyr::select(full_name, ipa_air_pass) |&gt;\n  dplyr::arrange(dplyr::desc(ipa_air_pass)) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n                  full_name ipa_air_pass\n1                Joe Burrow   0.06736333\n2               Sam Darnold   0.04033663\n3             Lamar Jackson   0.03292716\n4            Tua Tagovailoa   0.03281935\n5               Brock Purdy   0.03000549\n6                 Drew Lock  -0.02737783\n7                    Bo Nix  -0.02808707\n8  Dorian Thompson-Robinson  -0.03832239\n9           Spencer Rattler  -0.05005097\n10       Anthony Richardson  -0.07074920"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#modeling-delta_textrmyac",
    "href": "slides/raw_slides/slides_10.html#modeling-delta_textrmyac",
    "title": "STAT 479 Lecture 10",
    "section": "Modeling \\(\\Delta_{\\textrm{yac}}\\)",
    "text": "Modeling \\(\\Delta_{\\textrm{yac}}\\)\n\nFit a nearly-identical multilevel model using only completed passes\n\n\ncompletions &lt;-\n  pass2024 |&gt;\n  dplyr::filter(complete_pass==1)\n\nyac_model &lt;-\n  lmer(Delta_yac ~ 1 + (1|passer_player_id) + (1 | receiver_player_id) + (1 | defteam ) \n       + air_yards + shotgun + qb_hit + no_huddle + posteam_type + pass_location + receiver_position + rush_strength, \n       data = completions)"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#dealing-with-singularities",
    "href": "slides/raw_slides/slides_10.html#dealing-with-singularities",
    "title": "STAT 479 Lecture 10",
    "section": "Dealing with Singularities",
    "text": "Dealing with Singularities\n\nEstimated no passer-to-passer variation\nIntuitive: once catch is made, who threw it doesn’t make a difference in terms of EPA.\n\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Delta_yac ~ 1 + (1 | passer_player_id) + (1 | receiver_player_id) +  \n    (1 | defteam) + air_yards + shotgun + qb_hit + no_huddle +  \n    posteam_type + pass_location + receiver_position + rush_strength\n   Data: completions\n\nREML criterion at convergence: 32367.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.9533 -0.5437 -0.2533  0.2882  7.4637 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev.\n receiver_player_id (Intercept) 0.0190108 0.1379  \n passer_player_id   (Intercept) 0.0000000 0.0000  \n defteam            (Intercept) 0.0006814 0.0261  \n Residual                       0.9296857 0.9642  \nNumber of obs: 11627, groups:  \nreceiver_player_id, 468; passer_player_id, 90; defteam, 32\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)          1.710173   0.689922   2.479\nair_yards           -0.022666   0.001214 -18.675\nshotgun             -0.046899   0.025546  -1.836\nqb_hit               0.037119   0.039172   0.948\nno_huddle           -0.174386   0.027107  -6.433\nposteam_typehome    -0.007157   0.018083  -0.396\npass_locationmiddle -0.057884   0.024339  -2.378\npass_locationright   0.011441   0.020532   0.557\nreceiver_positionDL -1.567487   1.193518  -1.313\nreceiver_positionLB -0.728621   0.974493  -0.748\nreceiver_positionOL -1.046652   0.781368  -1.340\nreceiver_positionQB -0.728794   0.974704  -0.748\nreceiver_positionRB -0.515251   0.689574  -0.747\nreceiver_positionTE -0.837277   0.689576  -1.214\nreceiver_positionWR -0.847828   0.689446  -1.230\nrush_strength        0.382583   0.105680   3.620\n\n\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#computing-ipa-for-yac",
    "href": "slides/raw_slides/slides_10.html#computing-ipa-for-yac",
    "title": "STAT 479 Lecture 10",
    "section": "Computing IPA for YAC",
    "text": "Computing IPA for YAC\n\nUsing ranef, we can extract YAC-specific IPAs for passers, receivers, and defenses\nTop-10 based on \\(\\textrm{IPA}^{(C)}_{\\textrm{yac}, c}\\) contains many well-known receivers\n\n\n\n         full_name ipa_yac_rec\n1  Marvin Mims Jr.   0.4038113\n2    Khalil Shakir   0.2763021\n3  KaVontae Turpin   0.2225070\n4    Xavier Worthy   0.2179003\n5     Tucker Kraft   0.2141390\n6     Brock Bowers   0.1885809\n7    Austin Ekeler   0.1830305\n8   Antonio Gibson   0.1818104\n9    Ja'Marr Chase   0.1778265\n10  Raheem Mostert   0.1729994"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#handling-qb-runs",
    "href": "slides/raw_slides/slides_10.html#handling-qb-runs",
    "title": "STAT 479 Lecture 10",
    "section": "Handling QB Runs",
    "text": "Handling QB Runs\n\nPlay-by-play data doesn’t distinguish between\n\nDesigned QB runs\nScrambles on broken plays\n\nWe will fit two separate models for EPA on rushing plays\n\n\nqb_runs &lt;-\n  rush2024 |&gt;\n  dplyr::filter(rusher_position == \"QB\")\n\nnonqb_runs &lt;-\n  rush2024 |&gt;\n  dplyr::filter(rusher_position != \"QB\")"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#multilevel-models",
    "href": "slides/raw_slides/slides_10.html#multilevel-models",
    "title": "STAT 479 Lecture 10",
    "section": "Multilevel Models",
    "text": "Multilevel Models\n\nOverviewCodeTop \\(\\textrm{IPA}^{(R)}_{r}\\) Values\n\n\n\nQB Run Model:\n\nFixed effects: shotgun, no_huddle, posteam_type, pass_strength\nRandom intercepts: rusher_player_id & defteam\n\nNon-QB Model\n\nFixed effects: same as QB-model + rusher_position & run_context\nRandom intercepts: same as QB-model\n\nUse ranef() to extract \\(\\textrm{IPA}^{(R)}_{r}\\) for Runner r\n\n\n\n\nqbrun_fit &lt;-\n  lmer(epa ~ 1 + (1 | rusher_player_id) + (1 | defteam) +\n         shotgun + no_huddle + posteam_type + pass_strength,\n       data = qb_runs)\nnonqb_run_fit &lt;-\n  lmer(epa ~ 1 + (1 | rusher_player_id) + (1 | defteam) +  \n         shotgun + no_huddle + posteam_type + rusher_position + run_context + pass_strength,\n       data = nonqb_runs)\n\n\n\n\n\n         full_name    ipa_run\n1    De'Von Achane 0.13626662\n2   Saquon Barkley 0.10131412\n3     Jahmyr Gibbs 0.09383069\n4      Jerome Ford 0.08473154\n5    Chuba Hubbard 0.08423388\n6     J.K. Dobbins 0.07766485\n7  Emari Demercado 0.07656891\n8    Dameon Pierce 0.06755530\n9      Rico Dowdle 0.06517368\n10     Taysom Hill 0.06219509"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#total-individual-points-added-above-average",
    "href": "slides/raw_slides/slides_10.html#total-individual-points-added-above-average",
    "title": "STAT 479 Lecture 10",
    "section": "Total Individual Points Added Above Average",
    "text": "Total Individual Points Added Above Average\n\nFor every offensive player, estimate how more EP they add per-play than league-average\n\nThrough the air (\\(\\textrm{IPA}^{(Q)}_{\\textrm{air}, q}\\), \\(\\textrm{IPA}^{(C)}_{\\textrm{air}, c}\\))\nAfter the catch (\\(\\textrm{IPA}^{(Q)}_{\\textrm{yac}, q}\\), \\(\\textrm{IPA}^{(C)}_{\\textrm{yac}, c}\\))\nRushing (\\(\\textrm{IPA}^{(R)}_{r}\\))\n\nMultiplying \\(\\textrm{IPA}\\) by number of attempts yields \\(\\textrm{IPAA}\\): total points added above average"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#computing-ipaa",
    "href": "slides/raw_slides/slides_10.html#computing-ipaa",
    "title": "STAT 479 Lecture 10",
    "section": "Computing IPAA",
    "text": "Computing IPAA\n\nCodePassersReceiversQB RushingNon-QB Rushing\n\n\n\npasser_ipaa &lt;-\n  pass2024 |&gt;\n  dplyr::group_by(passer_player_id) |&gt; \n  dplyr::summarise(n_pass = dplyr::n()) |&gt;\n  dplyr::rename(gsis_id = passer_player_id) |&gt; \n  dplyr::left_join(air_passer_effects, by = \"gsis_id\") |&gt; \n  dplyr::left_join(yac_passer_effects, by = \"gsis_id\") |&gt; \n  dplyr::mutate(\n    ipaa_air_pass = ipa_air_pass * n_pass,\n    ipaa_yac_pass = ipa_yac_pass * n_pass) |&gt;\n  dplyr::left_join(y = roster2024 |&gt; dplyr::select(gsis_id, full_name, position), by = \"gsis_id\")\n\n\n\n\npasser_ipaa |&gt;\n  dplyr::select(full_name, n_pass, ipa_air_pass, ipa_yac_pass, ipaa_air_pass, ipaa_yac_pass) |&gt;\n  dplyr::arrange(dplyr::desc(ipaa_air_pass)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 6\n   full_name      n_pass ipa_air_pass ipa_yac_pass ipaa_air_pass ipaa_yac_pass\n   &lt;chr&gt;           &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Joe Burrow        628       0.0674            0         42.3              0\n 2 Sam Darnold       512       0.0403            0         20.7              0\n 3 Lamar Jackson     454       0.0329            0         14.9              0\n 4 Brock Purdy       440       0.0300            0         13.2              0\n 5 Tua Tagovailoa    388       0.0328            0         12.7              0\n 6 Justin Herbert    484       0.0262            0         12.7              0\n 7 Baker Mayfield    562       0.0198            0         11.1              0\n 8 C.J. Stroud       509       0.0166            0          8.45             0\n 9 Russell Wilson    306       0.0231            0          7.07             0\n10 Geno Smith        554       0.0110            0          6.07             0\n\n\n\n\n\n\n# A tibble: 10 × 6\n   full_name         n_rec ipa_air_rec ipa_yac_rec ipaa_air_rec ipaa_yac_rec\n   &lt;chr&gt;             &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Tee Higgins         109      0.182     -0.0741          19.9       -8.07 \n 2 Amon-Ra St. Brown   141      0.139      0.0563          19.6        7.93 \n 3 Ladd McConkey       112      0.148     -0.0124          16.5       -1.38 \n 4 Terry McLaurin      117      0.118     -0.0612          13.7       -7.16 \n 5 DeVonta Smith        89      0.142     -0.0383          12.7       -3.41 \n 6 Jauan Jennings      113      0.111     -0.118           12.6      -13.3  \n 7 Jakobi Meyers       129      0.0928     0.00289         12.0        0.373\n 8 Justin Jefferson    153      0.0746     0.0566          11.4        8.66 \n 9 Courtland Sutton    135      0.0812    -0.156           11.0      -21.1  \n10 Jonnu Smith         111      0.0910     0.166           10.1       18.4  \n\n\n\n\n\n\n# A tibble: 10 × 4\n   full_name          ipa_qbrun n_qbrun ipaa_qbrun\n   &lt;chr&gt;                  &lt;dbl&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 Jayden Daniels         0.469     184       86.3\n 2 Josh Allen             0.703     108       75.9\n 3 Anthony Richardson     0.667      94       62.7\n 4 Jalen Hurts            0.307     167       51.3\n 5 Bo Nix                 0.481     102       49.1\n 6 Lamar Jackson          0.322     150       48.2\n 7 Kyler Murray           0.336     102       34.3\n 8 Brock Purdy            0.319      92       29.4\n 9 Patrick Mahomes        0.344      74       25.5\n10 Daniel Jones           0.283      88       24.9\n\n\n\n\n\n\n# A tibble: 10 × 4\n   full_name      ipa_run n_run ipaa_run\n   &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1 Saquon Barkley  0.101    346     35.1\n 2 De'Von Achane   0.136    203     27.7\n 3 Jahmyr Gibbs    0.0938   250     23.5\n 4 Chuba Hubbard   0.0842   250     21.1\n 5 Derrick Henry   0.0520   325     16.9\n 6 Rico Dowdle     0.0652   235     15.3\n 7 J.K. Dobbins    0.0777   195     15.1\n 8 Bijan Robinson  0.0477   304     14.5\n 9 Najee Harris    0.0481   263     12.7\n10 Chase Brown     0.0505   229     11.6"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#replacement-level",
    "href": "slides/raw_slides/slides_10.html#replacement-level",
    "title": "STAT 479 Lecture 10",
    "section": "Replacement Level",
    "text": "Replacement Level\n\nMultilevel models involved global-averages \\(\\mu_{Q}, \\mu_{C}, \\mu_{R}\\)\n\nRepresent the average latent intercept across theoretical super-population\nAssumption: observed players are randomly sampled from this super-population\n\nNot strictly correct to say IPA measures contribution above “league-average”\n\n\n\nWill define roster-based thresholds for “replacement level”\nConstruct a replacement-level “shadow”\n\nCompute average \\(\\textrm{IPA}\\) for replacement-level players\nShadow IPAA: multiply avg. replacement-level IPA by attempts\nCompare observed IPAA to shadow IPAA"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#defining-replacement-level",
    "href": "slides/raw_slides/slides_10.html#defining-replacement-level",
    "title": "STAT 479 Lecture 10",
    "section": "Defining Replacement Level",
    "text": "Defining Replacement Level\n\nMost rosters: 3 running backs, 4 wide receivers, 2 tight ends\nCreate separate thresholds for position & type of play\nFor passing plays:\n\nTop \\(32 \\times 4\\) WRs sorted by n_rec\nTop \\(32 \\times 2\\) TEs sorted by n_rec\nTop \\(32 \\times 3\\) RB\n\nFor running plays\n\nTop \\(32 \\times 3\\) RBs sorted by n_runs\nTop \\(32 \\times 1\\) WR & TEs sorted by n_runs"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#running-ipar",
    "href": "slides/raw_slides/slides_10.html#running-ipar",
    "title": "STAT 479 Lecture 10",
    "section": "Running IPAR",
    "text": "Running IPAR\n\nThresholdPlayersReplacement IPAShadow\n\n\n\nrb_run_threshold &lt;- run_ipaa |&gt; \n  dplyr::filter(position == \"RB\") |&gt;\n  dplyr::arrange(desc(n_run)) |&gt;\n  dplyr::slice(96) |&gt;\n  dplyr::pull(n_run)\n\nwrte_run_threshold &lt;- run_ipaa |&gt;\n  dplyr::filter(position %in% c(\"WR\", \"TE\")) |&gt;\n  dplyr::arrange(desc(n_run)) |&gt;\n  dplyr::slice(32) |&gt;\n  dplyr::pull(n_run)\n\n\n\n\nrun_ipaa &lt;- \n  run_ipaa |&gt;\n  dplyr::mutate(\n    repl_wrte = dplyr::case_when(\n      position %in% c(\"WR\", \"TE\") & n_run &lt; wrte_run_threshold ~ 1,\n      position %in% c(\"WR\", \"TE\") & n_run &gt;= wrte_run_threshold ~ 0,\n      !position %in% c(\"WR\", \"TE\") ~ NA),\n    repl_rb = dplyr::case_when(\n      position == \"RB\" & n_run &lt; rb_run_threshold ~ 1,\n      position == \"RB\" & n_run &gt;= rb_run_threshold ~ 0,\n      position != \"RB\" ~ NA))\n\n\n\n\nCompute average IPA for all replacement-level players\n\n\nrepl_wrte_ipa_run &lt;-  run_ipaa |&gt;\n  dplyr::filter(repl_wrte == 1) |&gt;\n  dplyr::pull(ipa_run) |&gt;\n  mean(na.rm = TRUE)\n\nrepl_rb_ipa_run &lt;- run_ipaa |&gt;\n  dplyr::filter(repl_rb == 1) |&gt;\n  dplyr::pull(ipa_run) |&gt;\n  mean(na.rm = TRUE)\n\n\n\n\nrun_ipar &lt;- run_ipaa |&gt;\n  dplyr::filter(position %in% c(\"WR\", \"RB\", \"TE\")) |&gt;\n   dplyr::mutate(\n    shadow_run = dplyr::case_when(\n      position %in% c(\"WR\", \"TE\") ~ n_run * repl_wrte_ipa_run,\n      position == \"RB\" ~ n_run * repl_rb_ipa_run),\n    ipar_run = ipaa_run - shadow_run)\n\n\n\n# A tibble: 10 × 3\n   full_name      ipaa_run ipar_run\n   &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1 Saquon Barkley     35.1     38.5\n 2 De'Von Achane      27.7     29.7\n 3 Jahmyr Gibbs       23.5     25.9\n 4 Chuba Hubbard      21.1     23.5\n 5 Derrick Henry      16.9     20.1\n 6 Rico Dowdle        15.3     17.6\n 7 Bijan Robinson     14.5     17.5\n 8 J.K. Dobbins       15.1     17.1\n 9 Najee Harris       12.7     15.2\n10 Chase Brown        11.6     13.8"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#computing-remaining-ipars",
    "href": "slides/raw_slides/slides_10.html#computing-remaining-ipars",
    "title": "STAT 479 Lecture 10",
    "section": "Computing Remaining IPAR’s",
    "text": "Computing Remaining IPAR’s\n\nVirtually same process for both passing IPAR’s\nTop 10 YAC IPAR\n\n\n\n# A tibble: 10 × 3\n   full_name        ipaa_yac_rec ipar_yac_rec\n   &lt;chr&gt;                   &lt;dbl&gt;        &lt;dbl&gt;\n 1 Ja'Marr Chase            31.1         31.3\n 2 Brock Bowers             28.9         30.2\n 3 Khalil Shakir            27.6         27.7\n 4 DJ Moore                 22.5         22.6\n 5 Brian Thomas Jr.         22.4         22.5\n 6 Xavier Worthy            21.4         21.4\n 7 Marvin Mims Jr.          21.0         21.0\n 8 Jonnu Smith              18.4         19.4\n 9 Puka Nacua               16.4         16.4\n10 Tucker Kraft             14.8         15.4"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#replacement-level-qb",
    "href": "slides/raw_slides/slides_10.html#replacement-level-qb",
    "title": "STAT 479 Lecture 10",
    "section": "Replacement-level QB",
    "text": "Replacement-level QB\n\nEvery offensive play runs through the QB\nIdea 1: designate one QB per team as non-replacement\n\nE.g., QB w/ most rushing & passing attempts\nProblem: assumes every NFL team has at least one non-replacement QB\n\nIdea 2: sort QBs by total number of passes & rushes & take top-32\nTop-10 IPAR via passing\n\n\n\n# A tibble: 10 × 3\n   full_name      ipa_air_pass ipar_air_pass\n   &lt;chr&gt;                 &lt;dbl&gt;         &lt;dbl&gt;\n 1 Joe Burrow           0.0674         44.6 \n 2 Sam Darnold          0.0403         22.6 \n 3 Lamar Jackson        0.0329         16.6 \n 4 Brock Purdy          0.0300         14.8 \n 5 Justin Herbert       0.0262         14.5 \n 6 Tua Tagovailoa       0.0328         14.2 \n 7 Baker Mayfield       0.0198         13.2 \n 8 C.J. Stroud          0.0166         10.4 \n 9 Russell Wilson       0.0231          8.21\n10 Geno Smith           0.0110          8.13"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#converting-points-to-wins",
    "href": "slides/raw_slides/slides_10.html#converting-points-to-wins",
    "title": "STAT 479 Lecture 10",
    "section": "Converting Points to Wins",
    "text": "Converting Points to Wins\n\nIPAR are on point-differential scale\nFind \\(\\alpha\\) & \\(\\beta\\) s.t. \\(\\textrm{Wins} \\approx \\alpha + \\beta \\times \\textrm{PtsDiff}\\)\nThen multiply IPAR values by \\(\\beta\\)\n\n\n\n\n\n\n\n\n\nFigure 1: Scoring differential is positively associated with wins"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#skill-position-war",
    "href": "slides/raw_slides/slides_10.html#skill-position-war",
    "title": "STAT 479 Lecture 10",
    "section": "Skill-Position WAR",
    "text": "Skill-Position WAR\n\nConvert each IPAR to wins scale & then sum\nTop-10 Skill Positions\n\n\n\n           full_name       war\n1      De'Von Achane 1.2408245\n2     Saquon Barkley 1.1551989\n3      Ja'Marr Chase 1.0545348\n4       Jahmyr Gibbs 0.8666242\n5        Jonnu Smith 0.8446721\n6  Amon-Ra St. Brown 0.8086111\n7       Brock Bowers 0.7438843\n8      Derrick Henry 0.6381019\n9         Puka Nacua 0.6084343\n10  Justin Jefferson 0.6012925"
  },
  {
    "objectID": "slides/raw_slides/slides_10.html#reminders",
    "href": "slides/raw_slides/slides_10.html#reminders",
    "title": "STAT 479 Lecture 10",
    "section": "Reminders",
    "text": "Reminders\n\nProject reports & presentations due at 12pm (noon) on Friday\nPlease also upload recording to shared Box folder (link on Canvas)\nI’ll announce peer review assignments by Thursday night\n\nYou must review 3 reports & presentations of 3 other teams\nDue on 10/17\nRubric + constructive comments"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#ncaa-d1-womens-hockey-2024-25",
    "href": "slides/raw_slides/slides_12.html#ncaa-d1-womens-hockey-2024-25",
    "title": "STAT 479 Lecture 12",
    "section": "NCAA D1 Women’s Hockey (2024-25)",
    "text": "NCAA D1 Women’s Hockey (2024-25)\n\nWIS went 38-1-2 & won the National Championship\nOSU went 29-8-0 & lost against WIS\nHead-to-Head: 2-1-1\n\n\n\nHow much better was WIS than OSU?\n\n\n\n\\[\n\\mathbb{P}(\\textrm{WIS beats OSU}) = \\textrm{???}\n\\]"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#the-basic-model",
    "href": "slides/raw_slides/slides_12.html#the-basic-model",
    "title": "STAT 479 Lecture 12",
    "section": "The Basic Model",
    "text": "The Basic Model\n\nSuppose there are \\(n\\) games played between \\(p\\) teams\nFor each team \\(j\\), there is a latent strength \\(\\lambda_{j}\\) \\[\n\\mathbb{P}(\\textrm{team i beats team j}) = \\frac{e^{\\lambda_{i}}}{e^{\\lambda_{i}} + e^{\\lambda_{j}}} = \\frac{1}{1 + e^{-1 \\times (\\lambda_{i} - \\lambda_{j})}}\n\\]\nLog-odds of \\(i\\) beating \\(j\\): \\(\\lambda_{i} - \\lambda_{j}.\\)"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#example-4-team-round-robin",
    "href": "slides/raw_slides/slides_12.html#example-4-team-round-robin",
    "title": "STAT 479 Lecture 12",
    "section": "Example: 4 Team Round-Robin",
    "text": "Example: 4 Team Round-Robin\n\nSuppose \\(\\lambda_{1} = 1, \\lambda_{2} = 0.5, \\lambda_{3} = 0.3\\) and \\(\\lambda_{4} = 0.\\) \\[\n\\mathbb{P}(\\textrm{team 1 beats team 2}) = \\frac{e^{1}}{e^{1} + e^{0.5}} \\approx 62\\%\n\\]\n\n\nSingle Prob.outerPairwise Probs.\n\n\n\nlambda &lt;- c(1, 0.5, 0.3, 0)\n1/(1 + exp(-1 * (lambda[1] - lambda[2])))\n\n[1] 0.6224593\n\n\n\n\n\nouter(X = lambda, Y = lambda, FUN = \"-\")\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.0  0.5  0.7  1.0\n[2,] -0.5  0.0  0.2  0.5\n[3,] -0.7 -0.2  0.0  0.3\n[4,] -1.0 -0.5 -0.3  0.0\n\n\n\n\n\nprobs &lt;- 1/(1 + exp(-1 * outer(X = lambda, Y = lambda, FUN = \"-\")))\ndiag(probs) &lt;- NA\nround(probs, digits = 3)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,]    NA 0.622 0.668 0.731\n[2,] 0.378    NA 0.550 0.622\n[3,] 0.332 0.450    NA 0.574\n[4,] 0.269 0.378 0.426    NA"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#example-tournament-simulation-i",
    "href": "slides/raw_slides/slides_12.html#example-tournament-simulation-i",
    "title": "STAT 479 Lecture 12",
    "section": "Example: Tournament Simulation I",
    "text": "Example: Tournament Simulation I\n\nConsider tournament w/ 4 teams and \\(\\lambda_{1} = 1, \\lambda_{2} = 0.5, \\lambda_{3} = 0.3\\) and \\(\\lambda_{4} = 0.\\)\nWhat is prob. that Team 3 finishes in top-2 in terms of wins?\nEnumerate all pairwise probabilities\n\n\n\n# A tibble: 6 × 3\n  player1 player2  prob\n    &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;\n1       1       2 0.622\n2       1       3 0.668\n3       1       4 0.731\n4       2       3 0.550\n5       2       4 0.622\n6       3       4 0.574"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#example-tournament-simulation-ii",
    "href": "slides/raw_slides/slides_12.html#example-tournament-simulation-ii",
    "title": "STAT 479 Lecture 12",
    "section": "Example: Tournament Simulation II",
    "text": "Example: Tournament Simulation II\n\nFlip 6 coins, one per match\n\nHeads: player1 wins; Tails: player2 wins\n\nrbinom(n, size, prob): simulate from a Binomial distribution\nCoin flips are special case with size = 1\n\nn: number of coins to flip\nprob: prob. of each coin landing heads\n\n\n\nset.seed(479)\noutcomes &lt;- rbinom(n = 6, size = 1, prob = design$prob)\ncbind(design, outcomes)\n\n  player1 player2      prob outcomes\n1       1       2 0.6224593        0\n2       1       3 0.6681878        1\n3       1       4 0.7310586        1\n4       2       3 0.5498340        0\n5       2       4 0.6224593        1\n6       3       4 0.5744425        1"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#example-tournament-simulation-iii",
    "href": "slides/raw_slides/slides_12.html#example-tournament-simulation-iii",
    "title": "STAT 479 Lecture 12",
    "section": "Example: Tournament Simulation III",
    "text": "Example: Tournament Simulation III\n\nTemporary TableCount Wins\n\n\n\nList players & winner of each match\n\n\ntmp_df &lt;-\n  design |&gt;\n  dplyr::select(player1, player2) |&gt;\n  dplyr::mutate(\n    outcome = outcomes,\n    winner = ifelse(outcome == 1, player1, player2),\n    player1 = factor(player1, levels = 1:4),\n    player2 = factor(player2, levels = 1:4),\n    winner = factor(winner, levels = 1:4))\ntmp_df\n\n# A tibble: 6 × 4\n  player1 player2 outcome winner\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt; &lt;fct&gt; \n1 1       2             0 2     \n2 1       3             1 1     \n3 1       4             1 1     \n4 2       3             0 3     \n5 2       4             1 2     \n6 3       4             1 3     \n\n\n\n\n\nCount wins by grouping on winner & counting occurrences\nIf one team loses all games, they won’t appear in grouped summary\ncomplete: fills in missing combinations\n\n\nwins &lt;-\n  tmp_df |&gt;\n  dplyr::group_by(winner) |&gt;\n  dplyr::summarise(Wins = dplyr::n()) |&gt;\n  dplyr::rename(Team = winner) |&gt;\n  tidyr::complete(Team, fill = list(Wins = 0))\nwins\n\n# A tibble: 4 × 2\n  Team   Wins\n  &lt;fct&gt; &lt;int&gt;\n1 1         2\n2 2         2\n3 3         2\n4 4         0"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#example-tournament-simulation-iv",
    "href": "slides/raw_slides/slides_12.html#example-tournament-simulation-iv",
    "title": "STAT 479 Lecture 12",
    "section": "Example: Tournament Simulation IV",
    "text": "Example: Tournament Simulation IV\n\nCodeTeam 1’s PerformanceRanking w/ TiesTeam Rankings\n\n\n\nReplay tournament 5000 times\n\n\nn_sims &lt;- 5000\nsimulated_wins &lt;- matrix(data = NA, nrow = n_sims, ncol = 4)\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  outcomes &lt;- rbinom(n = 6, size = 1, prob = design$prob)\n  wins &lt;-\n    design |&gt;\n    dplyr::select(player1, player2) |&gt;\n    dplyr::mutate(\n      outcome = outcomes,\n      winner = ifelse(outcome == 1, player1, player2),\n      winner = factor(winner, levels = unique(c(design$player1, design$player2)))) |&gt;\n  dplyr::group_by(winner) |&gt;\n  dplyr::summarise(Wins = dplyr::n()) |&gt;\n  dplyr::rename(Team = winner) |&gt;\n  tidyr::complete(Team, fill = list(Wins = 0))\n  simulated_wins[r,] &lt;- wins |&gt; dplyr::pull(Wins)\n}\n\n\n\n\ntable(simulated_wins[,1])\n\n\n   0    1    2    3 \n 163 1105 2198 1534 \n\n\n\n\n\nWe will rank teams by negative number of wins\nIn case of ties: assign minimum possible rank\nE.g.: say Teams 1 & 2 won 2 games each and 3 & 4 won 1 game each\n\nTeams 1 & 2 tied for 1st and Teams 3 & 4 ties for 3rd\n\n\n\nrank(-1*c(2,2,1,1), ties.method = \"min\")\n\n[1] 1 1 3 3\n\n\n\n\n\n\\(\\mathbb{P}(\\textrm{Team 3 is in top-2}) \\approx 53.1%\\)\n\n\nsimulated_ranks &lt;-\n  t(\n    apply(-1*simulated_wins, MARGIN = 1,  \n        FUN = rank, ties.method = \"min\")) \n table(simulated_ranks[,3])\n\n\n   1    2    3    4 \n1565 1092 1547  796 \n\n round(mean(simulated_ranks[,3] &lt;= 2), digits = 3)\n\n[1] 0.531"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#identifiability",
    "href": "slides/raw_slides/slides_12.html#identifiability",
    "title": "STAT 479 Lecture 12",
    "section": "Identifiability",
    "text": "Identifiability\n\nBasic BT model: \\(\\textrm{P}(i \\textrm{beats} j)\\) depends only on difference \\(\\lambda_{i} - \\lambda_{j}\\)\nProbabilities unchanged after constant shift. E.g.: \\(\\lambda_{j} \\rightarrow \\lambda_{j} + 5\\) for all \\(j\\)\nPractically: we can only estiamtes latent strengths up to an additive constant\nOften we fix one \\(\\lambda_{j} = 0\\) (a “reference team”)"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#overview",
    "href": "slides/raw_slides/slides_12.html#overview",
    "title": "STAT 479 Lecture 12",
    "section": "Overview",
    "text": "Overview\n\nGoal: Estimate \\(\\lambda_{j}\\)’s for all D1 Women’s Hockey Teams\nRequirement: a data table (one row per match) recording\n\nIdentities of the two teams\nIdentity of the winner\n\nScrape from USCHO website\n\nBefore 10/6: code on course website worked\nAfter 10/6: USCHO changed their backend (so scraping code won’t work…)\n\nWill post a complete data table on Canvas"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#preview-of-data",
    "href": "slides/raw_slides/slides_12.html#preview-of-data",
    "title": "STAT 479 Lecture 12",
    "section": "Preview of Data",
    "text": "Preview of Data\n\n\n# A tibble: 10 × 10\n   Day   Date       Time     Opponent OppScore Home  HomeScore OT    Notes Type \n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 Sat.  1/4/2025   1:00 CT  Minneso… 4        Lind… 1         &lt;NA&gt;  &lt;NA&gt;  NC   \n 2 Fri.  12/6/2024  6:00 ET  Connect… 2        New … 1         &lt;NA&gt;  &lt;NA&gt;  HE   \n 3 Fri.  12/6/2024  6:00 ET  Dartmou… 3        Clar… 5         &lt;NA&gt;  &lt;NA&gt;  EC   \n 4 Fri.  11/29/2024 6:00 ET  Colgate  3        Syra… 1         &lt;NA&gt;  &lt;NA&gt;  NC   \n 5 Sat.  1/18/2025  3:00 ET  Renssel… 0        Dart… 0         OT    DAR … EC   \n 6 Sat.  3/1/2025   4:30 ET  Maine    3        Bost… 4         &lt;NA&gt;  HEA … NC   \n 7 Fri.  1/3/2025   7:00 ET  Yale     8        Robe… 1         &lt;NA&gt;  &lt;NA&gt;  NC   \n 8 Fri.  11/1/2024  3:00 CT  Minneso… 2        Bemi… 1         &lt;NA&gt;  &lt;NA&gt;  WC   \n 9 Fri.  1/10/2025  6:00 ET  St. Law… 4        Union 2         &lt;NA&gt;  &lt;NA&gt;  EC   \n10 Sat.  11/16/2024 12:00 ET Vermont  0        Prov… 0         OT    VER … HE"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#data-preparation",
    "href": "slides/raw_slides/slides_12.html#data-preparation",
    "title": "STAT 479 Lecture 12",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nAppending a column for the winner\n\nMost games, HomeScore differs from OppScore\nOften when HomeScore==OppScore, game decided in shoot out\n\nTo determine winner of shootouts, must parse team name & abbreviation\n\nE.g. “WIS” for Wisconsin; “OSU” for Ohio State; etc.\n\nAdd column recording whether home team or away team won\nExtract exhibition & tournament games\n\nMostly by parsing the Notes column\n\nSee lecture notes for details"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#the-bradleyterry2-package",
    "href": "slides/raw_slides/slides_12.html#the-bradleyterry2-package",
    "title": "STAT 479 Lecture 12",
    "section": "The BradleyTerry2 Package",
    "text": "The BradleyTerry2 Package\n\nWe will use the BradleyTerry2 package\n\n\ndevtools::install_github(\"hturner/BradleyTerry2\")\n\n\nPackage has rather specific syntax, so it’s important to read documentation\n\n\nBradleyTerry2 expects a data table w/\n\nSeparate row for each home team - away team pair\nTwo columns recording the number of home & away team wins\n\n\n\nset.seed(123)\nresults |&gt; dplyr::filter(home.team == \"Wisconsin\") |&gt; dplyr::slice_sample(n=5)\n\n# A tibble: 5 × 4\n  home.team away.team       home.win away.win\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt;\n1 Wisconsin St. Cloud State        2        0\n2 Wisconsin St. Thomas             2        0\n3 Wisconsin Minnesota              3        0\n4 Wisconsin Ohio State             1        1\n5 Wisconsin Lindenwood             2        0"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#model-fitting",
    "href": "slides/raw_slides/slides_12.html#model-fitting",
    "title": "STAT 479 Lecture 12",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nCodeSummaryExtracting \\(\\hat{\\lambda}_{j}\\)’s\n\n\n\nBTm allows us to manually specify reference team (w/ \\(\\lambda_{j} = 0\\))\n\n\nfit &lt;-\n  BradleyTerry2::BTm(\n    outcome = cbind(home.win, away.win),  \n    player1 = home.team, player2 = away.team, \n    refcat = \"Assumption\", \n    data = results) \n\n\n\n\nsummary(fit)\n\n\nCall:\nBradleyTerry2::BTm(outcome = cbind(home.win, away.win), player1 = home.team, \n    player2 = away.team, refcat = \"Assumption\", data = results)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n..Bemidji State       3.8615     1.2828   3.010 0.002610 ** \n..Boston College      5.5371     1.2307   4.499 6.82e-06 ***\n..Boston University   5.7866     1.2267   4.717 2.39e-06 ***\n..Brown               4.7983     1.2178   3.940 8.14e-05 ***\n..Clarkson            5.8522     1.2137   4.822 1.42e-06 ***\n..Colgate             6.6950     1.2248   5.466 4.59e-08 ***\n..Connecticut         5.7772     1.2293   4.700 2.61e-06 ***\n..Cornell             7.2421     1.2586   5.754 8.72e-09 ***\n..Dartmouth           3.8239     1.2433   3.076 0.002100 ** \n..Franklin Pierce     0.8259     0.5436   1.519 0.128701    \n..Harvard             2.5128     1.3690   1.835 0.066441 .  \n..Holy Cross          4.1094     1.2214   3.364 0.000767 ***\n..Lindenwood          3.0888     1.2317   2.508 0.012150 *  \n..LIU                 1.6854     0.5869   2.872 0.004085 ** \n..Maine               4.2996     1.2360   3.479 0.000504 ***\n..Mercyhurst          5.1540     1.2094   4.262 2.03e-05 ***\n..Merrimack           3.8888     1.2264   3.171 0.001520 ** \n..Minnesota           7.3462     1.2685   5.791 6.98e-09 ***\n..Minnesota Duluth    6.7823     1.2722   5.331 9.75e-08 ***\n..Minnesota State     5.2957     1.2725   4.162 3.16e-05 ***\n..New Hampshire       4.6625     1.2191   3.824 0.000131 ***\n..Northeastern        5.6740     1.2210   4.647 3.37e-06 ***\n..Ohio State          7.8472     1.2946   6.061 1.35e-09 ***\n..Penn State          6.6326     1.2569   5.277 1.31e-07 ***\n..Post                0.3263     0.5404   0.604 0.545921    \n..Princeton           5.4717     1.2229   4.474 7.66e-06 ***\n..Providence          5.3356     1.2281   4.345 1.40e-05 ***\n..Quinnipiac          5.6338     1.2010   4.691 2.72e-06 ***\n..Rensselaer          4.3425     1.1913   3.645 0.000267 ***\n..RIT                 4.4201     1.2043   3.670 0.000242 ***\n..Robert Morris       3.0838     1.1764   2.621 0.008755 ** \n..Sacred Heart        1.3976     0.5531   2.527 0.011504 *  \n..St. Anselm          0.7039     0.5387   1.307 0.191287    \n..St. Cloud State     5.8507     1.2647   4.626 3.73e-06 ***\n..St. Lawrence        5.9259     1.2094   4.900 9.58e-07 ***\n..St. Michael's      -1.1317     0.6421  -1.762 0.077987 .  \n..St. Thomas          4.6868     1.2861   3.644 0.000268 ***\n..Stonehill           0.6387     0.5612   1.138 0.255106    \n..Syracuse            4.2144     1.1961   3.523 0.000426 ***\n..Union               4.5583     1.2002   3.798 0.000146 ***\n..Vermont             4.0937     1.2304   3.327 0.000877 ***\n..Wisconsin           9.3418     1.4304   6.531 6.53e-11 ***\n..Yale                5.4095     1.2138   4.457 8.32e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 815.15  on 474  degrees of freedom\nResidual deviance: 463.44  on 431  degrees of freedom\nAIC: 668.78\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nlambda_hat &lt;- BradleyTerry2::BTabilities(fit)\nlambda_hat[c(\"Wisconsin\", \"Ohio State\", \"Cornell\", \"Minnesota\"),]\n\n            ability     s.e.\nWisconsin  9.341843 1.430356\nOhio State 7.847175 1.294598\nCornell    7.242125 1.258623\nMinnesota  7.346209 1.268487"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#estimated-team-strengths",
    "href": "slides/raw_slides/slides_12.html#estimated-team-strengths",
    "title": "STAT 479 Lecture 12",
    "section": "Estimated Team Strengths",
    "text": "Estimated Team Strengths\n\n\n\n\n\n\n\n\nFigure 1: Most teams are favored against the reference team (Assumption College)"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#changing-reference-team",
    "href": "slides/raw_slides/slides_12.html#changing-reference-team",
    "title": "STAT 479 Lecture 12",
    "section": "Changing Reference Team",
    "text": "Changing Reference Team\n\nCodeVisualization\n\n\n\nfit_nh &lt;- update(fit, refcat = \"New Hampshire\")\nlambda_hat_nh &lt;- BradleyTerry2::BTabilities(fit_nh)\nlambda_hat_nh[c(\"Assumption\", \"New Hampshire\", \"Wisconsin\", \"Ohio State\"),]\n\n                ability      s.e.\nAssumption    -4.662547 1.2191462\nNew Hampshire  0.000000 0.0000000\nWisconsin      4.679297 0.9932217\nOhio State     3.184629 0.7887300\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Some teams are significantly stronger and some teams are significantly weaker than New Hampshire"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#ncaa-d1-championship",
    "href": "slides/raw_slides/slides_12.html#ncaa-d1-championship",
    "title": "STAT 479 Lecture 12",
    "section": "NCAA D1 Championship",
    "text": "NCAA D1 Championship\n\nCurrently, championship decided w/ a single game\n\n2024-25: WIS defeated OSU in overtime\n\nAccording to our model, WIS would win 81% of the time\n\n\n1/(1 + exp(-1 * (lambda_hat[\"Wisconsin\", \"ability\"] - lambda_hat[\"Ohio State\", \"ability\"])))\n\n[1] 0.8167779\n\n\n\nWhat if NCAA moved to a best-of-5 series?"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#simulating-series-once",
    "href": "slides/raw_slides/slides_12.html#simulating-series-once",
    "title": "STAT 479 Lecture 12",
    "section": "Simulating Series Once",
    "text": "Simulating Series Once\n\nwi_wins &lt;- 0 \nosu_wins &lt;- 0 \n\nwi_prob &lt;- \n  1/(1 + exp(-1 * (lambda_hat[\"Wisconsin\", \"ability\"] - lambda_hat[\"Ohio State\", \"ability\"])))\n\ngame_counter &lt;- 0 \noutcomes &lt;- rep(NA, times= 5)\nset.seed(481)\nwhile( wi_wins  &lt; 3 & osu_wins &lt; 3 & game_counter &lt; 5){ \n  game_counter &lt;- game_counter + 1\n  outcomes[game_counter] &lt;- rbinom(n = 1, size = 1, prob = wi_prob) \n  \n  if(outcomes[game_counter] == 1) wi_wins &lt;- wi_wins + 1 \n  else if(outcomes[game_counter] == 0) osu_wins &lt;- osu_wins + 1 \n\n}\nif(wi_wins == 3){ \n  winner &lt;- \"Wisconsin\"\n} else if(osu_wins == 3){\n  winner &lt;- \"Ohio State\"\n} else{\n  winner &lt;- NA\n}\n \ncat(\"Series ended after\", game_counter, \" games. Winner = \", winner, \"\\n\")\ncat(\"Wisconsin: \", wi_wins, \" Ohio State: \", osu_wins, \"\\n\")\ncat(\"Game results:\", outcomes, \"\\n\")\n\nSeries ended after 4  games. Winner =  Wisconsin \nWisconsin:  3  Ohio State:  1 \nGame results: 1 1 0 1 NA"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#repeated-simulations",
    "href": "slides/raw_slides/slides_12.html#repeated-simulations",
    "title": "STAT 479 Lecture 12",
    "section": "Repeated Simulations",
    "text": "Repeated Simulations\n\nRepeating the simulation 5,000 times (each time w/ different seed)\n\nWIS won series ~95% of the time\nSeries ended in 3 games about 55% of the time\nSeries went to 5 games about 13% of the time"
  },
  {
    "objectID": "slides/raw_slides/slides_12.html#looking-ahead",
    "href": "slides/raw_slides/slides_12.html#looking-ahead",
    "title": "STAT 479 Lecture 12",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nBasic BT model does not account for home-team advantage\nLecture 13: simulate entire Frozen 4"
  }
]