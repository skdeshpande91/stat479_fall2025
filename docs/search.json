[
  {
    "objectID": "lectures/lecture01.html",
    "href": "lectures/lecture01.html",
    "title": "Lecture 1: Boxscore Metrics",
    "section": "",
    "text": "Who is the best shooter in the NBA? How do we determine this using data? \nIn this lecture, we will practice using functions from the tidyverse suite of packages (especially dplyr) to manipulate tables of NBA box score data. Hopefully, much of the functionality we encounter in this lecture will be familiar to you. But, if you need a high-level refresher, I highly recommend the following resources:\n\nChapter 3 and Chapter 5 of R for Data Science.\nSection 1.9 and Chapter 3 of *Data Science: A First Introduction.\n\nWe will use the package hoopR to scrape NBA boxscore data. You should install the package using the code install.packages(\"hoopR\").",
    "crumbs": [
      "Lecture 1: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture01.html#motivation-the-best-shooting-season-in-the-nba",
    "href": "lectures/lecture01.html#motivation-the-best-shooting-season-in-the-nba",
    "title": "Lecture 1: Boxscore Metrics",
    "section": "",
    "text": "Who is the best shooter in the NBA? How do we determine this using data? \nIn this lecture, we will practice using functions from the tidyverse suite of packages (especially dplyr) to manipulate tables of NBA box score data. Hopefully, much of the functionality we encounter in this lecture will be familiar to you. But, if you need a high-level refresher, I highly recommend the following resources:\n\nChapter 3 and Chapter 5 of R for Data Science.\nSection 1.9 and Chapter 3 of *Data Science: A First Introduction.\n\nWe will use the package hoopR to scrape NBA boxscore data. You should install the package using the code install.packages(\"hoopR\").",
    "crumbs": [
      "Lecture 1: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture01.html#basic-box-score-statistics",
    "href": "lectures/lecture01.html#basic-box-score-statistics",
    "title": "Lecture 1: Boxscore Metrics",
    "section": "Basic box score statistics",
    "text": "Basic box score statistics\n\nScraping and wrangling box score data\nTo get all the boxscore data, we will use the function load_nba_player_box\n\nlibrary(tidyverse)\nraw_box &lt;-\n  hoopR::load_nba_player_box(seasons = 2002:(hoopR::most_recent_nba_season()))\n\nThe data table raw_box contains 813246 rows and 57 columns. Checking the column names, we see that there are columns for the numbers of field goals, three point shots, and free throws made and attempted.\n\ncolnames(raw_box)\n\n [1] \"game_id\"                           \"season\"                           \n [3] \"season_type\"                       \"game_date\"                        \n [5] \"game_date_time\"                    \"athlete_id\"                       \n [7] \"athlete_display_name\"              \"team_id\"                          \n [9] \"team_name\"                         \"team_location\"                    \n[11] \"team_short_display_name\"           \"minutes\"                          \n[13] \"field_goals_made\"                  \"field_goals_attempted\"            \n[15] \"three_point_field_goals_made\"      \"three_point_field_goals_attempted\"\n[17] \"free_throws_made\"                  \"free_throws_attempted\"            \n[19] \"offensive_rebounds\"                \"defensive_rebounds\"               \n[21] \"rebounds\"                          \"assists\"                          \n[23] \"steals\"                            \"blocks\"                           \n[25] \"turnovers\"                         \"fouls\"                            \n[27] \"plus_minus\"                        \"points\"                           \n[29] \"starter\"                           \"ejected\"                          \n[31] \"did_not_play\"                      \"active\"                           \n[33] \"athlete_jersey\"                    \"athlete_short_name\"               \n[35] \"athlete_headshot_href\"             \"athlete_position_name\"            \n[37] \"athlete_position_abbreviation\"     \"team_display_name\"                \n[39] \"team_uid\"                          \"team_slug\"                        \n[41] \"team_logo\"                         \"team_abbreviation\"                \n[43] \"team_color\"                        \"team_alternate_color\"             \n[45] \"home_away\"                         \"team_winner\"                      \n[47] \"team_score\"                        \"opponent_team_id\"                 \n[49] \"opponent_team_name\"                \"opponent_team_location\"           \n[51] \"opponent_team_display_name\"        \"opponent_team_abbreviation\"       \n[53] \"opponent_team_logo\"                \"opponent_team_color\"              \n[55] \"opponent_team_alternate_color\"     \"opponent_team_score\"              \n[57] \"reason\"                           \n\n\nNotice as well that there are columns for the game date (game_date), game id (game_id), and player (e.g., athlete_display_name). This suggests that each row corresponds to a unique combination of game and player and records the players individual statistics in that game.\nFor instance, here are the box score statistics for several players from a single game in 2011.\n\nraw_box |&gt;\n  filter(game_date == \"2011-06-12\") |&gt;\n  select(athlete_display_name, \n         field_goals_made, field_goals_attempted,\n         three_point_field_goals_made, three_point_field_goals_attempted,\n         free_throws_made, free_throws_attempted)\n\n── ESPN NBA Player Boxscores from hoopR data repository ───────── hoopR 2.1.0 ──\n\n\nℹ Data updated: 2025-07-24 06:40:48 CDT\n\n\n# A tibble: 30 × 7\n   athlete_display_name field_goals_made field_goals_attempted\n   &lt;chr&gt;                           &lt;int&gt;                 &lt;int&gt;\n 1 Dirk Nowitzki                       9                    27\n 2 Tyson Chandler                      2                     4\n 3 Jason Kidd                          2                     4\n 4 Shawn Marion                        4                    10\n 5 J.J. Barea                          7                    12\n 6 Brian Cardinal                      1                     1\n 7 Caron Butler                       NA                    NA\n 8 Ian Mahinmi                         2                     3\n 9 Rodrigue Beaubois                  NA                    NA\n10 DeShawn Stevenson                   3                     5\n# ℹ 20 more rows\n# ℹ 4 more variables: three_point_field_goals_made &lt;int&gt;,\n#   three_point_field_goals_attempted &lt;int&gt;, free_throws_made &lt;int&gt;,\n#   free_throws_attempted &lt;int&gt;\n\n\nAs a sanity check, we can cross-reference the data in our table with the box score from ESPN. Luckily, these numbers match up!\nIt turns out that raw_box contains much more data than we need. Specifically, it includes statistics from play-in and play-off games as well as data from some (but not all) All-Star games. Since we’re ultimately interested in identifying the best player-seasons in terms of shooting performance, we need to remove all play-off, play-in, and All-Star games from the dataset. Additionally, the column did_not_play contains a Boolean (i.e., logical) variable that is TRUE is the player did not play in the game and is FALSE if the player did not play in the game\n\nallstar_dates &lt;-\n  lubridate::date(c(\"2002-02-10\", \"2003-02-09\", \"2004-02-15\",\n    \"2005-02-20\", \"2006-02-19\", \"2007-02-18\", \n    \"2008-02-17\", \"2009-02-15\", \"2010-02-14\",\n    \"2011-02-20\", \"2012-02-26\", \"2013-02-17\", \n    \"2014-02-16\", \"2015-02-15\", \"2016-02-14\",\n    \"2017-02-19\", \"2018-02-18\", \"2019-02-17\",\n    \"2020-02-16\", \"2021-03-07\", \"2022-02-20\",\n    \"2023-02-19\", \"2024-02-18\", \"2025-02-16\"))\nreg_box &lt;-\n  raw_box |&gt;\n  filter(season_type == 2 & !did_not_play & !game_date %in% allstar_dates)\n\nLooking at the data table reg_box, we see that in about 9% of rows, the number of minutes played is missing. These likely correspond to players who were active but did not play or logged only a few seconds (generally at the end of games). We will replace these NA values with 0’s and, while doing so, rename some of the columns in reg_box.\n\nreg_box &lt;-\n  reg_box |&gt;\n1  rename(\n    Player = athlete_display_name,\n    FGM = field_goals_made,\n    FGA = field_goals_attempted,\n    TPM = three_point_field_goals_made,\n    TPA = three_point_field_goals_attempted,\n    FTM = free_throws_made, \n    FTA = free_throws_attempted) |&gt;\n2  mutate(FGM = ifelse(is.na(minutes), 0, FGM),\n         FGA = ifelse(is.na(minutes), 0, FGA),\n         TPM = ifelse(is.na(minutes), 0, TPM),\n         TPA = ifelse(is.na(minutes), 0, TPA),\n         FTM = ifelse(is.na(minutes), 0, FTM),\n         FTA = ifelse(is.na(minutes), 0, FTA)) |&gt;\n3  replace_na(list(minutes = 0))\n\n\n1\n\nRename several variables\n\n2\n\nFor those rows where minutes is NA, set the numbers of makes and attempts to 0\n\n3\n\nReplace missing minutes values with 0\n\n\n\n\nAt this point, every row of reg_box corresponds to a player-game combination. We ultimately wish to sum up the number of makes and misses of each shot type across an entire season for each player. To illustrate this, let’s focus on Dirk Nowitzki’s performance in the 2006-07 season when he won the league MVP award. Conceptually, we can accomplish this by first dividing the full data table into several smaller tables, one for each combination of player and season. Then, we can sum the number of field goals, three point shots, and free throws attempted and made by each player in each of their season. This is an example of the split-apply-combine strategy in which you “break up a big problem into manageable pieces, operate on each piece independently, and then put all the pieces back together.” (Wickham 2011). This functionality is implemented in dplyr using group_by()\n\nseason_box &lt;-\n  reg_box |&gt;\n  group_by(Player, season) %&gt;%\n  summarise(\n    FGM = sum(FGM),\n    FGA = sum(FGA),\n    TPM = sum(TPM),\n    TPA = sum(TPA),\n    FTM = sum(FTM),\n    FTA = sum(FTA),\n    minutes = sum(minutes),\n    n_games = n(),\n    .groups = \"drop\")\n\nThe data table season_box contains 11920 rows, each of corresponds to a single player-season combination. Here is a quick snapshot of some of the data for Dirk Nowitzki\n\nseason_box |&gt;\n  filter(Player == \"Dirk Nowitzki\") |&gt;\n  select(season, FGM, FGA, TPM, TPA, FTM, FTA)\n\n# A tibble: 18 × 7\n   season   FGM   FGA   TPM   TPA   FTM   FTA\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   2002   600  1258   139   350   440   516\n 2   2003   690  1489   148   390   483   548\n 3   2004   605  1310    99   290   371   423\n 4   2005   663  1445    91   228   615   708\n 5   2006   751  1564   110   271   539   598\n 6   2007   673  1341    72   173   498   551\n 7   2008   630  1314    79   220   478   544\n 8   2009   774  1616    61   170   485   545\n 9   2010   720  1496    51   121   536   586\n10   2011   610  1179    66   168   395   443\n11   2012   473  1034    78   212   318   355\n12   2013   332   707    63   151   164   191\n13   2014   633  1273   131   329   338   376\n14   2015   487  1062   104   274   255   289\n15   2016   498  1112   126   342   250   280\n16   2017   296   678    79   209    98   112\n17   2018   346   758   138   337    97   108\n18   2019   135   376    64   205    39    50",
    "crumbs": [
      "Lecture 1: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture01.html#from-totals-to-percentages",
    "href": "lectures/lecture01.html#from-totals-to-percentages",
    "title": "Lecture 1: Boxscore Metrics",
    "section": "From totals to percentages",
    "text": "From totals to percentages\nIn order to determine which player-season was the best in terms of shooting, we need to first define “best”. Perhaps the simplest definition is to find the player-season with the most made shots. We can identify this player-season by sorting the data in season_box by FGM in descending order with the arrange() function\n\nseason_box %&gt;%\n  arrange(desc(FGM))\n\n# A tibble: 11,920 × 10\n   Player             season   FGM   FGA   TPM   TPA   FTM   FTA minutes n_games\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n 1 Kobe Bryant          2006   949  2109   179   506   675   788    3184      78\n 2 LeBron James         2006   875  1823   127   379   601   814    3361      82\n 3 Kobe Bryant          2003   868  1924   124   324   601   713    3401      82\n 4 Shai Gilgeous-Ale…   2025   868  1680   165   444   604   673    2633      77\n 5 LeBron James         2018   857  1580   149   406   388   531    3024      82\n 6 Dwyane Wade          2009   854  1739    88   278   590   771    3048      82\n 7 Kevin Durant         2014   849  1688   192   491   703   805    3118      81\n 8 James Harden         2019   843  1909   378  1028   754   858    2870      78\n 9 Giannis Antetokou…   2024   837  1369    34   124   514   782    2573      73\n10 Tracy McGrady        2003   829  1813   173   448   576   726    2954      75\n# ℹ 11,910 more rows\n\n\nWhen we look at the ten “best” shooting seasons, we immediately recognize a lot of superstar players! On this basis, we might be satisfied evaluating shooting performances based only on the total number of shots. But taking a closer look, should we really consider Kobe Bryant’s 2002-03 and Shai Gilgeous-Alexander’s 2024-25 seasons to be equally impressive when Kobe took attempted 242 more shots than Shai in order to make 868 shots? Arguably, Shai’s 2024-25 season should rank higher than Kobe’s 2002-03 season because Shai was more efficient.\nThis motivates us to refine our definition of “best” by focusing on the percentage of field goals made rather than total number of field goals made.\n\nseason_box &lt;-\n  season_box |&gt;\n1  mutate(FGP = ifelse(FGA &gt; 0, FGM/FGA, NA_real_))\nseason_box |&gt; \n  arrange(desc(FGP)) |&gt;\n  select(Player, season, FGP)\n\n\n1\n\nFor players who attempted no field goals (i.e., FGA = 0), their field goal percentage is undefined.\n\n\n\n\n# A tibble: 11,920 × 3\n   Player           season   FGP\n   &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;\n 1 Ahmad Caver        2022     1\n 2 Alondes Williams   2025     1\n 3 Andris Biedrins    2014     1\n 4 Anthony Brown      2018     1\n 5 Braxton Key        2023     1\n 6 Chris Silva        2023     1\n 7 Dajuan Wagner      2007     1\n 8 DeAndre Liggins    2014     1\n 9 Donnell Harvey     2005     1\n10 Eddy Curry         2009     1\n# ℹ 11,910 more rows\n\n\nSorting the players by their \\(\\textrm{FGP},\\) we find that several players made 100% of their field goals. But very few of these players are immediately recognizable — and, indeed, none of them have been in the MVP conversation, despite the fact that they made all their shots!\nTo understand what’s going on, let’s take a look at the number of attempts.\n\nseason_box %&gt;% \n  arrange(desc(FGP)) %&gt;%\n  select(Player, season, FGP, FGA)\n\n# A tibble: 11,920 × 4\n   Player           season   FGP   FGA\n   &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ahmad Caver        2022     1     1\n 2 Alondes Williams   2025     1     2\n 3 Andris Biedrins    2014     1     1\n 4 Anthony Brown      2018     1     1\n 5 Braxton Key        2023     1     1\n 6 Chris Silva        2023     1     1\n 7 Dajuan Wagner      2007     1     1\n 8 DeAndre Liggins    2014     1     1\n 9 Donnell Harvey     2005     1     2\n10 Eddy Curry         2009     1     2\n# ℹ 11,910 more rows\n\n\nGiven the very low number of shots attempted in any of these player-season, claiming that any of these player-seasons are among the best ever would strain credulity! So, in order to determine the best shooting performance, we will need to threshold our data to players who took a minimum number of shots. For simplicity, let’s focus our attention on those players who attempted at least 400 field goals in a season (i.e., they attempted, on average, at least 5 shots per game).\n\n\n\nseason_box |&gt;\n  filter(FGA &gt;= 400) |&gt;\n  arrange(desc(FGP)) |&gt;\n  select(Player, season, FGP,FGA)\n\n# A tibble: 4,987 × 4\n   Player         season   FGP   FGA\n   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Daniel Gafford   2024 0.725   480\n 2 Walker Kessler   2023 0.720   414\n 3 DeAndre Jordan   2017 0.714   577\n 4 Rudy Gobert      2022 0.713   508\n 5 DeAndre Jordan   2015 0.710   534\n 6 Jarrett Allen    2025 0.706   640\n 7 Nic Claxton      2023 0.705   587\n 8 DeAndre Jordan   2016 0.703   508\n 9 Daniel Gafford   2025 0.702   403\n10 Daniel Gafford   2022 0.693   411\n# ℹ 4,977 more rows\n\n\nDo we really believe that these performances, all of which were made centers who mostly shoot at or near the rim, represent some of the best shooting performances of all time?\n\nEffective Field Goal Percentage\nA major limitation of FGP is that it treats 2-point shots the same as 3-point shots. As a result, the league-leader in FGP every season is usually a center whose shots mostly come from near the rim. Effective Field Goal Percentage (eFGP) adjusts FGP to account for the fact that a made 3-point shots is worth 50% more than a made 2-point shot. The formula for eFGP is \\[\n\\textrm{eFGP} = \\frac{\\textrm{FGM} + 0.5 \\times \\textrm{TPM}}{\\textrm{FGA}}\n\\]\n\nseason_box &lt;-\n  season_box |&gt;\n  mutate(\n    TPP = ifelse(TPA &gt; 0, TPM/TPA,NA_real_),\n    eFGP = (FGM + 0.5 * TPM)/FGA) \nseason_box %&gt;%\n  filter(FGA &gt;= 400) %&gt;%\n  arrange(desc(eFGP), desc(FGP)) %&gt;%\n  select(Player, season, eFGP, FGP, TPP, TPA, n_games)\n\n# A tibble: 4,987 × 7\n   Player         season  eFGP   FGP    TPP   TPA n_games\n   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Daniel Gafford   2024 0.725 0.725 NA         0      74\n 2 Walker Kessler   2023 0.721 0.720  0.333     3      74\n 3 DeAndre Jordan   2017 0.714 0.714  0         2      81\n 4 Rudy Gobert      2022 0.713 0.713  0         4      66\n 5 DeAndre Jordan   2015 0.711 0.710  0.25      4      82\n 6 Jarrett Allen    2025 0.706 0.706  0         5      82\n 7 Nic Claxton      2023 0.705 0.705  0         2      76\n 8 DeAndre Jordan   2016 0.703 0.703  0         1      77\n 9 Daniel Gafford   2025 0.702 0.702 NA         0      57\n10 Daniel Gafford   2022 0.693 0.693  0         1      72\n# ℹ 4,977 more rows\n\n\nWe see again that some of the best seasons, according to eFGP, were from centers, many of whom attempt few few three point shots. When filter out players who took at least 100 three point shots, we start to see other positions in the top-10.\n\nseason_box %&gt;%\n  filter(FGA &gt;= 400 & TPA &gt;= 100) %&gt;%\n  arrange(desc(eFGP), desc(FGP)) %&gt;%\n  select(Player, season, eFGP, FGP, TPP, TPA, n_games)\n\n# A tibble: 3,658 × 7\n   Player             season  eFGP   FGP   TPP   TPA n_games\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Kyle Korver          2015 0.671 0.487 0.492   449      75\n 2 Duncan Robinson      2020 0.667 0.470 0.446   606      73\n 3 Obi Toppin           2024 0.660 0.571 0.404   260      83\n 4 Nikola Jokic         2023 0.660 0.632 0.383   149      69\n 5 Joe Harris           2021 0.655 0.502 0.478   427      69\n 6 Joe Ingles           2021 0.652 0.489 0.451   406      67\n 7 Grayson Allen        2024 0.649 0.499 0.461   445      75\n 8 Michael Porter Jr.   2021 0.646 0.541 0.447   374      61\n 9 Mikal Bridges        2021 0.643 0.543 0.425   315      72\n10 Al Horford           2024 0.640 0.511 0.419   258      65\n# ℹ 3,648 more rows\n\n\n\n\nTrue Shooting Percentage\nBoth FGP and eFGP totally ignore free throws. Intuitively, we should expect the best shooter to be proficient at making two-and three-point shots as well as their free throws. One metric that accounts for all field goals, three pointers, and free throws is true shooting percentage (\\(\\textrm{TSP}\\)), whose formula is given by \\[\n\\textrm{TSP} = \\frac{\\textrm{PTS}}{2 \\times \\left(\\textrm{FGA} + (0.44 \\times \\textrm{FTA})\\right)},\n\\] where \\(\\textrm{PTS} =  \\textrm{FTM} + 2 \\times \\textrm{FGM} + \\textrm{TPM}\\) is the total number of points scored.\n\nseason_box &lt;-\n  season_box |&gt;\n  mutate(PTS = FTM + 2 * FGM + TPM,\n         TSP = PTS/(2 * (FGA + 0.44 * FTA)))\nseason_box %&gt;%\n  filter(FGA &gt;= 400 & TPA &gt;= 100) %&gt;%\n  arrange(desc(TSP), desc(eFGP), desc(FGP)) %&gt;%\n  select(Player, season, TSP, eFGP, FGP, TPP, n_games)\n\n# A tibble: 3,658 × 7\n   Player          season   TSP  eFGP   FGP   TPP n_games\n   &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 Nikola Jokic      2023 0.701 0.660 0.632 0.383      69\n 2 Kyle Korver       2015 0.699 0.671 0.487 0.492      75\n 3 Austin Reaves     2023 0.687 0.616 0.529 0.398      64\n 4 Duncan Robinson   2020 0.684 0.667 0.470 0.446      73\n 5 Dwight Powell     2019 0.682 0.637 0.597 0.307      77\n 6 Grayson Allen     2024 0.679 0.649 0.499 0.461      75\n 7 Kevin Durant      2023 0.677 0.614 0.560 0.404      47\n 8 Moritz Wagner     2024 0.676 0.636 0.601 0.330      80\n 9 Stephen Curry     2018 0.675 0.618 0.495 0.423      51\n10 Obi Toppin        2024 0.675 0.660 0.571 0.404      83\n# ℹ 3,648 more rows",
    "crumbs": [
      "Lecture 1: Boxscore Metrics"
    ]
  },
  {
    "objectID": "lectures/lecture02.html",
    "href": "lectures/lecture02.html",
    "title": "Lecture 2: Expected Goals",
    "section": "",
    "text": "During the EURO2022 football tournament, English player Beth Mead scored 6 goals. Here are screenshots (and links to videos) of three of those goals. Which is most impressive to you?\n\n\n\n\n\n\n\n\nMead shot against Austria link\n\n\n\n\n\n\n\nMead shot against Norway link\n\n\n\n\n\n\n\nMead shot against Sweden link\n\n\n\n\n\n\nFigure 1\n\n\n\nWe observed several important differences between these shots. For instance, Mead scored the goal against Austria in a one-on-one situation but had to shoot through several defenders against both Norway and Sweden. On this basis, should we view\nWe could argue endlessly about the qualitative differences between these shots. To make our discussion more precise, it is helpful to quantify these differences. One way — but certainly not the only way — is to ask what might happen if Mead were to repeat these three shots over and over again. In this thought experiment, we could compare the shots based on the relative proportion of times that the shots resulted in a goal. Comparing the these hypothetical long-run proportions to the actual observed shot outcomes allows us to how impressive the outcome was. For instance, the lob shot against Austria might look a lot less impressive if we knew that such a shot would very often result in a goal if repeated over and over again. Of course, Mead can’t actually repeat these shots over and over again. In this lecture, we will introduce the expected goals framework, which allows us to estimate those long-run frequencies of goals.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#motivation-beth-meads-performance-at-euro2022",
    "href": "lectures/lecture02.html#motivation-beth-meads-performance-at-euro2022",
    "title": "Lecture 2: Expected Goals",
    "section": "",
    "text": "During the EURO2022 football tournament, English player Beth Mead scored 6 goals. Here are screenshots (and links to videos) of three of those goals. Which is most impressive to you?\n\n\n\n\n\n\n\n\nMead shot against Austria link\n\n\n\n\n\n\n\nMead shot against Norway link\n\n\n\n\n\n\n\nMead shot against Sweden link\n\n\n\n\n\n\nFigure 1\n\n\n\nWe observed several important differences between these shots. For instance, Mead scored the goal against Austria in a one-on-one situation but had to shoot through several defenders against both Norway and Sweden. On this basis, should we view\nWe could argue endlessly about the qualitative differences between these shots. To make our discussion more precise, it is helpful to quantify these differences. One way — but certainly not the only way — is to ask what might happen if Mead were to repeat these three shots over and over again. In this thought experiment, we could compare the shots based on the relative proportion of times that the shots resulted in a goal. Comparing the these hypothetical long-run proportions to the actual observed shot outcomes allows us to how impressive the outcome was. For instance, the lob shot against Austria might look a lot less impressive if we knew that such a shot would very often result in a goal if repeated over and over again. Of course, Mead can’t actually repeat these shots over and over again. In this lecture, we will introduce the expected goals framework, which allows us to estimate those long-run frequencies of goals.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#working-with-soccer-event-data",
    "href": "lectures/lecture02.html#working-with-soccer-event-data",
    "title": "Lecture 2: Expected Goals",
    "section": "Working with soccer event data",
    "text": "Working with soccer event data\nWe will make use of high-resolution tracking data provided by the company StatsBomb, which was recently acquired by Hudl. As a bit of background, StatsBomb extracts player locations from game film using some pretty interesting computer vision techniques. To their great credit, StatsBomb releases a small snapshot of their data for public use1 We can access this data directly in R using the StatsBombR package.\nYou can check if the package is installed using the code \"StatsBombR\" %in% rownames(installed.packages()). If that code returns FALSE, then you should install the package using the code devtools::install_github(\"statsbomb/StatsBombR\").\nStatsBomb organizes its free data by competition/tournament. The screenshot below shows a table of all the available competitions. We can load this table into our R environment using the function StatsBombR::FreeCompetitions()\n\n\n\nScreenshot of the competitions covered in the public data\n\n\nEach competition and season have unique id and we can also see whether it was a men’s or women’s competition. To see which matches from selected competitions have publicly available data, we can pass the corresponding rows of this table to the function StatsBombR::FreeMatches(). For instance, hereare the first few rows and some selected columns from the matches from EURO 2022. StatsBomb graciously provided data for all the matches in the tournament. Figure 2 shows the table of matches from the 2022 EURO Competition; StatsBomb graciously provided data for all matches from the tournament, which can be obtained using the code below.\n\nStatsBombR::FreeCompetitions() |&gt;\n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt;\n  dplyr::select(match_id, home_team.home_team_name, away_team.away_team_name, home_score, away_score)\n\n[1] \"Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please credit StatsBomb as your data source when using the data and visit https://statsbomb.com/media-pack/ to obtain our logos for public use.\"\n[1] \"Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please credit StatsBomb as your data source when using the data and visit https://statsbomb.com/media-pack/ to obtain our logos for public use.\"\n\n\n# A tibble: 31 × 5\n   match_id home_team.home_team_n…¹ away_team.away_team_…² home_score away_score\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt;                       &lt;int&gt;      &lt;int&gt;\n 1  3835331 Sweden Women's          Switzerland Women's             2          1\n 2  3835324 Netherlands Women's     Sweden Women's                  1          1\n 3  3844384 England Women's         Spain Women's                   2          1\n 4  3847567 England Women's         Germany Women's                 2          1\n 5  3845506 England Women's         Sweden Women's                  4          0\n 6  3835335 Northern Ireland        England Women's                 0          5\n 7  3835323 Portugal Women's        Switzerland Women's             2          2\n 8  3835325 France Women's          Italy Women's                   5          1\n 9  3835320 Norway Women's          Northern Ireland                4          1\n10  3845507 Germany Women's         France Women's                  2          1\n# ℹ 21 more rows\n# ℹ abbreviated names: ¹​home_team.home_team_name, ²​away_team.away_team_name\n\n\nTo access the raw event-level data from a subset of matches, we need to pass the table above to the function StatsBombR::free_allevents(). StatsBomb also recommends running some basic pre-processing, all of which is nicely packaged together in the functions StatsBombR::allclean() and StatsBombR::get.opposingteam().\nAs an example, the code chunk below pulls out publicly available event data for every women’s international match.\n\nwi_events &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international) |&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam()\n\n\n1\n\nGet table of all available competitions\n\n2\n\nFind all women’s international competition\n\n3\n\nGet table of matches\n\n4\n\nGet all events\n\n5\n\nallclean() and get.opposingteam() run several pre-processing scripts that StatsBomb recommends.\n\n\n\n\n\n\n\n\n\n\nNavigating complex code\n\n\n\nIt is not easy to code complicated pipelines like the above in a single attempt. In fact, I had to build the code line-by-line. For instance, I initially ran just the first line and manually inspected the table of free competitions (using View()) to figure out which variables I needed to filter() on in the second line. It is very helpful to develop pipelines incrementally and to check intermediate results before putting everything together in one block of code.",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#sec-init-xg",
    "href": "lectures/lecture02.html#sec-init-xg",
    "title": "Lecture 2: Expected Goals",
    "section": "An initial expected goals model",
    "text": "An initial expected goals model\nSuppose we observe a dataset consisting of \\(n\\) shots. For each shot \\(i = 1, \\ldots, n,\\) let \\(Y_{i}\\) be a binary indicator of whether the shot resulted in a goal (\\(Y_{i} = 1\\)) or not (\\(Y_{i} = 0\\)). From the high-resolution tracking data, we can extract a potentially huge number of features about the shot at the moment of it was taken. Possible features include, but are certainly not limited to, the player taking the shot, the body part and side of the body used, the positions of the defenders and goal keepers, and contextual information like the score. Mathematically, we can collect all these features into a (potentially large) vector \\(\\boldsymbol{\\mathbf{X}}_{i}.\\)\nExpected goals (XG) models work by (i) positing an infinite super-population of shots represented by pairs \\((\\boldsymbol{\\mathbf{X}}, Y)\\) of feature vector \\(\\boldsymbol{\\mathbf{X}}\\) and binary outcome \\(Y\\); and (ii) assuming that the shots in our dataset constitute a random sample \\((\\boldsymbol{\\mathbf{X}}_{1}, Y_{1}), \\ldots, (\\boldsymbol{\\mathbf{X}}_{n}, Y_{n})\\) from that population.\n\n\n\n\n\n\nConditional Expectations\n\n\n\nFor each combination of features \\(\\boldsymbol{\\mathbf{x}}\\), the expect goals given \\(\\boldsymbol{\\mathbf{x}},\\) which we will denote by \\(\\textrm{XG}(\\boldsymbol{\\mathbf{X}})\\) is just the average value of \\(Y\\) among the (assumed infinite) sub-population of shots with features \\(\\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}.\\) Mathematically, XG is conditional expectation: \\[\n\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}],\n\\]\n\n\nBecause the shot outcome \\(Y\\) is binary, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is the proportion of goals scored within the sub-population of shots defined by the feature combinations \\(\\boldsymbol{\\mathbf{x}}.\\) In other words, it is the conditional probability of a goal given the shot features \\(\\boldsymbol{\\mathbf{x}}.\\) On this view, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) provides a quantitative answer to our motivating question “If we were to replay a particular shot over and over again, what fraction of the time does it result in a goal?”\nThe StatsBomb variable shot.body_part.name records the body part with which each shot was taken. Within our dataset of women’s international matches, we can see the breakdown of these body parts.\n\ntable(wi_events$shot.body_part.name)\n\n\n      Head  Left Foot      Other Right Foot \n       769       1024         23       2059 \n\n\nFor this analysis, we will focus on fitting XG models using data from shots taken with a player’s feet or head.\n\nwi_shots &lt;-\n  wi_events |&gt; # Adds opposing team information\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;  # Subsets only shot event data\n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\nLater, it will be useful for us to focus only on the shots from EURO2022, so we will also create a table euro2022_shots of all shots from that competition using similar code.\n\n\nCode\neuro2022_shots &lt;-\n  StatsBombR::FreeCompetitions() |&gt; \n  dplyr::filter(competition_id == 53 & season_id == 106) |&gt; \n  StatsBombR::FreeMatches() |&gt; \n  StatsBombR::free_allevents() |&gt; \n  StatsBombR::allclean() |&gt; \n  StatsBombR::get.opposingteam() |&gt;\n  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\nNow suppose we only include the body part in \\(\\boldsymbol{\\mathbf{X}}\\). If we had full access to the infinite super-population of women’s international shots, then we could compute \\[\\textrm{XG}(\\text{right-footed shot}) = \\mathbb{P}(\\text{goal} \\vert \\text{right-footed shot})\\] by (i) forming a sub-group containing only those right-footed shots and then (ii) calculating the proportion of goals scored within that sub-group. We could similarly compute \\(\\textrm{XG}(\\text{left-footed shot})\\) and \\(\\textrm{XG}(\\text{header})\\) by calcuating the proportion of goals scored within the sub-groups containing, resptively, only left-footed shots and only headers.\nOf course, we don’t have access to the infinite super-population of shots. However, on the assumption that our observed data constitute a sample from that super-population, we can estimate \\(\\textrm{XG}\\) by mimicking the idealized calculations described above:\n\nBreak the dataset of all observed shots in women’s international matches into several groups based on the body part\nWithin these two groups, compute the proportion of goals\n\nTo keep things simple, we dropped the 23 shots that were taken with a body part other than the feet or the head.\n\nxg_model1 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y), n = dplyr::n())\nxg_model1\n\n# A tibble: 3 × 3\n  shot.body_part.name   XG1     n\n  &lt;chr&gt;               &lt;dbl&gt; &lt;int&gt;\n1 Head                0.112   769\n2 Left Foot           0.113  1024\n3 Right Foot          0.102  2059\n\n\n\n\n\n\n\n\nGeneralizing our model results\n\n\n\nA key assumption of all XG models is that the observed data is a random sample drawn from the super-population. The only women’s internationals matches for which StatsBomb data were from the 2019 and 2023 World Cup and the 2022 EURO tournaments. These matches are arguably not highly representative of all women’s international matches, meaning that we should exercise some caution when using models fitted to these data to analyze matches from other competitions (e.g., an international friendly or a match in a domestic league).\n\n\nWe can now create a table of just Beth Mead’s shots from EURO 2022 and add a column with the XG for each shot. To do this, we first filter our table wi_shots using the player name (note, StatsBomb uses her full name!). Then, for every left-footed shot Mead attempted, we want to copy over the corresponding value from the table xg_model1, which in this case is 0.113. Similarly, we want to copy over the corresponding values for right-footed shots and headers from xg_model1 into our table for Mead’s shots. We can do this using an left join. In the code below, we actually create a temporary version of xg_model1 that drops the column recording the overall counts of the body part used for the shots in wi_shots. This way, when we perform the join, we don’t create a new column with these counts.\n\n1tmp_xg1 &lt;- xg_model1 |&gt; dplyr::select(!n)\n\nmead_shots &lt;-\n  euro2022_shots |&gt;\n  dplyr::filter(player.name == \"Bethany Mead\") |&gt;\n2  dplyr::left_join(y = tmp_xg1, by = c(\"shot.body_part.name\"))\n3rm(tmp_xg1)\n\n\n1\n\nA temporary copy of xg_model1 that doesn’t include the column n\n\n2\n\nCreate a column with estimated XG corresponding to the body part used for each shot\n\n3\n\nRemove the temporary copy\n\n\n\n\nWe can now look at the what our model says about the three goals from above. The first, against Austria in the 15th minute; the second, against Norway in the 37th minute, and the third against Sweden in the 33rd minute. When These turn out to be in rows 1, 4, and 14 of the table mead_shots\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, Y, XG1) |&gt;\n  dplyr::slice(c(1, 4, 14))\n\n# A tibble: 3 × 5\n  OpposingTeam    minute shot.body_part.name     Y   XG1\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Austria Women's     15 Right Foot              1 0.102\n2 Norway Women's      37 Left Foot               1 0.113\n3 Sweden Women's      33 Right Foot              1 0.102\n\n\nAccording to our first model, these impressiveness of these goals is pretty similar: our model put the respective chances of each shot resulting in a goal at about 10%, 11%, and 10%. But, watching the videos a bit more closely, this conclusion is not especially satisfying: Mead scored the first goal in a one-on-one situation but had to shoot through several defenders on the second and third goal. The discrepancy between our qualitative comparisons and our quantitative modeling results stems from the fact that we only conditioned on the body part and did not account for the other ways that the shots are different. In other words, our initial XG model is much too coarse to quantify the differences between the three chances that we believe are important.\n\nAccounting for additional features\nFor a more refined comparison, we need a finer XG model that conditions on more features, including ones that differ between the two shots. To this end, notice that Mead uses a different technique on the three shots: she lobs the ball into the net on the first goal, shoots the ball from the ground on the second goal, and scores the third goal off of a half-volley, striking the ball as it bounces up off the ground. The StatsBomb variable shot.technique.name records the technique of each shot type\n\ntable(wi_shots$shot.technique.name)\n\n\n     Backheel Diving Header   Half Volley           Lob        Normal \n           25            10           529            15          3015 \nOverhead Kick        Volley \n           14           244 \n\n\nBy conditioning on both body part and technique, we can begin to build a more refined XG model. The code to do this is almost identical to the code used in our first model. The only difference is that we now group by two variables shot.body_part.name and shot.technique_name. Because we are grouping by two variables, specify the argument .groups=\"drop\" argument when calling summarize; this prevents a (mostly innocuous) warning message2. We additionally append our new XG estimates to the table containing all of Mead’s shots.\n\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarize(XG2 = mean(Y), n = dplyr::n(), .groups = \"drop\")\n\ntmp_xg2 &lt;- xg_model2 |&gt; dplyr::select(-n)\nmead_shots &lt;-\n  mead_shots |&gt;\n  dplyr::inner_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\nrm(tmp_xg2)\n\nmead_shots |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, shot.technique.name, Y, XG2) |&gt;\n  dplyr::slice(c(1, 4, 14))\n\n# A tibble: 3 × 6\n  OpposingTeam    minute shot.body_part.name shot.technique.name     Y    XG2\n  &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n1 Austria Women's     15 Right Foot          Lob                     1 0.333 \n2 Norway Women's      37 Left Foot           Normal                  1 0.121 \n3 Sweden Women's      33 Right Foot          Half Volley             1 0.0801",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#more-sophisticated-xg-models",
    "href": "lectures/lecture02.html#more-sophisticated-xg-models",
    "title": "Lecture 2: Expected Goals",
    "section": "More sophisticated XG models",
    "text": "More sophisticated XG models\nAccording to our new XG model, the right-footed lob against Austria has a much higher XG than the other shots against Norway and Sweden, which seems much more reasonable than our previous model. But are we fully satisfied with this model? One could credibly argue that even though our model returns somewhat more sensible XG estimates, it is still too coarse for to meaningfully compare the shots above. After all, because it does not condition on distance, our model would return exactly the same XG for right-footed volleys taken one meter and 15 meters away from the goal. Similarly, we could try to account for the number of defenders between the shot and the goal and the position of the keeper.\nIf we had access to the infinite super-population of shots, conditioning on even more features is conceptually straightforward: we look at the corresponding sub-group of the super-population defined by a particular combination of features and compute the average \\(Y.\\) Unfortunately, with finite data, trying to “bin-and-average” using lots of features can lead to erratic estimates. For instance, here are the five largest and five smallest XG estimates based on body-part and shot technique.\n\nxg_model2 |&gt; \n  dplyr::arrange(dplyr::desc(XG2)) |&gt; \n  dplyr::filter(dplyr::row_number() %in% c(1:5, (dplyr::n()-4):dplyr::n()))\n\n# A tibble: 10 × 4\n   shot.body_part.name shot.technique.name    XG2     n\n   &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt; &lt;int&gt;\n 1 Right Foot          Lob                 0.333     12\n 2 Left Foot           Volley              0.162     74\n 3 Right Foot          Backheel            0.136     22\n 4 Left Foot           Normal              0.121    750\n 5 Head                Normal              0.113    759\n 6 Right Foot          Volley              0.0412   170\n 7 Head                Diving Header       0         10\n 8 Left Foot           Backheel            0          3\n 9 Left Foot           Lob                 0          3\n10 Left Foot           Overhead Kick       0          2\n\n\nBecause none of the 3 left-footed lobs in our dataset led to goals, our model estimates \\(\\textrm{XG}(\\text{left-footed lob})\\) as 0. Similarly, the rather large \\(\\textrm{XG}(\\text{right-footed lob})\\) of 33% is based on only 12 shots. Attempting to condition on even more variables would result in estimates based on even smaller sample sizes3.\nSo, it would appear that we’re stuck between a rock and a hard place. On the one hand, our XG model with two features is still too coarse to quantify important differences between the motivating shots. But, on the other hand, binning and averaging with even more features carries the risk of producing highly erratic, extreme, and somewhat nonsensical estimates4.\nStatistical models offer a principled approach to overcome these issues. We will explore several such models in Lecture 3. But for now, we will rely on a model developed by StatsBomb that accounts for a large number of features based on player locations (in two dimensions), the ball location (in three dimension), and other factors like the body part, shot technique, and the actions leading up to the shot (e.g., whether shot was taken off dribble or first touch). You can read more about their model here. Luckily for us, they include XG estimates for each shot in the public data, under the column shot.statsbomb_xg.\nFor instance, here are the XG estimates from StatsBomb’s model for all of Beth Mead’s goals\n\nmead_shots |&gt;\n  dplyr::filter(Y == 1) |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, \n                shot.technique.name, Y, shot.statsbomb_xg)\n\n# A tibble: 6 × 6\n  OpposingTeam     minute shot.body_part.name shot.technique.name     Y\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;\n1 Austria Women's      15 Right Foot          Lob                     1\n2 Norway Women's       33 Head                Normal                  1\n3 Norway Women's       37 Left Foot           Normal                  1\n4 Norway Women's       80 Left Foot           Volley                  1\n5 Northern Ireland     43 Left Foot           Normal                  1\n6 Sweden Women's       33 Right Foot          Half Volley             1\n# ℹ 1 more variable: shot.statsbomb_xg &lt;dbl&gt;\n\n\nRecall that XG quantifies a certain hypothetical long-term frequency of scoring a goal: if the shot was replayed under exactly the conditions quantified by the feature vector \\(\\boldsymbol{\\mathbf{x}}\\), \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is the proportion of times a goal is scored. So, according to StatsBomb’s proprietary model, if Mead repeatedly attempted the three shots from Figure 1, we should expect them to result in goals 36%, 44%, and 9% of the time. In other words, according to StatsBomb’s model, Meads goal against Sweden is much more impressive than her goals against Austria and Norway. One could argue, further, that this goal was somewhat lucky.\nWe can also look at the XG’s of the shots Mead took that didn’t result in goals.\n\nmead_shots |&gt;\n  dplyr::filter(Y == 0) |&gt;\n  dplyr::select(OpposingTeam, minute, shot.body_part.name, \n                shot.technique.name, Y, shot.statsbomb_xg)\n\n# A tibble: 9 × 6\n  OpposingTeam     minute shot.body_part.name shot.technique.name     Y\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;\n1 Norway Women's       29 Right Foot          Normal                  0\n2 Norway Women's       52 Right Foot          Volley                  0\n3 Northern Ireland      5 Head                Normal                  0\n4 Northern Ireland     15 Right Foot          Half Volley             0\n5 Northern Ireland     56 Right Foot          Normal                  0\n6 Northern Ireland     83 Right Foot          Normal                  0\n7 Sweden Women's        4 Head                Normal                  0\n8 Sweden Women's       19 Left Foot           Normal                  0\n9 Sweden Women's       46 Left Foot           Normal                  0\n# ℹ 1 more variable: shot.statsbomb_xg &lt;dbl&gt;\n\n\nWe see that most of Mead’s misses were on shots with very low XG values, indicating that none of these misses were especially unlucky. By summing the differences \\(Y_{i} - \\textrm{XG}_{i}\\) across all of her shots, we can quantify the degree to which Mead under- or over-performed the model expectations.\n\nsum(mead_shots$Y - mead_shots$shot.statsbomb_xg)\n\n[1] 2.896323\n\n\nWe conclude that during EURO 2022, Beth Mead scored 2.9 more goals than what StatsBomb’s XG model expected based on the contexts in which she attempted shots. We can repeat this calculation — summing over the difference between shot outcome \\(Y\\) and \\(\\textrm{XG}\\) — for all players in EURO 2022 to find the players that most over-performed and most under-performed the model expectations.\n\ngoe &lt;- \n  euro2022_shots |&gt;\n  dplyr::mutate(diff = Y - shot.statsbomb_xg) |&gt;\n  dplyr::group_by(player.name) |&gt;\n  dplyr::summarise(GOE = sum(diff),n = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(GOE))\ngoe\n\n# A tibble: 200 × 3\n   player.name               GOE     n\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Alexandra Popp          3.34     16\n 2 Bethany Mead            2.90     15\n 3 Alessia Russo           1.79     12\n 4 Francesca Kirby         1.79      5\n 5 Lina Magull             1.70     14\n 6 Ingrid Filippa Angeldal 1.37     10\n 7 Romée Leuchter          1.19      2\n 8 Hanna Ulrika Bennison   0.952     1\n 9 Nicole Anyomi           0.896     1\n10 Julie Blakstad          0.879     1\n# ℹ 190 more rows\n\n\nIt turns out that Alexandra Popp, the German captain, outperformed StatsBomb’s XG model expectations by an even wider margin than Beth Mead. Like Mead, Popp scored 6 goals during the tournament off a similar number of shots (16 for Popp and 15 for Mead). Interestingly, Mead won the Golden Boot because she had one more assist…",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture02.html#footnotes",
    "href": "lectures/lecture02.html#footnotes",
    "title": "Lecture 2: Expected Goals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd props to Hudl for continuing to make this data available!↩︎\nSee this StackOverflow post and the documentation for summarise for details.↩︎\nTry to convince yourself why this is the case!↩︎\nIndeed, it seems absurd to claim that, at least in women’s international soccer, players will never score off left-footed lobs! As the Statistician Dennis Lindley put it, we must “never believe in anything absolutely” and we should “leave a little probability for the moon being made of green cheese; it can be as small as 1 in a million, but have it there since otherwise an army of astronauts returning with samples of the said cheese will leave you unmoved” (Lindley 1985, sec. 6.7, available here). Lindley termed this principle “Cromwell’s Rule”, a reference to Oliver Cromwell’s quote “I beseech you, in the bowels of Christ, think it possible that you may be mistaken” from his letter to the Church of Scotland.↩︎",
    "crumbs": [
      "Lecture 2: Expected Goals"
    ]
  },
  {
    "objectID": "lectures/lecture06.html",
    "href": "lectures/lecture06.html",
    "title": "Lecture 6: Run expectancy",
    "section": "",
    "text": "The first game of the 2024 MLB Regular Season, between the Los Angeles Dodgers and the San Diego Padres, took place on March 20, 20241 During the game, Shohei Ohtani recorded two hits in his five plate appearances:\n\nIn the 3rd inning, with 2 outs and no runners on base, Ohtani singled into right field.\nIn the 8th inning, with 1 out and runners on first and second base, Ohtani singled into left center field, driving in one run.\n\nWhich of these singles was most valuable to Dodgers? And how much of that value should be credited to Ohtani specifically?\nBecause the second directly resulted in a run scoring, it is tempting to conclude that the second single was more valuable than the first. However, although the Dodgers didn’t score as a result of the first single, Ohtani reached first base, putting his team in a position to potential score more runs than if he had not reached first2. In terms of allocating credit, it seems natural to give Ohtani all the credit for whatever value was created on his first single. But on his second single, at least some of the value created is attributable to a baserunner scoring from second base. How should we divide credit between Ohtani and that baserunner?\nOver the next several lectures, we will work with pitch-level tracking data from Major League Baseball to answer these questions. In this lecture, we begin by computing run values, a metric that combines changes in the number of a team can expect to score and the number of runs actually scored in an at-bat. In Lecture 7, we will distribute the run value created in every at-bat between the batter and base runners involved in the at-bat. We will then begin Lecture 8 by distributing the negative of the run value created by th batting team between the pitcher and fielders involved in each at-bat. Finally, we will aggregate the total run value over average created by each player through their hitting, fielding, base running, and pitching and convert that aggregate into a measure of wins above replacement. Our development largely follows (Baumer, Jensen, and Matthews 2015), who created a transparent, open-source version of wins above replacement, a cornerstone of baseball analytics.",
    "crumbs": [
      "Lecture 6: Run expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#overview-allocation-of-credit-in-baseball",
    "href": "lectures/lecture06.html#overview-allocation-of-credit-in-baseball",
    "title": "Lecture 6: Run expectancy",
    "section": "",
    "text": "The first game of the 2024 MLB Regular Season, between the Los Angeles Dodgers and the San Diego Padres, took place on March 20, 20241 During the game, Shohei Ohtani recorded two hits in his five plate appearances:\n\nIn the 3rd inning, with 2 outs and no runners on base, Ohtani singled into right field.\nIn the 8th inning, with 1 out and runners on first and second base, Ohtani singled into left center field, driving in one run.\n\nWhich of these singles was most valuable to Dodgers? And how much of that value should be credited to Ohtani specifically?\nBecause the second directly resulted in a run scoring, it is tempting to conclude that the second single was more valuable than the first. However, although the Dodgers didn’t score as a result of the first single, Ohtani reached first base, putting his team in a position to potential score more runs than if he had not reached first2. In terms of allocating credit, it seems natural to give Ohtani all the credit for whatever value was created on his first single. But on his second single, at least some of the value created is attributable to a baserunner scoring from second base. How should we divide credit between Ohtani and that baserunner?\nOver the next several lectures, we will work with pitch-level tracking data from Major League Baseball to answer these questions. In this lecture, we begin by computing run values, a metric that combines changes in the number of a team can expect to score and the number of runs actually scored in an at-bat. In Lecture 7, we will distribute the run value created in every at-bat between the batter and base runners involved in the at-bat. We will then begin Lecture 8 by distributing the negative of the run value created by th batting team between the pitcher and fielders involved in each at-bat. Finally, we will aggregate the total run value over average created by each player through their hitting, fielding, base running, and pitching and convert that aggregate into a measure of wins above replacement. Our development largely follows (Baumer, Jensen, and Matthews 2015), who created a transparent, open-source version of wins above replacement, a cornerstone of baseball analytics.",
    "crumbs": [
      "Lecture 6: Run expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#obtaining-and-preparing-baseball-tracking-data",
    "href": "lectures/lecture06.html#obtaining-and-preparing-baseball-tracking-data",
    "title": "Lecture 6: Run expectancy",
    "section": "Obtaining and preparing baseball tracking data",
    "text": "Obtaining and preparing baseball tracking data\n\nHistory of tracking data\n\n\n\nAccessing StatCast Data\nMajor League Baseball hosts a public-facing web interface for accessing StatCast data. Using that interface, users can pull up data for individual players or about all pitches of a certain type. Powering this website is an application programming interface (API), which allows software applications to connect to the underlying StatCast database. It is through this API that the baseballR package acquires data. If you have not yet installed that package, you can do so using the code devtools::install_github(repo = \"BillPetti/baseballr\")\nThe baseballR package provides a function baseballr::statcast_search() that allows users query all StatCast data by date, player, or player type. One of the original baseballr authors, Bill Petti, wrote a wrapper function that uses baseballr::statcast_search() to pull down an entire season’s worth of pitch-by-pitch data; see this blog post for more the wrapper function code and this earlier post for details about its design. Since he published his original function, StatCast has added some new fields, necessitating a few changes to the original script. The code below defines a new scraper, which we will use in the course. An R script containing this code is available at this link. At a high level, the scraping function pulls data from StatCast on a week-by-week basis.\n\n\nShow the code\nannual_statcast_query &lt;- function(season) {\n  \n  data_base_column_types &lt;- \n    readr::read_csv(\"https://app.box.com/shared/static/q326nuker938n2nduy81au67s2pf9a3j.csv\")\n  \n  dates &lt;- \n    seq.Date(as.Date(paste0(season, '-03-01')),\n             as.Date(paste0(season, '-12-01')), \n             by = '4 days')\n  \n  date_grid &lt;- \n    tibble::tibble(start_date = dates, \n                   end_date = dates + 3)\n  \n  safe_savant &lt;- \n    purrr::safely(scrape_statcast_savant)\n  \n  payload &lt;- \n    purrr::map(.x = seq_along(date_grid$start_date),\n               ~{message(paste0('\\nScraping week of ', date_grid$start_date[.x], '...\\n'))\n                 payload &lt;- \n                   safe_savant(start_date = date_grid$start_date[.x], \n                               end_date = date_grid$end_date[.x], \n                               type = 'pitcher')\n                 return(payload)\n                 })\n  \n  payload_df &lt;- purrr::map(payload, 'result')\n  \n  number_rows &lt;- \n    purrr::map_df(.x = seq_along(payload_df),\n                  ~{number_rows &lt;- \n                    tibble::tibble(week = .x, \n                                   number_rows = length(payload_df[[.x]]$game_date))\n                  }) |&gt;\n    dplyr::filter(number_rows &gt; 0) |&gt;\n    dplyr::pull(week)\n  \n  payload_df_reduced &lt;- payload_df[number_rows]\n  \n  payload_df_reduced_formatted &lt;- \n    purrr::map(.x = seq_along(payload_df_reduced), \n               ~{cols_to_transform &lt;- \n                 c(\"pitcher\", \"fielder_2\", \"fielder_3\",\n                   \"fielder_4\", \"fielder_5\", \"fielder_6\", \"fielder_7\",\n                   \"fielder_8\", \"fielder_9\")\n               df &lt;- \n                 purrr::pluck(payload_df_reduced, .x) |&gt;\n                 dplyr::mutate_at(.vars = cols_to_transform, as.numeric) |&gt;\n                 dplyr::mutate_at(.vars = cols_to_transform, function(x) {ifelse(is.na(x), 999999999, x)})\n               character_columns &lt;- \n                 data_base_column_types |&gt;\n                 dplyr::filter(class == \"character\") |&gt;\n                 dplyr::pull(variable)\n               numeric_columns &lt;- \n                 data_base_column_types |&gt;\n                 dplyr::filter(class == \"numeric\") |&gt;\n                 dplyr::pull(variable)\n               integer_columns &lt;- \n                 data_base_column_types |&gt;\n                 dplyr::filter(class == \"integer\") |&gt;\n                 dplyr::pull(variable)\n               df &lt;- \n                 df |&gt;\n                 dplyr::mutate_if(names(df) %in% character_columns, as.character) |&gt;\n                 dplyr::mutate_if(names(df) %in% numeric_columns, as.numeric) |&gt;\n                 dplyr::mutate_if(names(df) %in% integer_columns, as.integer)\n               return(df)\n               })\n  \n  combined &lt;- payload_df_reduced_formatted |&gt;\n    dplyr::bind_rows()\n  \n  return(combined)\n}\n\n\nTo use this function, it is enough to run something like.\n\nraw_statcast2024 &lt;- annual_statcast_query(2024)\n\n\n\n\n\n\n\nWarning\n\n\n\nScraping a single season of StatCast data can take between 30 and 45 minutes. I highly recommend scraping the data for any season only once and saving the resulting data table in an .RData file that can be loaded into future R sessions.\n\nlibrary(tidyverse)\nraw_statcast2024 &lt;- annual_statcast_query(2024)\nsave(raw_statcast2024, file = \"raw_statcast2024.RData\")\n\nThese .RData files take between 75MB and 150MB of space. So, if you want to work with several seasons (e.g., going back all the way to 2008, the first season for which pitch tracking data is available), you will need about 2.5GB of storage space on your computer.\n\n\n\n\nWorking with StatCast Data\nThe function annual_statcast_query actually scrapes data not only from the regular season but also from the pre-season and the play-offs. The column game_type records the type of game in which each pitch was thrown. Looking at the StatCast documentation, we see that regular season pitches have game_type==\"R\".\n\ntable(raw_statcast2024$game_type, useNA = 'always')\n\n\n     D      F      L      R      S      W   &lt;NA&gt; \n  5182   2488   3540 695136  77056   1576      0 \n\n\nFor our analyses, we will work only with data from regular season games. Below, we filter to those pitches with game_type==\"R\" and remove those with non-sensical values like 4 balls or 3 strikes.\n\nstatcast2024 &lt;-\n  raw_statcast2024 |&gt; \n  dplyr::filter(game_type == \"R\") |&gt;\n  dplyr::filter(\n    strikes &gt;= 0 & strikes &lt; 3 & \n      balls &gt;= 0 & balls &lt; 4 & \n      outs_when_up &gt;= 0 & outs_when_up &lt; 3) |&gt;\n  dplyr::arrange(game_pk, at_bat_number, pitch_number)\n\nWe’re now left with 695,135 regular season pitches.\n\n\nPitch- and at-bat-level descriptions\nStatCast records a ton of information about each pitch including the type of pitch (pitch_type), the velocity of the pitch when it was released (vx0, vy0, vz0), the acceleration of the pitch at about the half-way point between the pitcher and batter (ax, ay, and az), and horizontal and vertical coordinates of the pitch as it crosses the front edge of home plate (plate_x) and (plate_y). The column type also records whether the pitch resulted ball (type=B), a strike (type=S), or whether the ball was put in play (type=X). For balls in play, StatCast also records things like the launch speed and angle (launch_speed, launch_angle), the coordinates on the field where the ball landed (hc_x) and (hc_y), and the position of the first fielder to touch the ball (hit_location). In Lecture 8, we will work with hc_x and hc_y to build a model that predicts the probability that each fielder makes an out on balls hit to a specific part of the field. As a bit of a preview, here is a plot of all the hc_x and hc_y values.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x, statcast2024$hc_y, \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, col = rgb(0,0,0, 0.1))\n\n\n\n\n\n\n\n\n\n\nAdding baserunner information\nThe columns on_1b, on_2b, and on_3b record who is on first, second, or third base at the beginning of each pitch (NA values indicate that nobody is on base). For convenience, we will add a new column to the data table that records the baserunner configuration using a string of 3 binary digits. If there is a runner on first base, the first digit will be a 1 and if there is not a runner on first base, the first digit will be a 0. Similarly, the second and third digits respectively indicate whether there are runners on second and third base. So, the string \"101\" indicates that there are runners on first and third base but not on second. To create the 3-digit binary string encoding baserunner configuration, notice that 1*(!is.na(on_1b)) will return a 1 if there is someone on first base and 0 otherwise. So by pasting together the results of 1*(!is.na(on_1b)), 1*(!is.na(on_2b)), and 1*(!is.na(on_3b)), we can form the 3-digit binary string described above. In following code, we also rename the column outs_when_up to Outs.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::mutate(\n    BaseRunner = \n      paste0(1*(!is.na(on_1b)),1*(!is.na(on_2b)),1*(!is.na(on_3b)))) |&gt;\n  dplyr::rename(Outs = outs_when_up)\n\n\n\nGetting player names\nFor every pitch, the StatCast dataset records the identities of the batter (batter), pitcher (pitcher), and the other fielders (fielders_2, …, fielders_9). However, it does not identify them by name, instead using an ID number, which is assigned by MLB Advanced Media. We can look up the corresponding player names using a database maintained by the Chadwick Register. The function baseballr::chadwick_player_lu downloads the Chadwick database and stores it as a data table in R. This database contains the names and MLB Advanced Media identifiers for players across all seasons, far more than we need for our purposes. So, in the code below, we first download the Chadwick player database and then extract only those players who appeared in the 2024 regular season. Like with the raw pitch-by-pitch data, I recommend that you download this player identity database once and save the table as an .RData object for future use.\n\n1player2024_id &lt;-\n  unique(\n    c(statcast2024$batter, statcast2024$pitcher,\n      statcast2024$on_1b, statcast2024$on_2b, statcast2024$on_3b,\n      statcast2024$fielder_2, statcast2024$fielder_3,\n      statcast2024$fielder_3, statcast2024$fielder_4,\n      statcast2024$fielder_5, statcast2024$fielder_6,\n      statcast2024$fielder_7, statcast2024$fielder_8,\n      statcast2024$fielder_9))\n\nchadwick_players &lt;- baseballr::chadwick_player_lu()\n2save(chadwick_players, file = \"chadwick_players.RData\")\n\nplayer2024_lookup &lt;-\n  chadwick_players |&gt;\n  dplyr::filter(!is.na(key_mlbam) & key_mlbam %in% player2024_id) |&gt;\n  dplyr::mutate(\n3    FullName = paste(name_first, name_last),\n4    Name = stringi::stri_trans_general(FullName, \"Latin-ASCII\"))\nsave(player2024_lookup, file = \"player2024_lookup.RData\")\n\n\n1\n\nThis vector contains the MLB Advanced Media ID for all players who appeared in 2024 as a batter, pitcher, fielder, or baserunner.\n\n2\n\nSave a local copy of the full Chadwick database.\n\n3\n\nCreates a column containing the player’s first and last name.\n\n4\n\nRemoves any accents or special characters, which we will need in Lecture 7.\n\n\n\n\n\n\nGetting Player Positions\nLater in Lecture 7, we will compare each batter’s hitting performance to the average performance of other batters who play the same position in the field. Unfortunately, the Chadwick database does not record the position of each player. Luckily, position information can be obtained using baseballr::mlb_batting_orders(), which retrieves the batting order for each MLB game. In this functions output, the column id contains the MLB Advanced Media id number for each player (i.e., key_mlbam in the data table player2024_lookup that we created above) and the column abbreviation contains the fielding position. For instance, Ohtani was listed as the Designated Hitter in that March 20, 2024 game between the Dodgers and the Padres.\n\nbaseballr::mlb_batting_orders(game_pk = 745444)\n\n── MLB Game Starting Batting Order data from MLB.com ─── baseballr 1.6.0.9002 ──\n\n\nℹ Data updated: 2025-07-30 20:58:12 CDT\n\n\n# A tibble: 18 × 8\n       id fullName         abbreviation batting_order batting_position_num team \n    &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;                &lt;chr&gt;\n 1 605141 Mookie Betts     SS           1             0                    away \n 2 660271 Shohei Ohtani    DH           2             0                    away \n 3 518692 Freddie Freeman  1B           3             0                    away \n 4 669257 Will Smith       C            4             0                    away \n 5 571970 Max Muncy        3B           5             0                    away \n 6 606192 Teoscar Hernánd… RF           6             0                    away \n 7 681546 James Outman     CF           7             0                    away \n 8 518792 Jason Heyward    RF           8             0                    away \n 9 666158 Gavin Lux        2B           9             0                    away \n10 593428 Xander Bogaerts  2B           1             0                    home \n11 665487 Fernando Tatis … RF           2             0                    home \n12 630105 Jake Cronenworth 1B           3             0                    home \n13 592518 Manny Machado    DH           4             0                    home \n14 673490 Ha-Seong Kim     SS           5             0                    home \n15 595777 Jurickson Profar LF           6             0                    home \n16 669134 Luis Campusano   C            7             0                    home \n17 642180 Tyler Wade       3B           8             0                    home \n18 701538 Jackson Merrill  CF           9             0                    home \n# ℹ 2 more variables: teamName &lt;chr&gt;, teamID &lt;int&gt;\n\n\nBecause baseballr::mlb_batting_orders() can be run using only one game identifier (i.e., game_pk) at a time, we need to loop over all of the unique game_pk values in statcast2024 to get all batting orders. In the code block below, we first create a wrapper function, which we call get_lineup around baseballr::mlb_batting_orders() that only retains the columns id and abbreviation and renames those columns.\n\nget_lineup &lt;- function(game_pk){\n  lineup &lt;- baseballr::mlb_batting_orders(game_pk = game_pk)\n  lineup &lt;-\n    lineup |&gt;\n    dplyr::mutate(game_pk = game_pk) |&gt;\n    dplyr::rename(key_mlbam = id, position = abbreviation) |&gt;\n    dplyr::select(game_pk, key_mlbam, position)\n  return(lineup)\n}\n\nNow, to get the batting orders for all regular season games from 2024, we could try writing a for loop that iterates over all unique game_pk values and writes the table to a list.\n\nall_lineups &lt;- list()\nunik_game_pk &lt;- unique(statcast2024$game_pk)\nfor(i in 1:length(unik_game_pk)){\n  all_lineups[[i]] &lt;- get_lineup(game_pk = unik_game_pk[i])\n}\n\nIf get_lineup() throws an error (e.g., because the batting orders for a particular game are not available), then the whole loop gets terminated. To avoid having to manually remove problematic games and re-start the loop, we will use purrr:possibly() to create a version of get_lineup() that returns NULL when it hits an error. We will also use purrr:map instead of writing an explict for loop.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code takes about an hour to run.\n\n\n\nposs_get_lineup &lt;- purrr::possibly(.f = get_lineup, otherwise = NULL) \nunik_game_pk &lt;- unique(statcast2024$game_pk)\n\n1block_starts &lt;- seq(1, length(unik_game_pk), by = 500)\nblock_ends &lt;- c(block_starts[-1], length(unik_game_pk))\n\nall_lineups &lt;- list()\nfor(b in 1:5){\n  tmp &lt;-\n    purrr::map(.x = unik_game_pk[block_starts[b]:block_ends[b]], \n               .f = poss_get_lineup, \n               .progress = TRUE)\n  all_lineups &lt;- c(all_lineups, tmp)\n}\n\nlineups2024 &lt;- \n2  dplyr::bind_rows(all_lineups) |&gt;\n  unique()\n3save(lineups2024, file = \"lineups2024.RData\")\n\n\n1\n\nI was not able to loop over the whole set of unique game_pk values at once. I found it useful to break the loop into blocks of about 500 games each.\n\n2\n\nEach element of all_lineup is a data table. This allows us to stack them on top of one another.\n\n3\n\nIt’s useful to save the table of batting orders just in case we need it again in the future\n\n\n\n\nThe table lineups2024 contains the starters and their position for all the games in our dataset. The following code determines the most commonly listed position for each player.\n\npositions2024 &lt;-\n  lineups2024 |&gt;\n  dplyr::group_by(key_mlbam, position) |&gt;\n1  dplyr::summarise(n = dplyr::n()) |&gt;\n2  dplyr::slice_max(order_by = n, with_ties = FALSE) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::select(key_mlbam, position)\nsave(positions2024, file = \"positions2024.RData\")\n\n\n1\n\nCounts the number of occurrences of each player-position combination\n\n2\n\nAfter we call dplyr::summarise(), the resulting data table is still grouped by batter. So, when we call dplyr::slice_max() it looks at all the rows corresponding to each player and extracts the one with highest count. This is how we determine the most commonly listed position.",
    "crumbs": [
      "Lecture 6: Run expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#expected-runs",
    "href": "lectures/lecture06.html#expected-runs",
    "title": "Lecture 6: Run expectancy",
    "section": "Expected Runs",
    "text": "Expected Runs\n\n\n\n\n\n\nDefinition\n\n\n\nFor each combination of the number of outs (\\(\\textrm{o} \\in \\{0,1,2\\}\\)) and base runner configurations (\\(\\textrm{br} \\in \\{\"000\", \"100\", \"010\", \"001\", \"110\", \"101\", \"011\", \"111\"\\}\\)), let \\(\\rho(\\textrm{o}, \\textrm{br})\\) be the average number of runs that a team scores in the remainder of the half-inning following a pitch thrown with \\(\\textrm{o}\\) outs and base runner configuration \\(\\textrm{br}.\\)\n\n\nComputing \\(\\rho(\\textrm{o}, \\textrm{br})\\) is conceptually straightforward: we need to divide our observed at-bats into 24 bins, one for each combination of \\((\\textrm{o}, \\textrm{br})\\) and then compute the average value of \\(R\\) within each bin. This is exactly the same “binning-and-averaging” procedure we used to fit our initial XG models in Lecture 2. We will do this using pitches taken in the first 8 innings of played in the 2024 regular season. We focus only on the first 8 innings because the 9th and extra innings are fundamentally different than the others. Specifically, the bottom half of the 9th (or later) innings is only played if the game is tied or the home team is trailing after the top of the 9th inning concludes. In those half-innings, if played, the game stops as soon as a winning run is scored. For instance, say that the home team is trailing by 1 runs in the bottom of the 9th and that there are runners on first and second base. If the batter hits a home run, the at-bat is recorded as resulting in only two runs (the tying run from second and the winning run from first). But the exact same scenario would result in 3 runs in an earlier inning.\n\nComputing runs scored in the half-inning\nSuppose that in a given at-bat \\(a\\) that there are \\(n_{a}\\) pitches. Within at-bat \\(a,\\) for each \\(i = 1, \\ldots, n_{a},\\) let \\(R_{i,a}\\) be the number of runs scored in the half-inning after that pitch (including any runs scored as a result of pitch \\(i\\)). So \\(R_{1,a}\\) is the number of runs scored in the half-inning after the first pitch, \\(R_{2,a}\\) is the number of runs scored subsequent to the second pitch, etc. Our first step towards building the necessary at-bat-level data set will be to append a column of \\(R_{i,a}\\) values to each season’s StatCast data.\nWe start by illustrating the computation using a single half-inning from the Dodgers-Padres game introduced earlier. The code below pulls out all pitches thrown in the top of the 8th inning of the game. During this inning, the Dodgers scored 4 runs.\n\ndodgers_inning &lt;-\n  statcast2024 |&gt;\n  dplyr::filter(game_pk == 745444 & inning == 8 & inning_topbot == \"Top\") |&gt;\n  dplyr::select(\n    at_bat_number, pitch_number, Outs, BaseRunner,\n    bat_score, post_bat_score, events, description, des,\n    type, on_1b, on_2b, on_3b, hc_x, hc_y, hit_location) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number)\n\nThe column bat_score records the batting team’s score before each pitch is thrown. The column post_bat_score records the batting team’s score after the the outcome of the pitch. For most of the 25 pitches, we find that bat_score is equal to post_bat_score; this is because only a few pitches result in scoring events.\n\nrbind(bat_score = dodgers_inning$bat_score, post_bat_score = dodgers_inning$post_bat_score)\n\n               [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nbat_score         1    1    1    1    1    1    1    1    1     1     1     1\npost_bat_score    1    1    1    1    1    1    1    1    1     1     1     1\n               [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\nbat_score          1     1     2     3     3     3     4     5     5     5\npost_bat_score     1     2     3     3     3     4     5     5     5     5\n               [,23] [,24] [,25]\nbat_score          5     5     5\npost_bat_score     5     5     5\n\n\nCross-referencing the table above with the play-by-play data, we see that the Dodgers score their second run after the 14th pitch of the half-inning (on a Enrique Hernández sacrifice fly); their third run on the very next pitch (Gavin Lux grounding into a fielder’s choice); and their fourth and fifth runs on consecutive pitches (on singles by Mookie Betts and Shohei Ohtani).\n\n\n\nDodgers-Padres Play-by-Play\n\n\nWe can verify this by looking at the variable des, which stores a narrative description about what happened during the at-bat.\n\ndodgers_inning$des[c(14,15, 18, 19)]\n\n[1] \"Enrique Hernández out on a sacrifice fly to left fielder José Azocar. Max Muncy scores.\"                                                                                             \n[2] \"Gavin Lux reaches on a fielder's choice, fielded by first baseman Jake Cronenworth. Teoscar Hernández scores. James Outman to 2nd. Fielding error by first baseman Jake Cronenworth.\"\n[3] \"Mookie Betts singles on a ground ball to left fielder José Azocar. James Outman scores. Gavin Lux to 2nd.\"                                                                           \n[4] \"Shohei Ohtani singles on a line drive to left fielder José Azocar. Gavin Lux scores. Mookie Betts to 2nd.\"                                                                           \n\n\nNotice that, because we’ve sorted the pitches in ascending order by at-bat and pitch number, the very last row of the table corresponds to the last pitch of the inning. Accordingly, the last value in the post_bat_score column is the batting team’s score at the end of the inning. Thus, to compute \\(R_{i,a}\\) for all pitches in this inning, it is enough to subtract the bat_score in each row from the last post_bat_score in the table. To access this last value, we use the function last().\n\ndplyr::last(dodgers_inning$post_bat_score) - dodgers_inning$bat_score\n\n [1] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 2 2 2 1 0 0 0 0 0 0\n\n\nWe now append a column with these values to our data table dodgers_inning\n\ndodgers_inning &lt;-\n  dodgers_inning |&gt;\n  dplyr::mutate(RunsRemaining = dplyr::last(post_bat_score) - bat_score)\n\n\n\nPutting it all together\nWe’re now ready to extend these calculation to every half-inning of every game in the season. To do this, we will take advantage of the group_by() command in dplyr to apply the same calculation to small groups defined by game and half-inning.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n2  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n3  dplyr::mutate(RunsRemaining = dplyr::last(post_bat_score) - bat_score) |&gt;\n  dplyr::ungroup()\n\n\n1\n\nDivide the data based on the combination of game and half-inning\n\n2\n\nArrange pitches in the appropriate temporal order\n\n3\n\nAdd column for how many runs were scored after each pitch\n\n\n\n\nNow we have the number of runs scored in the half-inning after each pitch. But to compute run expectancy, we need this quantity at the at-bat level and not at the pitch-level. Using our notation from before, note that \\(R_{1,a}\\) is the number of runs scored after the first pitch of at-bat \\(a.\\) So, to compute run expectancy, it is enough to pull out the first pitch from each at-bat (i.e., those pitches with pitch_number == 1) using the filter() function.\n\nexpected_runs &lt;-\n  statcast2024 |&gt;\n1  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(Outs, BaseRunner, RunsRemaining) |&gt;\n2  dplyr::group_by(Outs, BaseRunner) |&gt;\n3  dplyr::summarize(rho = mean(RunsRemaining), .groups = \"drop\")\n\n\n1\n\nGet the first pitch in every at-bat\n\n2\n\nSub-divide the data based on the 24 combinations of out and baserunner\n\n3\n\nCompute the mean of RunRemaining for each game state combination\n\n\n\n\nThe table expected_runs contains one row for every combination of outs and base-runner configuration. Traditionally, expected runs is reported using an \\(8\\times 3\\) matrix, with rows corresponding to base-runner configurations and columns corresponding to outs. We can re-format expected_runs to this matrix format using the pivot_wider() function\n\nexpected_runs |&gt; \n  tidyr::pivot_wider(\n    names_from = Outs,\n    values_from = rho,\n    names_prefix=\"Outs: \")\n\n# A tibble: 8 × 4\n  BaseRunner `Outs: 0` `Outs: 1` `Outs: 2`\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 000            0.488     0.262    0.0980\n2 001            1.43      0.972    0.352 \n3 010            1.07      0.672    0.347 \n4 011            2.03      1.44     0.612 \n5 100            0.897     0.529    0.228 \n6 101            1.90      1.22     0.502 \n7 110            1.49      0.926    0.449 \n8 111            2.31      1.58     0.815 \n\n\nIn the March 20, 2024 game against the Padres, Ohtani recorded his first hit in the 3rd inning on a pitch with 2 outs and no runners on base. Based on the game state at the start of the at-bat (i.e., Outs=2 and BaseRunner='000'), his team can expect to score 0.1 runs in the remainder of the half-inning. As a result of Ohtani’s single, he changed the game state to Outs = 2 and BaseRunner=100, a state from which his team can expect to score 0.22 runs, on average. So, although his at-bat did not directly result in a run scoring, Ohtani increased his team’s run expectancy by about 0.12 runs.",
    "crumbs": [
      "Lecture 6: Run expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#run-value",
    "href": "lectures/lecture06.html#run-value",
    "title": "Lecture 6: Run expectancy",
    "section": "Run value",
    "text": "Run value\nWhile expected runs is a really useful metric, for the purposes of allocating credit, we need to account not only for what we expect to happen but also what actually happened as a result of each at-bat.\n\n\n\n\n\n\nDefinition: Run Value\n\n\n\nThe run value of an at-bat is defined as the the number of runs scored in the at-bat plus the difference in expected runs from the starting to ending state. That is, denoting the number of runs scored in the at-bat as \\(\\textrm{RunsScored}\\) and the starting and ending states as \\((\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\) and \\((\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}),\\) then \\[\n\\textrm{RunValue} = \\textrm{RunsScored} + \\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}) - \\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\n\\]\n\n\nIn a sense, run value rewards batters and base runners for two things, actually scoring runs and putting their team in positions from which they could potentially score more runs.\nTo do compute the run value of each at-bat in the 2024 season, we must compute\n\nThe number of runs scored during each at-bat\nThe game state (i.e., the number of outs and the base-runner configuration) at the start and end of each at-bat\nThe change in expected runs during the at-bat (i.e., \\(\\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}) - \\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\)).\n\nWe will first develop the necessary code using the data from Dodger’s 8th inning from their game against the Padres. Then, we will deploy that code to the whole statcast2024 table by grouping by game_pk and at_bat_number.\n\nCalculating RunsScored\nStatCast numbers every at-bat within the game and every pitch within each at-bat. To compute the number of runs scored within each at-bat, we will:\n\nSort the pitches by at-bat number and then by pitch number in ascending order\nTake the different between the last value of post_bat_score and first value of bat_score within each at-bat.\n\nLet’s try to verify this by looking at pitches from the third, fourth, and fifth at-bats of Dodgers’ 8th inning against the Padres3\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 61:63) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, bat_score, description, post_bat_score)\n\n# A tibble: 7 × 5\n  at_bat_number pitch_number bat_score description   post_bat_score\n          &lt;int&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n1            61            1         1 ball                       1\n2            61            2         1 ball                       1\n3            61            3         1 ball                       1\n4            61            4         1 ball                       1\n5            62            1         1 foul                       1\n6            62            2         1 hit_into_play              2\n7            63            1         2 hit_into_play              3\n\n\nBased on the description column, we see that the first pitch of at-bat 62 was a foul ball and the second pitch was hit into play. When we look at the corresponding row (row 6) of the table, we see that that Dodgers’ pre-pitch score was 1 (bat_score = 1) and that they scored 1 run as a result of the hit (post_bat_score = 2). Reassuringly, the difference between the value of post_bat_score in row 6 (the last row for at-bat 62) and the value of bat_score in row 5 (the first row for at-bat 62) is 1. We can similarly verify our procedure works in at-bat 61: the fourth value of post_bat_score and the first value of bat_score are equal and the Dodgers did not score in this at-bat.\nWe can apply our procedure to the entirety of the Dodgers’ half-inning\n\ndodgers_inning &lt;-\n  dodgers_inning |&gt;\n  dplyr::group_by(at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(RunsScored = dplyr::last(post_bat_score) - dplyr::first(bat_score)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::arrange(at_bat_number, pitch_number)\ndodgers_inning |&gt; dplyr::select(at_bat_number, pitch_number, bat_score, description, post_bat_score)\n\n# A tibble: 25 × 5\n   at_bat_number pitch_number bat_score description   post_bat_score\n           &lt;int&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1            59            1         1 called_strike              1\n 2            59            2         1 ball                       1\n 3            59            3         1 ball                       1\n 4            59            4         1 foul                       1\n 5            59            6         1 blocked_ball               1\n 6            60            1         1 ball                       1\n 7            60            2         1 foul_tip                   1\n 8            60            3         1 hit_into_play              1\n 9            61            1         1 ball                       1\n10            61            2         1 ball                       1\n# ℹ 15 more rows\n\n\nWe can now apply this formula to all pitches in statcast2024 by grouping by game_pk and at_bat_number. We will also save a copy of the data table statcast2024 so that we can load it into future R sessions.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, at_bat_number) |&gt;\n2  dplyr::arrange(pitch_number) |&gt;\n3  dplyr::mutate(RunsScored = dplyr::last(post_bat_score) - dplyr::first(bat_score)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number)\n\nsave(statcast2024, file = \"statcast2024.RData\")\n\n\n1\n\nSub-divide the data based on individual at-bats\n\n2\n\nPlace pitches within each at-bat in sequential order\n\n3\n\nAdd column recording the number of runs scored in the at-bat ### Computing the starting and ending states\n\n\n\n\nExcept for the very last pitch in a team’s innings, the ending state of that pitch is, by definition, the starting state of the next pitch. In order to compute \\(\\rho(\\textrm{o}_{\\text{end}}, \\textrm{br}_{\\text{end}}),\\) and \\(\\rho(\\textrm{o}_{\\text{start}}, \\textrm{br}_{\\text{start}})\\) for each at-bat, we will first create a columns in statcast2024 that encode the game state at the beginning and end of the at-bat.\nTo build up our code, let’s continue with our running example of the Dodgers’ 8th inning, focusing on the at the second through fourth at-bats of the inning.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner)\n\n# A tibble: 9 × 4\n  at_bat_number pitch_number  Outs BaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;     \n1            60            1     0 100       \n2            60            2     0 100       \n3            60            3     0 100       \n4            61            1     0 110       \n5            61            2     0 110       \n6            61            3     0 110       \n7            61            4     0 110       \n8            62            1     0 111       \n9            62            2     0 111       \n\n\nWe start by creating new columns recording the Outs and BaseRunner values of the next pitch using dplyr::lead() function. 4.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner) |&gt;\n  dplyr::mutate(\n    next_Outs = dplyr::lead(Outs), next_BaseRunner = dplyr::lead(BaseRunner))\n\n# A tibble: 9 × 6\n  at_bat_number pitch_number  Outs BaseRunner next_Outs next_BaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;          \n1            60            1     0 100                0 100            \n2            60            2     0 100                0 100            \n3            60            3     0 100                0 110            \n4            61            1     0 110                0 110            \n5            61            2     0 110                0 110            \n6            61            3     0 110                0 110            \n7            61            4     0 110                0 111            \n8            62            1     0 111                0 111            \n9            62            2     0 111               NA &lt;NA&gt;           \n\n\nNow, within each at-bat, we can look at the last values of next_Outs and next_BaseRunner to figure out the ending state of the at-bat.\n\ndodgers_inning |&gt;\n  dplyr::filter(at_bat_number %in% 60:62) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner) |&gt;\n  dplyr::mutate(\n    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner)) |&gt;\n  dplyr::group_by(at_bat_number) |&gt;\n  dplyr::mutate(\n    endOuts = dplyr::last(next_Outs),\n    endBaseRunner = dplyr::last(next_BaseRunner)) |&gt;\n  dplyr::select(at_bat_number, pitch_number, Outs, BaseRunner, endOuts, endBaseRunner) |&gt;\n  dplyr::ungroup()\n\n# A tibble: 9 × 6\n  at_bat_number pitch_number  Outs BaseRunner endOuts endBaseRunner\n          &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        \n1            60            1     0 100              0 110          \n2            60            2     0 100              0 110          \n3            60            3     0 100              0 110          \n4            61            1     0 110              0 111          \n5            61            2     0 110              0 111          \n6            61            3     0 110              0 111          \n7            61            4     0 110              0 111          \n8            62            1     0 111             NA &lt;NA&gt;         \n9            62            2     0 111             NA &lt;NA&gt;         \n\n\n\nWe can repeat this code for all at-bats.\n\nrunValue2024 &lt;-\n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n2    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n3    end_Outs = dplyr::last(next_Outs),\n    end_BaseRunner = dplyr::last(next_BaseRunner)) |&gt; \n  dplyr::ungroup() |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n4  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(\n    game_pk, at_bat_number, \n    inning, inning_topbot, \n    Outs, BaseRunner, \n    RunsScored, RunsRemaining, \n    end_Outs, end_BaseRunner)\n\n\n1\n\nSub-divide into half-innings\n\n2\n\nGet values of Outs and BaseRunner for next pitch in the inning\n\n3\n\nGoes to last pitch of each at-bat and gets the next value of Outs and BaseRunner, which are the starting values of these variables in the next at-bat.\n\n4\n\nGets the first pitch from each at-bat\n\n\n\n\n\n\nComputing run-values\nNow that we have a table runValue containing information about the starting and ending states of each at-bat, we are ready to compute run-values. In particular, we can use a join (just like we did with XG in Lecture 2) to add in the values of the starting and ending expected runs.\nBefore doing that, though, we need to deal with the NA’s introduced by lead(). Looking at the at-bats from the Dodger’s 8th inning from our running example, we see that those NA’s correspond to the very last at-bat of the half-inning.\nBefore doing that, though, let’s take a quick look at the rows in the table corresponding to the Dodgers’ 8th inning from our running example\n\nrunValue2024 |&gt;\n  dplyr::filter(game_pk == 745444 & inning == 8 & inning_topbot == \"Top\") |&gt;\n  dplyr::select(at_bat_number, Outs, BaseRunner, end_Outs, end_BaseRunner)\n\n# A tibble: 8 × 5\n  at_bat_number  Outs BaseRunner end_Outs end_BaseRunner\n          &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;         \n1            59     0 000               0 100           \n2            60     0 100               0 110           \n3            61     0 110               0 111           \n4            62     0 111               1 110           \n5            63     1 110               1 110           \n6            64     1 110               1 110           \n7            65     1 110               1 110           \n8            66     1 110              NA &lt;NA&gt;          \n\n\nBecause the “end of the inning” state is not one of the 24 combinations of outs and baserunner configurations in the expected_runs table, we’re going to row to that table with Outs=3, BaseRunners='000', and rho = 0 (since the team cannot score any more runs in the inning once it is over!).\n\nexpected_runs &lt;-\n  expected_runs |&gt;\n  tibble::add_row(Outs=3, BaseRunner=\"000\", rho = 0)\n\nrunValue2024 &lt;-\n  runValue2024 |&gt;\n  dplyr::mutate(\n    end_Outs = ifelse(is.na(end_Outs), 3, end_Outs),\n    end_BaseRunner = ifelse(is.na(end_BaseRunner), '000', end_BaseRunner))\n\nWe’re now ready to use a join to append the starting and ending expected runs.\n\nend_expected_runs &lt;- \n  expected_runs |&gt;\n  dplyr::rename(\n    end_Outs = Outs,\n    end_BaseRunner = BaseRunner,\n    end_rho = rho)\n\nrunValue2024 &lt;-\n  runValue2024 |&gt;\n  dplyr::left_join(y = expected_runs, by = c(\"Outs\", \"BaseRunner\")) |&gt;\n  dplyr::left_join(y = end_expected_runs, by = c(\"end_Outs\", \"end_BaseRunner\")) |&gt;\n  dplyr::mutate(RunValue = RunsScored + end_rho - rho) |&gt;\n  dplyr::select(game_pk, at_bat_number, RunValue)\n\nrm(end_expected_runs)\nsave(runValue2024, file = \"runValue2024.RData\")",
    "crumbs": [
      "Lecture 6: Run expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#assessing-batter-production-with-run-value",
    "href": "lectures/lecture06.html#assessing-batter-production-with-run-value",
    "title": "Lecture 6: Run expectancy",
    "section": "Assessing batter production with run value",
    "text": "Assessing batter production with run value\nEach row in runValue2024 corresponds to a single at-bat in the 2024 regular season and is uniquely determined by the game identifier (game_pk) and at-bat number (at_bat_number). We’re now in a position to quantify how run value Ohtani created during the Dodgers-Padres game introduced above?\nTo do this, it is helpful to first look up Ohtani’s MLB Advanced Media ID number using our table player2024_lookup. Then, we can extract the rows of statcast2024 corresponding to the first pitch in each at-bat from the Dodgers-Padres game statcast2024. Then we can filter this subset to only those at-bats where Ohtani was the batter. Finally, we can join the associated run values.\n\nload(\"player2024_lookup.RData\")\n\n\nohtani_id &lt;- \n  player2024_lookup$key_mlbam[which(player2024_lookup$FullName == \"Shohei Ohtani\")]\n\nohtani_ab &lt;-\n  statcast2024 |&gt;\n  dplyr::filter(game_pk == 745444) |&gt;\n  dplyr::filter(pitch_number == 1 & batter == ohtani_id) |&gt;\n  dplyr::select(game_pk, at_bat_number, inning, des) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\")) |&gt;\n  dplyr::select(at_bat_number, RunValue, des)\nohtani_ab\n\n# A tibble: 5 × 3\n  at_bat_number RunValue des                                                    \n          &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                                                  \n1             2   -0.367 Shohei Ohtani grounds into a force out, shortstop Ha-S…\n2            18    0.130 Shohei Ohtani singles on a sharp line drive to right f…\n3            37   -0.367 Shohei Ohtani grounds into a force out, third baseman …\n4            52   -0.164 Shohei Ohtani grounds out softly, pitcher Wandy Peralt…\n5            65    1     Shohei Ohtani singles on a line drive to left fielder …\n\n\nOver the course of the entire game, Ohtani created a total of 0.2310438 in run value. The negative run value created in his first, third, and fourth at-bats (i.e., when he got out) is offset by the positive run value he created by singling in the top of the 3rd inning and driving in a run in the top of the 8th inning. Note the run value of his last at-at is exactly 1 because a single run scored but the base-runner configuration did not change as a result of the at-bat.\nBy repeating this calculation over the course of the entire 2024 regular season, we can identify those batters who created the most run value for their teams.\n\ntmp_lookup &lt;-\n  player2024_lookup |&gt;\n  dplyr::select(key_mlbam, Name) |&gt;\n  dplyr::rename(batter = key_mlbam)\n\nre24 &lt;-\n  statcast2024 |&gt; \n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::select(game_pk, at_bat_number, batter) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\")) |&gt;\n  dplyr::group_by(batter) |&gt;\n  dplyr::summarise(RE24 = sum(RunValue),N = dplyr::n()) |&gt;\n  dplyr::inner_join(y = tmp_lookup, by = \"batter\") |&gt;\n  dplyr::select(Name, RE24, N) |&gt;\n  dplyr::arrange(desc(RE24))\nre24\n\n# A tibble: 649 × 3\n   Name               RE24     N\n   &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n 1 Aaron Judge        89.0   675\n 2 Juan Soto          74.4   693\n 3 Shohei Ohtani      73.1   708\n 4 Bobby Witt         65.9   694\n 5 Brent Rooker       47.9   599\n 6 Vladimir Guerrero  45.0   671\n 7 Ketel Marte        41.2   562\n 8 Kyle Schwarber     40.9   672\n 9 Joc Pederson       39.2   433\n10 Jose Ramirez       39.1   657\n# ℹ 639 more rows\n\n\nWhen we compare our top-10 to FanGraph’s leaderboard for RE24, we see a lot of overlap. But there are some differences, especially with regards to the number of plate appearances and actual RE24 values. For the latter, FanGraph likely used a different expected run matrix. And the StatCast data is not complete; for instance, it is missing 3 games in which Judge played.",
    "crumbs": [
      "Lecture 6: Run expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture06.html#footnotes",
    "href": "lectures/lecture06.html#footnotes",
    "title": "Lecture 6: Run expectancy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis was also the first MLB game ever played in South Korea!↩︎\nIn fact, Ohtani went on to steal second base during the next at-bat, further increasing his team’s chancse of↩︎\nStatCast assigns each at-bat in a game a unique number. The third, fourth, and fifth at-bats during the Dodger’s 8th inning were the 61st, 62nd, and 63rd at-bats of the game.↩︎\nThe next value of a variable is undefined in the last row of a column, resulting in some NA’s. We’ll deal with those later on.↩︎",
    "crumbs": [
      "Lecture 6: Run expectancy"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#the-basic-bradley-terry-model",
    "href": "lectures/lecture13.html#the-basic-bradley-terry-model",
    "title": "Bradley-Terry Models",
    "section": "The basic Bradley-Terry model",
    "text": "The basic Bradley-Terry model\n\nEstimation\nConveniently, there is a nice R package **BradleyTerry2* that allows",
    "crumbs": [
      "Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#example",
    "href": "lectures/lecture13.html#example",
    "title": "Bradley-Terry Models",
    "section": "Example:",
    "text": "Example:",
    "crumbs": [
      "Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#from-estimation-to-simulation",
    "href": "lectures/lecture13.html#from-estimation-to-simulation",
    "title": "Bradley-Terry Models",
    "section": "From estimation to simulation",
    "text": "From estimation to simulation\nWhile estimating these model parameters is nice, how might be use them prospectively?\nOne idea is to simulate.\nGiven estimates \\(\\hat{\\lambda},\\) we can look at a series of matches and try to forecast.",
    "crumbs": [
      "Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture13.html#going-beyond-the-basic-model",
    "href": "lectures/lecture13.html#going-beyond-the-basic-model",
    "title": "Bradley-Terry Models",
    "section": "Going beyond the basic model",
    "text": "Going beyond the basic model\nThere are many many extensions of the model. Here are just a handful.\nThe basic BT model does not account for ties, making it difficult to apply, e.g., to professional soccer (football outside America).\nOne could envision fitting a Bradley-Terry model to each\n\nMulti-level BT models\nIt could very well be the case that \\(\\lambda_{t}\\) depends on factors related to the team \\(t\\) (e.g., its payroll, number of all-star players, strenght of schedule to date, etc). The BradleyTerry2 package allows us to fit multi-level models in which \\(\\lambda_{t}\\) is allowed to depend on covariates in a linear fashion.",
    "crumbs": [
      "Bradley-Terry Models"
    ]
  },
  {
    "objectID": "lectures/lecture11.html",
    "href": "lectures/lecture11.html",
    "title": "Lecture 11: Pitch Framing",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#overview",
    "href": "lectures/lecture11.html#overview",
    "title": "Lecture 11: Pitch Framing",
    "section": "Overview",
    "text": "Overview",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#historical-strike-probabilities",
    "href": "lectures/lecture11.html#historical-strike-probabilities",
    "title": "Lecture 11: Pitch Framing",
    "section": "Historical Strike Probabilities",
    "text": "Historical Strike Probabilities",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#an-initial-multi-level-model",
    "href": "lectures/lecture11.html#an-initial-multi-level-model",
    "title": "Lecture 11: Pitch Framing",
    "section": "An initial multi-level model",
    "text": "An initial multi-level model",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#a-more-involved-multi-level-model",
    "href": "lectures/lecture11.html#a-more-involved-multi-level-model",
    "title": "Lecture 11: Pitch Framing",
    "section": "A more involved multi-level model",
    "text": "A more involved multi-level model\nCatchers are not the only people who might influence the called strike probability!",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture11.html#exercises",
    "href": "lectures/lecture11.html#exercises",
    "title": "Lecture 11: Pitch Framing",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Lecture 11: Pitch Framing"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#stint-level-nba-data",
    "href": "lectures/lecture04.html#stint-level-nba-data",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Stint-level NBA data",
    "text": "Stint-level NBA data\n\nWhat is a “stint”\nUse hoopR to pull\nRefer to code elsewhere\n\n\nBuilding a design matrix",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#plusminus",
    "href": "lectures/lecture04.html#plusminus",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Plus/Minus",
    "text": "Plus/Minus",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#linear-regression",
    "href": "lectures/lecture04.html#linear-regression",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Linear Regression",
    "text": "Linear Regression\nAdjusted plus/minus attempts to account for the other 9 players.\nFormally, it assumes that there is some number \\(\\alpha_{p}\\) associated with each player \\(p\\) and an average home-court advantage \\(\\alpha_{0}\\) such that \\[\nY_{i} = \\alpha_{0} + \\alpha_{h_{i1}} + \\cdots + \\alpha_{h_{i5}} - \\alpha_{a_{i1}} - \\cdots - \\alpha_{a_{i5}} + \\epsilon_{i},\n\\] where the error term is assumed to have mean 0.\nHow can we estimate these using data?\nOne approach is least squares.\nRemember that big design matrix \\(X\\) from earlier? It turns out we can re-formulate this problem as \\[\n\\lVert Y - \\tilde{X} \\boldsymbol{\\alpha} \\rVert_{2}^{2}\n\\]\nA little bit of linear algebra shows that the least squares solution is obtained by\nBut before we can do this, there’s a slight problem: the matrix is not invertible. This is because the rows sum to 1!\n\nDropping players and restricting the scope\nHistorically, analysts have gotten around this issue by simply dropping the columns associated with players who played fewer than some fixed number of minutes. Traditionally this is 250 minutes.\nIn R, we can estimate the coefficients using lm",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture04.html#predictive-quality",
    "href": "lectures/lecture04.html#predictive-quality",
    "title": "Lecture 4: Adjusted Plus/Minus",
    "section": "Predictive quality",
    "text": "Predictive quality\nThe loop below trains an adjusted plus/minus model\nWe compare to a simple model that asserts all \\(\\alpha_{p} = 0\\) and that each \\(Y\\); In the training data; we then compare the average out-of-sample squared error.\nIf you have taken STAT 333, you’ll recognize this as a kind of generalization of \\(R^{2}.\\)\nAdjusted plus/minus really doesn’t seem to work that well!",
    "crumbs": [
      "Lecture 4: Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture08.html",
    "href": "lectures/lecture08.html",
    "title": "Lecture 8: Defensive creidt & replacement level",
    "section": "",
    "text": "Last lecture, we distributed the run value created in each at-bat between the batter and base runners. Aggregating over all at-bats, we computed each player’s \\(\\textrm{RAA}^{\\textrm{br}},\\) which quantifies how much more run value a player created through his base running than would otherwise be expected based on the starting game states and ending events of the at-bats in which he was involved. We also computed \\(\\textrm{RAA}^{\\textrm{b}},\\) which quantifies how much more run value a player created through his hitting than would be expected based on his position.\nAccording to the conservation of runs framework introduced by Baumer, Jensen, and Matthews (2015), whenever the batting team creates \\(\\delta\\) units of run value in at-bat, the fielding team (necessarily) gives up \\(\\delta\\) units of run value. Equivalently, the fielding team creates \\(-\\delta\\) units of run value. In this lecture, we discuss how to divide this \\(-\\delta\\) run value between the pitchers and fielders.\nFor at-bats \\(i\\) that end without the ball being put in play (i.e., a strikeout, walk, or a homerun), we will assign the entirety of the \\(-\\delta_{i}\\) run value to the pitcher. But for at-bats that result with balls put in play, we will divide \\(-\\delta_{i}\\) into two parts \\(\\delta^{(p)}_{i} = -\\delta \\times \\hat{p}_{i}\\) and \\(\\delta^{(f)} = -\\delta_{i} \\times (1 - \\hat{p}_{i})\\) where \\(\\hat{p}_{i}\\) is an estimate of the probability that play results in an out given the batted ball’s location. We will first allocate\n\n\nWe begin by loading several of the data tables created in Lecture 6 and Lecture 7 including statcast2024, which contains pitch-level data for all regular season pitches in 2024, runValue2024, which contains the \\(\\delta_{i}\\)’s for each regular season at-bat, and player2024_lookup, which contains the names and MLB Advanced Media identifiers for each player.\n\nload(\"statcast2024.RData\")\nload(\"runValue2024.RData\")\nload(\"player2024_lookup.RData\")\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n\nRecall from Lecture 6 that the StatCast variables hc_x and hc_y record the coordinates of each batted ball. When we plotted the available hc_x and hc_y values, we saw the outlines of the park, with home plate located around hc_x = 125 and hc_y = 200. We can also roughly make out the first and third base lines and the infield.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x, statcast2024$hc_y, \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\n\n\n\n\n\n\n\n\nIt is not immediately apparent, however, whether the first base line is on the left or right. Luckily, StatCastalso contains the official fielding position of the player who first fields the ball1. When we plot just the pitches first fielded by first basemen, we see that in the original (hc_x, hc_y) coordinate system, first base is on the right hand side of the plot.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x[statcast2024$hit_location==3], \n     statcast2024$hc_y[statcast2024$hit_location==3], \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[2], alpha.f = 0.1))\n\n\n\n\n\n\n\n\nAlthough MLB Stadiums are equipped with lots of very cool camera and ball tracking technology, the variables hc_x and hc_y are not derived from those technologies. Instead a stringer manually marks the location of the batted ball on a tablet so the coordinates hc_x and hc_y are expressed in units of pixels on that tablet. Jim Albert, who is one of the founding fathers of Statistical research in baseball, suggested the following transformation from the original (hc_x, hc_y) coordinate system to one that places home plate at the bottom of the plot (with coordinates (0,0)) and measures distances in feet. \\[\n\\begin{align}\nx &= 2.5 \\times (\\texttt{hc\\_x} - 125.42) & y &= 2.5 \\times (198.27 - \\texttt{hc\\_y})\n\\end{align}\n\\]\nThe following code implements this transformation, storing the new coordinates as x and y and then plots the batted ball locations. It also overlays the first and third base lines, the base paths and places points at the base locations.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::mutate(\n    x = 2.5 * (hc_x - 125.42),\n    y = 2.5 * (198.27 - hc_y))\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$x, \n     statcast2024$y, \n     xlab = \"x\", ylab = \"y\",\n     pch = 16, cex = 0.2, \n     xlim = c(-300, 300),\n     ylim = c(-100, 500),\n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\n\n1abline(a = 0, b = 1, col = oi_colors[5], lwd = 1)\nabline(a = 0, b = -1, col = oi_colors[5], lwd = 1)\n\n2lines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 2)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nThe function abline(a,b) plots the line \\(y = ax + b\\). In our new coordinate system the first and third base lines correspond to the lines \\(y = x\\) and \\(y = -x\\).\n\n2\n\nIn our new coordinate system, home plate is at (0,0). First base and third base are 90 feet away from home plate along the 45 degree lines, meaning that their coordinates are (90/sqrt(2), 90/sqrt(2)) and (-90/sqrt(2), 90/sqrt(2))`.\n\n\n\n\n\n\n\n\n\n\n\nIn Lecture 7, we created a table atbat2024 that contained the beginning and ending states of each at-bat, run value, ending event, and identities of the batter and baserunners. Today, we will build a similar data frame that contains the ending event, identities of the pitcher and fielders, and, if the at-bat resulted in a ball being put into play, the batted ball locations. Like the code we used to build atbat2024, we need to identify the last entry in the column events in each at bat as well as the last entry of the columns x, y, and hit_location. We will also keep the run value.\n\ndef_atbat2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n    end_events = dplyr::last(events),\n    end_x = dplyr::last(x),\n    end_y = dplyr::last(y),\n    end_type = dplyr::last(type),\n    end_hit_location = dplyr::last(hit_location)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::select(\n    game_date, game_pk, at_bat_number,\n    end_events, end_type, des,\n    batter,\n    pitcher, fielder_2, fielder_3,\n    fielder_4, fielder_5, fielder_6,\n    fielder_7, fielder_8, fielder_9,\n    end_x, end_y, end_hit_location) |&gt;\n  dplyr::rename(x = end_x, y = end_y) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\"))",
    "crumbs": [
      "Lecture 8: Defensive creidt & replacement level"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#overview",
    "href": "lectures/lecture08.html#overview",
    "title": "Lecture 8: Defensive creidt & replacement level",
    "section": "",
    "text": "Last lecture, we distributed the run value created in each at-bat between the batter and base runners. Aggregating over all at-bats, we computed each player’s \\(\\textrm{RAA}^{\\textrm{br}},\\) which quantifies how much more run value a player created through his base running than would otherwise be expected based on the starting game states and ending events of the at-bats in which he was involved. We also computed \\(\\textrm{RAA}^{\\textrm{b}},\\) which quantifies how much more run value a player created through his hitting than would be expected based on his position.\nAccording to the conservation of runs framework introduced by Baumer, Jensen, and Matthews (2015), whenever the batting team creates \\(\\delta\\) units of run value in at-bat, the fielding team (necessarily) gives up \\(\\delta\\) units of run value. Equivalently, the fielding team creates \\(-\\delta\\) units of run value. In this lecture, we discuss how to divide this \\(-\\delta\\) run value between the pitchers and fielders.\nFor at-bats \\(i\\) that end without the ball being put in play (i.e., a strikeout, walk, or a homerun), we will assign the entirety of the \\(-\\delta_{i}\\) run value to the pitcher. But for at-bats that result with balls put in play, we will divide \\(-\\delta_{i}\\) into two parts \\(\\delta^{(p)}_{i} = -\\delta \\times \\hat{p}_{i}\\) and \\(\\delta^{(f)} = -\\delta_{i} \\times (1 - \\hat{p}_{i})\\) where \\(\\hat{p}_{i}\\) is an estimate of the probability that play results in an out given the batted ball’s location. We will first allocate\n\n\nWe begin by loading several of the data tables created in Lecture 6 and Lecture 7 including statcast2024, which contains pitch-level data for all regular season pitches in 2024, runValue2024, which contains the \\(\\delta_{i}\\)’s for each regular season at-bat, and player2024_lookup, which contains the names and MLB Advanced Media identifiers for each player.\n\nload(\"statcast2024.RData\")\nload(\"runValue2024.RData\")\nload(\"player2024_lookup.RData\")\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n\nRecall from Lecture 6 that the StatCast variables hc_x and hc_y record the coordinates of each batted ball. When we plotted the available hc_x and hc_y values, we saw the outlines of the park, with home plate located around hc_x = 125 and hc_y = 200. We can also roughly make out the first and third base lines and the infield.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x, statcast2024$hc_y, \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\n\n\n\n\n\n\n\n\nIt is not immediately apparent, however, whether the first base line is on the left or right. Luckily, StatCastalso contains the official fielding position of the player who first fields the ball1. When we plot just the pitches first fielded by first basemen, we see that in the original (hc_x, hc_y) coordinate system, first base is on the right hand side of the plot.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$hc_x[statcast2024$hit_location==3], \n     statcast2024$hc_y[statcast2024$hit_location==3], \n     xlab = \"hc_x\", ylab = \"hc_y\",\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[2], alpha.f = 0.1))\n\n\n\n\n\n\n\n\nAlthough MLB Stadiums are equipped with lots of very cool camera and ball tracking technology, the variables hc_x and hc_y are not derived from those technologies. Instead a stringer manually marks the location of the batted ball on a tablet so the coordinates hc_x and hc_y are expressed in units of pixels on that tablet. Jim Albert, who is one of the founding fathers of Statistical research in baseball, suggested the following transformation from the original (hc_x, hc_y) coordinate system to one that places home plate at the bottom of the plot (with coordinates (0,0)) and measures distances in feet. \\[\n\\begin{align}\nx &= 2.5 \\times (\\texttt{hc\\_x} - 125.42) & y &= 2.5 \\times (198.27 - \\texttt{hc\\_y})\n\\end{align}\n\\]\nThe following code implements this transformation, storing the new coordinates as x and y and then plots the batted ball locations. It also overlays the first and third base lines, the base paths and places points at the base locations.\n\nstatcast2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::mutate(\n    x = 2.5 * (hc_x - 125.42),\n    y = 2.5 * (198.27 - hc_y))\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(statcast2024$x, \n     statcast2024$y, \n     xlab = \"x\", ylab = \"y\",\n     pch = 16, cex = 0.2, \n     xlim = c(-300, 300),\n     ylim = c(-100, 500),\n     col = adjustcolor(oi_colors[1], alpha.f = 0.1))\n\n1abline(a = 0, b = 1, col = oi_colors[5], lwd = 1)\nabline(a = 0, b = -1, col = oi_colors[5], lwd = 1)\n\n2lines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 2)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 2)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nThe function abline(a,b) plots the line \\(y = ax + b\\). In our new coordinate system the first and third base lines correspond to the lines \\(y = x\\) and \\(y = -x\\).\n\n2\n\nIn our new coordinate system, home plate is at (0,0). First base and third base are 90 feet away from home plate along the 45 degree lines, meaning that their coordinates are (90/sqrt(2), 90/sqrt(2)) and (-90/sqrt(2), 90/sqrt(2))`.\n\n\n\n\n\n\n\n\n\n\n\nIn Lecture 7, we created a table atbat2024 that contained the beginning and ending states of each at-bat, run value, ending event, and identities of the batter and baserunners. Today, we will build a similar data frame that contains the ending event, identities of the pitcher and fielders, and, if the at-bat resulted in a ball being put into play, the batted ball locations. Like the code we used to build atbat2024, we need to identify the last entry in the column events in each at bat as well as the last entry of the columns x, y, and hit_location. We will also keep the run value.\n\ndef_atbat2024 &lt;-\n  statcast2024 |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n    end_events = dplyr::last(events),\n    end_x = dplyr::last(x),\n    end_y = dplyr::last(y),\n    end_type = dplyr::last(type),\n    end_hit_location = dplyr::last(hit_location)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::select(\n    game_date, game_pk, at_bat_number,\n    end_events, end_type, des,\n    batter,\n    pitcher, fielder_2, fielder_3,\n    fielder_4, fielder_5, fielder_6,\n    fielder_7, fielder_8, fielder_9,\n    end_x, end_y, end_hit_location) |&gt;\n  dplyr::rename(x = end_x, y = end_y) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\"))",
    "crumbs": [
      "Lecture 8: Defensive creidt & replacement level"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#estimating-out-probabilities",
    "href": "lectures/lecture08.html#estimating-out-probabilities",
    "title": "Lecture 8: Defensive creidt & replacement level",
    "section": "Estimating out probabilities",
    "text": "Estimating out probabilities\nRecall that we need to estimate the probability that a batted ball results in an out given its coordinate. To do so, we first need to extract those at-bats that resulted in a ball being put in play. The variable end_type in def_atbat2024 records whether the last pitch of each at-bat resulted in a strike (end_type = S), a ball (end_type = B), or a ball being hit into play (end_type = X). As a sanity check, let’s take a look at the ending event for all at-bats with end_type = X. Reassuringly, none of them can occur without a ball being hit.\n\ntable(def_atbat2024$end_events[def_atbat2024$end_type == \"X\"], useNA = 'always')\n\n\n                   double               double_play               field_error \n                     7608                       336                      1093 \n                field_out           fielders_choice       fielders_choice_out \n                    72233                       373                       306 \n                force_out grounded_into_double_play                  home_run \n                     3408                      3152                      5326 \n                 sac_bunt                   sac_fly       sac_fly_double_play \n                      446                      1222                        13 \n                   single                    triple               triple_play \n                    25363                       685                         1 \n                     &lt;NA&gt; \n                        0 \n\n\nAs another sanity check, when we tabulate the end_events for all at-bats with end_type != X, we do not see any of the hitting events from above.\n\ntable(def_atbat2024$end_events[def_atbat2024$end_type != \"X\"], useNA = 'always')\n\n\n                             catcher_interf          hit_by_pitch \n                  308                    97                  1977 \n            strikeout strikeout_double_play          truncated_pa \n                40145                   107                   304 \n                 walk                  &lt;NA&gt; \n                14029                     0 \n\n\nSo, we can isolate those at-bats with a ball put in play by filtering on end_type.\n\nbip &lt;-\n  def_atbat2024 |&gt;\n  dplyr::filter(end_type == \"X\")\n\nThe columns end_events includes values fielders_choice and fielders_choice_out. Presumably, balls recorded as fielders_choice did not result in an out while those recorded as fielders_choice_out did. To check whether this is the case, we will search from the string “out” in the description of the at-bat (des) for all balls with event = fielders_choice\n\ntable(grepl(\"out\", bip$des[bip$end_events == \"fielders_choice\"]))\n\n\nFALSE  TRUE \n  371     2 \n\n\nCuriously, there are two instances where the play is recorded as a fielder’s choice but the string “out” appears. Investigating further, in one instance the string “out” appears as part of a player’s name while in the other, the play actually did result in an out.\n\nwhich(grepl(\"out\", bip$des) & bip$end_events == \"fielders_choice\")\n\n[1] 10235 65138\n\nbip$des[which(grepl(\"out\", bip$des) & bip$end_events == \"fielders_choice\")]\n\n[1] \"Mike Trout reaches on a fielder's choice, fielded by shortstop David Hamilton. Anthony Rendon to 3rd. Nolan Schanuel to 2nd. Throwing error by shortstop David Hamilton.\"        \n[2] \"Ryan Jeffers reaches on a fielder's choice, fielded by second baseman Colt Keith. Byron Buxton scores. Ryan Jeffers out at 2nd, catcher Jake Rogers to first baseman Mark Canha.\"\n\n\nWe will manually change the value of end_events in row 65138 to “fielders_choice_out”. For fitting our out probability model, we will focus only on those at-bats that did not end with a home run. We will also drop at-bats for which the batted ball locations are missing.\n\nbip$end_events[65138] &lt;- \"fielders_choice_out\"\nout_events &lt;- \n  c(\"double_play\", \"field_out\", \"fielders_choice_out\",\n    \"force_out\", \"grounded_into_double_play\", \n    \"sac_bunt\", \"sac_fly\", \"sac_fly_double_play\",\n    \"triple_play\")\nbip &lt;-\n  bip |&gt;\n  dplyr::filter(end_events != \"home_run\" & !is.na(x) & !is.na(y)) |&gt;\n  dplyr::mutate(Out = ifelse(end_events %in% out_events, 1, 0)) \n\n\nBinning and averaging\nOne potential approach works by dividing the field into small rectangular regions and computing the proportion of balls landing within each region that result in an out. To this end, we will first create a grid of 3ft x 3ft squares that covers the range of our batted ball locations.\n\nrange(bip$x)\n\n[1] -287.725  290.575\n\nrange(bip$y)\n\n[1] -91.85 459.35\n\n\n\n1grid_sep &lt;- 3\n2x_grid &lt;- seq(from = -300, to = 300, by = grid_sep)\ny_grid &lt;- seq(from = -100, to = 500, by = grid_sep)\n3raw_grid &lt;- expand.grid(x = x_grid, y = y_grid)\n\n\n1\n\nSeparation between grid points in each dimension.\n\n2\n\nCreates a sequence of evenly-spaced points\n\n3\n\nCreates a table with every combination of values in x_grid and y_grid\n\n\n\n\nThe table grid contains many combinations of x and y that do not represent plausible batted ball locations. In the figure below, we plot all batted ball locations and overlay all the grid points.\n\nplot(bip$x, bip$y, \n     xlim = c(-300, 300), ylim = c(-100, 500),\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.2))\npoints(raw_grid$x, raw_grid$y, pch = 16, cex = 0.2, \n       col = adjustcolor(oi_colors[3], alpha.f = 0.3))\n\n\n\n\n\n\n\n\nWe will remove those (x,y) pairs in grid for which either\n\nx + y &lt; -100: these are locations far below the third base line\ny - x &lt; -100: these are locations far below the first base line\nsqrt(x^2 +y^2 &lt; 580): these are locations very far from home plate\n\n\ngrid &lt;-\n  raw_grid |&gt;\n  dplyr::filter(y + x &gt; -100 & y - x &gt; -100 & sqrt(x^2 + y^2) &lt; 580)\nplot(bip$x, bip$y, \n     xlim = c(-300, 300), ylim = c(-100, 500),\n     pch = 16, cex = 0.2, \n     col = adjustcolor(oi_colors[1], alpha.f = 0.2))\npoints(grid$x, grid$y, pch = 16, cex = 0.2, \n       col = adjustcolor(oi_colors[3], alpha.f = 0.3))\n\n\n\n\n\n\n\n\nWe can use the cut() function to divide the x and y values in bip into small bins. Here is a quick example of how cut works: we first create a vector with a few numbers. The breaks argument of cut tells it the end points of each bin.\n\nx &lt;- c(0.11, 0.23, 0.45, 0.67, 0.99, 0.02) \ncut(x, breaks = seq(0, 1, by = 0.1))  \n\n[1] (0.1,0.2] (0.2,0.3] (0.4,0.5] (0.6,0.7] (0.9,1]   (0,0.1]  \n10 Levels: (0,0.1] (0.1,0.2] (0.2,0.3] (0.3,0.4] (0.4,0.5] ... (0.9,1]\n\n\nBelow, we first cut the values of x and y that appear in bip. Then, using a grouped summary, we can compute the number of balls hit to each bin and the proportion of those balls that result in an out.\n\nbin_probs &lt;-\n  bip |&gt;\n  dplyr::select(x, y, Out) |&gt;\n  dplyr::mutate(\n1    x_bin = cut(x, breaks = seq(-300-grid_sep/2, 300+grid_sep/2, by = grid_sep)),\n    y_bin = cut(y, breaks = seq(-100 - grid_sep/2, 500+grid_sep/2, by = grid_sep))) |&gt;\n  dplyr::group_by(x_bin, y_bin) |&gt;\n  dplyr::summarise(\n    out_prob = mean(Out), \n    n_balls = dplyr::n(),\n    .groups = \"drop\")\n\n\n1\n\nOffsetting by grid_sep/2 ensures that the x and y values in grid are the centers of the squares in our grid (and not one of the corners)\n\n\n\n\nWe can now bin the x and y values contained in grid and, using a left_join(), append a column that contains the estimated probability of an out hit to each 3ft x 3ft square in our grid. Note that if there is a combination of x_bin and y_bin in grid that is not contained in bin_prob, the estimated probability will be NA. These are grid squares in which no balls were hit.\n\ngrid_probs_bin &lt;-\n  grid |&gt;\n  dplyr::mutate(\n    x_bin = cut(x, breaks = seq(-300-grid_sep/2, 300+grid_sep/2, by = 3)),\n    y_bin = cut(y, breaks = seq(-100-grid_sep/2, 500+grid_sep/2, by = 3))) |&gt;\n  dplyr::left_join(y = bin_probs, by = c(\"x_bin\", \"y_bin\"))\n\nWe can now make a heatmap of our estimated probabilities by looping over the rows in grid_prob_bins, drawing a small rectangle with corners (x-1.5, y-1.5), (x + 1.5, y-1.5), (x+1.5, y+1.5), and (x-1.5, y+1.5), and shading the rectangle based on the corresponding out probability.\n\n\n1col_list &lt;- colorBlindness::Blue2DarkRed18Steps\n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\n2plot(1, type = \"n\",\n3     xlim = c(-300, 300), ylim = c(-100, 500),\n4     xaxt = \"n\", yaxt = \"n\", bty = \"n\",\n5     main = \"Naive out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n6  rect(xleft = grid_probs_bin$x[i] - grid_sep/2,\n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n       col = ifelse(is.na(grid_probs_bin$out_prob[i]), \n7                    adjustcolor(oi_colors[1], alpha.f = 0.2),\n8                    adjustcolor(rgb(colorRamp(col_list,bias=1)(grid_probs_bin$out_prob[i])/255),\n                                alpha.f = 0.5)),\n9       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nThis is a popular diverging color palette, which ranges from blue to red, that is color-blind friendly. See here for more details.\n\n2\n\nSpecifying type = \"n\" tells R to make an empty plot\n\n3\n\nManually set the horizontal and vertical limits of the plotting area\n\n4\n\nxaxt = \"n\" and yaxt = \"n\" suppress the axis lines and bty = \"n\" suppresses the bounding box\n\n5\n\nmain controls the title of plot while setting xlab = \"\" and ylab = \"\" suppresses horizontal and vertical axis labels\n\n6\n\nTo use rect(), we must pass the coordinates of the bottom left and top right corners of the rectangle\n\n7\n\nIf there were no balls hit to a particular rectangle, we will color it gray. Otherwise, we will color it according to the fitted probability\n\n8\n\nThe expression rgb(colorRamp(col_list, bias=1)(grid_probs_bin$out_prob)/255) maps the fitted probability to a color in col_list using linear interpolation. Here 0% maps to dark blue and 100% maps to dark red.\n\n9\n\nSuppresses the border of the rectangle\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized additive models\nOur naively estimated out probabilities leave much to be desired. For one thing, there are lots of “gaps” in the fitted surface where our initial model can’t make a prediction. These are the cells in our grid into which no balls were hit. Of much greater concern are the sharp discontinuities evident in the figure. In fact, there are many regions where the fitted out probability jumps from 0% to 100% to 0% in the span of about 6 feet. Such discontinuities are artifacts of the small sample sizes within some of the bins. Indeed, we find that about 85% of the grid cells contain 10 or fewer observations\n\nmean(bin_probs$n_balls &lt;= 10)\n\n[1] 0.8541364\n\n\nLike we did to predict XG using shot distance, we can overcome this challenge by building a statistical model. In particular, we would like to model the log-odds of an out as a function of the batted balls location. That is, \\[\n\\log\\left(\\frac{\\mathbb{P}(\\textrm{out})}{1 - \\mathbb{P}(\\textrm{out})} \\right) = s(x,y),\n\\] where \\(s\\) is a smooth function of the batted balls location. What form should \\(s\\) take?\nOne possibility is to assume that there are parameters \\(\\beta_{0}, \\beta_{x},\\) and \\(\\beta_{y}\\) such that \\[\n\\log\\left(\\frac{\\mathbb{P}(\\textrm{out})}{1 - \\mathbb{P}(\\textrm{out})} \\right) = \\beta_{0} + \\beta_{x}x + \\beta_{y}y\n\\] We can estimate these parameters using glm() and then compute the fitted probability for every cell in our grid.\n\nlogit_fit &lt;-\n  glm(Out ~ x + y,\n      family = binomial(link = \"logit\"), data = bip)\ngrid_logit_preds &lt;-\n  predict(object = logit_fit, \n1          newdata = grid, type = \"response\")\ngrid_logit_cols &lt;- \n2  rgb(colorRamp(col_list,bias=1)(grid_logit_preds)/255)\n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", \n     xlim = c(-300, 300), ylim = c(-100, 500), \n     xaxt = \"n\", yaxt = \"n\", bty = \"n\", \n     main = \"Logistic regression out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n  rect(xleft = grid_probs_bin$x[i] - grid_sep/2, \n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n3       col = adjustcolor(grid_logit_cols[i], alpha.f = 0.5),\n       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n1\n\nTo get fitted values on the probability scale, we need to specify type = \"response\"\n\n2\n\nCreates a vector mapping the fitted probability at each grid location to a color between the two extremes\n\n3\n\nUnlike with binning-and-averaging, our parametric model is able to make predictions at grid cells not present in the training data. So, we don’t need to check whether the predicted probability is NA\n\n\n\n\n\n\n\n\n\n\n\nThe fitted logistic regression model predicts that the probability of an out decreases as one moves vertically away from home plate. The model further predicts that balls hit into the outfield tend not to result in outs, which is at odds with what we see in the data. Indeed, our earlier plot revealed pockets of high out probability in the outfield, roughly corresponding to the usual positioning of the left, right, and center fielders.\nThe root cause of the conflict between our data and our model predictions is the model’s strong — and totally unrealistic — assumption that the log-odds of an out is linear in both \\(x\\) and \\(y.\\) A more accurate model would allow the log-odds to vary non-linearly.\nGeneralized Additive Models (or GAMs) are an elegant way to introduce non-linearity and create spatially smooth maps. In our context, a GAM model expresses \\(s(x,y)\\) as a linear combination of a large number of two-dimensional splines functions. Each spline function is a piecewise polynomial that is localized to a small region of \\((x,y)\\) space, meaning that it is non-zero inside the region and zero outside the region. Such spline functions can be linearly combined to approximate really complicated functions arbitrarily well2\nWe can fit GAMs in R using the mgcv package. The package provides two fitting functions, gam() and bam(), which is recommended for very large data sets like bip. Both functions are similar lm() and glm() in that they require users to specify a formula and, in the case of non-continuous outcomes, a link function. The main difference is the term s(), which is used to signal to gam() or bam() that we want to smooth over whatever variables appear inside of the brackets of s(). In our case, because we wish to smooth over both x and y, we will include the terms s(x,y)\n\n\n\n\n\n\nWarning\n\n\n\nFitting our out probability model takes a few minutes\n\n\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\ngam_fit &lt;-\n  bam(formula = Out ~ s(x,y), \n      family = binomial(link=\"logit\"), data = bip)\n\ngrid_gam_preds &lt;- \n  predict(object = gam_fit,\n          newdata = grid, type = \"response\")\ngrid_gam_cols &lt;-   \n  rgb(colorRamp(col_list,bias=1)(grid_gam_preds)/255) \n\npar(mar = c(1,1,5,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\", \n     xlim = c(-300, 300), ylim = c(-100, 500), \n     xaxt = \"n\", yaxt = \"n\", bty = \"n\", \n     main = \"GAM out probabilities\", xlab = \"\", ylab = \"\")\n\nfor(i in 1:nrow(grid_probs_bin)){\n  rect(xleft = grid_probs_bin$x[i] - grid_sep/2, \n       ybot = grid_probs_bin$y[i] - grid_sep/2,\n       xright = grid_probs_bin$x[i] + grid_sep/2,\n       ytop = grid_probs_bin$y[i] + grid_sep/2,\n       col = adjustcolor(grid_gam_cols[i], alpha.f = 0.5),\n       border = NA)\n}\nabline(a = 0, b = 1, col = oi_colors[5], lwd = 2) \nabline(a = 0, b = -1, col = oi_colors[5], lwd = 2)\n\nlines(x = c(0,90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3) \nlines(x = c(0,-90/sqrt(2)), y = c(0, 90/sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\nlines(x = c(-90/sqrt(2), 0), y = c(90/sqrt(2), 90*sqrt(2)), col = \"white\", lwd = 3)\n\n\npoints(x = 0, y = 0, pch = 16, cex = 0.8, col = \"white\")\npoints(x = 90/sqrt(2), y = 90/sqrt(2), pch = 16, cex = 0.8, col = \"white\")\npoints(x = -90/sqrt(2), y = 90/sqrt(2), pch = 16,cex = 0.8, col = \"white\")\npoints(x = 0, y = 90*sqrt(2), pch = 16, cex = 0.8, col = \"white\")\n\n\n\n\n\n\n\n\nThe out probabilities fitted by the GAM appear much more reasonable: balls hit within the in-field and close to the outfielders tend to be result in outs while balls hit in the gap between the infield and outfield tend not to result in outs.\n\n\nDividing run value between pitcher and fielders\nRecall that if at-bat \\(i\\) results in a ball put in play, we will distribute \\(\\delta_{i}^{(f)} = -\\delta_{i} \\times \\hat{p}_{i}\\) to the fielders and assign \\(\\delta_{i}^{(p)} = -\\delta \\times (1 - \\hat{p}_{i})\\) to the pitcher. The following code adds a column to def_atbat2024 containing the predicted out probabilities from our fitted GAM. Note that this column contains NA values for those at-bats that did not result in a ball in play (e.g., they ended with a walk or a strikeout or a home run) For the vast majority of these at-bats, we will manually set \\(\\hat{p}_{i} = 0\\) so that the pitcher gets all the credit (i.e., \\(\\delta^{(p)}_{i} = -\\delta_{i}\\)).\n\nall_preds &lt;-\n  predict(object = gam_fit, \n1          newdata = def_atbat2024,\n          type = \"response\")\n2def_atbat2024$p_out &lt;- all_preds\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    p_out = dplyr::case_when(\n3      is.na(p_out) & end_events == \"home_run\" ~ 0,\n4      is.na(p_out) & end_type != \"X\" ~ 0,\n      .default = p_out))\n\n\n1\n\nWhen x and y are NA (i.e., whenever the at-bat doesn’t end with a ball in play), predict will return NA.\n\n2\n\nAdds fitted out probabilities to the data table\n\n3\n\nFor home runs, manually set p_out = 0 (so pitcher gets all credit/blame)\n\n4\n\nFor at-bats that end with a ball or strike, set p_out = 0 (so pitcher gets all credit/blame)\n\n\n\n\nEven after manually setting \\(\\hat{p}_{i} = 0\\) on the at-bats that didn’t end with the ball in play, there are still some NA values in the column p_out. On further inspection, it looks like these were at-bats in which the ball was put in play but are missing hit locations.\n\ntable(def_atbat2024$end_events[is.na(def_atbat2024$p_out)])\n\n\n                   double                 field_out grounded_into_double_play \n                        2                         5                         1 \n                   single                    triple \n                        4                         1 \n\nsum(is.na(def_atbat2024$x[is.na(def_atbat2024$p_out)]))\n\n[1] 13\n\n\nWe will remove these rows from our calculations\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::filter(!is.na(p_out))\n\nWe can now finally compute \\(\\delta^{(f)}_{i}\\) and \\(\\delta^{(p)}_{i}\\) for each at-bat.\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    delta_p = -1 * (1 - p_out) * RunValue,\n    delta_f = -1 * p_out * RunValue)",
    "crumbs": [
      "Lecture 8: Defensive creidt & replacement level"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#fielding-run-values",
    "href": "lectures/lecture08.html#fielding-run-values",
    "title": "Lecture 8: Defensive creidt & replacement level",
    "section": "Fielding run values",
    "text": "Fielding run values\nIf a ball is hit towards deep left field and results in the fielding team creating a large \\(-\\delta\\), how much blame should the third baseman, who is on the opposite side of the park receive? Following Baumer, Jensen, and Matthews (2015), we will apportion \\(\\delta_{i}^{(f)},\\) the portion of the run value \\(-\\delta_{i}\\) attributable to the fielding in at-bat \\(i\\), based on each fielder’s responsibility for making an out on the batted ball. Specifically, we will assign \\(w_{i,\\ell} \\times \\delta_{i}^{(f)}\\) to the fielder playing position \\(\\ell \\in \\{1, 2, \\ldots, 9\\}\\) during the at-bat where \\[\nw_{i,\\ell} = \\frac{\\hat{p}_{i,\\ell}}{\\hat{p}_{i,1} + \\cdots + \\hat{p}_{i,9}}\n\\] and \\(\\hat{p}_{i,\\ell}\\) is the probability that fielder \\(\\ell\\) makes the out given the location of the batted ball.\nTo estimate the \\(\\hat{p}_{i,\\ell}\\)’s, we will fit 9 separate GAMs, one for each fielding position. For fielding position \\(\\ell,\\) we will create a new variable that is equal to 1 if the ball in play resulted in an out was initially fielded by the player at position \\(\\ell.\\) Unlike our overall out probability model, which we fit using data from all batted balls, each position-specific GAM will be fitted using data from the subset of batted balls that are reasonably close to the typical position location. For instance, when fitting the model for the first basemen, we won’t include data from balls hit into deep left field. We compute the coordinates of the typical position by taking the average value of the x and y coordinates of all batted balls fielded by players at position \\(\\ell.\\) We fit each position-specific GAM using batted balls hit within 150 ft of this typical location[^adhoc]  [^adhoc]: This choice is decidedly ad hoc. You should experiment with different cut-offs to see how the downstream results change!\n\n\n\n\n\n\nWarning\n\n\n\nThis code takes between 25 and 40 minutes to run\n\n\n\n1mid_x &lt;- rep(NA, times = 9)\nmid_y &lt;- rep(NA, times = 9)\n2fit_list &lt;- list()\nfit_time &lt;- rep(NA, times = 9)\nfor(l in 1:9){\n  print(paste0(\"Starting l = \", Sys.time()))\n  \n3  mid_x[l] &lt;- mean(bip$x[bip$end_hit_location == l], na.rm = TRUE)\n  mid_y[l] &lt;- mean(bip$y[bip$end_hit_location==l], na.rm = TRUE)\n  \n  train_df &lt;-\n    bip |&gt;\n4    dplyr::mutate(newOut = ifelse(Out == 1 & end_hit_location == l, 1, 0)) |&gt;\n    dplyr::filter(!is.na(x) & !is.na(y)) |&gt;\n5    dplyr::filter( sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150) |&gt;\n    dplyr::select(x,y,newOut) \n\n6  fit_time[l] &lt;-\n    system.time(\n7      fit_list[[l]] &lt;-\n        bam(formula = newOut ~ s(x, y),\n8            family = binomial(link=\"logit\"),data = train_df))[\"elapsed\"]\n  \n  \n  \n  tmp_df &lt;-\n    def_atbat2024 |&gt; \n    dplyr::mutate(\n9      x = ifelse(sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150, x, NA),\n      y = ifelse(sqrt( (x - mid_x[l])^2 + (y - mid_y[l])^2 ) &lt; 150, y, NA)\n    )\n    \n10  preds &lt;- predict(object = fit_list[[l]], newdata = tmp_df, type = \"response\")\n11  def_atbat2024[[paste0(\"p_out_\", l)]] &lt;- preds\n}\n12save(fit_list, file = \"position_gam_fits.RData\")\n\n\n1\n\nVectors containing coordinates for the typical fielder location\n\n2\n\nHolds all fitted objects, so that we can use them later to visualize position-specific out probabilities\n\n3\n\nCompute typical coordinates for position l\n\n4\n\nCreate position-specific outcome variable\n\n5\n\nSubsets to batted balls within 150 ft of the typical location\n\n6\n\nIt’s often helpful to track how long it takes to fit a model. system.time() can return, among other things, the elapsed time. We save the time for fitting each position’s model in the vector fit_time\n\n7\n\nInstead of creating a different object for each position’s fitted model, we will store all the fitted models in a single list, which can be saved.\n\n8\n\nsystem.time() returns three different time measures. The most pertinent one is the actual elapsed time, stored in the slot “elapsed”.\n\n9\n\nWe only want model predictions for balls hit within 150ft of the typical location. Passing NA x and y values to predict() is a hack to suppress model predictions for balls hit outside this radius. We will later over-write the resulting NA’s with 0’s.\n\n10\n\nfit_list[[l]] is the fitted object return by bam for position l.\n\n11\n\nCreates a column in def_atbat2024 holding the\n\n12\n\nSave’s the list containing all position-specific fits\n\n\n\n\n[1] \"Starting l = 2025-08-01 18:04:01.373716\"\n[1] \"Starting l = 2025-08-01 18:05:16.554675\"\n[1] \"Starting l = 2025-08-01 18:06:24.439367\"\n[1] \"Starting l = 2025-08-01 18:07:51.014745\"\n[1] \"Starting l = 2025-08-01 18:09:18.34922\"\n[1] \"Starting l = 2025-08-01 18:10:40.416933\"\n[1] \"Starting l = 2025-08-01 18:12:00.566097\"\n[1] \"Starting l = 2025-08-01 18:12:41.239513\"\n[1] \"Starting l = 2025-08-01 18:13:21.521715\"\n\n\nThe columns p_out_1, …, p_out_9 in def_atbat2024 contain several NA values. Some of these correspond to at-bats that did not result in ball being put in play (i.e., those with end_type = S or end_type = B) while others correspond to at-bats in which the ball was hit far away from the typical location of the fielder. For these latter at-bats, we will manually set the position-specific out probabilities as follows: 1. For home runs and balls hit outside the 150ft radius for each position, we set p_out_* = 0 2. For at-bats ending with a strike or ball, we keep p_out_* = NA. 3. For balls in play with missing x and y coordinates, we set p_out_* = NA\n\n\nCode\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    p_out_1 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[1])^2 + (y - mid_y[1])^2) &gt;= 150 ~ 0,\n      .default = p_out_1),\n    p_out_2 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[2])^2 + (y - mid_y[2])^2) &gt;= 150 ~ 0,\n      .default = p_out_2),\n    p_out_3 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[3])^2 + (y - mid_y[3])^2) &gt;= 150 ~ 0,\n      .default = p_out_3),\n    p_out_4 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[4])^2 + (y - mid_y[4])^2) &gt;= 150 ~ 0,\n      .default = p_out_4),\n    p_out_5 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[5])^2 + (y - mid_y[5])^2) &gt;= 150 ~ 0,\n      .default = p_out_5),\n    p_out_6 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[6])^2 + (y - mid_y[6])^2) &gt;= 150 ~ 0,\n      .default = p_out_6),\n    p_out_7 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[7])^2 + (y - mid_y[7])^2) &gt;= 150 ~ 0,\n      .default = p_out_7),\n    p_out_8 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[8])^2 + (y - mid_y[8])^2) &gt;= 150 ~ 0,\n      .default = p_out_8),\n    p_out_9 = dplyr::case_when(\n      end_events == \"home_run\" ~ 0, \n      end_type %in% c(\"S\", \"B\") ~ NA,\n      is.na(x) | is.na(y) ~ NA,\n      sqrt( (x - mid_x[9])^2 + (y - mid_y[9])^2) &gt;= 150 ~ 0,\n      .default = p_out_9))\n\n\nWe now normalize the \\(\\hat{p}_{i,\\ell}\\) values to get the weights \\(w_{i,\\ell}\\) measuring the responsibility of each fielder for making an out in the at-bat.\n\ndef_atbat2024 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(\n    total_weight = p_out_1 + p_out_2 + p_out_3 +\n      p_out_4 + p_out_5 + p_out_6 +\n      p_out_7 + p_out_8 + p_out_9,\n    w1 = p_out_1/total_weight,\n    w2 = p_out_2/total_weight,\n    w3 = p_out_3/total_weight,\n    w4 = p_out_4/total_weight,\n    w5 = p_out_5/total_weight,\n    w6 = p_out_6/total_weight,\n    w7 = p_out_7/total_weight,\n    w8 = p_out_8/total_weight,\n    w9 = p_out_9/total_weight)\n\nNow, for each fielding position, we can compute \\(w_{i,\\ell} \\times \\delta^{(f)}_{i}\\) and aggregate these values across players at that position to compute \\(\\textrm{RAA}_{\\ell}^{(f)},\\) which quantifies the total run value created by the player through their fielding at position \\(\\ell.\\) Negative values of \\(\\textrm{RAA}_{\\ell}^{(f)}\\) suggest that the player’s fielding at position \\(\\ell\\) netted positive run value for the batting team, thereby negatively impacthing his own team.\nHere is the calculation for first basemen (fielding position number \\(\\ell = 3\\)). Taking a looking at the largest \\(\\textrm{RAA}_{\\ell}^{f}\\) values, we see several recent Golden Glove winners who are known for their fielding prowess (Santana, Walker, Goldschmidt, Olson, Guerrero).\n\nraa_f3 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f3 = delta_f * w3) |&gt;\n  dplyr::group_by(fielder_3) |&gt;\n  dplyr::summarize(RAA_f3 = sum(RAA_f3, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_3)\n\nraa_f3 |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, RAA_f3) |&gt;\n  dplyr::arrange(dplyr::desc(RAA_f3)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   Name              RAA_f3\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 Carlos Santana      55.7\n 2 Christian Walker    54.6\n 3 Paul Goldschmidt    52.4\n 4 Bryce Harper        51.1\n 5 Matt Olson          51.0\n 6 Ryan Mountcastle    48.7\n 7 Vladimir Guerrero   47.1\n 8 Michael Toglia      46.5\n 9 Freddie Freeman     46.2\n10 Josh Naylor         46.2\n\n\nThe following code repeats this calculation for all fielding positions.\n\nraa_f1 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f1 = delta_f * w1) |&gt;\n  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarize(RAA_f1 = sum(RAA_f1, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = pitcher) |&gt;\n  dplyr::select(key_mlbam, RAA_f1)\n\nraa_f2 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f2 = delta_f * w2) |&gt;\n  dplyr::group_by(fielder_2) |&gt;\n  dplyr::summarize(RAA_f2 = sum(RAA_f2, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_2) |&gt;\n  dplyr::select(key_mlbam, RAA_f2)\n\nraa_f3 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f3 = delta_f * w3) |&gt;\n  dplyr::group_by(fielder_3) |&gt;\n  dplyr::summarize(RAA_f3 = sum(RAA_f3, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_3) |&gt;\n  dplyr::select(key_mlbam, RAA_f3)\n\nraa_f4 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f4 = delta_f * w4) |&gt;\n  dplyr::group_by(fielder_4) |&gt;\n  dplyr::summarize(RAA_f4 = sum(RAA_f4, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_4) |&gt;\n  dplyr::select(key_mlbam, RAA_f4)\n\nraa_f5 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f5 = delta_f * w5) |&gt;\n  dplyr::group_by(fielder_5) |&gt;\n  dplyr::summarize(RAA_f5 = sum(RAA_f5, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_5) |&gt;\n  dplyr::select(key_mlbam, RAA_f5)\n\nraa_f6 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f6 = delta_f * w6) |&gt;\n  dplyr::group_by(fielder_6) |&gt;\n  dplyr::summarize(RAA_f6 = sum(RAA_f6, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_6) |&gt;\n  dplyr::select(key_mlbam, RAA_f6)\n\nraa_f7 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f7 = delta_f * w7) |&gt;\n  dplyr::group_by(fielder_7) |&gt;\n  dplyr::summarize(RAA_f7 = sum(RAA_f7, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_7) |&gt;\n  dplyr::select(key_mlbam, RAA_f7)\n\nraa_f8 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f8 = delta_f * w8) |&gt;\n  dplyr::group_by(fielder_8) |&gt;\n  dplyr::summarize(RAA_f8 = sum(RAA_f8, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_8) |&gt;\n  dplyr::select(key_mlbam, RAA_f8)\n\nraa_f9 &lt;-\n  def_atbat2024 |&gt;\n  dplyr::mutate(RAA_f9 = delta_f * w9) |&gt;\n  dplyr::group_by(fielder_9) |&gt;\n  dplyr::summarize(RAA_f9 = sum(RAA_f9, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = fielder_9) |&gt;\n  dplyr::select(key_mlbam, RAA_f9)\n\nLike we did when computing \\(\\textrm{RAA}^{(br)}\\) in Lecture 7, we will concatenate raa_f1, …, raa_f9 and sum every players total contributions across all fielding positions.\n\nraa_f &lt;-\n  raa_f1 |&gt;\n  dplyr::full_join(y = raa_f2, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f3, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f4, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f5, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f6, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f7, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f8, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f9, by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(\n    list(RAA_f1=0, RAA_f2=0, RAA_f3=0, RAA_f4=0, \n         RAA_f5=0, RAA_f6 = 0, RAA_f7=0, RAA_f8 = 0, RAA_f9=0)) |&gt;\n  dplyr::mutate(\n    RAA_f = RAA_f1 + RAA_f2 + RAA_f3 + RAA_f4 + \n      RAA_f5 + RAA_f6 + RAA_f7 + RAA_f8 + RAA_f9) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_f, RAA_f1, RAA_f2, RAA_f3, RAA_f4, RAA_f5,\n                RAA_f6, RAA_f7, RAA_f8, RAA_f9)",
    "crumbs": [
      "Lecture 8: Defensive creidt & replacement level"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#pitching-run-values",
    "href": "lectures/lecture08.html#pitching-run-values",
    "title": "Lecture 8: Defensive creidt & replacement level",
    "section": "Pitching run values",
    "text": "Pitching run values\nRecall that \\(\\delta_{i}^{(p)}\\) is the amount of run value created by the pitcher in at-bat \\(i.\\) Summing this value across all at-bats involving each pitcher, we obtain each pitchers \\(\\textrm{RAA}^{(p)}.\\)\n\nraa_p &lt;-\n  def_atbat2024 |&gt;\n  dplyr::select(pitcher, delta_p) |&gt;\n  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarise(RAA_p = sum(delta_p, na.rm = TRUE)) |&gt;\n  dplyr::rename(key_mlbam = pitcher) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_p)\n\nWe see that the two 2024 Cy Young Award winners3, Tarik Skubal and Chris Sale, have the highest \\(\\textrm{RAA}^{(p)}\\) values.\n\nraa_p |&gt;\n  dplyr::arrange(dplyr::desc(RAA_p)) |&gt;\n  dplyr::slice_head(n=10)\n\n# A tibble: 10 × 3\n   Name            key_mlbam RAA_p\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt;\n 1 Tarik Skubal       669373 14.7 \n 2 Chris Sale         519242 14.6 \n 3 Ryan Walker        676254 13.5 \n 4 Cade Smith         671922 13.3 \n 5 Paul Skenes        694973 12.3 \n 6 Emmanuel Clase     661403 11.2 \n 7 Kirby Yates        489446  9.99\n 8 Griffin Jax        643377  9.53\n 9 Edwin Uceta        670955  9.23\n10 Garrett Crochet    676979  9.13\n\n\nWe will save both raa_f and raa_p for later use\n\nsave(raa_f, raa_p, file = \"raa_defensive2024.RData\")",
    "crumbs": [
      "Lecture 8: Defensive creidt & replacement level"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#replacement-level",
    "href": "lectures/lecture08.html#replacement-level",
    "title": "Lecture 8: Defensive creidt & replacement level",
    "section": "Replacement Level",
    "text": "Replacement Level\n\nload(\"raa_offensive2024.RData\")\n\nraa &lt;-\n  raa_b |&gt;\n  dplyr::select(-Name) |&gt;\n  dplyr::full_join(y = raa_br |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_p |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_f |&gt; dplyr::select(-Name), by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(list(RAA_b = 0, RAA_br = 0, RAA_f = 0, RAA_p = 0)) |&gt;\n  dplyr::mutate(RAA = RAA_b + RAA_br + RAA_f + RAA_p) |&gt;\n  dplyr::left_join(y = player2024_lookup |&gt; dplyr::select(key_mlbam, Name), by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, RAA_b, RAA_br, RAA_f, RAA_p)\n\n\\(RAA\\) is a comprehensive measure of a player’s performance that accounts not only for the actual runs scored (or given up) due to his contributions but also the changes in the expected runs that can be attributed to his play. We constructed \\(RAA\\) so that larger numbers indicate better performance. While the absolute \\(RAA\\) values are useful on their own, they become even more useful — and interpretable — when calibrated to measure performance relative to a baseline player. As argued by Baumer, Jensen, and Matthews (2015), although comparing an individual players \\(RAA\\) to the league-average value is intuitive, average players are themselves quite valuable. Further, it is not reasonable to expect a team to be able to replace any player with a league average one. Thus, it is more useful to compare each player’s performance relative to a replacement-level player.\nAs discussed in class (and also in Section 3.8 of Baumer, Jensen, and Matthews (2015)), existing definitions of replacement level are fairly arbitrary. For instance, back in 2010, FanGraphs asserted that a team of players making the minimum MLB salary would win 29.7% of its games (so between 48 and 49 games). Similar definitions have been adopted by other sites like Baseball Propsectus and Baseball Reference. Unfortunately, there is little empirical justification for this number.\nWe will instead use Baumer, Jensen, and Matthews (2015)’s transparent, roster-based definition of replacement level, which is motivated and computed as follows: 1. Each of the 30 major league teams typically carries 25 players, 13 of whom are position players and 12 of whom are pitchers 2. On any given day, there are generally \\(12 \\times 30 = 360\\) pitchers and \\(13 \\times 30 = 390\\) active players. 3. So, we will treat the 390 position players with the most plate appearances and the 360 pitchers who faced the most batters as non-replacement level and everyone else as replacement-level.\nRemember that our table def_atbat2024 contains information from all available at-bats in the 2024 regular season. We can use the data in this table to count the number of plate appearances for each non-pitcher and number of batters faced by each pitcher. To this end, we first create lists of the MLB Advanced Media IDs of all pitchers and all non-pitchers who appear in our dataset.\n\n1all_players &lt;- unique(\n  c(def_atbat2024$batter, def_atbat2024$pitcher, def_atbat2024$fielder_2,\n    def_atbat2024$fielder_3, def_atbat2024$fielder_4, def_atbat2024$fielder_5,\n    def_atbat2024$fielder_6, def_atbat2024$fielder_7, def_atbat2024$fielder_8, def_atbat2024$fielder_9))\n2pitchers &lt;- unique(def_atbat2024$pitcher)\n\n3position_players &lt;- all_players[!all_players %in% pitchers]\n\n\n1\n\nGet the ID for all players\n\n2\n\nGet the ID for all pitchers\n\n3\n\nPull out the ID for all non-pitchers\n\n\n\n\nUsing grouped summaries, we can count the number of at-bats faced by each position player as a batter and by each pitcher. Since the overwhelming majority of at-bats involved only one batter, these counts effectively tell us the number of batters faced by each pitcher.\n\nposition_pa &lt;-\n  def_atbat2024 |&gt;\n1  dplyr::filter(batter %in% position_players) |&gt;\n  dplyr::group_by(batter) |&gt;\n2  dplyr::summarise(n = dplyr::n()) |&gt;\n3  dplyr::arrange(dplyr::desc(n)) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\npitcher_pa &lt;-\n  def_atbat2024 |&gt;\n4  dplyr::group_by(pitcher) |&gt;\n  dplyr::summarise(n = dplyr::n()) |&gt;\n  dplyr::arrange(dplyr::desc(n)) |&gt;\n5  dplyr::rename(key_mlbam = pitcher)\n\n\nrepl_position_players &lt;- position_pa$key_mlbam[-(1:390)]\nrepl_pitchers &lt;- pitcher_pa$key_mlbam[-(1:360)]\n\ncat(\"Cut-off for position players:\", position_pa$n[390], \"\\n\")\ncat(\"Cut-off for pitchers:\", pitcher_pa$n[360], \"\\n\")\n\n\n1\n\nThis removes at-bats in which a pitcher is hitting\n\n2\n\nCounts the number of at-bats in which each position player is batting\n\n3\n\nArranges the counts in decreasing order so that we can determine replacement-level cut-offs\n\n4\n\nSince the vector pitchers is just the unique values of def_atbat2024$pitcher, there is no need to filter\n\n5\n\nGet the IDs of the position players and pitchers outside, respectively, the top 390 and 360 numbers of plate appearances\n\n\n\n\nCut-off for position players: 131 \nCut-off for pitchers: 204 \n\n\nUltimately, we wish to compare each player’s \\(RAA\\) to the \\(RAA\\) that would have been created had the player been replaced by a replacement-level player. To estimate this counter-factual \\(RAA\\), we will divide the total \\(RAA\\) produced by all replacement-level players by the total number of plate appearances..\n\nrepl_position_raa &lt;-\n  raa |&gt;\n  dplyr::filter(key_mlbam %in% repl_position_players) |&gt;\n1  dplyr::inner_join(y = position_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n)\n\nrepl_pitch_raa &lt;-\n  raa |&gt;\n  dplyr::filter(key_mlbam %in% repl_pitchers) |&gt;\n  dplyr::inner_join(y = pitcher_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n) \n\nrepl_avg_pos &lt;- sum(repl_position_raa$RAA)/sum(repl_position_raa$n)\nrepl_avg_pitch &lt;- sum(repl_pitch_raa$RAA)/sum(repl_pitch_raa$n)\n\n\n1\n\nUsing an inner join ensures that we only append the n values for replacement-level players\n\n\n\n\nWe see that the replacement-level RAA per-at-bat is 0 for position players and -0.02 for pitchers. Multiplying the the replacement-level per-at-bat RAA values by the number of plate appearances faced by each non-replacement-level player gives us an estimate of how each player’s replacement-level “shadow” would perform. Finally, using the heuristic of 10 runs per win, dividing the difference between the actual RAA and the RAA for the replacement-level shadow yields our wins above replacement for position players.\n\nposition_war &lt;-\n  raa |&gt;\n  dplyr::filter(!key_mlbam %in% repl_position_players) |&gt;\n  dplyr::inner_join(y = position_pa, by = \"key_mlbam\") |&gt; \n  dplyr::select(Name, key_mlbam, RAA, n) |&gt;\n  dplyr::mutate(shadowRAA = n * repl_avg_pos) |&gt;\n  dplyr::mutate(WAR = (RAA - shadowRAA)/10)\n\nposition_war |&gt;\n  dplyr::arrange(dplyr::desc(WAR)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 6\n   Name              key_mlbam   RAA     n shadowRAA   WAR\n   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Bobby Witt           677951 165.    694     -1.36 16.6 \n 2 Gunnar Henderson     683002 117.    702     -1.37 11.8 \n 3 Elly De La Cruz      682829 114.    679     -1.33 11.5 \n 4 Jose Ramirez         608070 113.    657     -1.28 11.5 \n 5 Zach Neto            687263 110.    590     -1.15 11.1 \n 6 Marcus Semien        543760 108.    701     -1.37 10.9 \n 7 Ketel Marte          606466 101.    562     -1.10 10.2 \n 8 Vladimir Guerrero    665489  99.0   671     -1.31 10.0 \n 9 Francisco Lindor     596019  98.5   689     -1.35  9.98\n10 Jose Altuve          514888  98.2   661     -1.29  9.95\n\n\nWe can run a similar calculation for pitchers.\n\npitch_war &lt;-\n  raa |&gt;\n  dplyr::filter(!key_mlbam %in% repl_pitchers) |&gt;\n  dplyr::inner_join(y = pitcher_pa, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA, n) |&gt;\n  dplyr::mutate(shadowRAA = n * repl_avg_pitch) |&gt;\n  dplyr::mutate(WAR = (RAA - shadowRAA)/10)\n\npitch_war |&gt;\n  dplyr::arrange(dplyr::desc(WAR)) |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 6\n   Name            key_mlbam   RAA     n shadowRAA   WAR\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Tarik Skubal       669373 17.2    730    -17.7   3.49\n 2 Chris Sale         519242 17.2    702    -17.0   3.42\n 3 Zack Wheeler       554430 10.1    759    -18.4   2.86\n 4 Paul Skenes        694973 14.6    495    -12.0   2.66\n 5 Garrett Crochet    676979  9.91   596    -14.5   2.44\n 6 Bryce Miller       682243  7.66   681    -16.5   2.42\n 7 Seth Lugo          607625  2.47   813    -19.7   2.22\n 8 Ryan Walker        676254 14.7    302     -7.33  2.21\n 9 Reynaldo Lopez     625643  8.82   542    -13.2   2.20\n10 Cade Smith         671922 14.2    289     -7.02  2.12",
    "crumbs": [
      "Lecture 8: Defensive creidt & replacement level"
    ]
  },
  {
    "objectID": "lectures/lecture08.html#footnotes",
    "href": "lectures/lecture08.html#footnotes",
    "title": "Lecture 8: Defensive creidt & replacement level",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe official position numberings are: 1 (pitcher), 2 (catcher), 3 (first baseman), 4 (second baseman), 5 (third baseman), 6 (shortstop), 7 (left fielder), 8 (center fielder), and 9 (right fielder).↩︎\nIf you’re keen to learn more about splines, consider taking STAT 351 (Introduction to Nonparametric Statistics). A lot of the mathematical theory underpinning smoothing splines was developed by Prof. Grace Wahba, who was a long-serving member of the faculty here.↩︎\nThe Cy Young Award is given to the best pitchers in the National League and American League.↩︎",
    "crumbs": [
      "Lecture 8: Defensive creidt & replacement level"
    ]
  },
  {
    "objectID": "guides/plot_vertical_statsbomb.html",
    "href": "guides/plot_vertical_statsbomb.html",
    "title": "Plotting StatsBomb Data Vertically",
    "section": "",
    "text": "In Lectures 2 and 3, we built several expected goals (xg) models using data provided by StatsBomb. StatsBomb pre-processes their raw tracking data so that attacking play is oriented from left to right. I find it somewhat more aesthetically pleasing to visualize shot data vertically with the goal line on top and the half-line at the bottom.\nThis page defines a function to transform StatsBomb shot location data so that it can be plotted vertically instead of horizontally. As you continue to work with StatsBomb data, feel free to use these functions.",
    "crumbs": [
      "Plotting StatsBomb Data Vertically"
    ]
  },
  {
    "objectID": "guides/plot_vertical_statsbomb.html#overview",
    "href": "guides/plot_vertical_statsbomb.html#overview",
    "title": "Plotting StatsBomb Data Vertically",
    "section": "",
    "text": "In Lectures 2 and 3, we built several expected goals (xg) models using data provided by StatsBomb. StatsBomb pre-processes their raw tracking data so that attacking play is oriented from left to right. I find it somewhat more aesthetically pleasing to visualize shot data vertically with the goal line on top and the half-line at the bottom.\nThis page defines a function to transform StatsBomb shot location data so that it can be plotted vertically instead of horizontally. As you continue to work with StatsBomb data, feel free to use these functions.",
    "crumbs": [
      "Plotting StatsBomb Data Vertically"
    ]
  },
  {
    "objectID": "guides/plot_vertical_statsbomb.html#a-digression-on-coordinate-systems",
    "href": "guides/plot_vertical_statsbomb.html#a-digression-on-coordinate-systems",
    "title": "Plotting StatsBomb Data Vertically",
    "section": "A digression on coordinate systems",
    "text": "A digression on coordinate systems\nStatsBomb converts their raw location data to a standardized pitch of dimensions 120y x 80y. Appendix 2 of their Open Data Specification document shows the standardized pitch along with several landmarks like the penalty area and goalkeeper’s box.\n\n\n\nStatsBomb Coordinate System\n\n\nNotice that the origin (i.e., the point with coordinates c(0,0)) is in the top-left corner so that the \\(x\\) coordinate increases as you more from left to right and the \\(y\\) coordinate increases as you move from top to bottom. This coordinate system is standard for 2D computer graphics1 and is often referred to as the “left-handed” coordinate system.\nWhen we build R graphics, we typically use the more conventional “right-handed” coordinate system in which the \\(y\\) coordinates increases when we move from bottom. For instance, to create a plot region with \\(0 \\leq x \\leq 120\\) and \\(0 \\leq y \\leq 80,\\) we might use something like the following code\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0)) # sets up margins for plot\nplot(1, type = \"n\", # signals that we want an empty plot\n     xlab = \"x\", ylab = \"y\", # labels axes\n     xlim = c(0,120), ylim = c(0,80))\n\n\n\n\n\n\n\n\nLuckily, there is a quick “fix”2 that allows us to generate plots in R using the StatsBomb coordinate system: instead of specifying ylim = c(0,80), we simply specify ylim = c(80,0). This new specification tells R that when we move from bottom-to-top, the \\(y\\) coordinate should go from 80 to 0.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\",\n     xlim = c(0, 120), ylim = c(80, 0),\n     xlab = \"x\", ylab = \"y\")\n\n\n\n\n\n\n\n\nNow that we have the coordinate system sorted, we will hard-code the coordinates several landmarks on the pitch. Doing so will help us determine an appropriate transformation of coordinates to produce a vertically oriented pitch. The main landmarks are the corners of the pitch; the corners of the left and right penalty boxes; the corners of the left and right goalkeepers’ areas; the corners of the left and right nets; the locations of the goal posts; and the half-circle.\n\n# corners of pitch\ntop_left_corner &lt;- c(0,0)\nbot_left_corner &lt;- c(0,80)\ntop_right_corner &lt;- c(120,0)\nbot_right_corner &lt;- c(120, 80)\n\n# endpoints of half-line\ntop_halfline &lt;- c(60,0)\nbot_halfline &lt;- c(60, 80)\n\n# corners of left penalty area\ntop_left_penl &lt;- c(0, 18)\nbot_left_penl &lt;- c(0,62)\ntop_right_penl &lt;- c(18,18)\nbot_right_penl &lt;- c(18,62)\n\n# corners of left goalkeeper area\ntop_left_gkl &lt;- c(0,30)\nbot_left_gkl &lt;- c(0,50)\ntop_right_gkl &lt;- c(6,30)\nbot_right_gkl &lt;- c(6,50)\n\n# left goal posts\ntop_postl &lt;- c(0,36)\nbot_postl &lt;- c(0,44)\n\n# corners of left net\ntop_left_netl &lt;- c(-2,36)\nbot_left_netl &lt;- c(-2, 44)\ntop_right_netl &lt;- c(0, 36)\nbot_right_netl &lt;- c(0,44)\n\n# corners of right penalty area\ntop_left_penr &lt;- c(102,18)\nbot_left_penr &lt;- c(102,62)\ntop_right_penr &lt;- c(120,18)\nbot_right_penr &lt;- c(120,62)\n# corners of right goal keeper's area\ntop_left_gkr &lt;- c(114,30)\nbot_left_gkr &lt;- c(114,50)\ntop_right_gkr &lt;- c(120,30)\nbot_right_gkr &lt;- c(120,50)\n# right goal posts\ntop_postl &lt;- c(120,36)\nbot_postl &lt;- c(120,44)\n# corners of left net\ntop_left_netr &lt;- c(120,36)\nbot_left_netr &lt;- c(120, 44)\ntop_right_netr &lt;- c(122, 36)\nbot_right_netr &lt;- c(122,44)\n\n# half-circles\nleft_halfcirc &lt;-\n  data.frame(x = 60 + 10*cos(seq(from = pi/2, to = 3*pi/2, length = 100)),\n             y = 40 - 10 * sin(seq(from = pi/2, to = 3*pi/2, length = 100)))\n\nright_halfcirc &lt;-\n  data.frame(x = 60 + 10*cos(seq(from =-pi/2, to = pi/2, length = 100)),\n             y = 40 - 10 * sin(seq(from = -pi/2, to = pi/2, length = 100)))\n\nWe can now add in the penalty and goalkeepers’ areas to the pitch using the rect function. This function draws a rectangle whose bottom left coordinate is c(xleft, ybottom) and whose top right coordinate is c(xright, ytop). We also add in the half line and circle\n\npar(mar = c(3,3,2,1), \n    mgp = c(1.8, 0.5, 0), \n    xpd = TRUE) # xpd allows plotting in margins\nplot(1, type = \"n\",\n     xlab = \"x\", ylab = \"y\",\n     xlim = c(0,120), ylim = c(80,0))\n# boundaries of pitch\nrect(xleft = bot_left_corner[1], ybottom = bot_left_corner[2], \n     xright = top_right_corner[1], ytop = top_right_corner[2])\n# left penalty area\nrect(xleft = bot_left_penl[1], ybottom = bot_left_penl[2],\n     xright = top_right_penl[1], ytop = top_right_penl[2])\n# left goalkeeper's area\nrect(xleft = bot_left_gkl[1], ybottom = bot_left_gkl[2],\n     xright = top_right_gkl[1], ytop = top_right_gkl[2])\n# left net\nrect(xleft = bot_left_netl[1], ybottom = bot_left_netl[2],\n     xright = top_right_netl[1], ytop = top_right_netl[2])\n\n# right penalty area\nrect(xleft = bot_left_penr[1], ybottom = bot_left_penr[2],\n     xright = top_right_penr[1], ytop = top_right_penr[2])\n# right goalkeeper's area\nrect(xleft = bot_left_gkr[1], ybottom = bot_left_gkr[2],\n     xright = top_right_gkr[1], ytop = top_right_gkr[2])\n# right net\nrect(xleft = bot_left_netr[1], ybottom = bot_left_netr[2],\n     xright = top_right_netr[1], ytop = top_right_netr[2])\n# half-line\nlines(x = c(top_halfline[1], bot_halfline[1]),\n      y = c(top_halfline[2], bot_halfline[2]))\n# left half-circle \nlines(x = left_halfcirc$x, y = left_halfcirc$y)\n# right half-circle\nlines(x = right_halfcirc$x, y = right_halfcirc$y)",
    "crumbs": [
      "Plotting StatsBomb Data Vertically"
    ]
  },
  {
    "objectID": "guides/plot_vertical_statsbomb.html#translation-and-rotation",
    "href": "guides/plot_vertical_statsbomb.html#translation-and-rotation",
    "title": "Plotting StatsBomb Data Vertically",
    "section": "Translation and Rotation",
    "text": "Translation and Rotation\nWe will form a vertical pitch layout by composing the following three transformations:\n\nTranslate the pitch so the origin is at mid-field, which is located at \\((60,40).\\) Mathematically, the transformation is \\((x,y) \\rightarrow (x-60, y - 40)\\)\nRotate the pitch so that (i) the bottom right corner goes to the top right; (ii) the top right corner goes to the top left; (iii) the top left corner goes to the bottom left; and (iv) the bottom left corner goes to the bottom right. Mathematically, the transformation is \\((x,y) \\rightarrow (y, -x)\\)3.\nTranslate the rotated frame so that the origin is in the top left corner. Mathematically, the transformation is \\((x,y) \\rightarrow (x+40,y+60).\\)\n\nPutting these steps together, we have the transformation \\[\n(x,y) \\rightarrow (x-60, y-40) \\rightarrow (y-40, 60-x) \\rightarrow (y,120-x)\n\\] It will be useful to define functions for these transformations\n\ntransform_x &lt;- function(x,y){return(y)}\ntransform_y &lt;- function(x,y){return(120-x)}\n\nRecall that the function rect draws a rectangle using the coordinates of its bottom left and top right corners. We can compute the coordinates of the bottom left (resp. top right) corner of a rectangle in a vertical orientation by transforming the coordinates of the top left (resp. bottom right) corners of a rectangle in the original horizontal orientation.\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(1, type = \"n\",\n     xlim = c(0, 80), ylim = c(120,0), # note the different limits!\n     xlab = \"x\", ylab = \"y\")\n# boundaries of pitch\nrect(xleft = transform_x(top_left_corner[1], top_left_corner[2]),\n     ybottom = transform_y(top_left_corner[1], top_left_corner[2]),\n     xright = transform_x(bot_right_corner[1], bot_right_corner[2]),\n     ytop = transform_y(bot_right_corner[1], bot_right_corner[2]))\n# original left penalty area now on bottom\nrect(xleft = transform_x(top_left_penl[1], top_left_penl[2]),\n     ybottom = transform_y(top_left_penl[1], top_left_penl[2]),\n     xright = transform_x(bot_right_penl[1], bot_right_penl[2]),\n     ytop = transform_y(bot_right_penl[1], bot_right_penl[2]))\n# original left goalkeeper's area now on bottom\nrect(xleft = transform_x(top_left_gkl[1], top_left_gkl[2]),\n     ybottom = transform_y(top_left_gkl[1], top_left_gkl[2]),\n     xright = transform_x(bot_right_gkl[1], bot_right_gkl[2]),\n     ytop = transform_y(bot_right_gkl[1], bot_right_gkl[2]))\n# original left net now on bottom\nrect(xleft = transform_x(top_left_netl[1], top_left_netl[2]),\n     ybottom = transform_y(top_left_netl[1], top_left_netl[2]),\n     xright = transform_x(bot_right_netl[1], bot_right_netl[2]),\n     ytop = transform_y(bot_right_netl[1], bot_right_netl[2]))\n\n# original right penalty area now on top\nrect(xleft = transform_x(top_left_penr[1], top_left_penr[2]),\n     ybottom = transform_y(top_left_penr[1], top_left_penr[2]),\n     xright = transform_x(bot_right_penr[1], bot_right_penr[2]),\n     ytop = transform_y(bot_right_penr[1], bot_right_penr[2]))\n# original right goalkeeper's area now on top\nrect(xleft = transform_x(top_left_gkr[1], top_left_gkr[2]),\n     ybottom = transform_y(top_left_gkr[1], top_left_gkr[2]),\n     xright = transform_x(bot_right_gkr[1], bot_right_gkr[2]),\n     ytop = transform_y(bot_right_gkr[1], bot_right_gkr[2]))\n# original right net now on bottom\nrect(xleft = transform_x(top_left_netr[1], top_left_netr[2]),\n     ybottom = transform_y(top_left_netr[1], top_left_netr[2]),\n     xright = transform_x(bot_right_netr[1], bot_right_netr[2]),\n     ytop = transform_y(bot_right_netr[1], bot_right_netr[2]))\n\n# half-line\nlines(x = transform_x( c(top_halfline[1], bot_halfline[1]), c(top_halfline[2], bot_halfline[2])),\n      y = transform_y( c(top_halfline[1], bot_halfline[1]), c(top_halfline[2], bot_halfline[2])))\n# original left half-circle now on bottom\nlines(x = transform_x(left_halfcirc$x, left_halfcirc$y),\n      y = transform_y(left_halfcirc$x, left_halfcirc$y))\n# original right half-circle now on top\nlines(x = transform_x(right_halfcirc$x, right_halfcirc$y),\n      y = transform_y(right_halfcirc$x, right_halfcirc$y))\n\n\n\n\n\n\n\n\nFor convenience, we can write a function that plots either the full pitch or the attacking in either the horizontal or vertical orientation. The function plot_pitch has two arguments:\n\nhalf: Set half = TRUE to plot the attacking half and half = FALSE to plot the full pitch. Default is TRUE\nvertical: Set vertical = TRUE to plot in a vertical orientation and vertical = FALSE to plot in a horizontal orientation. Default is TRUE\n\nYou can download an R script implementing this function from this link. If you save that script in your course or project repository, you can source it as needed. You can also unfold the next code block to see how the function plot_pitch is implemented.\n\n\nCode\nplot_pitch &lt;- function(half = TRUE, vertical = TRUE){\n  par(mar = c(2,1,4,1), # lower left & right margins but increase top margin\n      mgp = c(1.8, 0.5, 0), \n      xpd = TRUE) # allows plotting in the margin\n  if(vertical){\n    # plot vertical pitch\n    if(half){\n      # only plot the attacking half\n      plot(1, type = \"n\",\n           xlim = c(0,80), ylim = c(60,0),\n           xaxt = \"n\", yaxt = \"n\", # suppresses axis marks\n           xlab = NA, ylab = NA, # suppress labels\n           bty = \"n\") # suppresses the bounding box\n      # pitch\n      rect(xleft = transform_x(top_halfline[1], top_halfline[2]),\n           ybottom = transform_y(top_halfline[1], top_halfline[2]),\n           xright = transform_x(bot_right_corner[1], bot_right_corner[2]),\n           ytop = transform_y(bot_right_corner[1], bot_right_corner[2]))\n      # original right penalty area now on top\n      rect(xleft = transform_x(top_left_penr[1], top_left_penr[2]),\n           ybottom = transform_y(top_left_penr[1], top_left_penr[2]),\n           xright = transform_x(bot_right_penr[1], bot_right_penr[2]),\n           ytop = transform_y(bot_right_penr[1], bot_right_penr[2]))\n      # original right goalkeeper's area now on top\n      rect(xleft = transform_x(top_left_gkr[1], top_left_gkr[2]),\n           ybottom = transform_y(top_left_gkr[1], top_left_gkr[2]),\n           xright = transform_x(bot_right_gkr[1], bot_right_gkr[2]),\n           ytop = transform_y(bot_right_gkr[1], bot_right_gkr[2]))\n      # original right net now on top\n      rect(xleft = transform_x(top_left_netr[1], top_left_netr[2]),\n           ybottom = transform_y(top_left_netr[1], top_left_netr[2]),\n           xright = transform_x(bot_right_netr[1], bot_right_netr[2]),\n           ytop = transform_y(bot_right_netr[1], bot_right_netr[2]))\n      # half-circle\n      lines(x = transform_x(right_halfcirc$x, right_halfcirc$y),\n            y = transform_y(right_halfcirc$x, right_halfcirc$y))\n    } else{\n      # plot the full field\n      plot(1, type = \"n\",\n           xlim = c(0, 80), ylim = c(120,0), \n           xaxt = \"n\", yaxt = \"n\", # suppresses axis marks\n           xlab = NA, ylab = NA, # suppress labels\n           bty = \"n\") # suppresses the bounding box\n      # boundaries of pitch\n      rect(xleft = transform_x(top_left_corner[1], top_left_corner[2]),\n           ybottom = transform_y(top_left_corner[1], top_left_corner[2]),\n           xright = transform_x(bot_right_corner[1], bot_right_corner[2]),\n           ytop = transform_y(bot_right_corner[1], bot_right_corner[2]))\n      # original left penalty area now on bottom\n      rect(xleft = transform_x(top_left_penl[1], top_left_penl[2]),\n           ybottom = transform_y(top_left_penl[1], top_left_penl[2]),\n           xright = transform_x(bot_right_penl[1], bot_right_penl[2]),\n           ytop = transform_y(bot_right_penl[1], bot_right_penl[2]))\n      # original left goalkeeper's area now on bottom\n      rect(xleft = transform_x(top_left_gkl[1], top_left_gkl[2]),\n           ybottom = transform_y(top_left_gkl[1], top_left_gkl[2]),\n           xright = transform_x(bot_right_gkl[1], bot_right_gkl[2]),\n           ytop = transform_y(bot_right_gkl[1], bot_right_gkl[2]))\n      # original left net now on bottom\n      rect(xleft = transform_x(top_left_netl[1], top_left_netl[2]),\n           ybottom = transform_y(top_left_netl[1], top_left_netl[2]),\n           xright = transform_x(bot_right_netl[1], bot_right_netl[2]),\n           ytop = transform_y(bot_right_netl[1], bot_right_netl[2]))\n      # original right penalty area now on top\n      rect(xleft = transform_x(top_left_penr[1], top_left_penr[2]),\n           ybottom = transform_y(top_left_penr[1], top_left_penr[2]),\n           xright = transform_x(bot_right_penr[1], bot_right_penr[2]),\n           ytop = transform_y(bot_right_penr[1], bot_right_penr[2]))\n\n      # original right goalkeeper's area now on top\n      rect(xleft = transform_x(top_left_gkr[1], top_left_gkr[2]),\n           ybottom = transform_y(top_left_gkr[1], top_left_gkr[2]),\n           xright = transform_x(bot_right_gkr[1], bot_right_gkr[2]),\n           ytop = transform_y(bot_right_gkr[1], bot_right_gkr[2]))\n      # original right net now on bottom\n      rect(xleft = transform_x(top_left_netr[1], top_left_netr[2]),\n           ybottom = transform_y(top_left_netr[1], top_left_netr[2]),\n           xright = transform_x(bot_right_netr[1], bot_right_netr[2]),\n           ytop = transform_y(bot_right_netr[1], bot_right_netr[2]))\n      # half-line\n      lines(x = transform_x( c(top_halfline[1], bot_halfline[1]), c(top_halfline[2],bot_halfline[2])),\n            y = transform_y( c(top_halfline[1], bot_halfline[1]), c(top_halfline[2], bot_halfline[2])))\n      # original left half-circle now on bottom\n      lines(x = transform_x(left_halfcirc$x, left_halfcirc$y),\n            y = transform_y(left_halfcirc$x, left_halfcirc$y))\n      # original right half-circle now on top\n      lines(x = transform_x(right_halfcirc$x, right_halfcirc$y),\n            y = transform_y(right_halfcirc$x, right_halfcirc$y))\n    }# closes if/else checking whether to plot attacking half\n  } else{\n    # plot horizontal pitch\n    if(half){\n      plot(1, type = \"n\",\n           xlim = c(60,120), ylim = c(80, 0),\n           xaxt = \"n\", yaxt = \"n\", # suppresses axis marks\n           xlab = NA, ylab = NA,# suppresses labels\n           bty = \"n\") # suppresses the bounding box\n      # boundaries of attacking half\n      rect(xleft = bot_halfline[1], ybottom = bot_halfline[2], \n           xright = top_right_corner[1], ytop = top_right_corner[2])\n      # right penalty area\n      rect(xleft = bot_left_penr[1], ybottom = bot_left_penr[2],\n           xright = top_right_penr[1], ytop = top_right_penr[2])\n      # right goalkeeper's area\n      rect(xleft = bot_left_gkr[1], ybottom = bot_left_gkr[2],\n           xright = top_right_gkr[1], ytop = top_right_gkr[2])\n      # right net\n      rect(xleft = bot_left_netr[1], ybottom = bot_left_netr[2],\n           xright = top_right_netr[1], ytop = top_right_netr[2])\n      # right half-circle\n      lines(x = right_halfcirc$x, y = right_halfcirc$y)\n    } else{\n      plot(1, type = \"n\",\n           xlim = c(0,120), ylim = c(80,0),\n           xaxt = \"n\", yaxt = \"n\", # suppresses axis marks\n           xlab = NA, ylab = NA, # suppress labels\n           bty = \"n\") # suppresses the bounding box\n      # boundaries of pitch\n      rect(xleft = bot_left_corner[1], ybottom = bot_left_corner[2], \n           xright = top_right_corner[1], ytop = top_right_corner[2])\n      # left penalty area\n      rect(xleft = bot_left_penl[1], ybottom = bot_left_penl[2],\n           xright = top_right_penl[1], ytop = top_right_penl[2])\n      # left goalkeeper's area\n      rect(xleft = bot_left_gkl[1], ybottom = bot_left_gkl[2],\n           xright = top_right_gkl[1], ytop = top_right_gkl[2])\n      # left net\n      rect(xleft = bot_left_netl[1], ybottom = bot_left_netl[2],\n           xright = top_right_netl[1], ytop = top_right_netl[2])\n      # right penalty area\n      rect(xleft = bot_left_penr[1], ybottom = bot_left_penr[2],\n           xright = top_right_penr[1], ytop = top_right_penr[2])\n      # right goalkeeper's area\n      rect(xleft = bot_left_gkr[1], ybottom = bot_left_gkr[2],\n           xright = top_right_gkr[1], ytop = top_right_gkr[2])\n      # right net\n      rect(xleft = bot_left_netr[1], ybottom = bot_left_netr[2],\n           xright = top_right_netr[1], ytop = top_right_netr[2])\n      # half-line\n      lines(x = c(top_halfline[1], bot_halfline[1]),\n            y = c(top_halfline[2], bot_halfline[2]))\n      # left half-circle \n      lines(x = left_halfcirc$x, y = left_halfcirc$y)\n      # right half-circle\n      lines(x = right_halfcirc$x, y = right_halfcirc$y)\n    }\n  } # closes if/else checking whether to plot horizontally or vertically\n  \n}",
    "crumbs": [
      "Plotting StatsBomb Data Vertically"
    ]
  },
  {
    "objectID": "guides/plot_vertical_statsbomb.html#example-visualizing-beth-meads-goals-at-euro-2022",
    "href": "guides/plot_vertical_statsbomb.html#example-visualizing-beth-meads-goals-at-euro-2022",
    "title": "Plotting StatsBomb Data Vertically",
    "section": "Example: Visualizing Beth Mead’s goals at EURO 2022",
    "text": "Example: Visualizing Beth Mead’s goals at EURO 2022\nWe’ll use our new function plot_pitch() to illustrate all 5 of Beth Mead’s goals at EURO 2022.\n\nlibrary(tidyverse)\noi_colors &lt;- \n  palette.colors(palette = \"Okabe-Ito\")\n\nThe code in the next block, which is very similar to that used in Lecture 2, does the following things:\n\nLoads & preprocesses the event data from EURO 2022 (competition_id = 53 and season_id = 106)\nFilters to all goals scored by Beth Mead.\nSelects only the starting and ending locations of the shots (in the horizontal orientation)\nComputes the starting and ending coordinates in the vertical orientation of each shot\n\nWe will first plot the goals on the full horizontal pitch\n\nplot_pitch(half = FALSE, vertical = FALSE)\nfor(i in 1:nrow(mead_goals)){\n  lines(x = c(mead_goals$location.x[i], mead_goals$shot.end_location.x[i]),\n        y = c(mead_goals$location.y[i], mead_goals$shot.end_location.y[i]),\n         col = oi_colors[i+1])\n}\nmtext(\"Beth Mead Goals (EURO 2022)\", side = 3, line = 1)\nmtext(\"Created with free data from StatsBomb\\n https://github.com/statsbomb/open-data\",\n      side = 1, line = 1)\n\n\n\n\n\n\n\n\nAnd here are the same goals but in the attacking half in a vertical orientation\n\nplot_pitch(half = TRUE, vertical = TRUE)\nfor(i in 1:nrow(mead_goals)){\n  lines(x = c(mead_goals$vert_location.x[i], mead_goals$vert_shot.end_location.x[i]),\n        y = c(mead_goals$vert_location.y[i], mead_goals$vert_shot.end_location.y[i]),\n         col = oi_colors[i+1])\n}\nmtext(\"Beth Mead Goals (EURO 2022)\", side = 3, line = 1)\nmtext(\"Created with free data from StatsBomb\\n https://github.com/statsbomb/open-data\",\n      side = 1, line = 1)",
    "crumbs": [
      "Plotting StatsBomb Data Vertically"
    ]
  },
  {
    "objectID": "guides/plot_vertical_statsbomb.html#footnotes",
    "href": "guides/plot_vertical_statsbomb.html#footnotes",
    "title": "Plotting StatsBomb Data Vertically",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee here for the gory details or check out the Wikipedia entry on 2D graphics.↩︎\nI found this solution here after Googling “R change origin top left”.↩︎\nCounter-clockwise rotations in the usual right-handed coordinate system (with \\(y\\) increasing from bottom-to-top) are clockwise rotations in the left-handed coordinate system we are using, in which \\(y\\) increases from top-to-bottom. We want to rotate 90 degrees counter-clockwise in the right-handed system, which is equivalent to a 270 degree clockwise rotation in the left-handed system. See this Wikipedia entry for more details.↩︎",
    "crumbs": [
      "Plotting StatsBomb Data Vertically"
    ]
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lecture Notes",
    "section": "",
    "text": "Lecture notes will be posted here."
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Guides",
    "section": "",
    "text": "In this section, you will find pages containing additional background information about the methods and datasets introduced in Lecture. You will also find much more comprehensive code and guides for fitting the relevant models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "",
    "text": "Welcome to STAT 479 (Special Topics in Statistics)! This iteration of the course will focus on sports analytics.\nLectures notes, instructions for the course project, and additional tutorials and exercises will be posted to this website. So, please bookmark this page and check it regularly throughout the course."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "STAT 479 (Fall 2025): Sports Analytics",
    "section": "Course information",
    "text": "Course information\nDescription: Illustrates the use of statistical modeling and data science techniques to derive actionable insights from sports data. Emphasizes not only technical calculation of advanced metrics but also on written and oral communication to other data scientists and to non-technical audience. Topics may include: deriving team rankings from paired competitions; measuring an individual player’s contribution to their team’s overall success; assessing player performance and team strategy in terms of expected outcomes; forecasting the impact of new rule changes using simulation; and creating new metrics using high-resolution player tracking data.\nLearning Outcomes: Throughout the course you will\n\nImplement appropriate statistical methods to assess player and team performance\nWork with play-by-play and high-resolution tracking data\nProvide constructive and actionable feedback on your peers’ analytic reports\nBuild a personal portfolio of sports data analyses\n\nLocation & Schedule: Tuesdays & Thursdays, 11:00am-12:15pm, 1524 Morgridge Hall\nInstructor & Office Hours: Sameer Deshpande (sameer.deshpande@wisc.edu). Office Hours TBA."
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Contains additional exercises to reproduce and extend analyses shown in lecture."
  },
  {
    "objectID": "exercises/exercises1_boxscore.html",
    "href": "exercises/exercises1_boxscore.html",
    "title": "Constructing Advanced Metrics Using Box Score Data",
    "section": "",
    "text": "We will rely on data from the Lahman Database.\n\n\n\nif(!\"Lahman\" %in% rownames(installed.packages())){\n  message(\"Package `Lahman' not already installed. Installing now\")\n  install.packages(\"Lahman\")\n} else{\n  library(Lahman)\n}",
    "crumbs": [
      "Constructing Advanced Metrics Using Box Score Data"
    ]
  },
  {
    "objectID": "exercises/exercises1_boxscore.html#setup-installing-the-lahman-package",
    "href": "exercises/exercises1_boxscore.html#setup-installing-the-lahman-package",
    "title": "Constructing Advanced Metrics Using Box Score Data",
    "section": "",
    "text": "if(!\"Lahman\" %in% rownames(installed.packages())){\n  message(\"Package `Lahman' not already installed. Installing now\")\n  install.packages(\"Lahman\")\n} else{\n  library(Lahman)\n}",
    "crumbs": [
      "Constructing Advanced Metrics Using Box Score Data"
    ]
  },
  {
    "objectID": "guides/getting_started.html",
    "href": "guides/getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "This course will make extensive use of the R programming language through the RStudio integrated development environment (IDE). Because the formal pre-requisites for this course are STAT 333 or 340, you are expected to have previous experience using the R programming language.\nThe course will also use version control (using git and GitHub) and Quarto for publishing the results of your analyses. This page contains information",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "guides/getting_started.html#computing-r-rstudio",
    "href": "guides/getting_started.html#computing-r-rstudio",
    "title": "Getting Started",
    "section": "Computing (R & RStudio)",
    "text": "Computing (R & RStudio)\n\n\n\n\n\n\nWarning\n\n\n\nI will assume fluency with basic R functionality (e.g., assignment, writing and executing scripts, saving data objects, setting environments, installing and loading packages), data manipulation with dplyr and other tidyverse packages, and visualization using either base R graphics or ggplot2. I will additionally assume some familiarity with fitting statistical models in R and interpreting their output (e.g., using lm and glm).\nIf you do not meet the formal course prerequisites and/or have not used R in a previous course, this is not the right course for you.\n\n\n\nAdditional R resources\nHaving issued that warning (prereq-warning?), you will see some new R functionality in the course. As the focus is on answer sports problems, we will not spend significant classtime going over new functions, packages, or techniques. If you find that there are gaps in your R knowledge, you are expected to fill them on your own time. Here are some helpful resources\n\nR for Data Science\nData Science: A first introduction\n\n\n\nInstallation\nWhile you are expected to have used R in previous courses (see warning), I strongly recommend installing the latest version of both R and RStudio at the beginning of the course. As of the time of this writing, that is R version 4.5 and RStudio version 2025.05.\nYou can download a version of R specific to for your operating system from this website. After install R, you should download and re-install RStudio from this website.\n\n\n\n\n\n\nTip\n\n\n\nWhenever you update your version of R, you need to re-install the packages; this is a perennial source of frustration for many R users] and some good-natured humor from others (who also manually re-installs packages after every update!)\n\n\n\n\nRequired Packages\nThroughout the course, we will make extensive use of several packages in the tidyverse, primarily for data loading, pre-processing, and manipulation. We will also make extensive use of the packages glmnet, ranger, and xgboost for model fitting. We will occasionally also use ggplot2 for creating visualizations.\nAs the course progresses, we will introduce and install new package as required. For the most part, these packages will be specific to a particular sport. Every package that we will use in this class is available through either (i) the Comprehensive R Archive Network (CRAN) or (ii) a public GitHub repository maintained by the packager developer. We typically install CRAN packages using the install.packages() command. To install packages hosted on GitHub, we will use the install_github function in the devtools package (which itself is available on CRAN)\n\n\n\n\n\n\nBase packages\n\n\n\nPrior to Lecture 2, please make sure you have installed the tidyverse packages as well as devtools, ggplot2, glmnet, ranger, and xgboost.\n\ninstall.packages(c(\"devtools\", \"tidyverse\", \"ggplot2\", \"glmnet\", \"ranger\", \"xgboost\"))\n\n\n\n\n\nColorblind-friendly graphics\nI am especially partial to the Okabe-Ito color palette. Throughout the course notes, you will see snippets like\n\noi_colors &lt;-\n  palette.colors(palette = \"Okabe-Ito\")\n\nin which we explicitly create a vector holding colors from this color palette.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "guides/getting_started.html#version-control-git-github",
    "href": "guides/getting_started.html#version-control-git-github",
    "title": "Getting Started",
    "section": "Version control (Git & GitHub)",
    "text": "Version control (Git & GitHub)\n\n\nAdditional Resources",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "guides/getting_started.html#quarto",
    "href": "guides/getting_started.html#quarto",
    "title": "Getting Started",
    "section": "Quarto",
    "text": "Quarto\n\nInstallation\n\n\nPublishing",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#multi-level-models",
    "href": "lectures/lecture09.html#multi-level-models",
    "title": "Lecture 9: WAR for Football",
    "section": "Multi-level models",
    "text": "Multi-level models\n\n\nMotivation:\nrouped data, when we fit linear models there is an assumption about independence across observations In the context of our football data, there’s actually quite a bit of grouping structure: play-to-play, it is not likely that the observations are similarly independent as they involve a lot of the same players. If a player is very skilled, we would expect the outcome to be high.",
    "crumbs": [
      "Lecture 9: WAR for Football"
    ]
  },
  {
    "objectID": "lectures/lecture09.html#a-more-elaborate-multi-level-model",
    "href": "lectures/lecture09.html#a-more-elaborate-multi-level-model",
    "title": "Lecture 9: WAR for Football",
    "section": "A more elaborate multi-level model",
    "text": "A more elaborate multi-level model\nUp to this point, we fit models that gave all credit to the quarterback. This is patently absurd.\nWe need to also allocate credit to recievers.\nAnd we need to adjust for many many covariates",
    "crumbs": [
      "Lecture 9: WAR for Football"
    ]
  },
  {
    "objectID": "lectures/lecture10.html",
    "href": "lectures/lecture10.html",
    "title": "Lecture 10: nflWAR II",
    "section": "",
    "text": "Last lecture, we introduce two-level models.\nOur plan will be to compute the player’s average effect (their “individual points added (iPA)”) for three different types of offensive production: rushing players, passing plays (air yards), and passing plays (yards after catch)",
    "crumbs": [
      "Lecture 10: nflWAR II"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#recap-overview",
    "href": "lectures/lecture10.html#recap-overview",
    "title": "Lecture 10: nflWAR II",
    "section": "",
    "text": "Last lecture, we introduce two-level models.\nOur plan will be to compute the player’s average effect (their “individual points added (iPA)”) for three different types of offensive production: rushing players, passing plays (air yards), and passing plays (yards after catch)",
    "crumbs": [
      "Lecture 10: nflWAR II"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#passing-model-1-air-yards",
    "href": "lectures/lecture10.html#passing-model-1-air-yards",
    "title": "Lecture 10: nflWAR II",
    "section": "Passing Model 1: Air yards",
    "text": "Passing Model 1: Air yards",
    "crumbs": [
      "Lecture 10: nflWAR II"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#passing-model-2-yards-after-catch",
    "href": "lectures/lecture10.html#passing-model-2-yards-after-catch",
    "title": "Lecture 10: nflWAR II",
    "section": "Passing Model 2: Yards after Catch",
    "text": "Passing Model 2: Yards after Catch",
    "crumbs": [
      "Lecture 10: nflWAR II"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#rushing-model",
    "href": "lectures/lecture10.html#rushing-model",
    "title": "Lecture 10: nflWAR II",
    "section": "Rushing Model",
    "text": "Rushing Model",
    "crumbs": [
      "Lecture 10: nflWAR II"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#individual-points-above-average-replacement-level",
    "href": "lectures/lecture10.html#individual-points-above-average-replacement-level",
    "title": "Lecture 10: nflWAR II",
    "section": "Individual Points Above Average & Replacement Level",
    "text": "Individual Points Above Average & Replacement Level\n\nComputing a player’s iPAA\n\n\n\nPosition-specific replacement levels",
    "crumbs": [
      "Lecture 10: nflWAR II"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#from-points-to-wins",
    "href": "lectures/lecture10.html#from-points-to-wins",
    "title": "Lecture 10: nflWAR II",
    "section": "From points to wins",
    "text": "From points to wins",
    "crumbs": [
      "Lecture 10: nflWAR II"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#ridge-regression",
    "href": "lectures/lecture05.html#ridge-regression",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Ridge Regression",
    "text": "Ridge Regression",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#predictive-comparison",
    "href": "lectures/lecture05.html#predictive-comparison",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Predictive Comparison",
    "text": "Predictive Comparison",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture05.html#weighted-rapm",
    "href": "lectures/lecture05.html#weighted-rapm",
    "title": "Lecture 5: Regularized Adjusted Plus/Minus",
    "section": "Weighted RAPM",
    "text": "Weighted RAPM",
    "crumbs": [
      "Lecture 5: Regularized Adjusted Plus/Minus"
    ]
  },
  {
    "objectID": "lectures/lecture07.html",
    "href": "lectures/lecture07.html",
    "title": "Lecture 7: Wins Above Replacement I",
    "section": "",
    "text": "Last lecture, we compute the run value of each at-bat in the 2024 MLB regular season. Run value is the sum of (i) the number of runs scored in the at-bat and (ii) the change in the number of runs the batting team is expected to score in the remainder of the half-inning. This change in expected runs is driven by the change in the combination of the number of outs and baserunner configuration. At the end of last lecture, we ranked players based on their run value totals, aggregating over all their at-bats. While the resulting rankings did appear to pass the “eye test” — both Aaron Judge and Shohei Ohtani created some of the largest run values — the metric implicitly gives batters all the credit for creating run value.\nOver the next two lectures, we will develop our own version of wins above replacement. Our development largely follows that of Baumer, Jensen, and Matthews (2015) but with some important differences.\n\n\nThe central idea — what (Baumer, Jensen, and Matthews 2015) call the “conservation of runs” framework — is that if the batting team gains \\(\\delta_{i}\\) units of run value during an at-bat, the fielding team gains \\(-\\delta_{i}\\) units of run value during that same at-bat. In this lecture, we will apportion \\(\\delta_{i}\\) between the batters and the baserunners involved in at-bat \\(i\\). Next lecture, we will apportion \\(-\\delta_{i}\\) between the pitcher and fielders involved in at-bat \\(i.\\)",
    "crumbs": [
      "Lecture 7: Wins Above Replacement I"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#recap-roadmap",
    "href": "lectures/lecture07.html#recap-roadmap",
    "title": "Lecture 7: Wins Above Replacement I",
    "section": "",
    "text": "Last lecture, we compute the run value of each at-bat in the 2024 MLB regular season. Run value is the sum of (i) the number of runs scored in the at-bat and (ii) the change in the number of runs the batting team is expected to score in the remainder of the half-inning. This change in expected runs is driven by the change in the combination of the number of outs and baserunner configuration. At the end of last lecture, we ranked players based on their run value totals, aggregating over all their at-bats. While the resulting rankings did appear to pass the “eye test” — both Aaron Judge and Shohei Ohtani created some of the largest run values — the metric implicitly gives batters all the credit for creating run value.\nOver the next two lectures, we will develop our own version of wins above replacement. Our development largely follows that of Baumer, Jensen, and Matthews (2015) but with some important differences.\n\n\nThe central idea — what (Baumer, Jensen, and Matthews 2015) call the “conservation of runs” framework — is that if the batting team gains \\(\\delta_{i}\\) units of run value during an at-bat, the fielding team gains \\(-\\delta_{i}\\) units of run value during that same at-bat. In this lecture, we will apportion \\(\\delta_{i}\\) between the batters and the baserunners involved in at-bat \\(i\\). Next lecture, we will apportion \\(-\\delta_{i}\\) between the pitcher and fielders involved in at-bat \\(i.\\)",
    "crumbs": [
      "Lecture 7: Wins Above Replacement I"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#data-preparation",
    "href": "lectures/lecture07.html#data-preparation",
    "title": "Lecture 7: Wins Above Replacement I",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo divide up offensive run value, we need to cretae a data table whose rows correspond to individual at-bats. This data table must, at a minimum, contain the starting and ending outs and baserunner configurations as well as the identities of the baserunners at the start and end of the at-bat. We will also want to include the columns event and des, which record the events and a narrative description of what happened in the at-bat.\n\nload(\"statcast2024.RData\")\nload(\"runValue2024.RData\")\nraw_atbat2024 &lt;- \n  statcast2024 |&gt;\n1  dplyr::group_by(game_pk, inning, inning_topbot) |&gt;\n  dplyr::arrange(at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(\n2    next_Outs = dplyr::lead(Outs),\n    next_BaseRunner = dplyr::lead(BaseRunner),\n    next_on_1b = dplyr::lead(on_1b),\n    next_on_2b = dplyr::lead(on_2b),\n    next_on_3b = dplyr::lead(on_3b)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::group_by(game_pk, at_bat_number) |&gt;\n  dplyr::arrange(pitch_number) |&gt;\n  dplyr::mutate(\n3    end_Outs = dplyr::last(next_Outs),\n    end_BaseRunner = dplyr::last(next_BaseRunner), # \n4    end_on_1b = dplyr::last(next_on_1b),\n    end_on_2b = dplyr::last(next_on_2b), \n    end_on_3b = dplyr::last(next_on_3b),  \n5    end_events = dplyr::last(events)) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::filter(pitch_number == 1) |&gt;\n  dplyr::arrange(game_date, game_pk, at_bat_number, pitch_number) |&gt;\n  dplyr::mutate(end_bat_score = bat_score + RunsScored, end_fld_score = fld_score,\n         end_Outs = ifelse(is.na(end_Outs), 3, end_Outs)) |&gt;\n  dplyr::select(game_date, game_pk, at_bat_number, inning, inning_topbot,\n         Outs, BaseRunner, batter, on_1b, on_2b, on_3b, bat_score, fld_score, \n         end_Outs, end_BaseRunner, end_on_1b, end_on_2b, end_on_3b, end_bat_score, end_fld_score,\n         end_events, des) |&gt;\n  dplyr::inner_join(y = runValue2024, by = c(\"game_pk\", \"at_bat_number\"))\n\n\n1\n\nDivide by game and half-inning\n\n2\n\nGets the value of several variables from the next pitch in the half-inning\n\n3\n\nGoes to last pitch of each at bat and gets next value of variable. That is, the starting value of the first pitch in the next at-bat.\n\n4\n\nFor instance, this looks up who’s on first at end of the current at-bat/start of the next at-bat.\n\n5\n\nThe variable events tells us what happened during the plate-appearance\n\n\n\n\n\nFilling in missing events\nThe column end_events in our data table raw_atbat2024 records what happened as a result of the at-bat. There are 308 rows with a missing entry.\n\ntable(raw_atbat2024$end_events)\n\n\n                                     catcher_interf                    double \n                      308                        97                      7608 \n              double_play               field_error                 field_out \n                      336                      1093                     72233 \n          fielders_choice       fielders_choice_out                 force_out \n                      373                       306                      3408 \ngrounded_into_double_play              hit_by_pitch                  home_run \n                     3152                      1977                      5326 \n                 sac_bunt                   sac_fly       sac_fly_double_play \n                      446                      1222                        13 \n                   single                 strikeout     strikeout_double_play \n                    25363                     40145                       107 \n                   triple               triple_play              truncated_pa \n                      685                         1                       304 \n                     walk \n                    14029 \n\n\nThe column des includes a much more detailed description of what happened during the plate appearance. A cursory look through the values of des corresponding to rows with missing end_events reveals that several of these at-bats ended with a walk, involved an automatic strike1, or an inning-ending pick off2\n\nraw_atbat2024 |&gt;\n  dplyr::filter(end_events == \"\") |&gt;\n  dplyr::slice_head(n = 15) |&gt;\n  dplyr::select(Outs, end_Outs, des)\n\n# A tibble: 15 × 3\n    Outs end_Outs des                                                           \n   &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                                                         \n 1     0        0 Mookie Betts walks.                                           \n 2     2        2 Freddie Freeman walks.                                        \n 3     2        3 Xander Bogaerts strikes out on automatic strike.              \n 4     2        2 Héctor Neris intentionally walks Wyatt Langford.              \n 5     2        2 Logan Webb intentionally walks Ha-Seong Kim.                  \n 6     2        2 Cole Ragans intentionally walks Carlos Santana.               \n 7     1        2 Andrew Vaughn strikes out on automatic strike.                \n 8     2        3 Oneil Cruz strikes out on automatic strike.                   \n 9     2        3 Pitcher Bryce Miller picks off Wilyer Abreu at on throw to sh…\n10     1        1 Yohan Ramírez intentionally walks Christian Yelich.           \n11     1        2 Alex Kirilloff strikes out on automatic strike.               \n12     1        1 Tony Kemp walks. James McCann to 2nd.                         \n13     2        3 With Anthony Rendon batting, Zach Neto picked off and caught …\n14     2        3 Miguel Sanó strikes out on automatic strike.                  \n15     2        3 With Vinnie Pasquantino batting, Bobby Witt Jr. picked off an…\n\n\nThe following code manually corrects the missing values for end_events\n\natbat2024 &lt;-\n  raw_atbat2024 |&gt;\n  dplyr::mutate(\n    end_events = dplyr::case_when(\n      end_events == \"\" & grepl(\"walk\", des) ~ \"walk\",\n      end_events == \"\" & grepl(\"strikes out\", des) ~ \"strikeout\",\n1      end_events == \"\" & end_Outs == 3 ~ \"truncated_pa\",\n2      end_events == \"\" & grepl(\"flies out\", des) ~ \"field_out\",\n      .default = end_events))\n\n\n1\n\nAfter accounting for the walks and strike outs on automatic strikes, all but one of the at-bats that still had a missing end_events value involved a pick-off that ended the inning\n\n2\n\nThe remaining at-bat involved a fly out that was caught in foul territory.",
    "crumbs": [
      "Lecture 7: Wins Above Replacement I"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#adjusted-run-values",
    "href": "lectures/lecture07.html#adjusted-run-values",
    "title": "Lecture 7: Wins Above Replacement I",
    "section": "Adjusted Run Values",
    "text": "Adjusted Run Values\nWe want to give credit to the batter and base runners for creating value over and above what would been expected given the game state and the actual outcome of the at-bat. More precisely, recall that \\(\\delta_{i}\\) is the run value created in at-bat \\(i.\\) We will denote the game state at the beginning of the at-bat with \\(\\textrm{g}_{i}\\) and the ending event with \\(\\textrm{e}_{i}.\\) We form the game state variable \\(\\textrm{g}\\) by concatenating the Outs and BaseRunners and separating them with a period so that \\(\\textrm{g} = \"0.101\"\\) corresponds to a situation with no outs and runners on first and third base.\n\natbat2024 &lt;-\n  atbat2024 |&gt;\n  dplyr::mutate(GameState = paste(Outs, BaseRunner, sep = \".\"))\n\nWe will assume that the run value created in each at-bat beginning in state \\(\\textrm{g}\\) and ending with event \\(\\textrm{e}\\) is equal to the average run value created in all at-bats with the same beginning and end plus some mean-zero error. That is, for each at-bat \\(i\\), \\[\n\\delta_{i} = \\mathbb{E}[\\delta \\vert \\textrm{g} = \\textrm{g}_{i}, \\textrm{e} = \\textrm{e}_{i}] + \\varepsilon_{i},\n\\] The average run value \\(\\mu:=\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}]\\) represents the average run value created in at-bats that begin in state \\(\\textrm{g}\\) and end with the event \\(\\textrm{e}.\\)\nIt is tempting to compute the expectation \\(\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}]\\) using the “binning-and-averaging” approach we took when developing our initial XG models back in Lecture 2. Unfortunately, such a procedure is liable to yield extreme and erratic answers as the number of bins is quite large. To wit, there are 24 distinct game states (i.e., combinations of outs and base runners) and 21 different events.\nThe 2024 dataset contains only 373 of the 504 total combinations of game state and ending event. Of the observed combinations, there is a huge disparity in the relative frequencies. Some combinations (e.g., triples with no outs and runners on second and third) occurred just once while others (e.g., striking out with no outs and nobody on) occurred close to 10,000 times.\n\natbat2024 |&gt;\n  dplyr::count(Outs, BaseRunner, end_events) |&gt;\n  dplyr::arrange(n) |&gt;\n  dplyr::slice(c(1:5, (dplyr::n()-4):dplyr::n()))\n\n# A tibble: 10 × 4\n    Outs BaseRunner end_events                n\n   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;\n 1     0 001        catcher_interf            1\n 2     0 001        fielders_choice_out       1\n 3     0 011        triple                    1\n 4     0 101        strikeout_double_play     1\n 5     0 101        triple_play               1\n 6     1 000        strikeout              7490\n 7     0 000        strikeout              9814\n 8     2 000        field_out             11563\n 9     1 000        field_out             14573\n10     0 000        field_out             20148\n\n\nInstead of “binning and averaging”, like we did with our distance-based XG models in Lecture 3, we will fit a statistical model. A natural starting model asserts that there are numbers \\(\\alpha_{0.000}, \\ldots, \\alpha_{2.111}\\) and \\(\\alpha_{\\textrm{catcher\\_interf}}, \\ldots, \\alpha_{\\textrm{walk}}\\) such that for all game states \\(\\textrm{g}\\) and ending events \\(\\textrm{e},\\) \\[\n\\mathbb{E}[\\delta \\vert \\textrm{g}, \\textrm{e}] = \\alpha_{\\textrm{g}} + \\alpha_{\\textrm{e}}.\n\\]\nUnder the assumed model, the average run value created by hitting a single when there are two outs and no runners on is \\(\\alpha_{\\textrm{2.000}} + \\alpha_{\\textrm{single}}\\) while the average run value created by hitting a single when there are no outs and runners on first and second is \\(\\alpha_{\\textrm{0.110}} + \\alpha_{\\textrm{single}}.\\)\nBecause we do not know the exact values of the \\(\\alpha_{g}\\)’s and \\(\\alpha_{e}\\)’s, we need to estimate them using our data. Perhaps the simplest way is by solving a least squares minimization problem \\[\n\\hat{\\boldsymbol{\\alpha}} = \\textrm{argmin} \\sum_{i = 1}^{n}{(\\delta_{i} - \\alpha_{g_{i}} - \\alpha_{e_{i}})^2},\n\\] where \\(g_{i}\\) and \\(e_{i}\\) record the game state and event of at-bat \\(i.\\)\nSolving this problem is equivalent to fitting a linear regression model without an intercept3. We can do this in R using the lm() function and including -1 in the formula argument4. In the following code, we create a temporary data frame that extracts just the run values \\(\\delta\\), game states \\(\\textrm{g}\\), and ending events \\(\\textrm{e}\\) from atbats2024 and convert the game state and event variables into factors.\n\ntmp_df &lt;-\n  atbat2024 |&gt;\n  dplyr::select(RunValue, GameState, end_events) |&gt;\n  dplyr::mutate(\n    GameState = factor(GameState),\n    end_events = factor(end_events))\n\nstate_event_fit &lt;-\n  lm(RunValue ~ -1 + GameState + end_events, data = tmp_df)\n\nUsing the summary() function, we can take a quick look at the \\(\\alpha\\)’s.\n\nsummary(state_event_fit)\n\n\nCall:\nlm(formula = RunValue ~ -1 + GameState + end_events, data = tmp_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.65924 -0.10891  0.01957  0.08867  2.24020 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \nGameState0.000                       0.344318   0.024078  14.300  &lt; 2e-16 ***\nGameState0.001                       0.240733   0.027701   8.690  &lt; 2e-16 ***\nGameState0.010                       0.392981   0.024448  16.074  &lt; 2e-16 ***\nGameState0.011                       0.272952   0.026280  10.386  &lt; 2e-16 ***\nGameState0.100                       0.391419   0.024160  16.201  &lt; 2e-16 ***\nGameState0.101                       0.237505   0.025347   9.370  &lt; 2e-16 ***\nGameState0.110                       0.407975   0.024500  16.652  &lt; 2e-16 ***\nGameState0.111                       0.369993   0.025731  14.379  &lt; 2e-16 ***\nGameState1.000                       0.348466   0.024094  14.463  &lt; 2e-16 ***\nGameState1.001                       0.253997   0.024902  10.200  &lt; 2e-16 ***\nGameState1.010                       0.354195   0.024327  14.560  &lt; 2e-16 ***\nGameState1.011                       0.265455   0.025049  10.597  &lt; 2e-16 ***\nGameState1.100                       0.396792   0.024121  16.450  &lt; 2e-16 ***\nGameState1.101                       0.332078   0.024655  13.469  &lt; 2e-16 ***\nGameState1.110                       0.412645   0.024311  16.974  &lt; 2e-16 ***\nGameState1.111                       0.359217   0.024860  14.449  &lt; 2e-16 ***\nGameState2.000                       0.356264   0.024096  14.785  &lt; 2e-16 ***\nGameState2.001                       0.346744   0.024549  14.124  &lt; 2e-16 ***\nGameState2.010                       0.318496   0.024256  13.131  &lt; 2e-16 ***\nGameState2.011                       0.321323   0.024888  12.911  &lt; 2e-16 ***\nGameState2.100                       0.351540   0.024132  14.567  &lt; 2e-16 ***\nGameState2.101                       0.363065   0.024474  14.835  &lt; 2e-16 ***\nGameState2.110                       0.354515   0.024269  14.608  &lt; 2e-16 ***\nGameState2.111                       0.338946   0.024651  13.750  &lt; 2e-16 ***\nend_eventsdouble                     0.398473   0.024206  16.461  &lt; 2e-16 ***\nend_eventsdouble_play               -1.293011   0.027320 -47.328  &lt; 2e-16 ***\nend_eventsfield_error                0.097087   0.025100   3.868  0.00011 ***\nend_eventsfield_out                 -0.589994   0.024071 -24.510  &lt; 2e-16 ***\nend_eventsfielders_choice            0.342555   0.027015  12.680  &lt; 2e-16 ***\nend_eventsfielders_choice_out       -0.963020   0.027678 -34.794  &lt; 2e-16 ***\nend_eventsforce_out                 -0.713014   0.024403 -29.219  &lt; 2e-16 ***\nend_eventsgrounded_into_double_play -1.195243   0.024442 -48.901  &lt; 2e-16 ***\nend_eventshit_by_pitch              -0.001119   0.024636  -0.045  0.96376    \nend_eventshome_run                   1.043303   0.024271  42.985  &lt; 2e-16 ***\nend_eventssac_bunt                  -0.443274   0.026600 -16.664  &lt; 2e-16 ***\nend_eventssac_fly                   -0.341349   0.025120 -13.589  &lt; 2e-16 ***\nend_eventssac_fly_double_play       -0.887624   0.070048 -12.672  &lt; 2e-16 ***\nend_eventssingle                     0.115000   0.024099   4.772 1.83e-06 ***\nend_eventsstrikeout                 -0.618027   0.024083 -25.662  &lt; 2e-16 ***\nend_eventsstrikeout_double_play     -1.041349   0.033231 -31.337  &lt; 2e-16 ***\nend_eventstriple                     0.704276   0.025700  27.404  &lt; 2e-16 ***\nend_eventstriple_play               -2.133826   0.238222  -8.957  &lt; 2e-16 ***\nend_eventstruncated_pa              -0.625189   0.026806 -23.323  &lt; 2e-16 ***\nend_eventswalk                      -0.024163   0.024135  -1.001  0.31675    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2369 on 178488 degrees of freedom\nMultiple R-squared:  0.7682,    Adjusted R-squared:  0.7682 \nF-statistic: 1.344e+04 on 44 and 178488 DF,  p-value: &lt; 2.2e-16\n\n\nWe estimate \\(\\hat{\\alpha}_{2.000} \\approx 0.356\\) and \\(\\hat{\\alpha}_{single} \\approx 0.115.\\) So, according to our fitted model the average run value created by singling when there are two outs and no runners on is about \\(0.471.\\)\n\n\n\n\n\n\nStatistical significance & model assumptions\n\n\n\nYou’ll notice that summary() returns a lot of inferential output (e.g., standard errors, p-values). These are computed under an additional assumption that the true errors \\(\\varepsilon_{i}\\) are independent and following a mean-zero normal distribution with constant variance. Since our main interest is prediction, we’re really not interested in checking whether, say, \\(\\alpha_{0:010}\\) is statistically significantly different than zero. So, we will not check whether the usual multiple linear model assumptions nor will we attempt. If you did want to make inferential statements about our model parameters, you would need to first check that the multiple linear multiple assumptions are not grossly violated.\n\n\nEquipped with our estimated model parameters, for each at-bat \\(i,\\) let \\(\\hat{\\mu}_{i} = \\hat{\\alpha}_{\\textrm{g}_{i}} + \\hat{\\alpha}_{\\textrm{e}_{i}}\\) and let \\(\\eta_{i} = \\delta_{i} - \\hat{\\mu}_{i}.\\) In terms of dividing credit between the batter and the base runner, we will follow Baumer, Jensen, and Matthews (2015) and attribute \\(\\hat{\\mu}_{i}\\) to the batter’s hitting in at-bat \\(i\\) and \\(\\eta_{i}\\) to the base running in that at-bat. We will add columns to atbat2024 holding the values of \\(\\hat{\\mu}\\) (mu) and \\(\\eta\\) (eta).\n\natbat2024$eta &lt;- state_event_fit$residuals\natbat2024$mu &lt;- state_event_fit$fitted.values\nsave(atbat2024, file = \"atbat2024.RData\")",
    "crumbs": [
      "Lecture 7: Wins Above Replacement I"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#baserunning-run-values",
    "href": "lectures/lecture07.html#baserunning-run-values",
    "title": "Lecture 7: Wins Above Replacement I",
    "section": "Baserunning run values",
    "text": "Baserunning run values\nOhtani’s second hit against the Padres on March 20, 2024 was a single with runners on first and second and no outs. While Ohtani and the runner originally on first advanced one base, the runner originally on second scored. Because this latter runner advanced more than what might have been otherwise expected, it makes sense to give him a larger share of the \\(\\eta_{i}\\) than to the first two runners, who only advanced one base on a single. Following (Baumer, Jensen, and Matthews 2015, sec. 3.2), the amount of base running run value \\(\\eta_{i}\\) that we assign to base runner \\(j\\) in at-bat \\(i\\) will be proportional to \\(\\kappa_{ij} = \\mathbb{P}(K &lt; k_{ij} \\vert \\textrm{e}_{i}),\\) where \\(k_{ij}\\) is the number of bases actually advanced by the base runner.\nEssentially, \\(\\kappa_{ij}\\) is the probability that a typical base runner advanced at most the \\(k_{ij}\\) bases advanced by base runner \\(j\\) in at-bat \\(i\\) following event \\(\\textrm{e}_{i}.\\) If the base runner does worse than expected (e.g., not advancing from second on a single), then \\(\\kappa_{ij}\\) will be very small. But if the base runner does better than expected (e.g., scoring from second on a single), then \\(\\kappa_{ij}\\) will be larger. When computing \\(\\kappa_{ij}\\) it is crucial that we condition on the actual ending event \\(\\textrm{e}_{i}.\\) After all, while we may want to penalize a runner for not advancing from second on a single, we definitely don’t want to penalize a runner for not advancing from second following a strike out!\n\nComputing baserunner advancement\nUnfortunately, StatCast does not compute the number of bases that each runner advances during each at-bat. The following code implements a function that determines the number of bases advanced by the runner on first (if any). It works by first checking whether there is anyone on 1b at the start of the at-bat. If so, it checks whether that player is on first, second, or third base at the end of the at-bat. If not, it parses the at-bat description contained in des and looks for a sentence containing the player’s name. If that sentence contains the words “out” or “caught stealing”, it sets the number of bases advanced to 0. But, if the sentnce contains the word “score”, it sets the number of bases advanced to 3, since the runner scored from first.\n\nload(\"player2024_lookup.RData\")\n#| label: mvt-1b-function\nmvt_1b &lt;- function(on_1b, Outs, bat_score,\n                   end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_1b)){\n    # there was someone on 1st base at the start of the at-bat\n1    if(!is.na(end_on_1b) & on_1b == end_on_1b) mvt &lt;- 0\n2    if(!is.na(end_on_2b) & on_1b == end_on_2b) mvt &lt;- 1\n3    if(!is.na(end_on_3b) & on_1b == end_on_3b) mvt &lt;- 2\n\n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on first\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_1b)]\n      # Start by splitting it a string\n      play_split &lt;- \n        stringr::str_split_1(\n          string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n          pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      \n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 3 # player scored from 1st\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      } \n    }\n  } \n  return(mvt)\n}\n\n\n1\n\nRunner remained on first, so they advanced 0 bases\n\n2\n\nRunner advanced 1 base (first to second)\n\n3\n\nRunner advanced 2 bases (first to third)\n\n\n\n\nWe similarly define functions to track the number of bases advanced by the runners on second and third base and by the batter. For brevity, we have folded the code.\n\n\nCode\nmvt_2b &lt;- function(on_2b, Outs, bat_score,\n                   end_on_2b, end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_2b)){\n    # there was someone on 2nd base at the start of the at-bat\n    if(!is.na(end_on_2b) & on_2b == end_on_2b) mvt &lt;- 0 # runner remained on 2nd\n    if(!is.na(end_on_3b) & on_2b == end_on_3b) mvt &lt;- 1 # runner advanced to 3rd\n    \n    #if(end_Outs == 3) mvt &lt;- 0 # inning ended ; there may be some edge cases here\n    # e.g., in last at-bat there may be a wild pitch\n    # https://www.espn.com/mlb/playbyplay/_/gameId/401568474 where runner scores and then batter gets out to end the inning\n    \n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on second\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_2b)]\n      # Start by splitting it a string\n      play_split &lt;- \n        stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                    pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      \n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 2 # player scored from 2nd\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      } \n    }\n  } \n  return(mvt)\n}\n\n\nmvt_3b &lt;- function(on_3b, Outs, bat_score,\n                   end_on_3b, end_Outs, end_bat_score,\n                   des){\n  mvt &lt;- NA\n  if(!is.na(on_3b)){\n    if(!is.na(end_on_3b) & on_3b == end_on_3b) mvt &lt;- 0 # runner remained on 3rd\n    \n    if(is.na(mvt)){\n      # either there are no baserunners at end of inning or\n      # there are baserunners but none of them started on second\n      # we need to parse the play\n      # Start by grabbing the player name\n      player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == on_3b)]\n      play_split &lt;- \n        stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                    pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n      check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n      if(any(check)){\n        # found something with player name in it\n        play &lt;- play_split[check]\n        if( any(grepl(pattern = \"out\", x = play) | grepl(pattern = \"caught stealing\", x = play))) mvt &lt;- 0 # player got out\n        else if(any(grepl(pattern = \"score\", x = play))) mvt &lt;- 1 # player scored from 3rd\n      } else{\n        # player name is not present in play description; and they're not on base\n        # if they got caught stealing in the middle of the at-bat this may not be recorded\n        # check if Outs &lt; end_Outs\n        if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      }\n    }\n  } \n  return(mvt)\n}\n\n\nmvt_batter &lt;- function(batter, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score, des)\n{\n  mvt &lt;- NA\n  if(!is.na(end_on_1b) & batter == end_on_1b) mvt &lt;- 1 # batter advanced to 1st\n  else if(!is.na(end_on_2b) & batter == end_on_2b) mvt &lt;- 2 # batter advanced to 2nd\n  else if(!is.na(end_on_3b) & batter == end_on_3b) mvt &lt;- 3 # batter advanced to 3rd\n  else{\n    # batter is not on base\n    # look up player name\n    player_name &lt;- player2024_lookup$Name[which(player2024_lookup$key_mlbam == batter)]\n    \n    play_split &lt;-\n      stringr::str_split_1(string = stringi::stri_trans_general(des, \"Latin-ASCII\"),\n                           pattern = \"(?&lt;=[[:punct:]])\\\\s(?=[A-Z])\")\n    \n    check &lt;- sapply(play_split, FUN = grepl, pattern = player_name)\n    if(any(check)){\n      # found something with player name in it\n      play &lt;- play_split[check]\n      if( any(grepl(pattern = \"out\", x = play))) mvt &lt;- 0 # player got out\n      else if(any(grepl(pattern = \"score\", x = play) | grepl(pattern = \"home\", x = play))) mvt &lt;- 4 # batter scored\n      else if(end_Outs == 3 | Outs &lt; end_Outs & bat_score == end_bat_score) mvt &lt;- 0\n      else mvt &lt;- NA\n    }\n  }\n  return(mvt)\n}\n\n\nWe can now apply these functions to every row of our data frame.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code takes a few minutes to run.\n\n\n\nbaserunning &lt;-\n  atbat2024 |&gt;\n  dplyr::rowwise() |&gt;\n  dplyr::mutate(\n    mvt_batter = mvt_batter(batter, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b, end_Outs, end_bat_score, des),\n    mvt_1b = mvt_1b(on_1b, Outs, bat_score, end_on_1b, end_on_2b, end_on_3b,end_Outs, end_bat_score, des),\n    mvt_2b = mvt_2b(on_2b, Outs, bat_score, end_on_2b, end_on_3b, end_Outs, end_bat_score, des),\n    mvt_3b = mvt_3b(on_3b, Outs, bat_score, end_on_3b, end_Outs, end_bat_score, des)) |&gt;\n  dplyr::ungroup() \n\n\n\nCumulative baserunning probabilities\nNow that we have computed the \\(k_{ij}\\)’s — that is, the number of bases each base runner advanced in each at-bat — we are ready to compute the cumulative base running probabilities \\(\\mathbb{P}(K \\leq k  \\vert \\textrm{e}).\\) In the following code, we first group at-bats by the ending event and then compute the proportion of times that the baserunner advances at most \\(k\\) bases. We also set the cumulative probability to zero for situations when there isn’t a runner on a particular base.\n\nbr_batter_probs &lt;-\n  baserunning |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_batter &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_batter &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_batter &lt;= 2, na.rm = TRUE),\n    kappa_3 = mean(mvt_batter &lt;= 3, na.rm = TRUE),\n    kappa_4 = mean(mvt_batter &lt;= 4, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_batter\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_batter\") |&gt;\n  dplyr::mutate(\n    mvt_batter = ifelse(mvt_batter == \"NA\", NA, mvt_batter),\n    mvt_batter = as.numeric(mvt_batter))\nbr_1b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_1b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_1b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_1b &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_1b &lt;= 2, na.rm = TRUE),\n    kappa_3 = mean(mvt_1b &lt;= 3, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_1b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_1b\") |&gt;\n  dplyr::mutate(mvt_1b = ifelse(mvt_1b == \"NA\", NA, mvt_1b),\n                mvt_1b = as.numeric(mvt_1b))\n\nbr_2b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_2b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_2b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_2b &lt;= 1, na.rm = TRUE),\n    kappa_2 = mean(mvt_2b &lt;= 2, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_2b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_2b\") |&gt;\n  dplyr::mutate(mvt_2b = ifelse(mvt_2b == \"NA\", NA, mvt_2b),\n                mvt_2b = as.numeric(mvt_2b))\n\nbr_3b_probs &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_3b)) |&gt;\n  dplyr::group_by(end_events) |&gt;\n  dplyr::summarize(\n    kappa_0 = mean(mvt_3b &lt;= 0, na.rm = TRUE),\n    kappa_1 = mean(mvt_3b &lt;= 1, na.rm = TRUE),\n    kappa_NA = 0) |&gt;\n  tidyr::pivot_longer(cols = tidyr::starts_with(\"kappa_\"),\n                      names_to = \"mvt_3b\",\n                      names_prefix = \"kappa_\",\n                      values_to = \"kappa_3b\") |&gt;\n  dplyr::mutate(mvt_3b = ifelse(mvt_3b == \"NA\", NA, mvt_3b),\n                mvt_3b = as.numeric(mvt_3b))\n\nThe table br_1b_probs contains the cumulative base running probabilities for runners who start on first base broken down by ending event. We find that in about 64.7% of singles, the runner on first advances one base or fewer while in 95.7% of singles, the runner on first advances two bases or fewer.\n\nbr_1b_probs |&gt;\n  dplyr::filter(end_events == \"single\")\n\n# A tibble: 5 × 3\n  end_events mvt_1b kappa_1b\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 single          0   0.0194\n2 single          1   0.647 \n3 single          2   0.957 \n4 single          3   1     \n5 single         NA   0     \n\n\n\n\nBaserunning runs above average\nNow that we have the cumulative base running probabilities, we’re (finally) ready to compute \\(\\kappa_{ij}.\\) To do so, we will use inner_join()’s to add columns to our baserunning data table with columns for the batter and runners on first, second, and third. Note, whenever there is no baserunner on first base (i.e., on_1b = NA), we will set the corresponding \\(\\kappa\\) to 0. Because we want to divide all of \\(\\eta_{i}\\) amongst the base runners, we need to normalize the \\(\\kappa_{ij}\\) values to sum to 1 within each at-bat. The columns norm_batter, norm_1b, norm_2b, and norm_3b contain these normalized weights.\n\nbaserunning &lt;-\n  baserunning |&gt;\n  dplyr::inner_join(y = br_batter_probs, by = c(\"end_events\", \"mvt_batter\")) |&gt;\n  dplyr::inner_join(y = br_1b_probs, by = c(\"end_events\", \"mvt_1b\")) |&gt;\n  dplyr::inner_join(y = br_2b_probs, by = c(\"end_events\", \"mvt_2b\")) |&gt;\n  dplyr::inner_join(y = br_3b_probs, by = c(\"end_events\", \"mvt_3b\")) |&gt;\n  dplyr::mutate(\n    total_kappa = kappa_batter + kappa_1b + kappa_2b + kappa_3b,\n    norm_batter = kappa_batter/total_kappa,\n    norm_1b = kappa_1b/total_kappa,\n    norm_2b = kappa_2b/total_kappa,\n    norm_3b = kappa_3b/total_kappa)\n\nTo illustrate our calculations so far, let’s look at Ohtani’s at-bats from that game against the Padres. First, we see that Ohtani reached first base in all except his fourth at-bat. So, for these four at-bats, his mvt_batter value is 1.\n\nload(\"player2024_lookup.RData\")\nohtani_id &lt;- \n  player2024_lookup$key_mlbam[which(player2024_lookup$FullName == \"Shohei Ohtani\")]\n\nbaserunning |&gt;\n  dplyr::filter(game_pk == 745444 & batter == ohtani_id) |&gt;\n  dplyr::select(at_bat_number, mvt_batter, des)\n\n# A tibble: 5 × 3\n  at_bat_number mvt_batter des                                                  \n          &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                                                \n1             2          1 Shohei Ohtani grounds into a force out, shortstop Ha…\n2            18          1 Shohei Ohtani singles on a sharp line drive to right…\n3            37          1 Shohei Ohtani grounds into a force out, third basema…\n4            52          0 Shohei Ohtani grounds out softly, pitcher Wandy Pera…\n5            65          1 Shohei Ohtani singles on a line drive to left fielde…\n\n\nIn his second at-bat, Ohtani singled with no runners on. So, he should get credit for creating all the base running run value above average on that at-bat. In contrast, we argued earlier that when he drove in a run in his fifth at-bat, the runner who scored from second should get a bit more credit than Ohtani and the runner on first, who only advanced one base. Looking at the weights norm_1b, norm_2b, and norm_batter for this at-bat, we see that indeed, we’re assigning a bit more weight to the runner on second than the runner on first.\n\nbaserunning |&gt;\n  dplyr::filter(game_pk == 745444 & batter == ohtani_id) |&gt;\n  dplyr::select(at_bat_number, mvt_1b, mvt_2b, mvt_3b, mvt_batter, norm_1b, norm_2b, norm_3b, norm_batter)\n\n# A tibble: 5 × 9\n  at_bat_number mvt_1b mvt_2b mvt_3b mvt_batter norm_1b norm_2b norm_3b\n          &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1             2      0     NA     NA          1   0.491   0           0\n2            18     NA     NA     NA          1   0       0           0\n3            37      0     NA     NA          1   0.491   0           0\n4            52     NA     NA     NA          0   0       0           0\n5            65      1      2     NA          1   0.247   0.382       0\n# ℹ 1 more variable: norm_batter &lt;dbl&gt;\n\n\nRecall that \\(\\eta_{i}\\) represents the run value above average generated in at-bat \\(i\\) due to base running. Whenever there is a runner on first at the start of the at-bat, the quantity \\(\\kappa_{i,\\textrm{1b}}/\\sum_{j}{\\kappa_{ij}} \\times \\eta_{i}\\) reflects the run value above average generated in the at-bat due to the base running of the runner initially on first. For each player, we can aggregate these values across all at-bats in which they are on first.\n\nraa_1b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_1b)) |&gt;\n  dplyr::mutate(RAA_1b = kappa_1b * eta) |&gt;\n  dplyr::group_by(on_1b) |&gt;\n  dplyr::summarise(RAA_1b = sum(RAA_1b)) |&gt;\n  dplyr::rename(key_mlbam = on_1b)\n\nWe can similarly compute the run value above average created by the runners on second base and third base as well as by the batter.\n\nraa_2b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_2b)) |&gt;\n  dplyr::mutate(RAA_2b = kappa_2b * eta) |&gt;\n  dplyr::group_by(on_2b) |&gt;\n  dplyr::summarise(RAA_2b = sum(RAA_2b)) |&gt;\n  dplyr::rename(key_mlbam = on_2b)\n\nraa_3b &lt;-\n  baserunning |&gt;\n  dplyr::filter(!is.na(on_3b)) |&gt;\n  dplyr::mutate(RAA_3b = kappa_3b * eta) |&gt;\n  dplyr::group_by(on_3b) |&gt;\n  dplyr::summarise(RAA_3b = sum(RAA_3b)) |&gt;\n  dplyr::rename(key_mlbam = on_3b)\n\nraa_batter &lt;-\n  baserunning |&gt;\n  dplyr::mutate(RAA_batter = kappa_batter * eta) |&gt;\n  dplyr::group_by(batter) |&gt;\n  dplyr::summarise(RAA_batter = sum(RAA_batter)) |&gt;\n  dplyr::rename(key_mlbam = batter)\n\nFinally, we can aggregate the total run value above average that each player creates from their base running, which we can \\(\\textrm{RAA}^{\\textrm{br}}.\\)\n\nraa_br &lt;-\n  raa_batter |&gt;\n  dplyr::full_join(y = raa_1b, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_2b, by = \"key_mlbam\") |&gt;\n  dplyr::full_join(y = raa_3b, by = \"key_mlbam\") |&gt;\n  tidyr::replace_na(list(RAA_batter = 0, RAA_1b = 0, RAA_2b = 0, RAA_3b = 0)) |&gt;\n  dplyr::mutate(RAA_br = RAA_batter + RAA_1b + RAA_2b + RAA_3b) |&gt;\n  dplyr::inner_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_br, RAA_batter, RAA_1b, RAA_2b, RAA_3b)\n\nThe player with the largest \\(\\textrm{RAA}^{\\textrm{br}}\\), Jose Ramirez, is known for his aggressive baserunning5",
    "crumbs": [
      "Lecture 7: Wins Above Replacement I"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#batting",
    "href": "lectures/lecture07.html#batting",
    "title": "Lecture 7: Wins Above Replacement I",
    "section": "Batting",
    "text": "Batting\nWe now need to distribute \\(\\hat{\\mu}_{i}\\) to each batter. As noted in (Baumer, Jensen, and Matthews 2015, sec. 3.3), we need to take care to calibrate each batter’s hitting performance with the expected performance by position.\n\n\nAverage run value by position\n\nload(\"positions2024.RData\")\nbatting &lt;-\n  atbat2024 |&gt;\n  dplyr::select(batter, mu) |&gt;\n  dplyr::rename(key_mlbam = batter) |&gt;\n  dplyr::left_join(y = positions2024, by = \"key_mlbam\")\n\navg_mu_position &lt;-\n  batting |&gt;\n  dplyr::group_by(position) |&gt;\n  dplyr::summarise(avg_mu = mean(mu))\n\nWe can now go through and subtract the position average from each \\(\\hat{\\mu}_{i}\\) in batting. This represents the run value above average created by the batter’s hittign in the at-bat. In the following code, we aggregate these at-bat level run values above average for each player. We denote the total \\(\\textrm{RAA}^{\\textrm{b}}.\\)\n\nraa_b &lt;-\n  batting |&gt;\n  dplyr::left_join(y = avg_mu_position, by = \"position\") |&gt;\n  dplyr::mutate(mu_aa = mu - avg_mu) |&gt;\n  dplyr::group_by(key_mlbam) |&gt;\n  dplyr::summarise(RAA_b = sum(mu)) |&gt;\n  #dplyr::summarise(RAA_b = sum(mu_aa)) |&gt;\n  dplyr::left_join(y = player2024_lookup, by = \"key_mlbam\") |&gt;\n  dplyr::select(Name, key_mlbam, RAA_b)\n\nWe find a lot of very good batters amongst the players with highest \\(\\textrm{RAA}^{\\textrm{b}}\\)’s\n\nraa_b |&gt;\n  dplyr::arrange(dplyr::desc(RAA_b)) |&gt;\n  dplyr::slice_head(n=10)\n\n# A tibble: 10 × 2\n   Name              RAA_b\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Aaron Judge        84.6\n 2 Juan Soto          64.3\n 3 Shohei Ohtani      59.6\n 4 Bobby Witt         59.0\n 5 Vladimir Guerrero  45.0\n 6 Gunnar Henderson   42.7\n 7 Ketel Marte        41.3\n 8 Jose Ramirez       35.9\n 9 Brent Rooker       34.8\n10 William Contreras  33.5",
    "crumbs": [
      "Lecture 7: Wins Above Replacement I"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#looking-ahead",
    "href": "lectures/lecture07.html#looking-ahead",
    "title": "Lecture 7: Wins Above Replacement I",
    "section": "Looking ahead",
    "text": "Looking ahead\nWe’ve distributed the run value \\(\\delta_{i}\\) created in each at-bat between the batter and base runners and computed season total runs values above average based on batting \\(\\textrm{RAA}^{\\textrm{b}}\\) and base running \\(\\textrm{RAA}^{\\textrm{br}}.\\) Next lecture, we will distribute \\(-\\delta_{i}\\) between the pitcher and fielders involved in each at-bat. So that we don’t have to repeat our earlier calculations, we will save raa_br and raa_b\n\nsave(raa_br, raa_b, file = \"raa_offensive2024.RData\")",
    "crumbs": [
      "Lecture 7: Wins Above Replacement I"
    ]
  },
  {
    "objectID": "lectures/lecture07.html#footnotes",
    "href": "lectures/lecture07.html#footnotes",
    "title": "Lecture 7: Wins Above Replacement I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStarting in 2023, Major League Baseball implemented a pitch timer. Batters who were not in the batter’s box and alert to the pitcher by the 8-second mark of the timer are penalized with an automatic strike. See the rules here.↩︎\nWhen this happens, Statcast usually records it as a truncated plate appearance (truncated_pa).↩︎\nTry proving this mathematically!↩︎\nCheck out the documentation for R’s formula interface here. Specifically, look at the bullet point about the - operations under “Details”↩︎\nSee this article about his base running from earlier this year.↩︎",
    "crumbs": [
      "Lecture 7: Wins Above Replacement I"
    ]
  },
  {
    "objectID": "lectures/lecture12.html#a-multi-level-model",
    "href": "lectures/lecture12.html#a-multi-level-model",
    "title": "Ranking professional golfers",
    "section": "A multi-level model",
    "text": "A multi-level model",
    "crumbs": [
      "Ranking professional golfers"
    ]
  },
  {
    "objectID": "lectures/lecture03.html",
    "href": "lectures/lecture03.html",
    "title": "Lecture 3: Estimating XG",
    "section": "",
    "text": "Last lecture, we fit two very basic models for XG. The first used only body part to predict the probability of the shot resulting in a goal while the second used both body part and shot technique. By looking at a few of Beth Mead’s shots from EURO 2022, we decided that the second model was a better because it assigned a higher XG to her one-on-one lob than the shot through multiple defenders. In this lecture, we will\nWe will continue to work with all shot event data from women’s international competitions that StatsBomb makes publicly available. Using code from Lecture 2, we can load in all the shot event data and create a binary outcome recording whether the shot resulted in a goal.\n\nwi_shots &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international)|&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam() |&gt;\n6  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n7  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\n1\n\nGets the table of all available competitions\n\n2\n\nIdentifies all women’s international competitions\n\n3\n\nGets match-level data from all women’s international competitions\n\n4\n\nGets event-level data\n\n5\n\nRuns StatsBomb’s recommend pre-processing\n\n6\n\nSubsets to shot-event data taken with either the foot or head\n\n7\n\nCreates a column Y\n\n\n\n\nWe will also re-construct the two simple XG models, one that conditions only on body part and the other that conditions on body part and technique. We then create a data table with columns containing the body part, technique, and outcome of the shot as well as the XG predictions from both models.\n\nxg_model1 &lt;- \n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\") \n\nsimple_preds &lt;-\n  wi_shots |&gt;\n  dplyr::select(Y, shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::left_join(y = xg_model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::left_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#overview",
    "href": "lectures/lecture03.html#overview",
    "title": "Lecture 3: Estimating XG",
    "section": "",
    "text": "Last lecture, we fit two very basic models for XG. The first used only body part to predict the probability of the shot resulting in a goal while the second used both body part and shot technique. By looking at a few of Beth Mead’s shots from EURO 2022, we decided that the second model was a better because it assigned a higher XG to her one-on-one lob than the shot through multiple defenders. In this lecture, we will\nWe will continue to work with all shot event data from women’s international competitions that StatsBomb makes publicly available. Using code from Lecture 2, we can load in all the shot event data and create a binary outcome recording whether the shot resulted in a goal.\n\nwi_shots &lt;-\n1  StatsBombR::FreeCompetitions() |&gt;\n2  dplyr::filter(competition_gender == \"female\" & competition_international)|&gt;\n3  StatsBombR::FreeMatches() |&gt;\n4  StatsBombR::free_allevents() |&gt;\n5  StatsBombR::allclean() |&gt;\n  StatsBombR::get.opposingteam() |&gt;\n6  dplyr::filter(type.name == \"Shot\" & shot.body_part.name != \"Other\") |&gt;\n7  dplyr::mutate(Y = ifelse(shot.outcome.name == \"Goal\", 1, 0))\n\n\n1\n\nGets the table of all available competitions\n\n2\n\nIdentifies all women’s international competitions\n\n3\n\nGets match-level data from all women’s international competitions\n\n4\n\nGets event-level data\n\n5\n\nRuns StatsBomb’s recommend pre-processing\n\n6\n\nSubsets to shot-event data taken with either the foot or head\n\n7\n\nCreates a column Y\n\n\n\n\nWe will also re-construct the two simple XG models, one that conditions only on body part and the other that conditions on body part and technique. We then create a data table with columns containing the body part, technique, and outcome of the shot as well as the XG predictions from both models.\n\nxg_model1 &lt;- \n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nxg_model2 &lt;-\n  wi_shots |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\") \n\nsimple_preds &lt;-\n  wi_shots |&gt;\n  dplyr::select(Y, shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::left_join(y = xg_model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::left_join(y = xg_model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-model-comparison",
    "href": "lectures/lecture03.html#sec-model-comparison",
    "title": "Lecture 3: Estimating XG",
    "section": "Comparing XG models",
    "text": "Comparing XG models\nOur first XG model conditions only on the body part used to take the shot while our second model additionally conditions on the technique. Consequently, the first model returns exactly the same predicted XG for a right-footed half-volley (e.g., this Beth Mead goal against Sweden in EURO 2022) as it does for a right-footed backheel shot (e.g., this Alessia Russo goal from the same game). In contrast, our second model, which accounts for both the body part and the shot technique, can distinguish between these two shots and return different XG predictions. Intuitively, we might expect the second model’s predictions to be more accurate because it leverages more information.\nTo make this more concrete, suppose we have observed a shots dataset of pairs \\((\\boldsymbol{\\mathbf{x}}_{1}, y_{1}), \\ldots (\\boldsymbol{\\mathbf{x}}_{n}, y_{n})\\) of feature vectors \\(\\boldsymbol{\\mathbf{X}}\\) and binary indicators \\(Y\\) recording whether the shot resulted in a goal or not. Recall from Lecture 2 that a key assumption of XG models is the observed dataset comprises a sample from some infinite super-population of shots. For each feature combination \\(\\boldsymbol{\\mathbf{x}},\\) the corresponding \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) is defined to be the conditional expectation \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}}) := \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}].\\) Because \\(Y\\) is a binary indicator, \\(\\textrm{XG}(\\boldsymbol{\\mathbf{x}})\\) can be interpreted as the probability that a shot with features \\(\\boldsymbol{\\mathbf{x}}\\) results in a goal.\nNow, suppose we have used our data to fit an XG model. We can go back to every shot \\(i\\) in our dataset and use each fitted model to obtain the predicted XG, which we denote1 as \\(\\hat{p}_{i}.\\) We are ultimately interested in assessing how close \\(\\hat{p}_{i}\\) is to the actual observed \\(Y_{i}.\\)\nIn Statistics and Machine Leaning, there are three common ways to assess the discrepancy between a probability forecast \\(\\hat{p}_{i}\\) and a binary outcome: misclassification rate, Brier score, and logloss.\n\nMisclassification Rate\nMisclassification rate is the coarsest measure of discrepancy between a predicted probability and a binary outcome. A forecast \\(\\hat{p}_{i} &gt; 0.5\\) (resp. \\(\\hat{p}_{i} &lt; 0.5\\)) reflects the fact that we believe it more likely than not that \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\)). So, intuitively, we should expect a highly accurate model to return forecasts \\(\\hat{p}_{i}\\) that were greater (resp. less) than 0.5 whenever \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\)). A model’s misclassification rate is the proportion of times that the model deviates from this expectation (i.e., when it returns forecasts greater than 50% when \\(y = 0\\) and returns forecasts less than 50% when \\(y = 1.\\))\nFormally, given a binary observations \\(y_{1}, \\ldots, y_{n}\\) and predicted probabilities \\(\\hat{p}_{1}, \\ldots, \\hat{p}_{n},\\) the misclassification rate is defined as \\[\n\\textrm{MISS} = n^{-1}\\sum_{i = 1}^{n}{\\mathbb{I}(y_{i} \\neq \\mathbb{I}(\\hat{p}_{i} \\geq 0.5))},\n\\] where \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) equals 1 when \\(\\hat{p}_{i} \\geq 0.5\\) and equals 0 otherwise and \\(\\mathbb{I}(y_{i} \\neq \\mathbb{I}(\\hat{p}_{i} \\geq 0.5))\\) equals 1 when \\(y_{i}\\) is not equal to \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) and equals 0 otherwise. On the view that misclassification rate captures the number of times our predictions are on the wrong side of the 50% boundary, we prefer models with smaller misclassification rate.\nIn the context of XG, misclassification rate measures the proportion of times we predict an XG higher than 0.5 for a non-goal or an XG lower than 0.5 for a goal.\n\n1misclass &lt;- function(y, phat){\n  return( mean( (y != 1*(phat &gt;= 0.5))))\n}\n\n2cat(\"BodyPart misclassification\",\n    round(misclass(simple_preds$Y, simple_preds$XG1), digits = 3), \"\\n\")\ncat(\"BodyPart+Technique misclassificaiton\", \n    round(misclass(simple_preds$Y, simple_preds$XG2), digits = 3), \"\\n\")\n\n\n1\n\nA helper function that computes misclassification rate.\n\n2\n\ncat() prints output to the R console and it is usually good to round numeric output to 2 or 3 decimal points.\n\n\n\n\nBodyPart misclassification 0.107 \nBodyPart+Technique misclassificaiton 0.107 \n\n\nOur two simple models have identical misclassification rates of about 10.7%. At first glance this is a bit surprising because the two models return different predicted XG values\n\ncat(\"Unique XG1 values:\", sort(unique(simple_preds$XG1)), \"\\n\")\n\nUnique XG1 values: 0.1015056 0.1118336 0.1132812 \n\ncat(\"Unique XG2 values:\", sort(unique(simple_preds$XG2)), \"\\n\")\n\nUnique XG2 values: 0 0.04117647 0.06770833 0.08011869 0.08333333 0.1108898 0.113307 0.1213333 0.1363636 0.1621622 0.3333333 \n\n\nOn closer inspection, we find that the two simple models both output predicted XG values that are all less than 0.5. So, the thresholded values \\(\\mathbb{I}(\\hat{p}_{i} \\geq 0.5)\\) are identical for both models’ \\(\\hat{p}_{i}\\)’s. Just for comparison’s sake, let’s compute the misclassification rate of the proprietary StatsBomb XG model\n\ncat(\"StatsBomb misclassificaiton\", \n    round(misclass(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 3), \"\\n\")\n\nStatsBomb misclassificaiton 0.088 \n\n\nWe see that StatsBomb’s model has a slightly smaller misclassification rate than our two simple models, but the gap is not very large.\n\n\nBrier Score & log-loss\nA major drawback of misclassification rate is that it only penalizes predictions for being on the wrong side of 50% but does not penalize predictions based on how far away they are from 0 or 1. For instance, if one model returns \\(\\hat{p} = 0.999\\) and another model returns \\(\\hat{p} = 0.501,\\) the thresholded forecasts \\(\\mathbb{I}(\\hat{p}_{i} &gt; 0.5)\\) are identical.\nThe Brier score and log-loss both take into account how far \\(\\hat{p}\\) is from \\(Y,\\) albeit in slightly different ways. The Brier score is defined as \\[\n\\text{Brier} = n^{-1}\\sum_{i = 1}^{n}{(y_{i} - \\hat{p}_{i})^2}.\n\\] The quantity \\((y_{i} - \\hat{p}_{i})^{2}\\) is small whenever (i) \\(y_{i} = 1\\) and \\(\\hat{p}_{i}\\) is very close to 1 or (ii) \\(y_{i} = 0\\) and \\(\\hat{p}_{i}\\) is very close to 0. The quantity is very large when \\(y_{i} = 1\\) (resp. \\(y_{i} = 0\\) and \\(\\hat{p}\\) is very close to 0 (resp. very closer to 1).\nLog-loss, which is also known as cross-entropy loss in the Machine Learning literature, more aggressively penalizes wrong forecasts. It is defined as \\[\n\\textrm{LogLoss} = -1 \\times \\sum_{i = 1}^{n}{\\left[ y_{i} \\times \\log(\\hat{p}_{i}) + (1 - y_{i})\\times\\log(1-\\hat{p}_{i})\\right]}.\n\\] Notice that if \\(y_{i} = 1\\) as \\(\\hat{p}_{i} \\rightarrow 0,\\) the term \\(y_{i} \\times \\log{\\hat{p}_{i}} \\rightarrow -\\infty.\\) So, while the Brier score is mathematically bounded between 0 and 1, the average log-loss can be arbitrarily large. Essentially, log-loss heavily penalizes predictions that are confident (i.e., very close to 0 or 1) but wrong (i.e., different than \\(y_{i}\\).)\n\nbrier &lt;- function(y, phat){\n  return(mean( (y - phat)^2 ))\n}\n\ncat(\"BodyPart Brier\", \n    round(brier(simple_preds$Y, simple_preds$XG1), digits = 4), \"\\n\")\n\nBodyPart Brier 0.0953 \n\ncat(\"BodyPart+Technique Brier\", \n    round(brier(simple_preds$Y, simple_preds$XG2) , digits = 4), \"\\n\")\n\nBodyPart+Technique Brier 0.0947 \n\ncat(\"StatsBomb Brier\",\n    round(brier(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 4), \"\\n\")\n\nStatsBomb Brier 0.0706 \n\n\n\nlogloss &lt;- function(y, phat){\n  \n1  if(any(phat &lt; 1e-12)) phat[phat &lt; 1e-12] &lt;- 1e-12\n  if(any(phat &gt; 1-1e-12)) phat[phat &gt; 1-1e-12] &lt;- 1e-12\n  return(-1 * mean( y * log(phat) + (1-y) * log(1-phat)))\n}\n\ncat(\"BodyPart LogLoss:\", \n    round(logloss(simple_preds$Y, simple_preds$XG1), digits = 3), \"\\n\")\ncat(\"BodyPart+Technique LogLoss:\", \n    round(logloss(simple_preds$Y, simple_preds$XG2) , digits = 3), \"\\n\")\ncat(\"StatsBomb LogLoss:\",\n    round(logloss(wi_shots$Y, wi_shots$shot.statsbomb_xg), digits = 3), \"\\n\")\n\n\n1\n\nSince \\(\\log(0) = -\\infty,\\) we often truncate very small or very large predictions when computing average log-loss\n\n\n\n\nBodyPart LogLoss: 0.339 \nBodyPart+Technique LogLoss: 0.336 \nStatsBomb LogLoss: 0.252",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-train-test",
    "href": "lectures/lecture03.html#sec-train-test",
    "title": "Lecture 3: Estimating XG",
    "section": "The training/testing paradigm",
    "text": "The training/testing paradigm\nBased on the preceding calculation, it is tempting to conclude that our second, more complex model, is more accurate than the first model. After all, it achieved a slightly smaller Brier score and log-loss. There is, however, a small catch: we assessed the model’s performance using exactly the same data that we used to fit the model.\nGenerally speaking, if we used the data \\((\\boldsymbol{\\mathbf{x}}_{1}, y_{1}), \\ldots, (\\boldsymbol{\\mathbf{x}}_{n}, y_{n})\\) to produce an estimate \\(\\hat{f}\\) of the conditional expectation function \\(f(\\boldsymbol{\\mathbf{x}}) = \\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}].\\) We can use this estimate to compute predictions \\(\\hat{y}_{i} = \\hat{f}(\\boldsymbol{\\mathbf{x}})\\) of the originally observed outcomes \\(y_{i}.\\) While we would like each \\(\\hat{y}_{i} \\approx y_{i},\\) we are often more interested in determining whether \\(\\hat{f}(\\boldsymbol{\\mathbf{x}}^{\\star})\\) is approximately equal to \\(y^{\\star}\\) for some new pair \\((\\boldsymbol{\\mathbf{x}}^{\\star}, y^{\\star})\\) not included in the original dataset. That is, we are often interested in knowing how well the fitted model can predict previously unseen data from the same infinite super-population. Moreover, between two models, we tend to prefer the one that yields smaller out-of-sample or testing error. That is, in the context of our XG models, we would prefer the one that yielded smaller misclassification rate, Brier score, and/or log-loss on shots not used to fit our initial models.\nIn practice, we rarely have access to a separate set of data that we can hold-out of training. Instead, we often (i) divide our data into two parts; (ii) fit our model on one part (the training data); and (iii) assess the predictive performance on the second part (the testing data). Commonly, we use 75% or 80% of the data to train a model and set the remaining part aside as testing data. Moreover, we often create the training/testing split randomly.\nThe next codeblock illustrates this workflow. To help create training/testing splits, we start by giving every row its own unique ID. Then, we randomly select a fixed number of rows to form our training dataset.\n\nn &lt;- nrow(wi_shots)\n1n_train &lt;- floor(0.75 * n)\nn_test &lt;- n - n_train\n\nwi_shots &lt;-\n  wi_shots |&gt;\n2  dplyr::mutate(id = 1:n)\n\n\n3set.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n4  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n5  dplyr::anti_join(y = train_data, by = \"id\")\n\n\n1\n\nWe will use about 75% of the data for training and the remainder for testing\n\n2\n\nAssign a unique ID number to each row in wi_shots\n\n3\n\nWe often manually the randomization seed, which dictates how R generates (pseudo)-random numbers (see here), to ensure full reproducibility of our analyses. Another user running this code should get the same results.\n\n4\n\nThe function slice_sample() returns a random subset of rows.\n\n5\n\nThe function anti_join() extracts the rows in wi_shots whose IDs are not contained in train_data\n\n\n\n\nAs a sanity check, we can check whether any of the id’s in test_data are also in train_data:\n\nany(train_data$id %in% test_data$id)\n\n[1] FALSE\n\n\nNow that we have a single training and testing split, let’s fit our two simple XG models and then computing the average training and testing losses\n\nmodel1 &lt;- \n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nmodel2 &lt;-\n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\ntrain_preds &lt;-\n  train_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ntest_preds &lt;-\n  test_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ncat(\"BodyPart train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG1), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG1), digits = 3), \"\\n\")\n\nBodyPart train log-loss: 0.334 test log-loss: 0.356 \n\ncat(\"BodyPart+Technique train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG2), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG2), digits = 3), \"\\n\")\n\nBodyPart+Technique train log-loss: 0.331 test log-loss: 0.351 \n\n\nFor this training/testing split, we see that the more complex model, which conditioned on both body-part and technique, produced slightly smaller log-loss than the simple model, which only conditioned on body-part. But this may not necessarily always hold. For instance, if we drew a different training/testing split (e.g., by initially setting a different randomization seed), we might observe the opposite.\n\nset.seed(478)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\")\n\nmodel1 &lt;- \n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name) |&gt;\n  dplyr::summarise(XG1 = mean(Y))\nmodel2 &lt;-\n  train_data |&gt;\n  dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n  dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\ntrain_preds &lt;-\n  train_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ntest_preds &lt;-\n  test_data |&gt;\n  dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n  dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\ncat(\"BodyPart train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG1), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG1), digits = 3), \"\\n\")\n\nBodyPart train log-loss: 0.339 test log-loss: 0.341 \n\ncat(\"BodyPart+Technique train log-loss:\",\n    round(logloss(train_preds$Y, train_preds$XG2), digits = 3), \n    \"test log-loss:\",\n    round(logloss(test_preds$Y, test_preds$XG2), digits = 3), \"\\n\")\n\nBodyPart+Technique train log-loss: 0.335 test log-loss: 0.364 \n\n\nOn this second random split, the simpler model had better in-sample and out-of-sample performance.\nIn practice, instead of relying on a single training/testing split, we often average the in- and out-of-sample losses across several different random splits. In the code below, we repeat the above exercise using 100 random training/testing splits. Notice that in our for loop, we manually set the seed in a predictable way to ensure reproducibility.\n\nn_sims &lt;- 100\n1train_logloss &lt;-\n  matrix(nrow = 2, ncol = n_sims,\n         dimnames = list(c(\"XG1\",\"XG2\"), c()))\ntest_logloss &lt;- \n  matrix(nrow = 2, ncol = n_sims,\n         dimnames = list(c(\"XG1\",\"XG2\"), c()))\n\nfor(r in 1:n_sims){\n2  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train) \n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n  \n  model1 &lt;- \n    train_data |&gt;\n    dplyr::group_by(shot.body_part.name) |&gt;\n    dplyr::summarise(XG1 = mean(Y))\n  \n  model2 &lt;-\n    train_data |&gt;\n    dplyr::group_by(shot.body_part.name, shot.technique.name) |&gt;\n    dplyr::summarise(XG2 = mean(Y), .groups = \"drop\")\n\n  train_preds &lt;-\n    train_data |&gt;\n    dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n    dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n\n  test_preds &lt;-\n    test_data |&gt;\n    dplyr::inner_join(y = model1, by = c(\"shot.body_part.name\")) |&gt;\n    dplyr::inner_join(y = model2, by = c(\"shot.body_part.name\", \"shot.technique.name\"))\n  \n3  train_logloss[\"XG1\", r] &lt;- logloss(train_preds$Y, train_preds$XG1)\n  train_logloss[\"XG2\",r] &lt;- logloss(train_preds$Y, train_preds$XG2)\n  \n  test_logloss[\"XG1\", r] &lt;- logloss(test_preds$Y, test_preds$XG1)\n  test_logloss[\"XG2\",r] &lt;- logloss(test_preds$Y, test_preds$XG2)\n}\n\n4cat(\"XG1 training logloss:\", round(mean(train_logloss[\"XG1\",]), digits = 3), \"\\n\")\ncat(\"XG2 training logloss:\", round(mean(train_logloss[\"XG2\",]), digits = 3), \"\\n\")\n\ncat(\"XG1 test logloss:\", round(mean(test_logloss[\"XG1\",]), digits = 3), \"\\n\")\ncat(\"XG2 test logloss:\", round(mean(test_logloss[\"XG2\",]), digits = 3), \"\\n\")\n\n\n1\n\nWe will save the training and testing log-loss for each training/testing split in a matrix\n\n2\n\nWe create each split using a different randomization seed. Many other choices are possible but simply adding the fold number r to some base seed (479 in this case) is perhaps the simplest approach.\n\n3\n\nWe write the training and testing log-losses for the r-th training/testing split to the r-th column of the matrices train_logloss and test_logloss\n\n4\n\nGet the average loss within each row\n\n\n\n\nXG1 training logloss: 0.34 \nXG2 training logloss: 0.336 \nXG1 test logloss: 0.339 \nXG2 test logloss: 0.345 \n\n\nBased on this analysis, we conclude that the simpler model provides better out-of-sample predictions than the more complex model. In other words, the more complex model appears to over-fit the data!",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#sec-logit",
    "href": "lectures/lecture03.html#sec-logit",
    "title": "Lecture 3: Estimating XG",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nAlthough the model that conditions only on body-part has smaller out-of-sample loss than the model that additionally accounts for shot technique, the model is still not wholly satisfactory. This is because it fails to account for important spatial information that we intuitively believe affects goal probability. For instance, we might intuitively expect a model that accounts for the distance between the shot and goal to be more accurate. Unfortunately, it is not easy to apply the same “binning and averaging” approach that we used for our first two models to condition on distance. This is because, with finite data, we will not have much data (if any) for all possible values of distance. As we saw in Lecture 2, averaging outcomes within bins with very small sample sizes can lead to extreme and erratic estimates. One way to overcome this challenge is to fit a statistical model.\nLogistic regression is the canonical statistical model for predicting a binary outcome \\(Y\\) using a vector \\(\\boldsymbol{\\mathbf{X}}\\) of \\(p\\) numerical features \\(X_{1}, \\ldots, X_{p}\\)2.\nThe model asserts that the log-odds that \\(Y = 1\\) is a linear function of the features. That is, \\[\n\\log\\left(\\frac{\\mathbb{P}(Y= 1 \\vert \\boldsymbol{\\mathbf{X}})}{\\mathbb{P}(Y = 0 \\vert \\boldsymbol{\\mathbf{X}})}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p}.\n\\] In R, we can fit logistic regression models using the function glm() but we must specify the argument family = binomial(link = \"logit\").\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit1 &lt;- glm(Y~DistToGoal, data = train_data, family = binomial(\"logit\"))\n\nThe summary function provides a snapshot about the fitted model, showing the individual parameter estimates, their associated standard errors, and whether or not they are statistically significantly different than zero.\n\nsummary(fit1)\n\n\nCall:\nglm(formula = Y ~ DistToGoal, family = binomial(\"logit\"), data = train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.429695   0.144511  -2.973  0.00294 ** \nDistToGoal  -0.112567   0.009972 -11.288  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1930.9  on 2888  degrees of freedom\nResidual deviance: 1765.3  on 2887  degrees of freedom\nAIC: 1769.3\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe see that the estimate of \\(\\beta_{1},\\) the coefficient capturing DistToGoal’s effect on the log-odds of a goal, is negative (-0.11) and that this is statistically significantly different than 0 (the associated p-value is &lt;2e-16). This is reassuring as we might intuitively expect the probability of a goal to decrease the further away a shot is from the goal. Recall that the intercept term \\(\\beta_{0}\\) quantifies the log-odds of a goal when DistToGoal = 0 (i.e., when a shot is taken from the middle of the goal line). Our model estimates this intercept to about -0.43, which, on the probability scale is about 39%. On the face of it, this doesn’t make a ton of sense: surely shots taken essentially from the middle of the goal line should go in much more frequently.\nWe can resolve this apparent contradiction by taking a closer look at the range of DistToGoal values in our dataset:\n\nrange(train_data$DistToGoal)\n\n[1]  1.860108 92.800862\n\n\nSince our model was trained on shots that came between 1.8 and 92 yards away, the suspiciously low 39% forecast for goal line shots represents a fairly significant extrapolation. In general, we need to exercise caution when using such extrapolations in any downstream analysis.\nCrucially, we can use our fitted model to make predictions about shots not seen in our training data. In R, we make predictions using the predict() function. The function takes three arguments: 1. object: this is the output from our glm call (i.e., the fitted model object), which we have here saved as fit1 2. newdata: this is a data table containing the features of the observations for which we would like predictions. The table that we pass here needs to have columns for every feature used in the model. 3. type: When object is the result from fitting a logistic regression model, predict will return predictions on the log-odds scale (i.e, \\(\\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_[1} + \\cdots + \\hat{\\beta}_{p}X_{p}\\)) by default. To get predictions on the probability scale, we need to set type = \"response\"\nIn the code below, we obtain predictions for both the training and testing observations and then compute the average log-loss for this training/testing split.\n\ntrain_pred1 &lt;- \n  predict(object = fit1,\n          newdata = train_data,\n          type = \"response\") \ntest_pred1 &lt;-\n  predict(object = fit1,\n          newdata = test_data,\n          type = \"response\")\n\ncat(\"Dist training logloss:\", round(logloss(train_data$Y, train_pred1), digits = 3), \"\\n\")\n\nDist training logloss: 0.306 \n\ncat(\"Dist testing logloss:\", round(logloss(test_data$Y, test_pred1), digits = 3), \"\\n\")\n\nDist testing logloss: 0.307 \n\n\nAt least for this training/testing split, the logistic regression model that conditions on distance is a bit more accurate than the simpler models. Before concluding that this model indeed is more accurate than the body-part based model, we will repeat the above calculation using 100 training/testing splits.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal, data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist training logloss:\", round(mean(train_logloss), digits = 3), \"\\n\")\n\nDist training logloss: 0.306 \n\ncat(\"Dist testing logloss:\", round(mean(test_logloss), digits = 3), \"\\n\")\n\nDist testing logloss: 0.305 \n\n\nSo, accounting for the shot distance appears to yield more accurate predictions than account for just body-part. This immediately raises the question “how much more accurate would predictions be if we accounted for shot distance, body-part, and technique?”\n\nAccount for multiple predictors\nIt turns out to be relatively straightforward to answer this question using glm(). Specifically, we can include more variables in the formula argument. Note that we first convert the variables shot.body_part.name and shot.technique.name to factors.\n\nwi_shots &lt;-\n  wi_shots |&gt;\n  dplyr::mutate(shot.body_part.name = factor(shot.body_part.name))\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit &lt;- \n  glm(formula = Y~DistToGoal + shot.body_part.name, \n      data = train_data, family = binomial(\"logit\"))\n\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal + shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -0.75354    0.16662  -4.523 6.11e-06 ***\nDistToGoal                    -0.14462    0.01145 -12.626  &lt; 2e-16 ***\nshot.body_part.nameLeft Foot   1.02331    0.19192   5.332 9.72e-08 ***\nshot.body_part.nameRight Foot  1.03471    0.17388   5.951 2.67e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1930.9  on 2888  degrees of freedom\nResidual deviance: 1722.9  on 2885  degrees of freedom\nAIC: 1730.9\n\nNumber of Fisher Scoring iterations: 6\n\n\nLooking at the output of summary(fit), we see that the model includes several more parameters. There is a slope associated with left-footed and right-footed shots. Internally, glm() has one-hot-encoded the categorical predictor shot.body_part.name and decomposed the log-odds of a goal as \\[\n\\beta_{0} + \\beta_{1}\\textrm{DistToGoal} + \\beta_{LeftFoot}\\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{RightFoot} \\mathbb{I}(\\textrm{RightFoot})\n\\] Suppose a shot is taken from distance \\(d.\\) The model makes different predictions based on the body part used to attempt the shot: * If the shot was taken with a header then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d\\) * If the shot was taken with the left foot, then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d + \\beta_{\\textrm{LeftFoot}}\\) * If the shot was taken with the right foot, then the log-odds of a goal is \\(\\beta_{0} + \\beta_{1}d + \\beta_{\\textrm{RightFoot}}\\) Reassuringly, the estimates for \\(\\beta_{\\textrm{RightFoot}}\\) and \\(\\beta_{\\textrm{LeftFoot}}\\) are positive, indicating that for a fixed distance, the log-odds of scoring a goal on a left- or right-footed shot is higher than headers.\nBy repeatedly fitting and assessing our new model using 100 random training/testing splits, we discover that account for both body part and shot distance results in even more accurate predictions.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal + shot.body_part.name, \n             data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist+BodyPart training logloss:\", round(mean(train_logloss), digits = 3), \"\\n\")\n\nDist+BodyPart training logloss: 0.298 \n\ncat(\"Dist+BodyPart testing logloss:\", round(mean(test_logloss), digits = 3), \"\\n\")\n\nDist+BodyPart testing logloss: 0.296 \n\n\n\n\nInteractions\nOur most accurate model so far accounts for both shot distance and the body part used to attempt the shot. Taking a closer look at the assumed log-odds of a goal, however, reveals a potentially important limitation: the effect of distance is assumed to be the same for all shot-types. That is, the model assume that moving one yard further away decreases the log-odds of a goal of a right-footed shot by exactly the same amount as it would a header.\nInteractions between factors in a regression model allow the effect of one factor to vary based on the value of another factor. Using R’s formula interface, we can introduce interactions between two variables using *.\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train)\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") \n\nfit &lt;- \n  glm(formula = Y~DistToGoal * shot.body_part.name, \n      data = train_data, family = binomial(\"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = Y ~ DistToGoal * shot.body_part.name, family = binomial(\"logit\"), \n    data = train_data)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                               0.48927    0.45700   1.071  0.28434\nDistToGoal                               -0.29471    0.05500  -5.358  8.4e-08\nshot.body_part.nameLeft Foot             -0.29355    0.55786  -0.526  0.59874\nshot.body_part.nameRight Foot            -0.33824    0.50859  -0.665  0.50601\nDistToGoal:shot.body_part.nameLeft Foot   0.15524    0.05870   2.645  0.00818\nDistToGoal:shot.body_part.nameRight Foot  0.15902    0.05674   2.803  0.00507\n                                            \n(Intercept)                                 \nDistToGoal                               ***\nshot.body_part.nameLeft Foot                \nshot.body_part.nameRight Foot               \nDistToGoal:shot.body_part.nameLeft Foot  ** \nDistToGoal:shot.body_part.nameRight Foot ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1930.9  on 2888  degrees of freedom\nResidual deviance: 1714.3  on 2883  degrees of freedom\nAIC: 1726.3\n\nNumber of Fisher Scoring iterations: 6\n\n\nMathematically, the formula specification Y ~ DistToGoal * shot.body_part.name tells R to fit the model that expresses the log-odds of a goal as \\[\n\\begin{align}\n\\beta_{0} + \\beta_{\\textrm{LeftFoot}} * \\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{RightFoot}} * \\mathbb{I}(\\textrm{RightFoot}) &+ ~ \\\\\n[\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:LeftFoot}}*\\mathbb{I}(\\textrm{LeftFoot}) + \\beta_{\\textrm{DistToGoal:RightFoot}}\\mathbb{I}(\\textrm{RightFoot})] \\times \\textrm{DistToGoal} &  \n\\end{align}\n\\] where \\(\\mathbb{I}(RightFoot)\\) is an indicator of whether or not the shot was taken with the right foot.\nNow for a given shot taken at distance \\(d\\), the log-odds of a goal are: * \\(\\beta_{0} + \\beta_{\\textrm{DistToGoal} * d\\) is the shot was a header. * \\(\\beta_{0} + \\beta_{\\textrm{LeftFoot}} + (\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:LeftFoot}}) * d\\) if the shot was taken with the left foot. * \\(\\beta_{0} + \\beta_{\\textrm{RightFoot}} + (\\beta_{\\textrm{DistToGoal}} + \\beta_{\\textrm{DistToGoal:RightFoot}}) * d\\) if the shot was taken with the right foot.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479+r)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train)\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") \n\n  fit &lt;- glm(Y~DistToGoal*shot.body_part.name, \n             data = train_data, family = binomial(\"logit\"))\n  \n  train_preds &lt;-\n    predict(object = fit,\n            newdata = train_data,\n            type = \"response\")\n  \n  test_preds &lt;-\n    predict(object = fit,\n            newdata = test_data,\n            type = \"response\")\n  \n  train_logloss[r] &lt;-\n    logloss(train_data$Y, train_preds)\n  test_logloss[r] &lt;-\n    logloss(test_data$Y, test_preds)\n}\ncat(\"Dist*BodyPart training logloss:\", round(mean(train_logloss), digits = 4), \"\\n\")\n\nDist*BodyPart training logloss: 0.2961 \n\ncat(\"Dist*BodyPart testing logloss:\", round(mean(test_logloss), digits = 4), \"\\n\")\n\nDist*BodyPart testing logloss: 0.295 \n\n\nInterestingly, adding the interaction between distance and body part did not seem to improve the test-set log-loss by much.",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#random-forests",
    "href": "lectures/lecture03.html#random-forests",
    "title": "Lecture 3: Estimating XG",
    "section": "Random forests",
    "text": "Random forests\nRemember how we used the function StatsBombR::allclean() to apply some of StatsBomb’s suggested pre-processing? It turns out that this function creates several features related to shot attempt including3 * Distances from the shot the center of the goal line (DistToGoal); from the shot to the goal keeper (DistSGK); from the goalkeeper to the center of the goal line (DistToGK) * The horizontal angles between the shot and goalkeeper to the center of the goal line (AngleToGoal & AngleToKeeper) and the difference between these angles (AngleDeviation) * The distances between the shot and the two nearest defenders (distance.ToD1 and distance.ToD2) * The number of defenders and whether the goal keeper is in the triangular area defined by the shot location and the two goal posts (the “cone”;DefendersInCone and InCone.GK) * The sum of the inverse distances between the shot location the locations of all defenders (density) and those defenders in the cone. A small density indicates that defenders are very far from the shot location. * The area of the smallest square that covers the locations of all center-backs and full-backs (DefArea)\nThe code below pulls out several of these features and creates a training/testing split\n\nshot_vars &lt;-\n  c(\"Y\",\n    \"shot.type.name\", \n    \"shot.technique.name\", \"shot.body_part.name\",\n    \"DistToGoal\", \"DistToKeeper\", # dist. to keeper is distance from GK to goal\n    \"AngleToGoal\", \"AngleToKeeper\",\n    \"AngleDeviation\", \n    \"avevelocity\",\"density\", \"density.incone\",\n    \"distance.ToD1\", \"distance.ToD2\",\n    \"AttackersBehindBall\", \"DefendersBehindBall\",\n    \"DefendersInCone\", \"InCone.GK\", \"DefArea\")\n\nwi_shots &lt;-\n  wi_shots |&gt;\n  dplyr::mutate(\n    shot.type.name = factor(shot.type.name),\n    shot.body_part.name = shot.body_part.name,\n    shot.technique.name = shot.technique.name)\n\nset.seed(479)\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::slice_sample(n = n_train) |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\ntest_data &lt;-\n  wi_shots |&gt;\n  dplyr::anti_join(y = train_data, by = \"id\") |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\", shot_vars)))\n\ny_train &lt;- train_data$Y\ny_test &lt;- test_data$Y\n\ntrain_data &lt;-\n  train_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\ntest_data &lt;-\n  test_data |&gt;\n  dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n  dplyr::select(-id)\n\nHow much more predictive accuracy can we squeeze out of an XG model when we account for all these variables? While it is tempting to continue fitting logistic regression models, these models make fairly strong assumptions about how the log-odds of a goal change as we vary the features. Moreover, it becomes increasingly difficult to specify interactions, especially between continuous/numerical factors.\nRandom forests is a powerful regression approach that avoids specifying functional forms and allows for high-order interactions. At a high-level, a random forest model approximates a regression function \\(\\mathbb{E}[Y \\vert \\boldsymbol{\\mathbf{X}}]\\) using the sum of several piece-wise constant step functions. \nIn this course, we will fit random forests models using the ranger package. The main function of that package is called ranger(). Its syntax is somewhat similar to glm() insofar as both functions require us to specify a formula and pass our data in as a data frame (or tibble). Unlike glm(), which required us to specify the argument family = binomial(\"logit\") to fit binary outcomes, ranger() requires us to specify the argument probability=TRUE. This signals to the function that we want predictions on the probability scale and not the \\(\\{0,1\\}\\)-outcome scale.\nWe can also use predict() to get predictions from a model fitted with ranger(). But this involves slightly different syntax: instead of using the argument newdata, we need to use the argument data. Additionally, when making predictions based on a ranger model with probability = TRUE, the function predict returns a named list. One element of that list is called predictions and it is a matrix whose rows correspond to the rows of data and whose columns contain the probabilities \\(\\mathbb{P}(Y = 0)\\) and \\(\\mathbb{P}(Y = 1).\\) So, to get the predicting XG values, we need to look at the 2nd column of this matrix.\n\nfit &lt;-\n1  ranger::ranger(formula = Y~.,\n                 data = train_data, \n                 probability = TRUE)\ntrain_preds &lt;- \n  predict(object = fit,\n2          data = train_data)$predictions[,2]\n\ntest_preds &lt;- \n  predict(object = fit,\n3          data = test_data)$predictions[,2]\n\nlogloss(y_train, train_preds)\nlogloss(y_test, test_preds)\n\n\n1\n\nThe . is short-hand for “all of the columns in data except for what’s on the left-hand side of the ~”\n\n2\n\nWhen making a prediction using the output of ranger, we pass the data in using the data argument instead of newdata.\n\n3\n\nAs noted earlier, predict returns a list, one of whose elements is a two-column matrix containing the fitted probability. This matrix is stored in the list element predictions. The second column of this matrix contains the fitted probabilities of a goal (\\(Y = 1\\)).\n\n\n\n\n[1] 0.106593\n[1] 0.2697499\n\n\nAt least for this training/testing split, our in-sample log-loss is much smaller than our out-of-sample log-loss. This is not at all surprising: flexible regression models like random forests are trained to predict the in-sample data and so we should expect to see smaller training errors than testing errors.\nRepeating these calculations over 100 training/testing splits, we can conclude that our more flexible random forest model, which accounts for many more features and can accommodate many more interaction terms than our simple logistic regression model, does seem to out-perform the logistic regression models.\n\nn_sims &lt;- 100\ntrain_logloss &lt;- rep(NA, times = n_sims)\ntest_logloss &lt;- rep(NA, times = n_sims)\n\nfor(r in 1:n_sims){\n  set.seed(479)\n  train_data &lt;-\n    wi_shots |&gt;\n    dplyr::slice_sample(n = n_train) |&gt;\n    dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\n  test_data &lt;-\n    wi_shots |&gt;\n    dplyr::anti_join(y = train_data, by = \"id\") |&gt;\n    dplyr::select(dplyr::all_of(c(\"id\", shot_vars)))\n\n  y_train &lt;- train_data$Y\n  y_test &lt;- test_data$Y\n\n  train_data &lt;-\n    train_data |&gt;\n    dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n    dplyr::select(-id)\n  test_data &lt;-\n    test_data |&gt;\n    dplyr::mutate(Y = factor(Y, levels = c(0,1))) |&gt;\n    dplyr::select(-id)\n  \n  fit &lt;- ranger::ranger(\n    formula = Y~.,\n    data = train_data, \n    probability = TRUE)\n  train_preds &lt;- \n    predict(object = fit, data = train_data)$predictions[,2]\n\n  test_preds &lt;- \n    predict(object = fit, data = test_data)$predictions[,2]\n\n  train_logloss[r] &lt;- logloss(y_train, train_preds)\n  test_logloss[r] &lt;- logloss(y_test, test_preds)\n}\ncat(\"RandomForest training logloss:\", round(mean(train_logloss), digits = 4), \"\\n\")\n\nRandomForest training logloss: 0.1066 \n\ncat(\"RandomForest testing logloss:\", round(mean(test_logloss), digits = 4), \"\\n\")\n\nRandomForest testing logloss: 0.2697 \n\n\n\nComparison to StatsBomb’s model\nNow that we have a much more predictive model, we can compare our model estimates to those from StatsBomb’s proprietary model. In the code below, we re-fit a random forests model to the whole dataset. We then plot our XG estimates against StatsBomb’s.\n\ntrain_data &lt;-\n  wi_shots |&gt;\n  dplyr::select(dplyr::all_of(c(\"id\",shot_vars)))\n\nfull_rf_fit &lt;-\n  ranger::ranger(formula = Y~.,data = train_data, probability = TRUE)\n\npreds &lt;- predict(object = fit, data = train_data)$predictions[,2]\n\npar(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0))\nplot(wi_shots$shot.statsbomb_xg, preds,\n     pch = 16, cex = 0.5, col = rgb(0,0,0, 0.25),\n     xlim = c(0, 1), ylim = c(0,1),\n     xlab = \"StatsBomb's XG\", ylab = \"Our XG\", main = 'Comparison of XG estimates')\n\n\n\n\n\n\n\n\nIf our model perfectly reproduced StatsBomb’s, we would expect to see all the points in the figure line up on the 45-degree diagonal. In fact, we observe some fairly substantial deviations from StatsBomb’s estimates. In particular, there are some shots for which StatsBomb’s model returns an XG of about 0.8. Our random forests model, on the other hand, returns a range of XG values for those shots. We also see that there are some shots for which StatsBomb’s model returns very small XG;s but our model returns XG’s closer to 0.5\nUltimately, the fact that our model estimates do not match StatsBomb’s is not especially concerning. For one thing, we trained our model using only about 3800 shots from women’s international matches contained in the public data. StatsBomb, on the other hand, trained their model using their full corpus of match data.\nMore substantively, StatsBomb’s model also conditions on many more features than ours. If our model estimates exactly matched StatsBomb’s, that would indicate that these extra features offered no additional predictive value. Considering that StatsBomb’s XG models condition on the height of the ball when the shot was attempted, the differences seem less surprising.",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  },
  {
    "objectID": "lectures/lecture03.html#footnotes",
    "href": "lectures/lecture03.html#footnotes",
    "title": "Lecture 3: Estimating XG",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOur notation nods to the fact that predicted XG is just a predicted probability↩︎\nWe often convert categorical features like shot.body_part.name or shot.technique.name into several numerical features using one-hot encoding.↩︎\nsee this issue in the StatsBomb’s opendata GitHub repo for more information about the variable definitions. The code to compute these values is available here. Note that in the StatsBomb coordinate system, the center of the goal line is at (120,4).↩︎",
    "crumbs": [
      "Lecture 3: Estimating XG"
    ]
  }
]